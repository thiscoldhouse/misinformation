[{"title": "Yesterday's News: Benchmarking Multi-Dimensional Out-of-Distribution Generalization of Misinformation Detection Models", "year": "2024", "pdf_data": "Short Paper\nYesterday\u2019s News: Benchmarking\nMulti-Dimensional Out-of-Distribution\nGeneralization of Misinformation Detection\nModels\nIvo Verhoeven\u2217\u2217\u22171, Pushkar Mishra2, Ekaterina Shutova1\n1ILLC, University of Amsterdam\ni.o.verhoeven@uva.nl\n2Meta AI, London\nThis article introduces misinfo-general , a benchmark dataset for evaluating misinformation\nmodels\u2019 ability to perform out-of-distribution generalization. Misinformation changes rapidly,\nmuch more quickly than moderators can annotate at scale, resulting in a shift between the training\nand inference data distributions. As a result, misinformation detectors need to be able to perform\nout-of-distribution generalization, an attribute they currently lack. Our benchmark uses distant\nlabelling to enable simulating covariate shifts in misinformation content. We identify time, event,\ntopic, publisher, political bias, misinformation type as important axes for generalization, and we\nevaluate a common class of baseline models on each. Using article metadata, we show how this\nmodel fails desiderata, which is not necessarily obvious from classification metrics. Finally, we\nanalyze properties of the data to ensure limited presence of modelling shortcuts. We make the\ndataset and accompanying code publicly available1.\n1. Introduction\nThe field of misinformation detection aims to develop classification models that can\nreliably moderate online content. Despite burgeoning academic interest (Wu et al. 2019;\nZhou and Zafarani 2020) and impressive classification results on existing datasets,\nmis- and disinformation content continues to propagate online and cause significant\nsocietal harm. The rapid evolution of online content, which significantly outpaces model\ndevelopment cycles, partially explains this. News, and more generally all online content,\nis valued primarily for its novelty. It will often contain unseen entities, events and entity-\nevent relationships. Verifying this content manually using domain experts is prohibitively\nexpensive, and typically requires context not available when news is emerging.\nThe misinformation datasets used for misinformation detector training, however,\nare collections of yesterday\u2019s news, containing content collected well after the fact.\n\u2217Equal contribution\n\u2217\u2217Corresponding authors\n1https://github.com/ioverho/misinfo-general\n\u00a9 2025 Association for Computational LinguisticsarXiv:2410.18122v2  [cs.IR]  26 May 2025\nComputational Linguistics Under Review\nFurthermore, these datasets typically have a narrow focus on particular events or\nmisinformation forms. As a result, state-of-the-art moderation systems lag behind the\nnews landscape, and encounter inference-time data distributions that have shifted away\nfrom its training data distribution. For misinformation detection to be successful at\nmitigating harms during deployment, and especially in situations with limited social\nor historical context (i.e., when news is emerging), models will need to be robust to\ndistribution shifts.\nCurrently, this property of Out-of-Distribution (OoD) generalization is lacking in\nmany SoTA NLP models, and this is especially true for misinformation detection models.\nPerformance significantly degrades when evaluated on unseen:\nrtime periods (Bozarth and Budak 2020; Horne, N\u00f8rregaard, and Adali 2020;\nKochkina et al. 2023; Stepanova and Ross 2023),rpublishers (Rashkin et al. 2017; Zhou et al. 2021),revents (Lee et al. 2021; Cheng, Nazarian, and Bogdan 2021; Ding et al. 2022; Wu\nand Hooi 2022),rdomains (Hoy and Koulouri 2022; Kochkina et al. 2023; Verhoeven et al. 2024),rcultures, or languages (Horne, Gruppi, and Adal\u0131 2020; Chu, Xie, and Wang 2021;\nOzcelik et al. 2023).\nWe primarily attribute this to the present state of misinformation datasets. While plentiful,\nthese are often small, collected over a short time span, centered around specific events,\nbiased towards popular content, or contain a homogeneous set of publishers. These\nproperties are generally believed to be detrimental to the generalization capabilities of\nmodern NLP models, which require large, diverse pre-training datasets, especially when\ntext or labels are noisy.\nCreating a dataset that does enforce OoD generalization, however, is not easy. Given\nthe expense involved in collecting these datasets, prior attempts at doing so have\ninvariably had to make trade-offs in size, diversity, or label fidelity (see Section 3).\nAs a result, these datasets are not representative of the misinformation landscape, and\nevaluation with such datasets will overestimate model performance during deployment\n(A\u00efmeur, Amri, and Brassard 2023; Xiao and Mayer 2024; Kuntur et al. 2024)). Generating\nhigh-quality misinformation labels for a realistically sized, naturalistic dataset remains\nintractable due to the cost of domain experts and the inherent subjectivity present\nin online content. This article will not solve this problem. Instead, we focus on more\naccurately estimating the generalization gap.\nSpecifically, we present misinfo-general , a dataset meant for testing the gen-\neralization performance of automated misinformation detectors holistically. We do so\nby processing a distantly labelled series of corpora intended for publisher reliability\nlabelling. While this introduces noise into the labels, we argue that the scale and diversity\nof the data make it useful for generalizability evaluation. To mitigate said noise, we\nperform extensive pre-processing of the data (Section 4), and post-hoc testing of dataset\nproperties (Section 8). This should ensure a balance of article quantity and label quality,\nproviding one with rich publisher-level metadata, across a long time-span, covering a\nmultitude of events and topics.\nTo showcase the utility of such a benchmark, we identify and operationalize six\ngeneralization axes\u2014(1) time, (2) specific events, (3) topics, (4) publisher style, (5) political\nbias and (6) misinformation type. We then train a simple, yet representative baseline\nmodel. We find that generalization to different classes of publishers is particularly\nchallenging, whereas within-publisher variation across years is smaller than expected.\nUsing the metadata available to us, we provide additional analysis of publisher-level\n2\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\ndeterminants of performance, and find some undesirable model behaviors not discussed\nin prior literature: less frequent publishers see degraded performance, and models treat\ndifferent political biases differently. Juxtaposed to these results, we also find some initial\nevidence that the scale and diversity of this dataset can benefit model generalization\nability when trained on.\n2. Related Work\n2.1 Generalizable Misinformation Detection\nGeneralization abilities of misinformation classifiers have been tested in many settings,\nat smaller scales. Horne, N\u00f8rregaard, and Adali (2020) found that performance degrades\nquickly when evaluating on future events, which Bozarth and Budak (2020) corroborate\nand extend to changes in domain. The same issues have also been reported in misinfor-\nmation detection in other modalities (Stepanova and Ross 2023; Verhoeven et al. 2024).\nZhou et al. (2021) find that models tend to overfit to publisher idiosyncrasies more than\narticle content, especially in publisher-level annotated datasets.\nResults on existing benchmark datasets are generally not indicative of downstream\nperformance. Kochkina et al. (2023) found that performance within one dataset vastly\noverestimates performance on other datasets or time spans. Even when controlling for\nthe time period or topic, Hoy and Koulouri (2022) found that models overfit to the\ntraining dataset and perform worse on similar but unseen datasets. In a recent systematic\nreview of the literature, Xiao and Mayer (2024) come to the conclusion that:\n. . . detection tasks are often meaningfully distinct from the challenges that online services\nactually face. Datasets and model evaluation are often non-representative of real-world\ncontexts, and evaluation frequently is not independent of model training (p. 1)\nThis sentiment matches the earlier discussion in A\u00efmeur, Amri, and Brassard (2023);\ncurrent misinformation benchmarks and evaluation setups can yield deceptively high\nperformance scores.\nDespite this paucity in benchmarks and labels, there has been some interest in\ndeveloping generalizable or adaptive misinformation detection techniques. This has\nbeen attempted through weak supervision (Shu et al. 2020), multitask training (Lee\net al. 2021), utilizing external agents (Ding et al. 2022; Mosallanezhad et al. 2022), data\nresampling or active learning (Hu et al. 2023), adversarial learning (Lin et al. 2022), or\ngradient-based meta-learning (Zhang et al. 2021a; Yue et al. 2022). While these research\ndirections are promising, their utility for out-of-distribution misinformation detection\nhas not been sufficiently tested on large, diverse benchmark data.\n2.2 Publisher Reliability Estimation\nA related field to misinformation classification, especially when utilizing publisher-level\nlabels, is publisher reliability estimation. Instead of yielding article level moderation\ndecisions, a publisher reliability model uses the content of one or many articles from one\npublisher to yield a reliability estimate of the publisher as a whole. This is a relatively\nwell-studied problem. At present, this is usually achieved through a mix of content-based\n(Rashkin et al. 2017; Bianchi et al. 2024) and metadata features (Baly et al. 2018a,b, 2019,\n2020a,b; Nakov et al. 2021).\n3\nComputational Linguistics Under Review\nRelative to article-level misinformation classifiers, publisher-level classification can\ngreatly reduce the computational cost needed for classification (Burdisso et al. 2024).\nHowever, this typically involves incorporating additional historical context, world-\nknowledge (Yang and Menczer 2024) or social context (Pratelli, Saracco, and Petrocchi\n2024). This can make publisher reliability models transductive instead of inductive\nlearners\u2014moderation decisions come from specific prior experience rather than general\nrules.\nThis mimics how moderators or users might analyze the reliability of a publisher,\npotentially before ingesting the contents of a specific article. However, such approaches\nmight fail in cases of where publishers are unknown, ambiguous or evolving. In those sit-\nuations, moderation decisions at the article-level is necessary. While misinfo-general\nis suited for either approach, we focus on testing the generalization of inductive article-\nlevel classifiers. These models naturally provide classification in cases where limited\ncontext or prior experience is available, and are required to depend entirely on specific\n(and hopefully non-spurious) general rules.\n3. Biases in Misinformation Datasets\nAt risk of repetition: misinformation models\u2019 performance degrades quickly under\ncovariate distribution shifts expected to occur during model deployment, an observation\nwhose cause we attribute to the datasets they were trained on. Due to the exorbitant cost\nof acquiring high-fidelity misinformation labels, misinformation datasets tend not to\nreflect the true variance in online (misinformation) content.\nTo illustrate this, we analyze common properties of datasets specifically constructed\nfor the development of misinformation detectors, by ways of an inexhaustive, yet\nrepresentative survey of existing misinformation datasets. We provide an overview of\nthese datasets in Table A.1. Furthermore, in this section, we (1) broadly categorize datasets\ninto different labelling methods; (2) provide specific examples of how misinformation is\ncollected and labelled; (3) discuss how these operationalizations can lead to biases in the\ndatasets; (4) and finally, provide a discussion on the merits and demerits of publisher-\nlevel labelled datasets for the purposes of model generalization.\n3.1 Dataset Labelling Granularity\nGenerally speaking, one can classify misinformation datasets into 3annotation schemes.\nListed from most fine-grained to most coarse-grained:\n1.Claim : experts fact-check individual (but complete) statements in isolation. Claims\nare usually small spans sourced from larger documents or utterances\n2.Article : experts label the overall veracity of entire documents. These can contain\nmany claims, whose factuality need not be consistent with each other\n3.Publisher : experts label publishers for their propensity for factual reporting, based\non historical records and prescribed authorial intent. These labels are often used\nas a proxy for finer-grained labels. The articles produced by publishers do not\nnecessarily have the same label as the publisher\nThe more fine-grained annotation methods yield high-quality labels, but can be\nprohibitively expensive to procure, or evaluate texts without the context those texts\nwould naturally have. Furthermore, these labelling methods are often forced to exclude\nunverifiable texts (e.g., highly subjective texts or opinions), despite these being prevalent\nin online discourse. On the other hand, the more coarse-grained annotation methods\n4\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\nrun the risk of introducing noise into the labels, by assuming consistency between finer-\ngrained labels. For example, an article may contain many factual statements, but a single\nblatant lie. Since there are increasingly fewer units at each level, however, labels are far\neasier to procure.\n3.2 Survey of Misinformation Datasets\nIn Table A.1 we present various misinformation datasets, their labelling granularity, their\nsize, and a description of how their data was sampled. In this subsection, we briefly\nexpand on some common trends on how misinformation data and labels were sourced.\n\u2018Claim\u2019-level annotations represent some of the oldest ( LIEDETECTOR (Mihalcea\nand Strapparava 2009)) and largest ( CREDBANK (Mitra and Gilbert 2015)) collections\nof misinformation text. The claims can be sourced from directly sampling social media\n(CREDBANK (Mitra and Gilbert 2015)) or sampling specific utterances flagged for\nreview (L IAR(Wang 2017), P OLITI FACT-OSLO (Poldvere, Uddin, and Thomas 2023)).\nWhile labels sourced from domain experts are dominant, using lay people as a\nmethod of crowdsourcing for either data or label collection has also proven popular. For\nthe former, as an example, articles are collected only if these were flagged by (trusted)\nusers of social media sites ( WEIBO 15(Ma et al.), WEIBO 17(Jin et al. 2017), WECHAT\n(Wang et al. 2020)). In some cases, lay volunteers were even used in the production\nof misinformation ( LIEDETECTOR (Mihalcea and Strapparava 2009), FAKENEWSAMT\n(P\u00e9rez-Rosas et al. 2018)).\nThe benefit of crowdsourcing is clear; especially at the article level, datasets that use\nexpert annotations ( BUZZFEED-WEBIS (Potthast et al. 2017), ALLCOTT & G ENTZKOW (All-\ncott and Gentzkow 2017), FAKENEWSCORPUS (Pathak and Srihari 2019)) tend to be much\nsmaller than those leveraging crowdsourcing. A common strategy to combat this, is to\nblend the \u2018Article\u2019 and \u2018Publisher\u2019 level labelling schemes ( FAKENEWSNET/GOSSIP COP\n(Shu et al. 2019), textscFakeNewsCorpus (Pathak and Srihari 2019), MM-COVID (Li\net al. 2020)). Either factual or misinformation articles are manually verified, and the\ncomplementary class is sampled from a set of publishers commonly associated with\nmisinformation or factual articles, respectively.\nA similar strategy is to blend the \u2018Article\u2019 and \u2018Claim\u2019 level labelling schemes. A claim\nmade in an article is annotated for veracity in annotation, and its label is propagated to\nthe entirety of the article ( FAKENEWSNET/POLITI FACT (Shu et al. 2019), COAID (Cui\nand Lee 2020), P OLITI FACT-OSLO (Poldvere, Uddin, and Thomas 2023)).\nThe most consistent method for generating large, diverse corpora, however, proves\nto be using \u2018Publisher-level labelling ( TSHP-17 (Rashkin et al. 2017), KAGGLE FAKE\nNEWS (Risdal 2016), SOME LIKE IT HOAX (Tacchini et al. 2017), FAKE VS SATIRE (Golbeck\net al. 2018), QP ROP (Barr\u00f3n-Cede\u00f1o et al. 2019)), as discussed above.\nTypically, the topics covered in the corpus are not further analyzed by dataset authors,\nalthough some datasets specifically focus on articles from various perspectives on the\nsame events ( MEDIA EVAL15(Boididou et al.), PHEME (Zubiaga, Liakata, and Procter\n2017), B UZZFEED-WEBIS (Potthast et al. 2017)).\nSimilarly, while most datasets are fairly general, some focus on specific domains.\nVery common are those focusing on social media or microblogging texts (CREDBANK\n(Mitra and Gilbert 2015), MEDIA EVAL15(Boididou et al.), WEIBO 15(Ma et al.), WEIBO 17\n(Jin et al. 2017), WECHAT (Wang et al. 2020)). Another common domain involves celebrity\nrumors, typically annotated for verification rather than veracity, and also commonly\nsourced from social media posts ( WEBDATASET CELEBRITY (P\u00e9rez-Rosas et al. 2018),\nFAKENEWSNET/GOSSIP COP(Shu et al. 2019)). During the COVID-19 pandemic, various\n5\nComputational Linguistics Under Review\nhealth-related datasets were introduced ( FAKEHEALTH (Dai, Sun, and Wang 2020), MM-\nCOVID (Li et al. 2020), FAKECOVID (Shahi and Nandini 2020), COAID (Cui and Lee\n2020)).\n3.3 Sources of Dataset Bias\nIn this subsection, we discuss how specific operationalizations can introduce bias in the\ndataset, adversely affecting model generalization performance.\nDiffering Definitions. Even among domain experts, there exists substantial disagreement\non what does and does not constitute misinformation (Altay et al. 2023). Recent\nsystematic reviews have found that this disagreement has carried over to the computer\nsciences (see for example Wu et al. (2019); Oshikawa, Qian, and Wang (2020); Zhou and\nZafarani (2020); A\u00efmeur, Amri, and Brassard (2023); Bodaghi et al. (2024); Xiao and Mayer\n(2024)). Indeed, the surveyed definitions of misinformation in Table A.1 seem to agree\non basic properties of misinformation, but disagree on the specific forms. As a result,\nthe forms of misinformation which are included can vary considerably. For example,\nmisinformation forms like \u2019Satire\u2019 and \u2019Propaganda\u2019 are either explicitly included or\nexcluded, proving to be especially divisive.\nInconsistent Label Sourcing. Another source of between-dataset variation, is the source\nof misinformation labels. While most datasets rely on domain experts, some use lay\nvolunteers to verify content, either explicitly ( CREDBANK (Mitra and Gilbert 2015),\nWEIBO 15(Ma et al.), WEIBO 17(Jin et al. 2017)) or implicitly ( SOME LIKE IT HOAX\n(Tacchini et al. 2017)).\nRecently, datasets have started using many misinformation sources ( FAKECOVID\n(Shahi and Nandini 2020), MUMIN (Nielsen and McConville 2022), MCFEND (Li et al.\n2024)). These can come from different countries and cultures, some of which are likely to\ndisagree on their misinformation definitions. Furthermore, this requires aggregating the\ndifferent misinformation labelling formats.\nMost misinformation definitions require specific authorial intent to deceive. However,\nin some datasets this is missing in the original content ( LIEDETECTOR (Mihalcea and\nStrapparava 2009), FAKENEWSAMT (P\u00e9rez-Rosas et al. 2018)), or ambiguous due to mis-\ninformation being defined as a lack of credible information ( FAKEHEALTH (Dai, Sun, and\nWang 2020), PHEME (Zubiaga, Liakata, and Procter 2017), FAKENEWSNET/GOSSIP COP\n(Shu et al. 2019)).\nFew publishers. Many datasets limit the number of publishers in either class. In some\ncases, this is due to deliberate scoping of the dataset ( BUZZFEED-WEBIS (Potthast et al.\n2017), FAKENEWS CORPUS (Pathak and Srihari 2019)), however in most cases this is\ndue to publisher scarcity. Misinformation annotators, like Snopes, Politifact, GossipCop,\netc., understandably tend to focus on verifiable misinformation pieces. As a result,\ndatasets sampling annotations from these sources incur a large positive bias. A common\nstrategy to counteract this is by including samples from a few mainstream publishers\n(TSHP-17 (Rashkin et al. 2017), MM-COVID (Li et al. 2020), COAID (Cui and Lee 2020),\nFAKENEWSNET(Shu et al. 2019)).\nAn unwanted side effect of having a small, homogeneous publisher set, is the\nintroduction of a modelling shortcut; misinformation classifiers no longer need to analyze\nthe veracity or intent of input content, but rather simply discriminate between a few\npublishers with unique idiosyncrasies. Similarly, in datasets where misinformation is\n6\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\nconstructed by editing factual information ( LIEDETECTOR (Mihalcea and Strapparava\n2009), FAKENEWSAMT (P\u00e9rez-Rosas et al. 2018)), the labels can be inferred by discrimi-\nnating between the stylistic preferences of the original texts\u2019 authors and those of the\neditors.\nFew events or topics. Similarly, many datasets sample content from a narrow time-span,\nor from a small set of events or topics. This can reduce the cost of generating labels, but\nwill likely induce overfit in automated moderation systems trained on these corpora.\nFocus on Obvious or Popular Misinformation. In several of the discussed datasets,\nmisinformation texts are collected based on user reports, or from third-party fact-checkers.\nThese run the risk of introducing a selection bias, resulting in a dataset that is not\nrepresentative of all produced misinformation.\nA secondary effect of this, is that unverifiable content (e.g., those relying purely\non opinion and speculation) are implicitly excluded. Some datasets explicitly exclude\nunverifiable content ( FAKENEWSNET(Shu et al. 2019)), whereas others include this as a\nspecific category ( BUZZFEED-WEBIS (Potthast et al. 2017)). Most datasets do not discuss\nunverifiable cases, despite these forming a sizable part of produced online content (see\nSection 8.3).\nConclusion. In short, we find that the realities of misinformation data collection results in\nmany datasets making a trade-off between label quality and corpus size. As a result, these\ndatasets introduce some bias, which we suggest as a primary reason for the reported\nbrittleness of misinformation detectors under covariate shift. Given that these covariate\nshifts are practically guaranteed in online content or news, testing misinformation\ndetectors before deployment for generalizability is crucial. Doing so, however, requires\nlarge, diverse datasets, which we have established is difficult to procure without bias. A\nrelated task, publisher reliability estimation, might provide an alternative.\n3.4 Publisher Reliability Datasets\nRelated to the task of misinformation detection is publisher reliability estimation (see\nSection 2.2. Given an article, or a set of articles, from some publisher, a reliability estimator\nhas to predict the overall publisher reliability.\nPublisher reliability is a broader concept than factuality, and considers many aspects\nof a publisher, which are not necessarily clear when analyzing articles or claims from\na publisher in isolation. These aspects include framing, publisher political or editorial\nbias, intended audience, sourcing practices, funding, etc. All of these factors are analyzed\non a large collection of a publishers\u2019 works, and used to provide an indication of\nthe trustworthiness of past and future releases. Ultimately, however, the factuality of\nproduced articles is an important dimension of publisher reliability.\nMuch like the publisher-level misinformation labelling scheme discussed above,\nit does not preclude less reliable publishers producing reliable content, or vice versa .\nIt merely suggests that this is less likely to occur. Reliable publishers often produce\nsensationalist or subjective content to draw in readership, whereas unreliable publishers\nmight intersperse their less reliable articles with more reliable ones to boost their\nperceived trustworthiness.\nImplicitly, by using publisher-level labels as a proxy for article-level reliability, we\n(as well as many \u2019Publisher\u2019-level datasets) make the assumption that the article-level\n7\nComputational Linguistics Under Review\nfactuality of an article from a reliable publisher is stochastically higher than that of an\narticle from a less reliable publisher.\n3.4.1 Measuring Generalization with Publisher Reliability Labels. Relative to misinfor-\nmation datasets, for generalizability aspects, publisher-reliability datasets are far easier to\nproduce at scale, and given publisher-level metadata, can be built specifically to enforce\ndiversity in both publishers and text. Furthermore, articles can be collected across much\nlonger time-spans, which naturally includes shifts in article topics or events.\nPerhaps most importantly, however, if a large enough set of publishers is collected,\nthe resulting dataset becomes a naturalistic view of published online content and\n(mis)information. Instead of a dataset including only verified misinformation, which\nare typically the least ambiguous or popular cases due to the selection bias of third-\nparty fact-checkers, the dataset is more aligned with online content as it would appear\npost deployment (Section 8). As a result, statistics about model evaluation are more\nrepresentative, and model developers can derive stronger conclusions.\nIn this article, we propose using a publisher-level reliability estimation dataset for\nthe purpose of evaluating the generalizability of article-level misinformation detectors.\nWhile this runs the risk of tarring all articles from a publisher with the same brush,\nwe believe the size and diversity of the dataset, along with access to publisher-level\nmetadata, can offset the induced bias, and still allow for conclusive inferences about\nmodel behavior under distribution shift.\nSpecifically, we assume that the effect of covariate distributional shifts on the predic-\ntive quality of a model is positively correlated between the two labelling approaches. In\nother words, we assume that model performance degrades under the same distributional\nshift in both labelling set-ups. Thus, implicitly we assume that the level of robustness\nto distributional shifts on a dataset like \u2018misinfo-general\u2018 serves as a good indicator for\nrobustness in article-level misinformation detection.\n4. The misinfo-general Dataset\nTo that end, we introduce misinfo-general , a benchmark for testing the general-\nization capacity of misinformation detection models, built on top of a series of noisy\npublisher-level datasets. While best suited for publisher reliability estimation models,\nwe instead use the publisher labels as a proxy for article labels.\nBased on the prior discussion, we foresee two sources of bias, (1) labels might not\nbe accurate at the article level, and (2) models will learn to infer the article\u2019s publisher\nand its label instead of inferring the label from the article. We take the following steps to\nmitigate these biases as much as possible:\n1. relabelling existing articles (Section 4.2)\n2. masking or removing publisher identifiable text in articles (Section 4.3)\n3. removing any article- and sentence-level duplicates (Section 4.3)\n4. masking self-references, along with other PII (Section 4.3)\nIn Section 8.1 we show that these pre-processing steps have made publisher identifi-\ncation from articles alone difficult.\nIn this section, we describe how we gather the dataset content and labels, and\ngenerate any additional metadata. In later sections, using article-level metadata, we\nspecifically test for model overfit to publisher style (Section 5 & Section 6), we show\nincluding a diverse set of publishers is beneficial to generalization performance (Section\n8\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\n7.2), and we try to find publishers with high degrees of mislabelling by assessing the\nnecessity of model memorization (Section 8.2).\n4.1 Article Provenance\nAll raw articles come from the various NewsLandscape (NELA) corpora produced by\nthe MELA lab2(2018; 2019; 2020; 2021; 2023). The corpora cover 2017\u20132022 (6 iterations)\nalmost continuously, with articles from a diverse group of publishers. In their original\nform, the 6 iterations together consist of 7.2million long-form articles.\nThe original authors\u2019 goal was to study the dynamic behavior of news and news\npublishers. They deemed existing corpora inadequate for their goals, because of (1) a\nsmall, relatively homogeneous collection of articles or publishers, (2) too narrow a focus\non specific events, (3) bias towards popular publishers, and (4) limited ground truth\nlabelling (Horne et al. 2018; Norregaard, Horne, and Adali 2019).\n4.2 Publisher Labelling\nFrom the 2018 iteration onwards, the NELA datasets come with publisher-level labels.\nHowever, due to inconsistencies across dataset iterations and the frequency of labelling\nerrors, we chose to relabel the dataset completely.\nSimilar to the initial NELA corpora labels, we scraped Media Bias/Fact Check3\n(MBFC). MBFC is a curated database of news publishers, with thorough analyses of\npublisher origins, bias, and credibility. Despite being run by lay volunteers, MBFC labels\ncorrelate well with professional fact-checking sources (Kiesel et al. 2019; Broniatowski\net al. 2022; Pratelli and Petrocchi 2022). MBFC labels have been used in many earlier\nworks (Rashkin et al. 2017; Baly et al. 2018a, 2020a; Burdisso et al. 2024; Casavantes et al.\n2024; Szwoch et al. 2024). MBFC labels are dynamic and annotations can change (although\nthis is infrequent). We use those available as of Oct. 2024, well after the publication dates\nof articles in the corpus.\nUsing the URL domain of the scraped articles, We first mapped all articles to a con-\nsistent set of publishers before removing any publishers known to be news aggregators\nor social media sites. This gives an article-publisher mapping that is consistent across\ndataset iterations, and removes cases of where articles were republished on different sites.\nEach publisher was linked to a publisher in the scraped MBFC database. We provide\nfurther detail in Appendix B.1. Ultimately, we identified 488distinct publishers, many\nof which were falsely attributed in NELA\u2019s original set of publishers. The metadata\navailable for each publisher is provided in Appendix B.6.\n4.3 Data Processing\nBeyond errors in the article-publisher and publisher-label mappings, the texts themselves\nfrequently contain duplicates or scraping errors. Of the 6.7M re-labelled articles, roughly\n\u224822% or1.5M articles were duplicates. Many of the remaining unique articles were\ndeemed malformed or semantically void. These contain either very little text, substantial\namounts of markup or include too many special tokens to be human-readable. We filter\nthese using a few simple rules (see Appendices B.2 and B.3). Altogether, we remove\n2https://melalab.github.io\n3https://mediabiasfactcheck.com/\n9\nComputational Linguistics Under Review\napproximately \u224843% of all downloaded articles. The final dataset contains 4.2million\ncleaned articles.\nIn the remaining texts, we mask various forms of private or identifiable information\n(PII), both to enhance safety and reduce the number of available classification \u2018shortcuts\u2019.\nWe furthermore standardize the copyright masking procedure introduced in Gruppi,\nHorne, and Adal\u0131 (2020). This introduces 4new special tokens: <copyright> replacing\nNELA\u2019s repeated @tokens, <twitter> ,<url> and<selfref> for any self-references.\nDespite our efforts, the datasets retain a level of \u2018noise\u2019 customary to data sourced\nfrom the internet. For example, articles from the same publisher tend to contain unique\nby-lines, attribution messages, or donation requests. Further cleaning efforts might\nreduce the realism of the benchmark.\n4.4 Topic Clustering\nOne of our aims is to test model generalization across different events and topics. To\ndiscover these, we used a modified variant of BERTopic (Grootendorst 2022) with a\ngte-large4(Li et al. 2023) backbone. This produced thousands of event clusters for\nevery dataset iteration, each with a TF-IDF representation vector. We aggregate these\nevents into overarching topics by applying spectral clustering to the adjacency matrix\ninduced by the inter-event cosine similarity of the TF-IDF matrix. We arbitrarily limit\nthe number of topics to 10, each with varying numbers of events in them. This process is\nfurther described in Appendix B.4.\nThis extends the work of Litterer, Jurgens, and Card (2023) on identifying \u2018news\nstorms\u2019 in the NELA corpora to a larger time-span, and a larger set of publishers.\n5. Generalization Taxonomy\nIn this section, we describe various dimensions along which we believe covariate shifts\nlikely to occur, and which are feasible to simulate using misinfo-general . We consider\na total of 6specific generalization axes.\nTime based generalization measures the extent to which changes in publisher style\naffect a model\u2019s predictions. The publishers considered in each split should be held\nconstant to avoid confounding with different publishers.\nEvolution of article content will also impact performance. We focus on two specific\nforms of such change: (1) due to spontaneous events , which we define as news-worthy\nhappenings with a definite and narrow time-span, or (2) due to evolving topics , which\nwe define as large, overarching collections of events that remain relatively static over a\nlong period. Across these events and topics, we expect markedly different language.\nThe distribution of publishers is also expected to change between training and\ninference time. All publishers exhibit some form of editorial bias or style, which can be\nmemorized by classification models. While models should use style to inform moderation\ndecisions, they should also not overfit to stylistic idiosyncrasies. One related, usually\nimplicit, expectation of misinformation detectors is a robustness to different political\nbiases ormisinformation types . Predictions ought to be based on a publisher\u2019s intent,\nnot their norms and values. By excluding these from training, we can test a model\u2019s\nability to generalize to different classes of publishers.\n4https://huggingface.co/thenlper/gte-large\n10\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\nTable 1\nA schematic overview of the generalization taxonomy. The left columns provide relevant\ngeneralization category and axis, whereas the right columns provide examples of in domain and\nout-of-distribution article sets.\nGeneralisation Axis In Distribution Out-of-Distribution\nTime TimeCNN AP Vox\n2018 2018 2018CNN AP Vox\n2017 2020 2019\nContentEvent not COVID-19 events COVID-19 events\nTopic Crime, Sports Elections\nPublisherPublisher CNN MSNBC OANN Reuters AP True Activist\nPolitical\nBiasAP Reuters Fox News\nCentre Centre RightVox Daily Beast True Activist\nLeft Left Left\nMisinfo\nTypeVox NYT OANN\nReliable Reliable QuestionableMSNBC 911Truth Age of Autism\nReliable Conspiracy Pseudosci.\n5.1 Data Splits\nTo operationalize these generalization axes, we build 6 (+1 baseline) train/test splits of\nthe dataset using the publisher-level metadata available to us. Each split is meant to\nsimulate one of the above described covariate shift scenarios, while ensuring minimal\ncross-scenario confounding.\nThroughout, we approximate the same 70/10/20% article proportions per train-\ning/validation/test split, respectively. The validation split, used for early stopping, is\nsampled i.i.d. from the training set. For all scenarios, we repeat each split independently\nfor each dataset year, for a total of 6 times. The only exception is the \u2018Event\u2019 axis, for\nwhich we combine all years into a single dataset.\nBriefly, we construct splits (schematically displayed in Table 2) for the scenarios as\nfollows:\n0.Uniform : standard stratified random splitting of articles into disjoint article sets.\nNo article meta-data is used\n1.Time : the training set consists of a single dataset year, while the test set contains\narticles from publishers seen during training in all other dataset years. This tests\nwithin publisher variation\n2.Event : the dataset has been annotated for several thousands of events, but we focus\non a singular one: the COVID-19 pandemic. We reserve all articles containing any\nrelated keywords for testing, and we train on all non-COVID articles\n3.Topic : we reserve the ksmallest topic clusters for the test set, such that these contain\nroughly 20% of all articles, and we train on the remaining articles\n4.Publisher : similarly, we reserve the kleast frequent publishers for the test set, such\nthat these contain roughly 20% of all articles, and we train on the remaining articles\n5.Political Bias : we reserve all articles from either all \u2018Left\u2019- or \u2018Right\u2019-biased\npublishers for testing, and train on articles from the opposite political bias, along\nwith any \u2018Center\u2019-biased publishers\n11\nComputational Linguistics Under Review\n6.Misinformation Type : similarly, we reserve all articles from either all \u2018Questionable\nSource\u2019 or \u2018Conspiracy-Pseudoscience\u2019 publishers for testing, and train on articles\nfrom the other misinformation class. We use an i.i.d. split of reliable articles to\nensure a similar class distribution in all splits\nWe include a substantially expanded description of each split\u2019s construction in\nAppendix C.\nIt is important to note that from the model\u2019s perspective, each scenario seems\nidentical. The same labels are present in each split, with roughly the same article counts\nin the same class proportions. Without additional context, one should expect similar\nperformance across these splits.\n6. Experiments and Results\nTo showcase the utility of misinfo-general for model training and evaluation, we\nuse a simple yet powerful baseline model. Specifically, we fine-tune an instance of\nDeBERTa-v35(He, Gao, and Chen 2023) where we reset the pooler and classification\nweights but freeze the model\u2019s remaining weights. To enable using dataset-specific\ntokens, we allow the token embedding layer to train with a very low learning rate.\nThe model\u2019s pre-training data included a closed-source news dataset (CC-News), dated\nbetween September 2016 and February 2019 (Liu et al. 2019), and thus should be easily\nadapted to misinfo-general . Similar architectures have shown surprisingly adequate\nperformance on other benchmark datasets, including various NELA versions (Pelrine,\nDanovitch, and Rabbany 2021; Zhou et al. 2021; Raza and Ding 2022).\nWe fine-tune the models on the different splits outlined in Section 5. We keep the\nhyperparameters and compute budget constant (which were tuned on the validation\nsets of the \u2018Uniform\u2019 splits), which we outline in Appendix D. Training occurs at the\narticle level, using publisher-level labels. We binarize the article publisher\u2019s MBFC label\nfor training labels: all \u2018Questionable Source\u2019, \u2018Conspiracy-Pseudoscience\u2019, and \u2018Satire\u2019\npublishers were deemed unreliable\u2212, and all others reliable+. Other publisher-label\nmappings have been used in other works, and is deserving of future research for this\ndataset.\nTo assess model performance at the article level, we employ the F1-score computed\nindependently for each class along, with the Matthews Correlation Coefficient (MCC).\nThe F1-score\u2019s interpretation is largely dependent on the class proportion (Flach and Kull\n2015), making it less suited to comparison across experiments, whereas MCC is more\nrobust to this (Chicco and Jurman 2020, 2022). MCC is 0 for random performance, and 1\nonly for perfect classification.\n6.1 OoD Generalization\nTable 2 displays the article level classification results for the various generalization splits\noutlined in Section 5. The larger the deviation between the in distribution (ID) articles in\nthe validation set and the out-of-distribution (OoD) articles in the test set, the worse we\nconsider the model\u2019s generalization performance.\nFirstly, we note that classification performance falls short of desired. While the F1-\nscore for the reliable+class tends to be high (in the range of 0.85\u22120.95at a\u223c60% class\n5https://huggingface.co/microsoft/deberta-v3-base\n12\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\nTable 2\nArticle-level classification performance comparing performance on the ID and OoD evaluation\nsets. The top row uses uniform splitting for both (OoD = ID), serving as a baseline value. \u2018Time\u2019\nbased splitting has strongly varying class proportions, making F1 values inappropriate.\nGeneralisation FormMCCF1\nReliableF1\nUnreliable\nID OoD \u2206 ID OoD \u2206 ID OoD \u2206\nUniform 0.46 0.46 0.00 0.86 0.86 0.00 0.57 0.57 0.00\nTime 0.46 0.33 -0.13 N/A\nEvent 0.43 0.46 0.03 0.87 0.86 -0.01 0.52 0.55 0.03\nTopic 0.46 0.38 -0.08 0.87 0.84 -0.03 0.56 0.50 -0.06\nPublisher 0.48 0.37 -0.10 0.87 0.84 -0.03 0.58 0.53 -0.05\nPolitical Bias\nLeft 0.49 0.30 -0.19 0.85 0.87 0.02 0.61 0.38 -0.23\nRight 0.56 0.19 -0.37 0.95 0.60 -0.34 0.58 0.26 -0.32\nMisinformation Type\nConsp.-PSci. 0.43 0.42 -0.01 0.87 0.82 -0.05 0.53 0.53 0.01\nQuestionable 0.43 0.23 -0.20 0.94 0.62 -0.33 0.41 0.25 -0.16\nproportion), classifying unreliable\u2212articles is considerably more difficult\u2014a trend that\nholds consistently across generalization forms. This is largely due to low recall scores\nfor the unreliable\u2212class. This is especially surprising given the high accuracy scores\nreported for similar models on other misinformation datasets.\nWe see no degradation in performance when applying the model to articles from an\nunseen event (here, the COVID-19 pandemic). Despite the introduction of many unseen\nterms to the articles\u2019 vocabulary, it appears the manner in which established publishers\ndiscuss this new event deviates little from preceding articles.\nBoth \u2018Publisher\u2019 and \u2018Topic\u2019 splitting show moderate decreases in MCC scores,\ncarried primarily by a decrease in the F1-scores for the unreliable\u2212class. Generalization\nto completely unseen publishers or topics, cases where one would expect distinctly\ndifferent linguistic style or vocabulary, is more challenging. The magnitude of this\nperformance degradation, however, is smaller than we initially expected. We attribute\nthis to two effects:\n1.More mainstream, prolific publishers are obscuring performance on publishers\nwith fewer articles (see Appendix B.8). We correct for this effect by including a\npublisher-level analysis in Section 7.1\n2.The training data is heterogeneous enough for the models to learn generalization\nacross publishers. We test for this in Section 7.2\nSince it is conceivable that different publishers prefer particular topics, we compute a\ncorrelation between the produced test sets. While we find a small but consistent overlap\nbetween the \u2018Publisher\u2019 and \u2018Topic\u2019 test sets, we do not believe this alone accounts for\nthe similarity in performance (see Appendix C.2).\nThe final two generalization axes exclude a particular misinformation type or\npolitical bias from the training set. For the former, we can see little to no effect when\n13\nComputational Linguistics Under Review\nremoving the \u2018Conspiracy-Pseudoscience\u2019 class of articles, but a drastic one if removing\n\u2018Questionable Source\u2019 articles. We posit this is due to the \u2018Questionable Source\u2019 being\nthe class of articles written with the explicit purpose of mimicking reliable+publishers,\nwhereas \u2018Conspiracy-Pseudoscience\u2019 tends to discuss completely separate topics. In\nother words, the conspiracy or pseudo-scientific articles tend to be easier to identify as\nunreliable\u2212.\nFor the \u2018Political Bias\u2019 generalization axis, we see an inability to generalize to\nopposing political biases. Training on center and right biased articles sees a 0.19drop in\nMCC, whereas training on center and left yields a drastic 0.37drop. While this is a form\nof publisher splitting, in both cases the magnitude of the degradation is substantially\nlarger. Especially for transfer to right-biased articles, there exists a drop for both reliable+\nand unreliable\u2212classification, indicating that it is more challenging for the model to\ndetermine article reliability.\n6.2 Generalization across time\nTable 3\nArticle level MCC scores of models trained\nwith uniform splitting on different years of\nthe dataset.\nEval\n2017 2018 2019 2020 2021 2022Train2017 0.50 0.43 0.41 0.40 0.40 0.38\n2018 0.29 0.42 0.43 0.39 0.41 0.37\n2019 0.26 0.38 0.44 0.40 0.41 0.40\n2020 0.34 0.39 0.47 0.47 0.47 0.45\n2021 0.31 0.37 0.46 0.46 0.47 0.45\n2022 0.33 0.38 0.46 0.45 0.46 0.46When applying the models to unseen years,\nwe find the models to be surprisingly ro-\nbust, as shown in Table 3. At the article\nlevel, despite consistent degradation in per-\nformance, proximal years tend to achieve\nsimilar scores. Only in very distant years\ndoes performance degrade dramatically.\nWe speculate that these differences are\ndue to differences in the various dataset\niterations, while publisher style or idiosyn-\ncrasies being relatively static. For example,\nall models not trained on the 2017 itera-\ntion perform poorly on the 2017 iteration\n(between 0.26and0.34MCC), whereas\nthe 2020 \u20132022 editions perform reasonably\nwell on each other\u2019s years. Indeed, visually, Table 3 correlates strongly with Appendix\nB.8 Table B.8, showing the amount of overlap in publishers across dataset years.\n6.3 LLM Performance\nWe compare the performance of the fine-tuned models to that of\nllama-3-8b-instruct6(Llama Team 2024), prompted to determine reliability\nof an article in a 0-shot setting with 512token context (see Appendix D.2).\nDespite the LLMs parameter count and the recency of its pre-training data, we find\nllama-3-8b-instruct to be inferior to the fine-tuned models as for the purpose of\narticle-level reliability classification. It manages an MCC of 0.25, compared to our fine-\ntuning models achieving 0.46on ID years, and 0.33on OoD years. With the advent of\nLLM-based search engines, this is a somewhat worrying result, and aligns with recent\nfindings of other works (Liu, Zhang, and Liang 2023; Yang and Menczer 2024).\nThe use of this modestly sized LLM already incurs a computational cost far greater\nthan the fine-tuning models used. In our experiments, using a single A100 GPU, the LLM\n6https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\n14\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\ntook ~ 70hours to yield a prediction for all articles in the corpus, whereas each of the\nfine-tuning models were trained and evaluated in ~ 12.5hours.\n7. Analysis of Model Generalization\n7.1 Determinants of Performance\ntrain size\nis_foreign\nLeft\nRight\nQuest\nConPSci\nSatire\nCon.\nPSci\nReliable\nQuest\nConPSci1\n01Political\nBiasLabelConPSci\nLevelFactuality\nFigure 1\nCoefficients of the determinants model,\nexpressed as effect sizes. Circles are\ncentered about the effect size, with lines\ngiving the 95% confidence interval.The analysis of our results, thus far, has\nbeen constrained to article-level classifica-\ntion. While this reflects how misinforma-\ntion classifiers interact with articles, it does\nnot match how we annotate the dataset\nand can obscure performance on smaller,\nless mainstream publishers. Ideally, as high-\nlighted by Baly et al. (2018b) and Burdisso\net al. (2024), classification performance is\nalso evaluated at the publisher-level, test-\ning which publisher properties aid or inter-\nfere with misinformation detection.\nTo that end, we employ a binomial\nlogistic regression on the average publisher-\nlevel accuracy7to assess which aspects of a\npublisher determine the achieved accuracy\nscore (for details and full model specifica-\ntion, see Appendix E.3). Unlike standard\nlogistic regression, the dependent variable\nis modelled as a ratio.Table 4\nMedian predicted publisher-level accuracies\naveraged over combinations of the MBFC\nlabel (rows) and political bias (columns).\nThe row headers correspond to (R) reliable,\n(Q) Questionable Source, (C) Conspiracy\nPseudoscience, and (S) Satire.\nLeft Center Right\nR86.22% 92 .90% 79 .03%\nQ46.13% 46 .11% 30 .92%\nC32.47% 79 .41% 31 .20%\nS7.10% \u2013 14.11%In Figure 1 we show coefficient mag-\nnitudes for several important determi-\nnants, expressed as effect sizes (Chinn 2000;\nLampinen et al. 2022). A positive effect\nsize indicates that the variable increases\nthe odds of accurate classification, ceteris\nparibus .\nThe size of the training set has a large\npositive effect, with a 1.91multiplicative in-\ncrease in the odds for each 10-fold increase\nin training samples. Thus, a publisher with\n1000 articles in the training set is 3.82times more likely to have correct classification in\nthe test set than a publisher with only 10 articles. Foreign publishers also prove easier to\nidentify.\nRelative to center-biased publishers, publishers on the left or right sides of the\npolitical spectrum see slightly degraded performance, even after controlling for the\npublisher label. Moreover, classification on the unreliable\u2212classes suffers more than on\nreliable+publishers. Since the odds ratios or effect sizes are difficult to interpret, we also\nprovide the estimated marginal mean publisher-level accuracy for those combinations in\nTable 4.\n7Defined as the ratio of correctly predicted articles to all articles for a publisher, i.e., true positive rate, recall.\n15\nComputational Linguistics Under Review\nDespite right biased unreliable\u2212sources being far more prevalent in the training\ndata, for both the \u2018Questionable Source\u2019 and \u2018Conspiracy-Pseudoscience\u2019 classes, model\nperformance is noticeably worse than on left and center biased sources. This somewhat\nconfirms the results in the \u2018Political Bias\u2019 rows of Table 2: models struggle disproportion-\nately to discriminate between reliable and unreliable\u2212right-biased articles.\nWe see a slight positive correlation with the MBFC \u2018Conspiracy-Pseudoscience\u2019 level.\nThe higher the value, the further the publishers\u2019 articles tend to deviate from convention.\nAs a result, strongly conspirational or pseudo-scientific publishers are 4.84and4.72\ntimes more easily identified than publishers where this effect is weaker.\nFinally, when looking at the factuality level (the propensity for a publisher to publish\nfactual articles) we find a positive interaction for reliable articles, and weakly negative\ninteractions for unreliable articles. The more a publisher goes against the expectation\n(factual for reliable for publishers, false for unreliable), the more difficult it becomes to\ndisambiguate the source.\n7.2 Effect of Publisher Diversity\nBaseline T op 1 T op 2 T op 30.20.40.60.8\nID\nOoD\nFigure 2\nMCC scores for different \u2018Publisher\u2019 split\ntest sets with only the top-1, 2 or 3 most\nprolific publishers retained per publisher\nclass. The \u2018Baseline\u2019 column corresponds to\nthe standard publisher splitting used in\ndescribed in 5. The lightly shaded circles\nprovide a value for each year of the dataset,\nthe solid circles their average. The blue\ncircles are ID publishers, the orange OoD\npublishers.Here we test to what extent the diversity of\nthe publishers present in the training data\nhas an effect on the generalization capacity\nof the models. We re-run the \u2018Publisher\u2019\nsplit experiment with smaller training sets,\nconstrained to only the most prolific pub-\nlishers. Specifically, for each MBFC label,\nwe only include the top-n most frequent\npublishers in the training data, while leav-\ning the test set untouched. While this re-\nduces the amount of variation in publishers\nconsiderably, it minimally affects the total\namount of data present. Each training set\nstill consists of hundreds of thousands of\narticles.\nFigure 2 displays the generalization\ngap (in terms of MCC) induced when in-\ncreasing publisher homogeneity. Especially\nwhen limiting performance to the Top-1\nmost common publishers, the models show\nincreased overfit to the training set. Where the \u2018Publisher\u2019 split saw a 0.1MCC delta,\nthis increased to an average degradation of 0.5. As the number of included publishers\nincreases, the generalization gap decreases and starts to converge to the previously seen\n\u2018Publisher\u2019 values. Notably, the variance in values is also substantially higher in the\nlimited publisher settings.\nFrom this, we conclude that (1) the splitting described in Section 5 to have minimally\naltered the heterogeneity present in the dataset, and (2) the models improve with\npublisher heterogeneity. The former finding suggests that the underestimation of the\ngeneralization gap will be especially egregious in datasets with a small pool of publishers\n(e.g., those that sample from a single reliable source to boost label balance). The latter,\ninstead, provides some initial evidence for the utility of using large, diverse publisher-\nlevel datasets for pre-training article-level misinformation detectors; while fine-tuning\n16\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\non high-fidelity labels is likely necessary, using distantly supervised datasets might\nencourage more robust models before fine-tuning.\n8. Analysis of Publisher-Level Labelling\n8.1 Publisher Identifiability\nThe use of publisher-level labels as a form of weak supervision, especially in misinfor-\nmation detection, can lead to models overfitting to publisher styles instead of article\nveracity. This was shown to be a serious concern by Zhou et al. (2021), and efforts to\nmitigate this effect at the data level were discussed in Section 4. Despite this, in Sections\n6.1 and 7.1 we still found models to overfit to specific publishers and publisher classes,\nand in Section 7.2 we found a negative correlation between publisher diversity and the\nmagnitude of the generalization gap.\nAs such, here we directly test the identifiability of the publisher from article content\nby replacing the misinformation labels with a unique identifier for each publisher. In\nother words, instead of classifying into the set {reliable+,unreliable\u2212}, the model\nclassifies into the set of all possible publishers.\nUsing the same learning algorithm, we find this to be a substantially more difficult\ntask. While models exhibit above random article-level performance, with an average\nMCC score of 0.18, this is much lower than scores achieved with misinformation labels.\nFurthermore, when aggregating F1-scores across classes proportionally according to\npublisher frequency (micro) we get 0.14, whereas with a flat average (macro) we obtain\na mere 0.04F1. In short, while it is possible to predict the publisher from an article with\nabove random performance, this is only really possible for the most prolific publishers,\nand this cannot entirely explain performance in misinformation classification.\n8.2 Publisher Memorization\nIn this subsection, we analyze to which extent models need to memorize specific\npublishers. If there exists a lot of disagreement between the label of an article and\nthe label assigned to its publisher, it is likely impossible to generalize to the unseen\npublisher from seen publishers; the labels of similar publishers clash. In this case, for\nclassification to be successful, it is necessary for the misinformation detector to memorize\npublisher idiosyncrasies.\nInspired by the works of Pleiss et al. (2020), Jenkins, Talafha, and Goodwin (2023) in\nautomated mislabelling detection, and Swayamdipta et al. (2020) on diagnosing dataset\nissues using \u2018dataset cartography\u2019, to estimate the necessity of memorization of specific\npublishers, we run an experiment comparing the average article confidence (mean logit\nassigned to the correct class) and disagreement (variance of logit assigned to the correct\nclass) of publishers when included or excluded from the dataset. If there is a large shift\nin either the confidence or disagreement of a publisher when in- or excluded, this might\nindicate the publishers\u2019 articles\u2019 labels are not aligned with those of similar publishers.\nWe rerun the 2021 uniform split experiment 15 times. We exclude each publisher in 5\nruns, at random, while taking care to minimize the number of exclusion set co-occurences\nand stratifying the exclusion across fine-grained MBFC publisher classes. As such, there\nshould always be similar publishers available to excluded ones.\nFigure 3 shows the effect of exclusion on the average confidence and disagreement\nscores for all publishers. Overall, and unsurprisingly, including a publisher during\ntraining increases average article confidence and decreases disagreement. However,\n17\nComputational Linguistics Under Review\n0.000 0.025 0.050 0.075 0.100 0.125 0.150\nDisagreement1\n012ConfidencePublisher Included in Training\n0.000 0.025 0.050 0.075 0.100 0.125 0.150\nDisagreement1\n012Publisher Excluded in Training\n6\n 4\n 2\n 0 2 4 6\nDisagreement5\n05ConfidenceNormalized  In/Exclusion\n0.000 0.025 0.050 0.075 0.100 0.125 0.150\nDisagreement1\n012 In/Exclusion\nFigure 3\nThe average article-level confidence and disagreement for different publishers. The top left panel\nshows scores when publishers are included in training, whereas the top right panel shows scores\nwhen publishers were excluded. The bottom left panel shows their difference, normalized. Only\npublishers with a shift magnitude above 2standard deviations (i.e., significantly far from origin)\nare shown with low opacity. The bottom right panel shows the direction of differences for\nsignificantly shifted publishers. In all panels, the size of the circles are proportional to a\npublisher\u2019s size in the dataset. Green colored circles represent reliable+publishers, whereas red\ncolored circles represent unreliable\u2212ones.\nfor most publishers, the shift between training in- or exclusion is small, and likely\nattributable to the inherent stochasticity of mini-batch training. We assume that these\npublishers\u2019 labels can largely be learned from similar publishers, and that these align\nwell with each other.\nWhile there are significant shifts, these mostly occur for the largest, typically\nwell-performing reliable+, publishers, and take the form of significantly increased\ndisagreement and decreased confidence when excluded. These include sources which\nMBFC believes to produce typically reliable, but sensationalist, subjective news (e.g., The\nSun, The Daily Mirror), or anti-US propaganda sources (e.g., Pravda Report, Asia Pacific\nResearch), as well as highly reputable sources (e.g., CBS News, BBC)8. Comparatively,\nmost large unreliable\u2212publishers see far smaller shifts.\nThere are substantially fewer significant shifts in the other quadrants (Figure 3,\nbottom left panel), and those that do show up tend to be for much smaller publishers.\nPublishers that see a significant reduction in confidence when included in training do\nexist, although these comprise a small minority with typically very few articles. Looking\nmore closely at such publishers, these include cases where site ownership changed\nduring article collection (Viral News Network, Infinite Unknown), or whose articles\n8These categorizations originate from MBFC, and do not reflect the authors\u2019 opinions.\n18\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\nare extremely noisy (Alternative Media TV), which would serve as good candidates for\nremoval. We also find particularly difficult cases here, like neutral, objectively written\narticles promoting climate change denial (Climate Etc)9.\nAll in all, while there appears to be some ambiguity in article labels, largely due to\npublisher editorial biases, we find no evidence of mislabelling beyond a level expected\nfor a corpus scraped from the internet. Despite having missed some publishers in the\ndata cleaning phase (see Appendix B), these represent a small minority of all publishers,\nand collectively contain a small minority of all included articles.\n8.3 Article Properties\nIn this subsection, we automatically analyze various properties of our dataset at the\narticle level, both to assess their presence for different classes of publishers, and their\ncorrelation with news reliability labels.\nSubjectivity analysis. The first property we annotate for is subjectivity. Objective news\npresents facts in a neutral, unbiased manner, and is commonly considered the antithesis\nof hyperpartisan or misinformation news, which is written specifically to incite an\nemotional response from readers, thereby inducing sharing (Bojic, Prodanovic, and\nSamala 2024). Subjectivity has shown some promise as a feature in discriminating\nreliable and unreliable news (Jeronimo et al. 2019). Despite this, reliable publishers also\nproduce subjective text, likely to drive engagement. This can make articles unverifiable,\nhampering article-level labelling.\nentirely\nobjective\nmostly\nobjective\nmixed\nmostly\nsubjective\nentirely\nsubjective0.00.10.20.30.4\n       7.94%               19.41%       \n       24.98%              28.42%       \n       19.28%       \nFigure 4\nThe estimated proportions for each\nsubjectivity level in misinfo-general .\nErrors bars give an approximate 95%\nconfidence interval (Agresti and Coull 1998)To assess the degree of objectivity, we\naskChatGPT-4o-mini to provide a rat-\ning for an article using a 5-point Likert\nscale, ranging from entirely objective to en-\ntirely subjective. While by no means SoTA,\nsimilar setups have shown reasonable per-\nformance in prior work (Galassi et al.; Stru\u00df\net al.; Shokri et al. 2024).\nFigure 4 shows the estimated propor-\ntions of each subjectivity level. Despite\nreliable news being in the majority, most\n(\u223c73%) articles have a subjectivity level of\n\u2018Mixed\u2019 or higher. In fact, \u2018Mostly Subjec-\ntive\u2019 articles seem to be most common.\nUpon inspection of the dataset, these\nannotations seem to match our findings.\nWhile unreliable news is substantially less\nlikely to present itself as objective, reliable\npublishers still publish a plethora of discussion and opinion pieces. This is especially\ntrue for publishers with a more pronounced political bias.\nWe repeat the binomial logistic regression analysis used in Section 7.1 to determine\nwhat publisher-level properties correlate with subjectivity. Unlike earlier, where the ease\nof classification correlated strongly with the form of misinformation, article subjectivity\ntends to correlate strongly with publisher political bias. Where an unreliable publisher\n9Idem.\n19\nComputational Linguistics Under Review\nReliableQuestionable\nSourceConspiracy-\nPseudoscience\nLeftSatire\nCenter RightEmotion\nJoy\nTrust\nFear\nSurprise\nSadness\nDisgust\nAnger\nAnticipation\nNeutral\nPropensity\n25%\n50%\n75%\nFigure 5\nThe association of emotions in articles with different publisher categories, as measured using\npointwise mutual information. Each circle represents one of Plutchik\u2019s 8 emotions, along with\nneutrality in their center. The area of a circle represents the strength of the association of the\nemotion with that publisher class, relative to the maximal association found. The legend provides\neach emotion\u2019s color, moving clockwise from the top.\nreduces the odds of an objective article by between 0.34\u22120.68, moving to a left or\nright political bias does so by between 0.24\u22120.25. In other words, both reliable and\nunreliable publishers produce subjective, potentially unverifiable articles, especially\nwhen publishing from a biased standpoint. The full model, including estimated marginal\nmedians, and prompt specification, can be found in Appendix E. We also compare the\nagreement with ChatGPT-4o , which seems to lean towards an even greater subjectivity\npropensity.\nEmotion analysis. Another property we annotate for is emotion. Affective language in\njournalistic texts has long been understudied, despite emotion and its effect playing an\nincreasingly important role in the modern media landscape (Koivunen et al. 2021). It\nis especially prevalent in unreliable news, and is used to both persuade readers and\nincite sharing (Alba-Juez and Mackenzie 2019). While the persuasiveness of emotional\nlanguage in fake news is a matter of debate (Martel, Pennycook, and Rand 2020; Phillips\net al. 2024), with prior work showing that high affective state in people after ingesting\nmisinformation being associated with both increased susceptibility (Martel, Pennycook,\nand Rand 2020; Bago et al. 2022) and skepticism (Horner et al. 2021; L\u00fchring et al.\n2024). From a computational perspective, however, combining emotion detection with\nmisinformation detection has shown some promise (Ghanem, Rosso, and Rangel 2020;\nZhang et al. 2021b). Especially low valence emotions like anger, sadness, anxiety, surprise,\nand fear are believed to be prevalent in misinformation texts (Liu et al. 2024).\nWe use a similar setup as above to annotate articles with one of eight Plutchik basic\nemotions (Plutchik 1980) and neutrality, a common emotion model used for annotation\nin NLP (Bostan and Klinger 2018).\n20\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\nFigure 5 shows the association of different emotions with different publisher\nclasses. Visually, these results largely mirror the objectivity analysis; both reliable and\nunreliable publishers use emotional language, with political bias being a more important\ndeterminant of affect than reliability. The most notable difference in association is with\n\u2018Neutral\u2019 and Center-Reliable publishers; relative to other publisher categories, neutral\nwriting occurs relatively often for this publisher class. Low valence emotions like \u2018Anger\u2019,\n\u2018Disgust\u2019 and \u2018Sadness\u2019 are prevalent throughout, but are especially associated with\nmore politically biased, less reliable publishers. Especially \u2018Satire\u2019 publishers seem to be\ncharacterized by relatively high amounts of \u2018Joy\u2019 and \u2018Surprise\u2019. \u2018Anticipation\u2019 is highly\nprevalent in articles discussing expected future events, which appears to occur often,\nacross all publisher classes.\nOverall, however, much like subjectivity, there exists a good balance of emotions\nacross the different publisher classes. While there is some correlation between emotion\nor subjectivity with publisher reliability, these appear to be insufficient to function as a\nshortcut for misinformation prediction.\nWhile we partially corroborate the finding that different emotions are present (or at\nleast, in different proportions) across publisher classes, ultimately there exists a good\nbalance of emotions across the different publisher classes. Much like subjectivity, simply\nusing the emotions present in an article to determine whether it comes from a reliable or\nunreliable publisher is likely insufficient.\nAs argued in Section 1, to reliably estimate the generalization gap of misinformation\ndetectors, it is crucial to have access to a naturalistic corpus of misinformation, which is\nrepresentative in terms of the diversity contained within. With this analysis, we have\nshown that for the properties of emotion and subjectivity, this diversity is present, with\nsubjective and emotional language being present in articles from both reliable and\nunreliable publishers.\n9. Conclusion\nIn the present state of the art, automated misinformation detectors cannot be safely\nor reliably deployed. While impressive performance is often reported, time and again,\npapers show that in more realistic settings, where out-of-distribution generalization is\nrequired, these models fail. We attribute this brittleness, backed by substantial evidence in\nprior work, to a failure of misinformation datasets to adequately simulate misinformation\ndetection scenarios. Due to the prohibitively expensive cost of procuring misinformation\nlabels, practically all surveyed misinformation datasets have had to make unrealistic\nassumptions, which introduce undesirable biases.\nTo accommodate recommended misinformation evaluation practices, and thereby\nenable the development of generalizable misinformation models, this article introduces\nmisinfo-general . It consists of a benchmark built on top of a cleaned, weakly super-\nvised corpus of online articles, which have rich article- and publisher-level metadata, and\nan operationalization of various generalization axes. We have shown that this dataset is\nchallenging for a common class of misinformation detection models, and especially so\nwhen generalization to unseen forms of content or publishers is required.\nThe metadata annotations enable us to further analyze the determinants of model\nperformance. We find large discrepancies across political biases and misinformation\nforms, but have also shown that increased diversity has a positive effect on generalization.\nWhile publisher-level labelling introduces noise, we believe the increased scale,\ndiversity, and affluence of metadata make up for this. The first two properties can enable\n21\nComputational Linguistics Under Review\nOoD generalization or robustness, whereas the latter enables evaluation, analysis and\npotentially generalization-aware training.\nWe make the dataset publicly available, and hope it will serve as a resource for OoD\ngeneralization-focused training and evaluation. While the implementation of trustworthy\nautomated misinformation detectors remains out of reach, we hope that this dataset\nat least makes evaluating and diagnosing generalization capacities of misinformation\ndetection models easy enough for wide-spread academic adoption.\n10. Limitations & Ethical Considerations\nWhile the dataset includes a diverse set of publishers, events and topics, ultimately the\npublisher metadata comes from a single source. The information provided by MBFC\nassumes a narrow, US-centric world-view. This is especially prominent when discussing\nforeign publishers from nations with geopolitical ambitions at odds with the U.S. As\nsuch, the metadata provides limited nuance, providing only a single perspective of an\ninherently subjective assessment. It is expected that across cultural backgrounds, the\npublisher information is bound to change.\nIn general, the news included in the dataset is US-centric, with the vast majority\nof publishers being American, producing articles for an American audience. This is\nexacerbated by us excluding all non-English articles. This means cross-lingual or cross-\ncultural generalization cannot be evaluated.\nThat said, we do discuss one weak form of cross-cultural transfer. Besides prioritizing\ndifferent events, the primary distinction between the political ideologies discussed\nin Section 5, are their differing norms and values. As such, the poor political bias\ngeneralization bodes ill for the more general cross-cultural generalization tasks.\nSpecific to our approach, we experimented with only a single publisher to class label\nmapping. By binarizing the labels, we leave no room for interpretation by the model. It\nis apparent that not all unreliable sources are equally unreliable, as shown in Section 7.1.\nSimilarly, reliable sources can produce sensational content. Prior work has used different\nmappings, and we suggest future approaches to test various labelling schemes. With the\nprovided metadata, we believe this can be easily implemented.\n10.1 Dataset Access and Licensing\nWe aim to make misinfo-general as easy to use as possible, but have had to make\nsome restrictions. The dataset contains texts that are toxic, hateful, or otherwise harmful\nto society if disseminated. The dataset itself or any derivative formats of it, like language\nmodels, should not be released for non-research purposes.\nThe NELA corpora were initially released under a CC0 1.010, essentially being\nreleased to the public domain. From January 1st, 2024, the NELA authors have de-\naccessioned their repository. Upon request, the authors note their desire to restrict usage\nto non-commercial research.\nGiven the potentially harmful content, and our colleagues wishes, we (re-)release our\ndataset under a more restrictive CC BY-NC-SA 4.011license. This allows for redistribution\nand adaption as necessary for academic research, while preventing commercial use-cases\nand requiring adaptions to maintain these restrictions.\n10https://creativecommons.org/publicdomain/zero/1.0/deed.en\n11https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en\n22\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\nThe texts themselves might further be copyrighted by their original publishers. We\nhave ensured that all texts have been \u2019poisoned\u2019 with tokens that occlude large parts of\nthe text, extending the efforts of the original NELA authors.\nWe have deliberately removed all social media content, and all hyperlinks to such\ncontent. We consider such content Personally identifiable information (PII), with limited\nuse in misinformation classification beyond author profiling. Such applications are\nfraught with ethical problems (Mishra, Yannakoudakis, and Shutova 2021), and likely\nonly induce overfitting in text-based classification.\nWe have released misinfo-general through two media that allow for restricted\naccess. Specifically, we use Harvard\u2019s Dataverse (which implements per-file access\nrestrictions) and HuggingFace\u2019s Dataset Hub (which implements repository-level gating).\nWe plan to review access applications manually, limiting use-cases to academic research\nonly.\n23\nComputational Linguistics Under Review\nA. Misinformation Dataset Survey\nTable A.1\nA long table with an (inexhaustive) sampling of misinformation datasets. Each row provides a\nsingle dataset, with name and citation, along with the labelling granularity (see Sec. 3), the dataset\nsize (where units \u2018k\u2019 and \u2018M\u2019 denote thousands and millions, respectively), and a short description\nof how the dataset authors generated misinformation labels.\nLie Detector\n(Mihalcea and Strapparava 2009)Claim 0.3k MTurkersaproduce short arguments that align and\noppose their stance on various topics\nCREDBANK\n(Mitra and Gilbert 2015)Claim 60M Many MTurkersaannotate tweets for veracity and\nverifiability, with the majority annotation becoming\nthe label\nWeibo15\n(Ma et al.)Article 5k The authors scraped user nominated misinformation\narticles from the Sina Weibo Community Manage-\nment centerb. Unannoted posts were included as\nfactual posts\nMediaEval15\n(Boididou et al.)Claim 12k The authors generated a list of events which were\nverified as true or false, and a collection of tweets\ndiscussing these events. The tweets were manually\nverified\nBuzzFeed-Webis\n(Potthast et al. 2017)Article 1.6k Articles from a small set of sources were manually\nrated between mostly true or mostly false by expert\njournalists from BuzzFeedc\nTSHP-17\n(Rashkin et al. 2017)Publisher 70k Trusted news articles were sampled from the Giga-\nword News corpus, whereas unreliable news was\nsampled from specific publishers.\nKaggle Fake News\n(Risdal 2016)Publisher 13k The authors scraped articles from unreliable sources\nusing a third-party tool. No reliable articles were\nincluded\nAllcott & Gentzkow\n(Allcott and Gentzkow 2017)Article 0.2k Verified fake news articles were scraped from Snopesd,\nPolitiFacteand BuzzFeedc. No reliable articles were\nincluded\nPHEME\n(Zubiaga, Liakata, and Procter 2017)Claim 5.8k Tweets related to 5 mainstream events were manually\nannotated as unverified rumour or verified\nLiar\n(Wang 2017)Claim 13k Short snippets from famous politicians scraped from\nthe PolitiFacteAPI\nWeibo17\n(Jin et al. 2017)Article,\nPublisher10k They take posts reported as false from trusted users,\nand take articles from mainstream publishers for\ntheir factual class\nSome Like it Hoax\n(Tacchini et al. 2017)Publisher 15.5k Articles were scraped from Facebook groups dedi-\ncated to sharing scientific or pseudo-scientific articlesDataset Label Size Description\nContinued on next page\n24\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\nFake vs Satire\n(Golbeck et al. 2018)Publisher 0.5k Articles were sampled from identified satire or fake\nnews sites. The authors constrained the number of\narticles per publisher to ensure a diverse publisher\nset. All ambiguous cases were removed\nFakeNewsAMT\n(P\u00e9rez-Rosas et al. 2018)Article 0.5k A small set of manually verified articles were taken\nfrom mainstream publishers, and minimally edited\nby MTurkersato produce misinformation\nWeb Dataset Celebrity\n(P\u00e9rez-Rosas et al. 2018)Article 0.5k To complement FakeNewsAMT, the authors collect\narticles from rumour and tabloid publications, and\nmanually verify articles using sites like GossipCopf\nFakeNewsNet\nPolitiFact\n(Shu et al. 2019)Claim,\nArticle23k The authors label an articles based on a claim made\nwithin, where the claim is labelled by PolitiFacte\nFakeNewsNet\nGossipCop\n(Shu et al. 2019)Article,\nPublisher23k Unverified rumour articles were taken from\nGossipCopf, with verified rumours coming from a\nfew mainstream publishers\nFakeNewsCorpus\n(Pathak and Srihari 2019)Article,\nPublisher0.7k ~700 articles were taken from questionable source\npublishers, and used as misinformation, and 26 ex-\npert labelled factual news articles. Satire and unveri-\nfiable news were explicitly excluded.\nQProp\n(Barr\u00f3n-Cede\u00f1o et al. 2019)Publisher 51k Uses MBFC to assign articles the label of their\npublisher. They manage to sample from 104 differ-\nent sources, although only include 10 progandistic\nsources.\nFakeHealth\n(Dai, Sun, and Wang 2020)Article 2.3k Both variants of the dataset (HealthStory and\nHealthRelease) include text manually verified by ex-\nperts from HealthNewsReview.orggon the credibility\nof the information provided\nMM-COVID\n(Li et al. 2020)Article,\nPublisher4.2k Articles with manual labels were collected from\nSnopesdand Poynterh, and to complement reliable\narticles, they sample from mainstream media sources\nFakeCovid\n(Shahi and Nandini 2020)Article 5.2k Specifically COVID articles with labels from Snopesd\nand Poynterhwere collected. The authors make sure\nto include labels from 92 separate organizations\nacross 105 countries\nWeChat\n(Wang et al. 2020)Article 4k The authors collected articles flagged by WeChat\nusers. A small subset was annotated by experts,\nwhile a larger subset was unannotated, meant for\nunsupervised trainingDataset Label Size Description\nContinued on next page\n25\nComputational Linguistics Under Review\nCoAID\n(Cui and Lee 2020)Claim,\nArticle,\nPublisher3.7k Misinformation articles about the COVID19 pan-\ndemic were scraped directly from various fact-\nchecking sources. Factual articles were scraped from\n9 reliable publishers. Claims were scraped from offi-\ncial government sites\nMuMIN\n(Nielsen and McConville 2022)Claim 13k The authors collected a set of 115 fact checking\norganisation from the Google Fact Check TooliAPI,\nand then collected all fact-checked claims from these\norganisations. They use a separate classifier to collate\ndifferent labelling schemas\nPolitiFact-Oslo\n(Poldvere, Uddin, and Thomas 2023)Claim,\nArticle2.7k Claims were extracted from PolitiFacte, and the post\nor article from which the claim originated was manu-\nally extracted. The authors specifically highlight the\nimportance of publisher-level metadata\nMCFEND\n(Li et al. 2024)Article 24K Articles annotated by various fact-checking organisa-\ntions around the world were collected, and manually\nmapped to a single annotation schema.Dataset Label Size Description\naMechanicalTurk: crowdsourced lay volunteers\nbWeibo Community Management Center: credible Weibo users can report posts\ncBuzzfeed: a digital media company\ndSnopes: expert journalist website for debunking misinformation\nePolitiFact: expert journalist website for fact checking politicians\nfGossipCop: a defunct website dedicated to fact-checking celebrity rumors\ngHealthNewsReviews: a defunct website dedicated to reviewing medical claims\nhPoynter: a global, non-profit organization with annotations from partnered organizations\niGoogle Fact Check Tool: a unified API for fact-checking annotations\n26\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\nYear 2017 2018 2019 2020 2021 2022 Total\nOriginal 0.14 0.70 1.12 1.78 1.86 1.78 7.24\nDe-aggregation & Labelling0.13 0.61 0.96 1.62 1.71 1.66 6.69\n-1% -9% -12% -5% -3% -3% -8%\nExact Deduplication0.12 0.55 0.86 1.34 1.39 1.32 5.58\n-6% -11% -12% -18% -20% -21% -23%\nCleaning0.12 0.53 0.71 1.12 1.16 1.11 4.74\n-3% -4% -17% -16% -17% -16% -35%\nNear Deduplication0.11 0.47 0.65 1.03 1.06 1.01 4.33\n-11% -12% -8% -8% -9% -9% -40%\nLanguage Detection0.11 0.47 0.64 1.02 1.05 0.99 4.16\n0% -1% -1% -1% -1% -1% -43%\nTable B.1\nSize of the datasets, in terms of millions of articles, after each step of cleaning, per year. The lower\npercentage gives the step-to-step reduction in size. The \u2019Total\u2019 column computes reduction relative\nto \u2019Original\u2019.\nB.misinfo-general Processing & Statistics\nWe downloaded the initial NELA corpora from the Harvard Dataverse, under a CC0 1.0\nlicense12. The corpora have since been de-accessioned, and can longer be downloaded.\nWe expand on this in Section 10.1.\nB.1 Re-Labelling\nAs a first processing step, we relabelled all publishers. This was done to 1) attribute\narticles to their original publisher (where possible), 2) ensure publisher information was\nup-to-date (MBFC had expanded their catalogue considerably) and 3) to mitigate the\neffects of publishers that might interfere with the learning signal.\nAn important class of publishers that belong under that third point are aggregation\nsites. Such sites either do not produce original content, or intersperse articles from (usually\nmore reputable) other sources through their content. While the collection of articles as\na whole might express some editorial bias, for the most part, these sorts of publisher\nintroduce noise into an already noisy labelling scheme.\nWe manually re-mapped all URL domains to a set of publishers consistent across\nyears, excluding all news aggregation platforms and social media sites (see Tables B.2\nand B.3). It should be noted that the 2018 edition of NELA did not contain URLs, making\nrelabelling in this manner impossible.\nTable B.8 shows the amount of publisher overlap exists between different dataset\nyears.\n12https://creativecommons.org/publicdomain/zero/1.0/deed.en\n27\nComputational Linguistics Under Review\nTable B.2\nPublishers removed for being news\naggregation sites, with article counts\npost de-aggregation.\nPublisher # Articles\ndrudgereport 92 186\nbonginoreport 17 895\nwhatreallyhappened 57 431\nthelibertydaily 6 346\nyahoonews 52 797\ntheduran 10 722\nTotal 237 377\nTable B.3\nPublishers removed for being social\nmedia sites, with article counts post\nde-aggregation.\nDomain # Articles\nsoundcloud.com 106\nyoutu.be 4 009\napps.apple.com 414\namazon.com 112\nfacebook 108\namazon 113\nyoutube 2 902\nplay.google.com 421\ntwitter 274\ninstagram 15\nreddit 4\ndnyuz.com 1 900\nTotal 10 378\nTable B.4\nFrequent duplicated articles.\nType Sample Text\nBanners Click for more article by Guest . . .\nPreviews To read the full blog, please check out the complete post . . .\nPrayers Our Father, who art in heaven, hallowed be thy Name . . .\nPodcast Descriptions Don\u2019t forget to tune in to . . .\nError Messages 403 Forbidden nginx\n28\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\nB.2 Deduplication\nModels trained on this dataset have to (implicitly) learn a mapping from an article to its\npublisher\u2019s class. As a result, any instances of duplicate content unique to a publisher\nor publisher class likely induces overfit. The model need only memorize those cases to\nmake a prediction, independent of the article content.\nTo minimize the effect of these duplicates, we apply several stages of deduplication.\n1.we only keep the first (as determined by publication date) instance of exact\nduplicates for each publisher. Besides plagiarism, the most common duplicates are\ndue to errors in scraping or parsing, resulting in artifacts (see Table B.4)\n2.we remove all articles with duplicate titles or URLs, across the entire dataset. This\nprimarily removes articles that are updated at a later stage (e.g., live-blogs or\nsummaries), or result from URL re-directions\n3.we remove all articles if more than 5% of its sentences are duplicates from the same\npublisher\nThe former two (\u2018Exact Deduplication\u2019 in Table B.1) can handle document-level\nduplicates, like plagiarized articles or updated blog-posts, but miss near duplicates.\nThe latter should filter out near duplicates, like lightly edited documents, (\u2018Near\nDeduplication\u2019 in Table B.1).\nAn added benefit of these deduplication steps is the identification of common\nscraping errors. In fact, we found this to be the most frequent form of duplication.\nWe list some common texts in Table B.4. Since the majority of such errors were automated\nresponses unique to individual publishers, we could filter these out with relative ease\nusing the described \u2018Exact Deduplication\u2019 approaches.\nUltimately, the separate deduplication steps together resulted in removing 3.1M\narticles, comprising roughly \u224822% the dataset after relabelling.\nB.3 Cleaning & Filtering\nMany of the included articles either include too many non-semantically relevant tokens\nor are otherwise malformed. This is aggravated by the NELA authors \u2019poisoning\u2019 the\ndataset (from 2019 onwards) with repeated \u2018@\u2019 tokens to avoid copyright infringement.\nWe do our best to clean the article texts, and remove those with no discernible semantic\ninformation.\nWe normalize all punctuation and remove any embedded URLs, HTML, or Mark-\ndown markup. SpaCy\u2019s (Montani et al. 2023) en_core_web_sm was used to annotate all\ntokens in the corpora. We identify self-references as named entities with a large longest\ncommon substring relative to the publisher\u2019s name. We also mask any sentences which\nSpaCy flags as being part of an email, URL or twitter handle. Finally, we standardize the\nNELA copyright poisoning, applying it to all dataset years equally.\nAll-in-all, this introduces 4 new special tokens: <copyright> replacing NELA\u2019s\nrepeated @tokens, <twitter> for X (formerly known as Twitter) handles, <url> for\nany URLs and <selfref> for any self-references.\nTo ensure all articles contained enough grammatical text to reasonably classify, we\nremoved any article which did not abide by the rules delineated in Table B.5. As a final\nstep, we use Lingua (Stahl 2024) to filter out any non-English texts.\nDespite removing almost half of the articles, the dataset retains a level of \u2019noise\u2019\ncustomary to data sourced from the internet. This is inherent to the domain, and further\ncleaning might negatively affect the realism of the benchmark. One type of noise that\n29\nComputational Linguistics Under Review\nTable B.5\nFiltering rules.\nRule V ALUE\nArticles must contain at least ${VALUE} tokens 16\nArticles must not contain more than ${VALUE} tokens 4096\nThe article must have a title -\nThe article must be at least ${VALUE} times longer than its title 3\nThe article must have a mean token length greater than ${VALUE} 2\nThe article must have a mean token length less than ${VALUE} 10\nThe article must have at most ${VALUE}% copyright tokens 20\ncould interfere with learning is the presence of stylistically unique substrings identifying\npublishers. For example, articles from the same publisher tend to contain similar by-lines,\nattribution messages, or donation requests.\nB.4 Event & Topic Clustering\nTo generate the metadata necessary for the \u2018Topic\u2019 generalization form (see Section 5\nor Appendix C), we opted for a bottom-up approach. This involved first clustering the\ndataset into thousands of fine-grained events, before clustering the event clusters into\noverarching topics.\nTo achieve this, we used a heavily modified variant of BERTopic (Grootendorst\n2022) with a gte-large13(Li et al. 2023) backbone.\nAfter embedding an entire year of the dataset, we first reduce their dimensionality\nusing mini-batched PCA, and whiten the data. We construct a vocabulary over all\nthe documents, by aggregating mini-batches of vocabularies. We then apply UMAP\ndimensionality reduction, using the PCA solution as initialization, and the cosine distance\nas the distance metric. Clustering was performed via HDBScan to mini-batches of the\nembeddings, assigning all articles to their nearest event cluster. Using the dataset-wide\nvocabulary, we generate a single TF-IDF representation matrix.\nAfter mini-batching, we generate a hierarchical clustering on top of the event-based\nTF-IDF matrix. We merge any events with a distance below 0.8 to mitigate the effect\nof mini-batching, and re-construct the TF-IDF representations. We deem these the final\nevent representations. Table B.6 provides the total number of events and the average\nnumber of articles contained within each.\nFinally, we apply a round of spectral clustering to these representations, compressing\nthese events into 10 groups, which we deem topics. The numbers of events in each topic\nvaries (see Table B.6).\nB.5 Metadata\nArticle content is stored in arrow files, accessed using HuggingFace\u2019s datasets library\n(HuggingFace 2021). We store all generated metadata in a duckdb database (M\u00fchleisen\n13https://huggingface.co/thenlper/gte-large\n30\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\narticles\nPKarticle_id VARCHAR\nyear UINT\nFKsource\u00a0 VARCHAR\ntitle VARCHAR\ncontent_hash VARCHAR\npublication_date DATE\nFKtopic_id\u00a0 UINT\nurl VARCHAR\ncovid_article BOOLsources\nPKsource VARCHAR\nurl VARCHAR\nlabel ENUM\nbias ENUM\nfactuality ENUM\ncountry VARCHAR\ntype VARCHAR\npopularity ENUM\ncredibility ENUM\nreasoning VARCHAR\nconspiracy ENUM\npseudosci ENUM\ncheck_date DATEtopics\nPKtopic_id UINT\nyear UINT\nrepresentation VARCHAR\nimbalanced_hyper_clusteUINT\nbalanced_hyper_cluster\u00a0UINT\nFigure B.1\nSchema for the generated metadata database. \u2019PK\u2019 indicates primary key, \u2019FK\u2019 denotes a foreign\nkey relationship.\nand Raasveldt 2024). Figure B.1 depicts the metadata schema. Table \u2019Sources\u2019 provides\ninformation sourced from MBFC on different publishers, whereas tables \u2019Articles\u2019 and\n\u2019Topics\u2019 provides information on the produced articles and events/topics, respectively.\nThe articles are all given unique article identifiers. This ensures we can quickly generate\nrelevant splits of the dataset and link these to article-level predictions.\nB.6 Publisher-level Metadata\nAll publisher metadata is stored in the \u2019Sources\u2019 table. Each source is identified by a\nunique name, linked in a 1-to-many relationship to articles in the \u2019Articles\u2019 table. The\nurl column provides a link to the MBFC page with the source metadata. The label\ncolumn provides the MBFC label, also shown in the lower rows of Table B.6. In column\nbias , the political bias (one of \u2019Extreme Left\u2019, \u2019Left\u2019, \u2019Left-Center\u2019, \u2019Least Biased\u2019, \u2019Pro-\nScience\u2019, \u2019Right-Center\u2019, \u2019Right\u2019 or \u2019Extreme Right\u2019) of the publisher is provided. The\nfactuality column provides an ordinal value for the propensity of a publisher to\nreport factual news. Columns country andtype provide information about the country\nof origin of the publisher, and their primary form of publication (e.g., TV , blog posts),\nrespectively. In column popularity , the average number of visits to the publisher\u2019s\nmain website is provided as an ordinal categorical value. credibility provides an\noverall assessment of the publishers\u2019 credibility, aggregating all other variables into a\nsingle score. Finally, the conspiracy andpseudosci columns provide the \u2018strength\u2019 of\nthe conspiracy or pseudo-science source. The larger this value is, the more these sources\ndeviate from the public or scientific consensus. This is only provided for publishers\nlabelled as \u2018Conspiracy-Pseudoscience\u2019.\nB.7 Article Statistics\nWe present various statistics at the article level for each iteration of the final\nmisinfo-general dataset in Table B.6. This includes the number of articles, the average\nnumber of tokens per article (post truncation), the number of events and the size of topics,\nand finally the label proportions, both in aggregated form as reliable+or unreliable\u2212\n, but also per publisher category. In total, the dataset comprises some 2B tokens once\ntruncated. Without truncation, this is likely substantially higher.\n31\nComputational Linguistics Under Review\nTable B.6\nVarious statistics on the dataset and labels. ARTICLE COUNTS provides the total number of articles\nin each dataset year. LABEL PROPORTION provides the relative occurrence of reliable vs. unreliable\narticles, whereas P UBLISHERS provides the number of such publishers present. Section\nTRUNCATED TOKEN COUNTS provides the mean and the 25, 50, 75th quantiles of the number of\ntokens per article. Note that is after truncation at 512; the raw articles tend to be much longer.\nEVENT CLUSTERING provides the number and average size (in articles) of events. We also provide\nthe number of events belonging to the smallest and largest topics. Sections RELIABLE LABELS and\nUNRELIABLE LABELS provides the proportion of each MBFC label in the dataset, split into\npublisher categories.\nStatistic 2017 2018 2019 2020 2021 2022\nArticle Counts 0.10M 0.46M 0.59M 1.02M 1.04M 0.99M\nPublishersReliable 43 99 134 189 183 184\nUnreliable 49 60 100 249 219 201\nTruncated\nToken\nCountsMean 398.53 383.59 430.22 432.87 428.15 435.90\nQ25 299 266 358 369 366 383\nQ50 488 448 512 512 512 512\nQ75 512 512 512 512 512 512\nEvent\nClustering# Events 2674 6288 7718 7931 8758 8336\nMean Event Size 38.67 73.38 76.87 128.49 118.24 119.29\nSmallest Topic Size 2.7K 20.5K 24.6K 27.7K 51.3K 48.4K\nLargest Topic Size 38.3K 94.1K 113.1K 192.3K 211.4K 209.4K\nLabel\nProportionreliable+61.39% 72.82% 75.91% 70.73% 71.36% 72.00%\nunreliable\u221238.61% 27.18% 24.09% 29.27% 28.64% 28.00%\nreliable+Left 17.91% 13.32% 11.13% 9.01% 8.76% 9.88%\nLeft-Center 24.09% 29.62% 35.19% 36.50% 34.53% 35.01%\nLeast Biased 0.93% 2.18% 4.38% 4.91% 5.13% 3.97%\nRight-Center 4.34% 12.66% 9.81% 8.70% 10.38% 9.52%\nRight 14.12% 15.05% 15.41% 11.40% 12.03% 13.08%\nPro-Science 0.21% 0.53% 0.55%\nunreliable\u2212Satire 1.70% 0.99% 0.82% 0.66% 0.48% 0.42%\nQuestionable Source 27.83% 20.14% 17.34% 19.41% 19.00% 18.45%\nConsp.-Pseudosci. 9.09% 6.05% 5.93% 9.20% 9.17% 9.13%\nIn Table B.7 we present descriptions of the various topics in the 2020 split of\nmisinfo-general . These topics are latent, and automatically generated from the inter-\nevent distance matrix. The space in which the articles were clustered into events is high-\ndimensional, and each cluster was assumed to be non-convex. As a result, it is difficult\nto find an \u2019average\u2019 representation of any event, and even more difficult to find one for\na topic (i.e., a cluster of clusters). Instead, we derived topic descriptions by sampling\nevents from each topic, and using the BERTopic generated event representation.\n32\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\nTable B.7\nAn example of the topics present in the 2020 split of misinfo-general . As described in Section\n5, to form the OoD set, the ksmallest topics are chosen, such collectively they (approximately)\ncomprise 20% of all articles.\n# Description # ArticlesLabels\nID/OoD\nreliable+unreliable\u2212\n7US Federal Elections 192k 18.8% 64.11% 35.89% ID\n9International Affairs 161k 15.8% 75.80% 24.20% ID\n8Health & COVID 149k 14.6% 70.85% 29.16% ID\n4Entertainment & Sports 112k 11.0% 86.30% 13.70% ID\n5Economy & Social Issues 112k 10.9% 63.50% 36.50% ID\n6Science & Technology 89k 8.7% 71.68% 28.31% ID\n3US Local Politics 66k 6.5% 68.85% 31.15% OoD\n2Crime & Justice 64k 6.2% 68.93% 31.07% OoD\n0 Conflict 45k 4.4% 69.37% 30.63% OoD\n1Biden Administration 28k 2.7% 60.43% 39.57% OoD\nB.8 Publisher Statistics\nAcross all dataset iterations, the vast majority of articles are written by a small minority\nof prolific publishers. This is shown in Figure B.2. The most prolific publishers have\nauthored between 40-60% of articles within a year. The next deciles manage only 20%,\nthen 10-15%, etc., with the smallest publishers authoring only a few articles. This effect\nseems to get more drastic for the later dataset years.\nAs discussed in Section 6 this can obscure poor performance when using article-\nlevel evaluation metrics. In such cases, the average performance score will tend to be\ndominated by the performance of the largest publishers, instead of the body of publishers\nas a whole. As shown in Figure 1 and Tables E.2 and E.3, performance on these less\nmainstream publishers tends to be substantially worse. As a result, the OoD performance\nmetrics in Table 2 likely underestimate the generalisation gap.\nThis dataset property means publisher-level analyses are necessary, in addition\nto the far more common article-level performance measures. That said, prominent\nmisinformation datasets tend to sample from a far more a smaller and homogenous set\nof publishers, with typically more balanced authorship proportions.\nAnother noteworthy statistic is the amount of overlap between publishers across\nthe dataset years. While the dataset as a whole contains 488distinct publishers, as can\nbe seen in Table B.6, each dataset year contains fewer total publishers. Furthermore, the\ndistribution of those publishers (in terms of article counts) shifts across the years. This is\npartially due to a changing collection methodology used by Horne et al.; Norregaard,\nHorne, and Adali; Gruppi, Horne, and Adal\u0131; Gruppi, Horne, and Adal\u0131; Gruppi, Horne,\nand Adal\u0131.\n33\nComputational Linguistics Under Review\n0.00.20.40.62017 2018 2019 2020 2021 2022\nFigure B.2\nThe authorship of articles in the dataset, aggregated into deciles. The right most column of each\npanel represents the top 10% most prolific publishers, the left most the bottom 90%.\n2017 2018 2019 2020 2021 2022\n2017 1.00 0.43 0.31 0.17 0.16 0.16\n2018 0.43 1.00 0.62 0.34 0.33 0.32\n2019 0.31 0.62 1.00 0.52 0.50 0.49\n2020 0.17 0.34 0.52 1.00 0.86 0.80\n2021 0.16 0.33 0.50 0.86 1.00 0.89\n2022 0.16 0.32 0.49 0.80 0.89 1.00\nTable B.8\nOverlap of publishers between years.As a result, the different dataset\nyears have different degrees of over-\nlap between each other. Table B.8\nshows the publisher intersection-over-\nunion overlaps between the 6differ-\nent dataset years. The largest block of\noverlap is between the years 2020-2022,\nwith overlaps across datasets being\nwell above 0.80. The IoU scores to other\ndataset years are far lower, with 2017\nhaving particularly low overlaps.\n34\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\nC. Additional Information on Generalization Axes and Splits\nUniform. For this generalization axis, we employ the industry standard method of\nstratified and shuffled train/validation/test splits. We maintain the ratios 80/10/20,\nrespectively, throughout. This is meant to form the baseline set of values, both in terms\nof ID-OoD articles performance, but also the expected delta between those.\nTime. Misinformation changes quickly, but models have been found to be brittle to\narticles outside the time-span of the training data (Bozarth and Budak 2020; Horne,\nN\u00f8rregaard, and Adali 2020). We train models on a dataset from a single year, and\nevaluate performance on all other years. To avoid conflating \u2018Time\u2019 and \u2018Publisher\u2019\nresults, we only evaluate on articles from publishers seen in the training set. The\nproportion of publishers\u2013and as a result, the reliable/unreliable proportions\u2013does change\ndrastically across dataset years.\nEvent. We define events as occurrences that span news articles with a definite time-\nspan and sudden occurrence. Novel events spawn articles with a markedly different\nvocabulary, introducing words, names, or terms not yet available to the model. Prior\nwork has shown models suffer when evaluated on unseen events (Lee et al. 2021; Ding\net al. 2022). The dataset contains a multitude of such events, but we focus specifically on\nthe COVID-19 pandemic; an event that particularly notable for its rapid onset and the\nvolume of misinformation it spawned (Bradd 2024). Models are trained on a subset of\narticles notcontaining COVID-19 keywords, and evaluated on ones that do. The article\ndiscovery-process is outlined in more detail in Appendix C.1.\nTopic. We define topics to be relatively static collections of events, wherein the style or\npublishers\u2019 opinions change little across years or decades. We discover such topics\nautomatically, bottom-up. Specifically, we apply a heavily modified variant of the\nBERTopic (Grootendorst 2022) algorithm to discover the many thousands of events\nthat occur in each year of the dataset. Each event is textually represented by a TF-IDF\nvector. To group events into topics, we apply a second round of spectral clustering on\nthe adjacency matrix induced by the inter-event cosine distance matrix. The number\nof articles contained by a topic differs substantially (see Table B.6). We reserve the n\nsmallest topics, which collectively contain roughly 20% of all articles for testing, and\ntrain on articles from all other topics.\nPublisher. All scraped articles are annotated with the news website, outlet, or publisher\nthat produced the article (see Figure B.1). In general, news publishers have some editorial\nbias or stance, making their corpus of articles distinctly different from that of other\npublishers. Prior work has found misinformation classifiers to be particularly sensitive to\nchanges in publisher (Barr\u00f3n-Cede\u00f1o et al. 2019; Bozarth and Budak 2020). We specifically\ntest for this by reserving the nsmallest publishers, which collectively contain roughly\n20% of all articles for the test set, and train on articles from all other publishers. We\nstratify this splitting across political bias, to ensure each political bias is represented\nequally in both the train and test sets.\nPolitical Bias. Separate from editorial bias, publishers tend to exhibit a political bias as\nwell. MBFC has annotated all publishers on a left-right political bias continuum. We map\nall \u2018Extreme Left\u2019 and \u2018Left\u2019 publishers to a coarser \u2018Left\u2019 group, and vice versa for the\n\u2018Right\u2019-side of the political spectrum. All other political biases (\u2018Center-Left\u2019, \u2018Center-\nRight\u2019, \u2018Least Biased\u2019 and \u2018Pro-Science\u2019) were mapped to \u2018Center\u2019 bias. We reserve either\n35\nComputational Linguistics Under Review\n0.20.40.60.8ProportionProportion of articles discussing COVID-19\nMar Jun Sep Dec Mar Jun Sep Dec Mar Jun Sep Dec0255075100Interest\n2020 2021 2022Google News-Search Interest for COVID-19\nFigure C.1\nThe top figure shows the proportion of articles published that day which discuss the COVID-19\npandemic. The solid line provides a LOESS smoothed trend line. The bottom figure provides a\nGoogle search interest for topics that fall under Coronavirus disease 2019. The different years\n(2020, 2021, 2022) are displayed as banded vertical columns.\nthe \u2018Left\u2019 or \u2018Right\u2019 group for evaluation, and train on the collection of articles from the\nopposite political bias, along with all \u2018Center\u2019 biased publishers.\nMisinformation Type. Misinformation presents in various forms. MBFC divides un-\nreliable publishers in three categories: \u2018Questionable-Source\u2019 (publishers that exhibit\nextreme bias or propaganda), \u2018Conspiracy-Pseudoscience\u2019 (publisher aligning with\nknown conspiracies or pseudo-scientific practices) and \u2018Satire\u2019 (publishers whose content\nis purposefully false for comedic effect). To test this form of generalization, we reserve\neither \u2018Questionable-Source\u2019 or \u2018Conspiracy-Pseudoscience\u2019 labels for evaluation, and\ntrain on all other articles. The test set includes a stratified sample of reliable sources as\nwell.\nC.1 Identifying COVID-19 Articles\nFor \u2018Event\u2019 splitting, we focus on a single event present over the latter 3 dataset years:\nthe COVID-19 pandemic. We first derive a set of COVID-19 keywords (e.g., \u2018sars-cov-2\u2019,\n\u2018lockdown\u2019, \u2018mask\u2019), and combine it with the set of keywords defined by (Gruppi, Horne,\nand Adal\u0131 2021). We include all articles from 2020-2022 that include any of these terms\nin the held-out test set. While this is bound to induce a large number of false positives,\nit ensures no COVID-19 related terms contaminate the models\u2019 learned vocabulary. In\nFigure C.1 we display the correlation between the found number of articles and the\namount of Google search volume.\n36\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\nYear IoURandom\nIoUExecess\nIoUTest Set Sizes\nPublisher Topic\n2017 0.1183 0.1084 9.07% 19764 20709\n2018 0.1227 0.1162 5.62% 109043 85832\n2019 0.1210 0.1133 6.80% 133465 110168\n2020 0.1386 0.1116 24.24% 203224 205873\n2021 0.1317 0.1055 24.82% 192315 203311\n2022 0.1318 0.0992 32.93% 172530 186986\nTable C.1\nThe amount of article_id overlap between the \u2018Publisher\u2019 and \u2018Topic\u2019 test sets. Column \u2018IoU\u2019\ngives the intersection over the union between the two held-out test sets. Column \u2018Over Random\u2019\ncompares this to the achieved value for 10000 simulated draws from a uniform distribution of the\nsame sizes as the actual test set sizes (given in the last two columns).\nC.2 Test Set Correlations\nPublishers have a tendency to prioritize different topics. This is especially prevalent for\nless mainstream publishers, which have far fewer resources to spend on the breadth of\ntheir reporting. It is therefore plausible that the \u2018Publisher\u2019 and \u2018Topic\u2019 splits have some\noverlap.\nWe test for this by comparing the amount of article level overlap in the held-out test\nsets of the \u2018Publisher\u2019 and \u2018Topic\u2019 to random draws of the same size from uncorrelated\nuniform distributions. The larger this overlap, the more correlated the two test sets\nare. We display the results of our test in Table C.1. While we find more overlap than\nrandom draws, the effect is not large. Even in the most egregious case, there is only a\n0.04difference in absolute IoU. As such, we believe the two different test sets measure\ndistinct generalization aspects.\n37\nComputational Linguistics Under Review\nD. Training Details\nHyperparameter Value\nseed 942\nmax_length 512\nval_prop 0.1\nbatchsize 64\npooler_dropout 0.1\nlr\nembeddings 5.0e-7\npooler 5.0e-5\nclassifier 1.0e-3\nweight_decay 0.1\nlr_scheduler\ntype \u2018polynomial\u2019\npower 3.163\nmax_steps 3.0e+6\nwarmup_ratio 0.0213\neval_prop 0.05\npatience 2\nTable D.1\nAn overview of important hyperparameters\nand their values. These were tuned on the\nvalidation set of the \u2018Uniform\u2019 split dataset.While DeBERTa-v3 models can theoret-\nically handle infinite sequences, to con-\nstrain memory requirements we truncate\ntokenization after the first 512tokens. This\nallowed for a batch size of 64during train-\ning, and 512during validation. We used\nAdam with weight decay (Kingma and Ba\n2017; Loshchilov and Hutter 2019) as the\noptimizer.\nSome other important training hyper-\nparameters are listed in Table D.1. We set\na high learning rate for the classifier and\npooler, but a very low value for the em-\nbeddings layer to avoid catastrophic forget-\nting. We employed a polynomial decaying\nlearning rate scheduler, which first linearly\nwarms up, before decaying according to\n\u03b7t=\u0012t\nmax_steps\u0013power\n\u03b7t\u22121\nFor each experiment, regardless of the size\nof the dataset, we allow for a maximum\nof3.0e+ 6update steps, evaluating on the\nvalidation set every 5%. We employ early\nstopping with patience of 2, through which\ntraining usually concludes well before reaching the maximum budget.\nD.1 Additional Training Details\nAll experiments were conducted on SNELLIUS, a Linux SLURM-based supercomputer.\nNodes consist of an Intel Xeon Platinum 8360Y CPU with 18 cores, 2.4 GHz speed, and a\nsingle NVIDIA A100 GPU accelerator (40 GiB of HMB2 memory), and 128 GiB of DDR4\nmemory.\nWe use Python 3.11 and PyTorch 2.2.2 built with CUDA 11.8. For training, we\nmake extensive use of utilities implemented in HuggingFace\u2019s transformers library:\ntransformers 4.37.2, datasets 2.19.0 and accelerate 0.30.1. All experiments were\nconducted under random seed 942. For local development, we use Ubuntu 20.04.6 LTS\n(GNU/Linux 5.15.90.1-microsoft-standard-WSL2 x86_64).\nD.2 LLM Inference\nTo conduct LLM inference, we use Meta\u2019s llama-3-8B-Instruct using the following\nprompt:\n\u00abSYS\u00bb\nYou are a content moderator working with journalistic articles. Your task is to identify\narticles from unreliable publishers.\n38\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\n\u00ab/SYS\u00bb\nDoes the following text come from a reliable news publisher?\nRespond with \u2019yes\u2019 or \u2019no\u2019.\nArticle: ${ARTICLE}\nDoes this article come from a reliable news publisher? \u2019yes\u2019 or \u2019no\u2019: [/INST]\nHere, ${ARTICLE} is replaced by the first 512tokens of each article.\nWe use the pmi_dc decision rule, introduced by Holtzman et al. (2021). We use the\nsame prompt without the article tokens as the domain conditional text. Empirically, this\nperforms slightly better across the entire corpus.\nDue to the (very) long articles prevalent in this dataset, using few-shot exemplars\nwas deemed intractable.\nD.3 Annotating Article Properties\nE. Obj. M. Obj. Mixed M. Subj. E. Subj.\nE. Obj. 5.71% 5.13% 0.39% 0.01%\nM. Obj. 1.80% 12.30% 3.76% 0.97%\nMixed 3.37% 15.19% 7.62% 0.49%\nM. Subj. 0.05% 0.39% 4.35% 14.70% 4.00%\nE. Subj. 0.83% 5.08% 13.77%\nTable D.2\nA confusion matrix between the subjectivity\nassessments of ChatGPT4o (rows) and\nChatGPT4o-mini (columns).Subjectivity. To annotate articles for sub-\njectivity, we used ChatGPT4o-mini , and\nChatGPT4o . The former was used to anno-\ntate the bulk of articles, and the latter to\nverify the overall quality of the annotations.\nWe used the following prompt:\n\u00abSYS\u00bb\nYou are a helpful assistant,\nhelping analyse the properties\nof news articles. Before a final answer,\nmake sure to explain your thinking.\n\u00ab/SYS\u00bb\nPlease classify how objective the\nfollowing article is.\nObjective articles take a neutral stance on topics, and focus on reporting factual news.\nSubjective articles instead focus on opinions, which are more difficult to verify and can\ntake specific stances for or against topics.\nThe title and body are provided. After you provide your reasoning, respond with one of\nentirely objective, mostly objective, mixed, mostly subjective, entirely subjective, and\nnothing else.\nArticle: ${ARTICLE}\nTable D.2 gives the classification correspondence between the two mod-\nels. Overall, we find that the mini model aligns well with the larger model,\nand that the larger model tends towards more subjective annotations, rather\nthan less. As such, it is plausible that the subjectivity within the dataset re-\nmains underreported. We include some examples of articles and their subjectivity\nannotations in https://github.com/ioverho/misinfo-general/tree/main/\nassets/subjectivity_annotations_examples .\nEmotion. We perform a similar analysis, but now for emotions present in the articles. We\nuse the following prompt, and only use ChatGPT4o-mini :\n\u00abSYS\u00bb\n39\nComputational Linguistics Under Review\nR\nQ\nC\nSanger\n anticipation\n disgust\nR\nQ\nC\nSfear\n joy\n neutral\nLeft Center RightR\nQ\nC\nSsadness\nLeft Center Rightsurprise\nLeft Center Righttrust\n0.00.20.40.60.81.0\nFigure D.1\nEmotion propensity, like Figure 5, but now in absolute values, and presented for each emotion in\nisolation. More intense, red colors signify a higher presence of an emotion, whereas cooler, blue\ncolors the opposite.\nYou are a helpful assistant, analysing the properties of news articles. Before a final\nanswer, make sure to explain your thinking.\n\u00ab/SYS\u00bb\nPlease identify the dominant emotions in the following article.\nWe will be using Plutchik\u2019s 8 basic emotions, along with a label for neutral:\n1. Joy: a feeling of happiness, pleasure, or contentment.\n2. Sadness: a feeling of loss, disappointment, or grief\n3. Trust: a sense of safety, security, and connection with others\n4.Disgust: a strong aversion to something unpleasant, often related to taste, smell, or moral\njudgments\n5. Fear: a response to perceived danger, leading to caution or escape behaviors\n6. Anger: a reaction to perceived threats or injustice\n7. Surprise: a reaction to something unexpected\n8.Anticipation: looking forward to or expecting something, which can bring excitement or\nanxiety\n9. Neutral: emotional balance, no strong positive or negative emotions are present\nTo respond, please list the emotions present in the article. When labelling the article for\nthe \u2019Neutral\u2019 emotion, please make sure no other emotion is present.\nThe article\u2019s title and body are provided. After you provide your reasoning, respond with\na list of joy, sadness, trust, disgust, fear, anger, surprise, anticipation, neutral, and nothing\nelse.\nArticle: ${ARTICLE}\nFigure D.1 presents the same data as in Figure 5, but now transposed (all publisher\nclass combinations per emotion), and in absolute values. We find emotion overall to be\npresent in all forms of articles and publishers.\n40\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\n-1 0 12017Left Center Right\n-1 0 12018\n-1 0 12019\n-1 0 12020\n-1 0 12021\nFP FN-1 0 12022\nFP FN FP FN\nFigure E.1\nProbabilities of making an error for publishers of a particular label and political bias, relative to\nthe probability of making a mistake for publishers of the same label. FP columns denote the\nprobability of error for reliable publishers, and FN vice versa. The higher\nE. Additional Results\nE.1 Political Bias\nTo further assess the model\u2019s interaction with political bias, we compute the complement\nof the expected publisher-level accuracy score for a political bias and publisher label\n(reliable or unreliable), and divide it by the marginal expected score:\n1\u2212E[ACCp|bias(p), yp]\n1\u2212E[ACCp|yp]\u22121\nessentially giving the shifted exponentiated pointwise mutual information (PMI) of\nmaking an error for a particular combination of publisher political bias and label,\np(\u02c6yp\u0338=y|bias(p), y)\np(\u02c6yp\u0338=y|y)\u22121 = exp ( pmi(\u02c6yp\u0338=y;bias(p)|y))\u22121\nThe higher this value is, the greater the association between making an error for a\nparticular political bias deviates from the expected value. If an error is made for a reliable\npublisher, this corresponds to a False Positive (FP) whereas an error for an unreliable\npublisher would correspond to a False Negative (FN). We introduce the shifting to ensure\n41\nComputational Linguistics Under Review\nno bias corresponds with 0, whereas positive and negative bias correspond to positive\nand negative values, respectively.\nIn Figure E.1 we display various such biases for each iteration of the corpus. As\nseen in Table E.2, performance degrades for the more extreme political biases. There\nis, however, a notable difference between left-biased and right-biased publishers, and\nespecially for the reliable right-biased publishers. The probability of the model generating\na False Positive is substantially higher (sometimes near twice as likely) than for reliable\nleftist or centre biased publishers. We conclude that the model tends to confuse articles\nfrom reliable right biased publishers with unreliable ones, far beyond what would be\nconsidered \u2019due to chance\u2019.\nOn the other hand, the model tends to be overly optimistic for centre-biased\npublishers, with lower FP probabilities and a tendency for slightly higher FN probabilities.\nAn unreliable publisher could therefore escape moderation by limiting its explicit political\nviews. While this is a relatively unlikely example, as most unreliable publishers are\nunreliable because of overt extreme political views, it does showcase an important blind-\nspot for these models.\nGiven the high-stakes nature of misinformation moderation decisions, this indicates\nthe presence of undesirable behaviour. In their current state, our misinformation detection\nmodels discriminate against publishers of a particular political group.\nE.2 Publisher Heterogeneity\nQuantity Top 1 Top 2 Top 3\nArticles 28% 43% 55%\nPublisher 11% 20% 30%\nTable E.1\nMean number of articles and publishers\nretained in the training set after limiting to\nthe top-n most prolific publishers. Note that\nthe number of articles and publishers\ndeviates from the distributions depicted in\nFigure B.2. In this case, we additionally\ncondition on the publisher\u2019s MBFC label.In Section 7.2 we tested for the effect of pub-\nlisher heterogeneity on generalising to un-\nseen publishers. We operationalized this by\nre-running the \u2018Publisher\u2019 split experiment\nwith only the top-n most prolific publishers\nfor each MBFC label, while leaving the test\nset untouched.\nThis dramatically reduces the amount\nof variation in publishers, while minimally\naffecting the total amount of data present.\nThis can be seen in Table E.1. Note that\nthese are expressed as percentages, but in\nabsolute terms constitute hundreds of thou-\nsands of articles.\nE.3 Determinants of Publisher-Level Accuracy\nTo estimate which factors impact accuracy at the publisher level. We define publisher\nlevel accuracy for a publisher pas,\nACCp=1\n|D(p)|X\nd\u2208D(p)1(\u02c6y(d)=y(p))\nor put otherwise, the expected rate of correct classification for all documents belonging\nto that publisher.\nTo determine which factors contribute to these scores, we use a multinomial logistic\nregression model. Specifically, we use statsmodels (Seabold and Perktold 2010) to\n42\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\nestimate the dependent variables as,\nX\nd\u2208D(p)1(\u02c6y(d)=y(p)) +1(\u02c6y(d)\u0338=y(p)) =\u03c3(\u03b20+KX\ni=1\u03b2ixi)\nWe use \u00b52\u00b7(1\u2212\u00b5)2as the variance function, Pearson\u2019s \u03c72as the scale value and the logit\nas the link function \u03c3.\nUniform Generalisation. The fully specified model is displayed in table E.2, with\nsome important variables\u2019 coefficients depicted graphically in Figure 1. The left most\ncolumn provides natural groupings of the different variables. The interpretation of the\ncoefficient magnitudes should be adjusted to consider the range of possible values for\nthe corresponding variable.\nIn the Generic category, we include the logarithm of the number of articles present\nin the training data, and whether the publisher is foreign or not. Both variables are\nassigned large coefficients. Every 10x increase of labelled articles increases the odds of\ncorrect classification by a factor of 1.91. Foreign publishers tend to be 2.91times as easy\nto classify relative to U.S. based publishers.\nThe Year category of variables includes a dummy variable for each iteration of the\ndataset. Here we find relatively little difference, despite statistical significance, with\ncoefficients falling between 0.85and1.15.\nWe further include dummy variables for the different political biases and MBFC\nlabels (assumes Centre-Reliable to be the default group). We furthermore include an\ninteraction effect between all political biases and MBFC labels. We find that unreliable\npublishers are substantially more difficult to classify, with all unreliable labels having\nlarge negative coefficients. This is especially true for the \u2019Questionable Source\u2019 category\nof articles, with log-odds shifts of \u22121.25and\u22121.84for left and right biased publishers,\nrespectively. Furthermore, corroborating the analysis in Appendix E.1, the models\nperform much more poorly for publishers on the right side of the political spectrum.\nIn the Strength block of variables, we include MBFC\u2019s strength of conspiracy or\npseudo-science. All \u2018Conspiracy-Psuedoscience\u2019 sources are rated on a scale, with larger\nvalues indicating a further deviation from the societal norm. Reliable sources a given a\ndefault score of 0. We find that there exists a slight, but consistent, positive correlation\nwith the strength of the pseudo-scientific or conspiratorial claims. In other words, the\nmore those publishers deviate from the status quo, the more easily these are identified.\nSimilarly, for the Factuality Score block, we include the MBFC score for a publisher\u2019s\npropensity for factual reporting as interaction effects with each MBFC label (excluding\n\u2018Satire\u2019). Again, we find a slight positive correlation for reliable sources, meaning that\nthe more often such publishers report factual information, the better classification\nperformance. For the unreliable publishers, however, the opposite holds: the less likely\nto present factual news, the easier the classification. In either case, an \u2019average\u2019 level of\nfactuality tends to correspond to more ambiguous cases, lying closing to the model\u2019s\ndecision boundary.\nTopic Distance. We repeat this analysis, but this time aggregating entities at the\nevent/label level, using the \u2018Topic\u2019 split results. The model specification is left as above,\nwith the introduction of a single additional variable: the minimum cosine distance to any\nevent in the training set. The coefficients are provided in Table E.3.\n43\nComputational Linguistics Under Review\nWhile comparison across Tables E.2 and E.3 is not directly possible, as each estimates\nusing different entities and different models, it is encouraging to see similar magnitudes\nfor most variables\u2019 coefficients.\nSurprisingly, we observe a very small but positive effect: \u03b2min_dist = 0.15. This implies\nthat a topic that is as far away as possible from all topics in the training set sees a 1.16\nincreased odds of correct classification. While this is a weak effect, it does suggest that\nevents more typical to a certain topic are easier to classify in than those near the topic\ncluster borders.\nAcknowledgments\n44\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\nGroup Parameter Range \u03b2exp(\u03b2)Std. Err. p 95% CI\nIntercept Intercept 1 -0.85 0.43 0.03 0.00 -0.91 -0.79\nGenericlog10(train count ) [0 ,\u221e]0.65 1.91 0.01 0.00 0.64 0.66\nForeign 0..1 1.07 2.91 0.01 0.00 1.06 1.08\nYear2018 0..1 0.07 1.08 0.02 0.00 0.04 0.11\n2019 0..1 0.14 1.15 0.02 0.00 0.10 0.17\n2020 0..1 -0.16 0.85 0.02 0.00 -0.19 -0.13\n2021 0..1 -0.06 0.94 0.02 0.00 -0.09 -0.03\n2022 0..1 0.00 1.00 0.02 0.85 -0.03 0.04\nPolitical\nBiasLeft 0..1 -0.39 0.68 0.01 0.00 -0.41 -0.37\nRight 0..1 -0.64 0.53 0.01 0.00 -0.66 -0.62\nLabelConspiracy-PseudoScience 0..1 -0.32 0.73 0.22 0.15 -0.76 0.11\nQuestionable Source 0..1 -2.40 0.09 0.02 0.00 -2.44 -2.37\nSatire 0..1 -1.58 0.21 0.18 0.00 -1.94 -1.23\nInteractionsLeft : Conspiracy-PseudoScience 0..1 -1.65 0.19 0.19 0.00 -2.02 -1.28\nLeft : Questionable Source 0..1 1.54 4.69 0.03 0.00 1.48 1.61\nLeft : Satire 0..1 -2.15 0.12 0.20 0.00 -2.54 -1.75\nRight : Conspiracy-PseudoScience 0..1 -1.70 0.18 0.19 0.00 -2.06 -1.33\nRight : Questionable Source 0..1 1.20 3.31 0.02 0.00 1.17 1.23\nRight : Satire 0..1 0.56 1.75 0.35 0.11 -0.12 1.25\nStrengthConspiracy 1..5 0.19 1.21 0.02 0.00 0.15 0.22\nPseudoScience 1..5 0.17 1.18 0.01 0.00 0.14 0.19\nFactuality\nScoreFactuality : Reliable 1..5 0.35 1.42 0.00 0.00 0.34 0.36\nFactuality : Conspiracy-PseudoScience 1..5 -0.09 0.92 0.03 0.00 -0.14 -0.04\nFactuality : Questionable Source 1..5 -0.13 0.88 0.01 0.00 -0.14 -0.11\nTable E.2\nCoefficients of the publisher-level determinants model, using the uniformly split models. Range\nindicates the possible values each variable can take, with ..indicating all natural numbers between\nthe extremes (inclusive). \u03b2provides the logistic parameter, and exp(\u03b2)its exponent (i.e. the\nlog-odds ratio). Column \u2019Std. Err.\u2019 provides the standard error, \u2019p\u2019 the corresponding p-value and\n\u201995% CI\u2019 the confidence interval. In the parameter names, the \u2019:\u2019 indicates an interaction term.\n45\nComputational Linguistics Under Review\nGroup Parameter Range \u03b2exp(\u03b2)Std. Err. p 95% CI\nIntercept Intercept 1 -0.49 0.61 0.05 0.00 -0.60 -0.38\nDistance Topic Distance [0, 1] 0.15 1.16 0.03 0.00 0.09 0.21\nGenericlog10(train count ) [0,\u221e] 0.56 1.75 0.01 0.00 0.54 0.58\nForeign 0..1 0.88 2.42 0.01 0.00 0.86 0.90\nYear2018 0..1 0.23 1.26 0.03 0.00 0.17 0.29\n2019 0..1 0.30 1.35 0.03 0.00 0.24 0.36\n2020 0..1 -0.18 0.83 0.03 0.00 -0.24 -0.13\n2021 0..1 -0.47 0.62 0.03 0.00 -0.53 -0.41\n2022 0..1 -0.27 0.76 0.03 0.00 -0.33 -0.21\nPolitical\nBiasLeft 0..1 -2.81 0.06 0.47 0.00 -3.74 -1.88\nRight 0..1 -2.58 0.08 0.03 0.00 -2.64 -2.52\nLabelConspiracy-PseudoScience 0..1 -1.51 0.22 0.25 0.00 -1.99 -1.03\nQuestionable Source 0..1 -0.54 0.58 0.01 0.00 -0.57 -0.51\nSatire 0..1 -0.73 0.48 0.02 0.00 -0.76 -0.70\nInteractionsLeft : Conspiracy-PseudoScience 0..1 -0.38 0.69 0.41 0.36 -1.18 0.43\nLeft : Questionable Source 0..1 2.11 8.27 0.05 0.00 2.02 2.21\nLeft : Satire 0..1 -1.83 0.16 0.31 0.00 -2.44 -1.22\nRight : Conspiracy-PseudoScience 0..1 -0.73 0.48 0.41 0.07 -1.53 0.06\nRight : Questionable Source 0..1 1.92 6.85 0.03 0.00 1.87 1.98\nRight : Satire 0..1 0.32 1.37 0.45 0.48 -0.56 1.19\nStrengthConspiracy 1..5 0.42 1.52 0.04 0.00 0.35 0.49\nPseudoScience 1..5 0.29 1.33 0.03 0.00 0.24 0.34\nFactuality\nScoreFactuality : Reliable 1..5 0.38 1.47 0.01 0.00 0.37 0.40\nFactuality : Conspiracy-PseudoScience 1..5 0.42 1.53 0.05 0.00 0.33 0.52\nFactuality : Questionable Source 1..5 -0.44 0.65 0.01 0.00 -0.46 -0.41\nTable E.3\nIdem, but now for the publisher-topic aggregated binomial logistic model. Since the aggregation\nlevel differs from that of the model presented in Table E.2, the coefficients are not directly\ncomparable.\n46\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\nReferences\nAgresti, Alan and Brent A. Coull. 1998.\nApproximate is Better than \u201cExact\u201d for\nInterval Estimation of Binomial\nProportions. The American Statistician ,\n52(2):119\u2013126.\nA\u00efmeur, Esma, Sabrine Amri, and Gilles\nBrassard. 2023. Fake news, disinformation\nand misinformation in social media: A\nreview. Social Network Analysis and Mining ,\n13(1):30.\nAlba-Juez, Laura and J Lachlan Mackenzie.\n2019. Emotion, lies, and \u201cbullshit\u201d in\njournalistic discourse: The case of fake\nnews.\nAllcott, Hunt and Matthew Gentzkow. 2017.\nSocial Media and Fake News in the 2016\nElection. Journal of Economic Perspectives ,\n31(2):211\u2013236.\nAltay, Sacha, Manon Berriche, Hendrik Heuer,\nJohan Farkas, and Steven Rathje. 2023. A\nsurvey of expert views on misinformation:\nDefinitions, determinants, solutions, and\nfuture of the field. Harvard Kennedy School\nMisinformation Review .\nBago, Bence, Leah R. Rosenzweig, Adam J.\nBerinsky, and David G. Rand. 2022.\nEmotion may predict susceptibility to fake\nnews but emotion regulation does not seem\nto help. Cognition & Emotion ,\n36(6):1166\u20131180.\nBaly, Ramy, Giovanni Da San Martino, James\nGlass, and Preslav Nakov. 2020a. We Can\nDetect Your Bias: Predicting the Political\nIdeology of News Articles. In Proceedings of\nthe 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP) ,\npages 4982\u20134991, Association for\nComputational Linguistics, Online.\nBaly, Ramy, Georgi Karadzhov, Dimitar\nAlexandrov, James Glass, and Preslav\nNakov. 2018a. Predicting Factuality of\nReporting and Bias of News Media Sources.\nInProceedings of the 2018 Conference on\nEmpirical Methods in Natural Language\nProcessing , pages 3528\u20133539, Association for\nComputational Linguistics, Brussels,\nBelgium.\nBaly, Ramy, Georgi Karadzhov, Jisun An,\nHaewoon Kwak, Yoan Dinkov, Ahmed Ali,\nJames Glass, and Preslav Nakov. 2020b.\nWhat Was Written vs. Who Read It: News\nMedia Profiling Using Text Analysis and\nSocial Media Context. In Proceedings of the\n58th Annual Meeting of the Association for\nComputational Linguistics , pages 3364\u20133374,\nAssociation for Computational Linguistics,\nOnline.Baly, Ramy, Georgi Karadzhov, Abdelrhman\nSaleh, James Glass, and Preslav Nakov.\n2019. Multi-Task Ordinal Regression for\nJointly Predicting the Trustworthiness and\nthe Leading Political Ideology of News\nMedia. In Proceedings of the 2019 Conference\nof the North American Chapter of the\nAssociation for Computational Linguistics:\nHuman Language Technologies, Volume 1\n(Long and Short Papers) , pages 2109\u20132116,\nAssociation for Computational Linguistics,\nMinneapolis, Minnesota.\nBaly, Ramy, Mitra Mohtarami, James Glass,\nLlu\u00eds M\u00e0rquez, Alessandro Moschitti, and\nPreslav Nakov. 2018b. Integrating Stance\nDetection and Fact Checking in a Unified\nCorpus. In Proceedings of the 2018 Conference\nof the North American Chapter of the\nAssociation for Computational Linguistics:\nHuman Language Technologies, Volume 2\n(Short Papers) , pages 21\u201327, Association for\nComputational Linguistics, New Orleans,\nLouisiana.\nBarr\u00f3n-Cede\u00f1o, Alberto, Israa Jaradat,\nGiovanni Da San Martino, and Preslav\nNakov. 2019. Proppy: Organizing the news\nbased on their propagandistic content.\nInformation Processing & Management ,\n56(5):1849\u20131864.\nBianchi, John, Manuel Pratelli, Marinella\nPetrocchi, and Fabio Pinelli. 2024.\nEvaluating Trustworthiness of Online\nNews Publishers via Article Classification.\nBodaghi, Arezo, Ketra A. Schmitt, Pierre\nWatine, and Benjamin C. M. Fung. 2024. A\nLiterature Review on Detecting, Verifying,\nand Mitigating Online Misinformation.\nIEEE Transactions on Computational Social\nSystems , 11(4):5119\u20135145.\nBoididou, Christina, Katerina Andreadou,\nSymeon Papadopoulos, Duc-Tien\nDang-Nguyen, Giulia Boato, Michael\nRiegler, and Yiannis Kompatsiaris.\nVerifying Multimedia Use at MediaEval\n2015.\nBojic, Ljubisa, Nikola Prodanovic, and\nAgariadne Dwinggo Samala. 2024.\nMaintaining Journalistic Integrity in the\nDigital Age: A Comprehensive NLP\nFramework for Evaluating Online News\nContent.\nBostan, Laura Ana Maria and Roman Klinger.\n2018. An Analysis of Annotated Corpora\nfor Emotion Classification in Text.\nBozarth, Lia and Ceren Budak. 2020. Toward\na Better Performance Evaluation\nFramework for Fake News Classification.\nProceedings of the International AAAI\n47\nComputational Linguistics Under Review\nConference on Web and Social Media ,\n14:60\u201371.\nBradd, Sam. 2024. Infodemic.\nhttps://www.who.int/health-\ntopics/infodemic.\nBroniatowski, David A., Daniel Kerchner,\nFouzia Farooq, Xiaolei Huang, Amelia M.\nJamison, Mark Dredze, Sandra Crouse\nQuinn, and John W. Ayers. 2022. Twitter\nand Facebook posts about COVID-19 are\nless likely to spread misinformation\ncompared to other health topics. PLoS\nONE , 17(1):e0261768.\nBurdisso, Sergio, Dairazalia S\u00e1nchez-Cort\u00e9s,\nEsa\u00fa Villatoro-Tello, and Petr Motlicek.\n2024. Reliability Estimation of News Media\nSources: Birds of a Feather Flock Together.\nCasavantes, Marco, Manuel Montes-y-G\u00f3mez,\nLuis Carlos Gonz\u00e1lez, and Alberto\nBarr\u00f3\u00f1-Cedeno. 2024. Propitter: A Twitter\nCorpus for Computational Propaganda\nDetection. In Advances in Soft Computing ,\npages 16\u201327, Springer Nature Switzerland,\nCham.\nCheng, Mingxi, Shahin Nazarian, and Paul\nBogdan. 2021. VRoC: Variational\nAutoencoder-aided Multi-task Rumor\nClassifier Based on Text.\nChicco, Davide and Giuseppe Jurman. 2020.\nThe advantages of the Matthews\ncorrelation coefficient (MCC) over F1 score\nand accuracy in binary classification\nevaluation. BMC Genomics , 21(1):6.\nChicco, Davide and Giuseppe Jurman. 2022.\nAn Invitation to Greater Use of Matthews\nCorrelation Coefficient in Robotics and\nArtificial Intelligence. Frontiers in robotics\nand AI , 9:876814.\nChinn, Susan. 2000. A simple method for\nconverting an odds ratio to effect size for\nuse in meta-analysis. Statistics in Medicine ,\n19(22):3127\u20133131.\nChu, Samuel Kai Wah, Runbin Xie, and\nYanshu Wang. 2021. Cross-Language Fake\nNews Detection. Data and Information\nManagement , 5(1):100\u2013109.\nCui, Limeng and Dongwon Lee. 2020. CoAID:\nCOVID-19 Healthcare Misinformation\nDataset.\nDai, Enyan, Yiwei Sun, and Suhang Wang.\n2020. Ginger Cannot Cure Cancer: Battling\nFake Health News with a Comprehensive\nData Repository.\nDing, Yasan, Bin Guo, Yan Liu, Yunji Liang,\nHaocheng Shen, and Zhiwen Yu. 2022.\nMetaDetector: Meta Event Knowledge\nTransfer for Fake News Detection. ACM\nTransactions on Intelligent Systems and\nTechnology .Flach, Peter and Meelis Kull. 2015.\nPrecision-Recall-Gain Curves: PR Analysis\nDone Right. In Advances in Neural\nInformation Processing Systems , volume 28,\nCurran Associates, Inc.\nGalassi, Andrea, Federico Ruggeri, Alberto\nBarr\u00f3n-Cede\u00f1o, Firoj Alam, Tommaso\nCaselli, Mucahid Kutlu, Julia Maria Stru\u00df,\nFrancesco Antici, Maram Hasanain, Juliane\nK\u00f6hler, Katerina Korre, Folkert Leistra,\nArianna Muti, Melanie Siegel,\nMehmet Deniz T\u00fcrkmen, Michael\nWiegand, and Wajdi Zaghouani. Notebook\nfor the CheckThat! Lab at CLEF 2023.\nGhanem, Bilal, Paolo Rosso, and Francisco\nRangel. 2020. An Emotional Analysis of\nFalse Information in Social Media and\nNews Articles. ACM Transactions on\nInternet Technology , 20(2):1\u201318.\nGolbeck, Jennifer, Matthew Mauriello, Brooke\nAuxier, Keval H. Bhanushali, Christopher\nBonk, Mohamed Amine Bouzaghrane,\nCody Buntain, Riya Chanduka, Paul\nCheakalos, Jennine B. Everett, Waleed\nFalak, Carl Gieringer, Jack Graney, Kelly M.\nHoffman, Lindsay Huth, Zhenya Ma,\nMayanka Jha, Misbah Khan, Varsha Kori,\nElo Lewis, George Mirano, William T.\nMohn IV , Sean Mussenden, Tammie M.\nNelson, Sean Mcwillie, Akshat Pant, Priya\nShetye, Rusha Shrestha, Alexandra\nSteinheimer, Aditya Subramanian, and\nGina Visnansky. 2018. Fake News vs Satire:\nA Dataset and Analysis. In Proceedings of\nthe 10th ACM Conference on Web Science ,\nWebSci \u201918, pages 17\u201321, Association for\nComputing Machinery, New York, NY,\nUSA.\nGrootendorst, Maarten. 2022. Bertopic:\nNeural topic modeling with a class-based\ntf-idf procedure. arXiv preprint\narXiv:2203.05794 .\nGruppi, Maur\u00edcio, Benjamin D. Horne, and\nSibel Adal\u0131. 2020. NELA-GT-2019: A Large\nMulti-Labelled News Dataset for The Study\nof Misinformation in News Articles.\nGruppi, Maur\u00edcio, Benjamin D. Horne, and\nSibel Adal\u0131. 2021. NELA-GT-2020: A Large\nMulti-Labelled News Dataset for The Study\nof Misinformation in News Articles.\nGruppi, Maur\u00edcio, Benjamin D. Horne, and\nSibel Adal\u0131. 2023. NELA-GT-2022: A Large\nMulti-Labelled News Dataset for The Study\nof Misinformation in News Articles.\nHe, Pengcheng, Jianfeng Gao, and Weizhu\nChen. 2023. DeBERTaV3: Improving\nDeBERTa using ELECTRA-Style\nPre-Training with Gradient-Disentangled\nEmbedding Sharing.\n48\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\nHoltzman, Ari, Peter West, Vered Shwartz,\nYejin Choi, and Luke Zettlemoyer. 2021.\nSurface Form Competition: Why the\nHighest Probability Answer Isn\u2019t Always\nRight. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language\nProcessing , pages 7038\u20137051, Association for\nComputational Linguistics, Online and\nPunta Cana, Dominican Republic.\nHorne, Benjamin D., William Dron, Sara\nKhedr, and Sibel Adali. 2018. Sampling the\nNews Producers: A Large News and\nFeature Data Set for the Study of the\nComplex Media Landscape.\nHorne, Benjamin D., Maur\u00edcio Gruppi, and\nSibel Adal\u0131. 2020. Do All Good Actors Look\nThe Same? Exploring News Veracity\nDetection Across The U.S. and The U.K.\nHorne, Benjamin D., Jeppe N\u00f8rregaard, and\nSibel Adali. 2020. Robust Fake News\nDetection Over Time and Attack. ACM\nTransactions on Intelligent Systems and\nTechnology , 11(1):1\u201323.\nHorner, Christy Galletta, Dennis Galletta,\nJennifer Crawford, and Abhijeet Shirsat.\n2021. Emotions: The Unexplored Fuel of\nFake News on Social Media. Journal of\nManagement Information Systems ,\n38(4):1039\u20131066.\nHoy, Nathaniel and Theodora Koulouri. 2022.\nExploring the Generalisability of Fake\nNews Detection Models. In 2022 IEEE\nInternational Conference on Big Data (Big\nData) , pages 5731\u20135740.\nHu, Beizhe, Qiang Sheng, Juan Cao,\nYongchun Zhu, Danding Wang, Zhengjia\nWang, and Zhiwei Jin. 2023. Learn over\nPast, Evolve for Future: Forecasting\nTemporal Trends for Fake News Detection.\nHuggingFace. 2021. Datasets: A community\nlibrary for natural language processing. In\nProceedings of the 2021 Conference on\nEmpirical Methods in Natural Language\nProcessing: System Demonstrations , pages\n175\u2013184, Association for Computational\nLinguistics, Online and Punta Cana,\nDominican Republic.\nJenkins, Thomas, Sameerah Talafha, and\nAdam Goodwin. 2023. An Ordered Sample\nConsensus (ORSAC) Method for Data\nCleaning Inspired by RANSAC: Identifying\nProbable Mislabeled Data.\nJeronimo, Caio Libanio Melo, Leandro Balby\nMarinho, Claudio E. C. Campelo, Adriano\nVeloso, and Allan Sales Da Costa Melo.\n2019. Fake News Classification Based on\nSubjective Language. In Proceedings of the\n21st International Conference on Information\nIntegration and Web-based Applications &Services , pages 15\u201324, ACM, Munich\nGermany.\nJin, Zhiwei, Juan Cao, Han Guo, Yongdong\nZhang, and Jiebo Luo. 2017. Multimodal\nFusion with Recurrent Neural Networks\nfor Rumor Detection on Microblogs. In\nProceedings of the 25th ACM International\nConference on Multimedia , MM \u201917, pages\n795\u2013816, Association for Computing\nMachinery, New York, NY, USA.\nKiesel, Johannes, Maria Mestre, Rishabh\nShukla, Emmanuel Vincent, Payam Adineh,\nDavid Corney, Benno Stein, and Martin\nPotthast. 2019. SemEval-2019 Task 4:\nHyperpartisan News Detection. In\nProceedings of the 13th International Workshop\non Semantic Evaluation , pages 829\u2013839,\nAssociation for Computational Linguistics,\nMinneapolis, Minnesota, USA.\nKingma, Diederik P . and Jimmy Ba. 2017.\nAdam: A Method for Stochastic\nOptimization.\nKochkina, Elena, Tamanna Hossain, Robert L.\nLogan, Miguel Arana-Catania, Rob Procter,\nArkaitz Zubiaga, Sameer Singh, Yulan He,\nand Maria Liakata. 2023. Evaluating the\ngeneralisability of neural rumour\nverification models. Information Processing\n& Management , 60(1):103116.\nKoivunen, Anu, Antti Kanner, Maciej Janicki,\nAuli Harju, Julius Hokkanen, and Eetu\nM\u00e4kel\u00e4. 2021. Emotive, evaluative,\nepistemic: A linguistic analysis of\naffectivity in news journalism. Journalism ,\n22(5):1190\u20131206.\nKuntur, Soveatin, Anna Wr\u00f3blewska, Marcin\nPaprzycki, and Maria Ganzha. 2024. Fake\nNews Detection: It\u2019s All in the Data!\nLampinen, Andrew, Ishita Dasgupta,\nStephanie Chan, Kory Mathewson,\nMh Tessler, Antonia Creswell, James\nMcClelland, Jane Wang, and Felix Hill.\n2022. Can language models learn from\nexplanations in context? In Findings 2022 ,\npages 537\u2013563, Association for\nComputational Linguistics, Abu Dhabi,\nUnited Arab Emirates.\nLee, Nayeon, Belinda Z. Li, Sinong Wang,\nPascale Fung, Hao Ma, Wen-tau Yih, and\nMadian Khabsa. 2021. On Unifying\nMisinformation Detection.\nLi, Yichuan, Bohan Jiang, Kai Shu, and Huan\nLiu. 2020. MM-COVID: A Multilingual and\nMultimodal Data Repository for\nCombating COVID-19 Disinformation.\nLi, Yupeng, Haorui He, Jin Bai, and Dacheng\nWen. 2024. MCFEND: A Multi-source\nBenchmark Dataset for Chinese Fake News\nDetection.\n49\nComputational Linguistics Under Review\nLi, Zehan, Xin Zhang, Yanzhao Zhang,\nDingkun Long, Pengjun Xie, and Meishan\nZhang. 2023. Towards General Text\nEmbeddings with Multi-stage Contrastive\nLearning.\nLin, Hongzhan, Jing Ma, Liangliang Chen,\nZhiwei Yang, Mingfei Cheng, and Chen\nGuang. 2022. Detect Rumors in Microblog\nPosts for Low-Resource Domains via\nAdversarial Contrastive Learning. In\nFindings of the Association for Computational\nLinguistics: NAACL 2022 , pages 2543\u20132556,\nAssociation for Computational Linguistics,\nSeattle, United States.\nLitterer, Benjamin, David Jurgens, and Dallas\nCard. 2023. When it Rains, it Pours:\nModeling Media Storms and the News\nEcosystem. In Findings of the Association for\nComputational Linguistics: EMNLP 2023 ,\npages 6346\u20136361, Association for\nComputational Linguistics, Singapore.\nLiu, Nelson F., Tianyi Zhang, and Percy Liang.\n2023. Evaluating Verifiability in Generative\nSearch Engines.\nLiu, Yinhan, Myle Ott, Naman Goyal, Jingfei\nDu, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin\nStoyanov. 2019. RoBERTa: A Robustly\nOptimized BERT Pretraining Approach.\nLiu, Zhiwei, Tianlin Zhang, Kailai Yang, Paul\nThompson, Zeping Yu, and Sophia\nAnaniadou. 2024. Emotion detection for\nmisinformation: A review. Information\nFusion , 107:102300.\nLlama Team, AI Meta. 2024. The Llama 3\nHerd of Models.\nLoshchilov, Ilya and Frank Hutter. 2019.\nDecoupled Weight Decay Regularization.\nL\u00fchring, Jula, Apeksha Shetty, Corinna\nKoschmieder, David Garcia, Annie\nWaldherr, and Hannah Metzler. 2024.\nEmotions in misinformation studies:\nDistinguishing affective state from\nemotional response and misinformation\nrecognition from acceptance. Cognitive\nResearch: Principles and Implications , 9(1):82.\nMa, Jing, Wei Gao, Prasenjit Mitra, Sejeong\nKwon, Bernard J Jansen, Kam-Fai Wong,\nand Meeyoung Cha. Detecting Rumors\nfrom Microblogs with Recurrent Neural\nNetworks.\nMartel, Cameron, Gordon Pennycook, and\nDavid G. Rand. 2020. Reliance on emotion\npromotes belief in fake news. Cognitive\nResearch: Principles and Implications , 5(1):47.\nMihalcea, Rada and Carlo Strapparava. 2009.\nThe lie detector: Explorations in the\nautomatic recognition of deceptive\nlanguage. In Proceedings of the ACL-IJCNLP2009 Conference Short Papers on -\nACL-IJCNLP \u201909 , page 309, Association for\nComputational Linguistics, Suntec,\nSingapore.\nMishra, Pushkar, Helen Yannakoudakis, and\nEkaterina Shutova. 2021. Modeling Users\nand Online Communities for Abuse\nDetection: A Position on Ethics and\nExplainability. In Findings of the Association\nfor Computational Linguistics: EMNLP 2021 ,\npages 3374\u20133385, Association for\nComputational Linguistics, Punta Cana,\nDominican Republic.\nMitra, Tanushree and Eric Gilbert. 2015.\nCREDBANK: A Large-Scale Social Media\nCorpus With Associated Credibility\nAnnotations. Proceedings of the International\nAAAI Conference on Web and Social Media ,\n9(1):258\u2013267.\nMontani, Ines, Matthew Honnibal, Matthew\nHonnibal, Adriane Boyd, Sofie Van\nLandeghem, and Henning Peters. 2023.\nExplosion/spaCy: V3.7.2: Fixes for APIs\nand requirements. Zenodo.\nMosallanezhad, Ahmadreza, Mansooreh\nKarami, Kai Shu, Michelle V . Mancenido,\nand Huan Liu. 2022. Domain Adaptive\nFake News Detection via Reinforcement\nLearning.\nM\u00fchleisen, Hannes and Mark Raasveldt. 2024.\nduckdb: DBI Package for the DuckDB Database\nManagement System . R package version\n1.0.0.9000,\nhttps://github.com/duckdb/duckdb-r.\nNakov, Preslav, Husrev Taha Sencar, Jisun An,\nand Haewoon Kwak. 2021. A Survey on\nPredicting the Factuality and the Bias of\nNews Media.\nNielsen, Dan Saattrup and Ryan McConville.\n2022. MuMiN: A Large-Scale Multilingual\nMultimodal Fact-Checked Misinformation\nSocial Network Dataset.\nNorregaard, Jeppe, Benjamin D. Horne, and\nSibel Adali. 2019. NELA-GT-2018: A Large\nMulti-Labelled News Dataset for The Study\nof Misinformation in News Articles.\nOshikawa, Ray, Jing Qian, and William Yang\nWang. 2020. A Survey on Natural\nLanguage Processing for Fake News\nDetection. In Proceedings of the Twelfth\nLanguage Resources and Evaluation\nConference , pages 6086\u20136093, European\nLanguage Resources Association, Marseille,\nFrance.\nOzcelik, Oguzhan, Arda Sarp Yenicesu, Onur\nYildirim, Dilruba Sultan Haliloglu,\nErdem Ege Eroglu, and Fazli Can. 2023.\nCross-Lingual Transfer Learning for\nMisinformation Detection: Investigating\n50\nVerhoeven, Mishra, Shutova Yesterday\u2019s News\nPerformance Across Multiple Languages.\nInProceedings of the 4th Conference on\nLanguage, Data and Knowledge , pages\n549\u2013558, NOVA CLUNL, Portugal, Vienna,\nAustria.\nPathak, Archita and Rohini Srihari. 2019.\nBREAKING! Presenting Fake News Corpus\nfor Automated Fact Checking. In\nProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics:\nStudent Research Workshop , pages 357\u2013362,\nAssociation for Computational Linguistics,\nFlorence, Italy.\nPelrine, Kellin, Jacob Danovitch, and\nReihaneh Rabbany. 2021. The Surprising\nPerformance of Simple Baselines for\nMisinformation Detection. In Proceedings of\nthe Web Conference 2021 , pages 3432\u20133441,\nACM, Ljubljana Slovenia.\nP\u00e9rez-Rosas, Ver\u00f3nica, Bennett Kleinberg,\nAlexandra Lefevre, and Rada Mihalcea.\n2018. Automatic Detection of Fake News.\nInProceedings of the 27th International\nConference on Computational Linguistics ,\npages 3391\u20133401, Association for\nComputational Linguistics, Santa Fe, New\nMexico, USA.\nPhillips, Samantha Cavanaugh, Sze Yuh Nina\nWang, Kathleen M. Carley, David Gertler\nRand, and Gordon Pennycook. 2024.\nEmotional language reduces belief in false\nclaims.\nPleiss, Geoff, Tianyi Zhang, Kilian Q\nWeinberger, and Ethan Elenberg. 2020.\nIdentifying Mislabeled Data using the Area\nUnder the Margin Ranking.\nPlutchik, ROBERT. 1980. Chapter 1 - A\nGENERAL PSYCHOEVOLUTIONARY\nTHEORY OF EMOTION. In Robert\nPlutchik and Henry Kellerman, editors,\nTheories of Emotion . Academic Press, pages\n3\u201333.\nPoldvere, Nele, Zia Uddin, and Aleena\nThomas. 2023. The PolitiFact-Oslo Corpus:\nA new dataset for fake news analysis and\ndetection. Information , 14(12).\nPotthast, Martin, Johannes Kiesel, Kevin\nReinartz, Janek Bevendorff, and Benno\nStein. 2017. A Stylometric Inquiry into\nHyperpartisan and Fake News.\nPratelli, Manuel and Marinella Petrocchi.\n2022. A Structured Analysis of Journalistic\nEvaluations for News Source Reliability.\nPratelli, Manuel, Fabio Saracco, and Marinella\nPetrocchi. 2024. Unveiling News\nPublishers Trustworthiness Through Social\nInteractions. In ACM Web Science\nConference , pages 139\u2013148, ACM, Stuttgart\nGermany.Rashkin, Hannah, Eunsol Choi, Jin Yea Jang,\nSvitlana Volkova, and Yejin Choi. 2017.\nTruth of Varying Shades: Analyzing\nLanguage in Fake News and Political\nFact-Checking. In Proceedings of the 2017\nConference on Empirical Methods in Natural\nLanguage Processing , pages 2931\u20132937,\nAssociation for Computational Linguistics,\nCopenhagen, Denmark.\nRaza, Shaina and Chen Ding. 2022. Fake news\ndetection based on news content and social\ncontexts: A transformer-based approach.\nInternational Journal of Data Science and\nAnalytics , 13(4):335\u2013362.\nRisdal. 2016. Getting Real about Fake News.\nSeabold, Skipper and Josef Perktold. 2010.\nstatsmodels: Econometric and statistical\nmodeling with python. In 9th Python in\nScience Conference .\nShahi, Gautam Kishore and Durgesh Nandini.\n2020. FakeCovid \u2013 A Multilingual\nCross-domain Fact Check News Dataset for\nCOVID-19.\nShokri, Mohammad, Vivek Sharma, Elena\nFilatova, Shweta Jain, and Sarah Levitan.\n2024. Subjectivity Detection in English\nNews using Large Language Models. In\nProceedings of the 14th Workshop on\nComputational Approaches to Subjectivity,\nSentiment, & Social Media Analysis , pages\n215\u2013226, Association for Computational\nLinguistics, Bangkok, Thailand.\nShu, Kai, Deepak Mahudeswaran, Suhang\nWang, Dongwon Lee, and Huan Liu. 2019.\nFakeNewsNet: A Data Repository with\nNews Content, Social Context and\nSpatialtemporal Information for Studying\nFake News on Social Media.\nShu, Kai, Guoqing Zheng, Yichuan Li,\nSubhabrata Mukherjee, Ahmed Hassan\nAwadallah, Scott Ruston, and Huan Liu.\n2020. Leveraging Multi-Source Weak Social\nSupervision for Early Detection of Fake\nNews.\nStahl, Peter M. 2024. Pemistahl/lingua-py.\nStepanova, Nataliya and Bj\u00f6rn Ross. 2023.\nTemporal Generalizability in Multimodal\nMisinformation Detection. In Proceedings of\nthe 1st GenBench Workshop on\n(Benchmarking) Generalisation in NLP , pages\n76\u201388, Association for Computational\nLinguistics, Singapore.\nStru\u00df, Julia Maria, Federico Ruggeri, Dimitar\nDimitrov, Andrea Galassi, Georgi Pachov,\nIvan Koychev, Preslav Nakov, Melanie\nSiegel, Michael Wiegand, Maram Hasanain,\nReem Suwaileh, and Wajdi Zaghouani.\nNotebook for the CheckThat! Lab at CLEF\n2024.\n51\nComputational Linguistics Under Review\nSwayamdipta, Swabha, Roy Schwartz,\nNicholas Lourie, Yizhong Wang, Hannaneh\nHajishirzi, Noah A. Smith, and Yejin Choi.\n2020. Dataset Cartography: Mapping and\nDiagnosing Datasets with Training\nDynamics. In Proceedings of the 2020\nConference on Empirical Methods in Natural\nLanguage Processing (EMNLP) , pages\n9275\u20139293, Association for Computational\nLinguistics, Online.\nSzwoch, Joanna, Mateusz Staszkow, Rafal\nRzepka, and Kenji Araki. 2024. Limitations\nof Large Language Models in Propaganda\nDetection Task. Applied Sciences ,\n14(10):4330.\nTacchini, Eugenio, Gabriele Ballarin, Marco\nL. Della Vedova, Stefano Moret, and Luca\nde Alfaro. 2017. Some Like it Hoax:\nAutomated Fake News Detection in Social\nNetworks.\nVerhoeven, Ivo, Pushkar Mishra, Rahel\nBeloch, Helen Yannakoudakis, and\nEkaterina Shutova. 2024. A (More) Realistic\nEvaluation Setup for Generalisation of\nCommunity Models on Malicious Content\nDetection.\nWang, William Yang. 2017. \"Liar, Liar Pants\non Fire\": A New Benchmark Dataset for\nFake News Detection. In Proceedings of the\n55th Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short\nPapers) , pages 422\u2013426, Association for\nComputational Linguistics, Vancouver,\nCanada.\nWang, Yaqing, Weifeng Yang, Fenglong Ma,\nJin Xu, Bin Zhong, Qiang Deng, and Jing\nGao. 2020. Weak Supervision for Fake\nNews Detection via Reinforcement\nLearning.\nWu, Jiaying and Bryan Hooi. 2022. Probing\nSpurious Correlations in Popular\nEvent-Based Rumor Detection Benchmarks.\nWu, Liang, Fred Morstatter, Kathleen M.\nCarley, and Huan Liu. 2019.\nMisinformation in Social Media: Definition,\nManipulation, and Detection. ACM\nSIGKDD Explorations Newsletter ,\n21(2):80\u201390.\nXiao, Madelyne and Jonathan Mayer. 2024.\nThe Challenges of Machine Learning for\nTrust and Safety: A Case Study on\nMisinformation Detection.\nYang, Kai-Cheng and Filippo Menczer. 2024.\nAccuracy and Political Bias of News Source\nCredibility Ratings by Large Language\nModels.\nYue, Zhenrui, Huimin Zeng, Ziyi Kou, Lanyu\nShang, and Dong Wang. 2022. Contrastive\nDomain Adaptation for EarlyMisinformation Detection: A Case Study\non COVID-19. In Proceedings of the 31st\nACM International Conference on Information\n& Knowledge Management , pages 2423\u20132433,\nACM, Atlanta GA USA.\nZhang, Qiang, Hongbin Huang, Shangsong\nLiang, Zaiqiao Meng, and Emine Yilmaz.\n2021a. Learning to Detect\nFew-Shot-Few-Clue Misinformation.\nZhang, Xueyao, Juan Cao, Xirong Li, Qiang\nSheng, Lei Zhong, and Kai Shu. 2021b.\nMining Dual Emotion for Fake News\nDetection. In Proceedings of the Web\nConference 2021 , pages 3465\u20133476, ACM,\nLjubljana Slovenia.\nZhou, Xiang, Heba Elfardy, Christos\nChristodoulopoulos, Thomas Butler, and\nMohit Bansal. 2021. Hidden Biases in\nUnreliable News Detection Datasets. In\nProceedings of the 16th Conference of the\nEuropean Chapter of the Association for\nComputational Linguistics: Main Volume ,\npages 2482\u20132492, Association for\nComputational Linguistics, Online.\nZhou, Xinyi and Reza Zafarani. 2020. A\nSurvey of Fake News: Fundamental\nTheories, Detection Methods, and\nOpportunities. ACM Computing Surveys ,\n53(5):1\u201340.\nZubiaga, Arkaitz, Maria Liakata, and Rob\nProcter. 2017. Exploiting Context for\nRumour Detection in Social Media. In\nSocial Informatics , pages 109\u2013123, Springer\nInternational Publishing, Cham.\n52", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Yesterday's News: Benchmarking Multi-Dimensional Out-of-Distribution Generalization of Misinformation Detection Models", "author": ["I Verhoeven", "P Mishra", "E Shutova"], "pub_year": "2024", "venue": "arXiv preprint arXiv:2410.18122", "abstract": "This article introduces misinfo-general, a benchmark dataset for evaluating misinformation  models' ability to perform out-of-distribution generalization. Misinformation changes rapidly,"}, "filled": false, "gsrank": 321, "pub_url": "https://arxiv.org/abs/2410.18122", "author_id": ["khMedPwAAAAJ", "bVcZ1qkAAAAJ", "jqOFBGoAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:Ri9kNRU9B2IJ:scholar.google.com/&output=cite&scirp=320&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D320%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=Ri9kNRU9B2IJ&ei=PrWsaIaUD7TWieoP1pCJ2AY&json=", "num_citations": 1, "citedby_url": "/scholar?cites=7063681701853278022&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:Ri9kNRU9B2IJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2410.18122"}}, {"title": "Predicting the factuality of reporting of news media using observations about user attention in their YouTube channels", "year": "2021", "pdf_data": "Predicting the Factuality of Reporting of News Media\nUsing Observations About User Attention in Their YouTube Channels\nKrasimira Bozhanova\nFMI, So\ufb01a University\n\u201cSt. Kliment Ohridski\u201d\nSo\ufb01a, Bulgaria\nkrasimira.bozhanova@gmail.comYoan Dinkov\nFMI, So\ufb01a University\n\u201cSt. Kliment Ohridski\u201d\nSo\ufb01a, Bulgaria\nyoandinkov@gmail.comIvan Koychev\nFMI and GATE, So\ufb01a University\n\u201cSt. Kliment Ohridski\u201d\nSo\ufb01a, Bulgaria\nkoychev@fmi.uni-sofia.bg\nMaria Castaldo\nUniv. Grenoble Alpes,\nCNRS, Inria, Grenoble INP,\nGIPSA-lab, F-38000, France\nmaria.castaldo@grenoble-inp.frTommaso Venturini\nUniv. Grenoble Alpes,\nCNRS, Inria, Grenoble INP,\nGIPSA-lab, F-38000, France\ntommaso.venturini@cnrs.frPreslav Nakov\nQatar Computing Research Institute\nHBKU\nDoha, Qatar\npnakov@hbku.edu.qa\nAbstract\nWe propose a novel framework for predict-\ning the factuality of reporting of news me-\ndia outlets by studying the user attention cy-\ncles in their YouTube channels. In particu-\nlar, we design a rich set of features derived\nfrom the temporal evolution of the number\nof views, likes, dislikes, and comments for a\nvideo, which we then aggregate to the channel\nlevel. We develop and release a dataset for the\ntask, containing observations of user attention\non YouTube channels for 489 news media. Our\nexperiments demonstrate both complementar-\nity and sizable improvements over state-of-the-\nart textual representations.\n1 Introduction\nDisinformation in the news and in social media\nis perceived as having a major impact on society,\ne.g., during the 2016 US Presidential election (Grin-\nberg et al., 2019) and the Brexit referendum (Gor-\nrell et al., 2018). During the COVID-19 pandemic\noutbreak, the moral panic (McLuhan, 1964) around\nonline disinformation grew to a whole new level\nasthe \ufb01rst global infodemic .1Nowadays, \ufb01ghting\ndisinformation online has been recognized as one\nof the most important issues societies around the\nworld are facing today.\nIn this paper, we highlight an aspect of disin-\nformation that is often neglected. Rather than ex-\namining the truth-value of individual piece of in-\nformation, we investigate the general quality of\nthe attention regimes in different news outlets, by\nanalyzing news media\u2019s YouTube channels.\n1MIT Technology Review: tinyurl.com/y8oschngYouTube is the largest and the most popular plat-\nform for video sharing with over two billion users\nand it is also the second most widely used news\nsource in USA, after Facebook.2While the plat-\nform has been scrutinized for the way in which\nit may amplify marginal and sometimes radical\ncontents (Munn, 2019; Ribeiro et al., 2020), the\nconnection between attention dynamics and disin-\nformation levels is still largely unexplored.\nHere, we do not focus on speci\ufb01c videos, but\nrather on entire YouTube channels of news media\nand their attention dynamics. In particular, we are\ninterested in differentiating the \u201cattention cycles\u201d\n(Downs, 1972; Leskovec et al., 2009) of YouTube\nchannels, that is to assess the rapidity and the steep-\nness with which their videos rise and fall in the\nconsideration of their audiences. While some out-\nlets encourage extensive and diverse discussions,\nother tend to concentrate everyone\u2019s attention on\nthe latest hot-button, thus distracting the public\nopinion instead of nourishing it (Venturini, 2019).\nOur contributions are the following:\n\u2022We propose to model the factuality of news\nmedia based on the user attention cycles in\ntheir respective YouTube channels.\n\u2022We release a specialized dataset for the task.3\n\u2022We show experimentally that considering\nattention cycles yields considerable perfor-\nmance gains on top of text representations\nfor predicting the factuality of news media.\n2http://tinyurl.com/y4apu58j\n3http://github.com/krasimira-bozhanova/youtube-\nattention-cycles-datasetarXiv:2108.12519v1  [cs.CL]  27 Aug 2021\nThe paper is organized as follows: Section 2\npresents related work. Section 3 describes our\ndataset. Section 4 discusses our methodology. Sec-\ntion 5 presents the experiments and results. Sec-\ntion 6 offers analysis and discussion. Section 7\nconcludes and points to directions for future work.\n2 Related Work\nSigni\ufb01cant efforts have been dedicated in the last\nyears to automating the detection of disinforma-\ntion(which is commonly referred to as fake news ),\nwhich we look at more closely in this section. At\nthe end, we mention previous attempts to use data\nfrom YouTube for media classi\ufb01cation tasks.\nAnalysis of the Content Many approaches have\nbeen proposed to analyze both the style and the con-\ntent of fake news. Using natural language process-\ning techniques, Horne and Adal\u0131 (2017) pointed\nout how fake news can be characterized by stylis-\ntic features such as the overuse of proper nouns,\npunctuation, capital letters, negation terms, and rep-\netitions in the text (Rubin et al., 2016). Fake news\nhas also been associated with intensity of sentiment\nand emotions, compared to mainstream news (Gi-\nachanou et al., 2019). Here, we also use textual\nrepresentations, but ( i) we focus mainly on analyz-\ning the user attention cycles in YouTube channels,\nand ( ii) we aim at categorizing entire news media\noutlets rather than individual pieces of news.\nAnalysis of the Response Many researchers\nused the user reactions in social media platforms\nto identify disinformation, e.g., the content and the\nnumber of replies to a piece of news, or the propa-\ngation of the content in the network. For instance,\nZhao et al. (2015) classi\ufb01ed disputed claims based\non their comments and reactions, assuming that, if\na claim is not true, at least some replies would ques-\ntion its factuality. Indeed, later studies (Ruchansky\net al., 2017; Nguyen et al., 2020) have shown that\nuser response features are quite important. Here,\nwe also focus on inspecting the user response and\nits potential link to disinformation, but we use user\nattention cycles in YouTube.\nAnalysis of Entire News Media Outlets Look-\ning at a news media outlet as a source of low-quality\ncontent is another way to approach the problem. In\nthese methods, features modelling the overall trust-\nworthiness of the source are used, such as ( i) Does\nthe news media outlet use veri\ufb01ed accounts on es-\ntablished platforms, such as Wikipedia and Twitter?(ii) If it does, do these accounts have a proper\ndescription, location, website references, etc?\n(iii) How does the URL of the media\u2019s website\nlook like? ( iv) Does the medium express political\nbias or sentiment? Baly et al. (2018) and Baly et al.\n(2020) used features motivated by these questions\nto achieve better results in combination with con-\ntent features and user pro\ufb01les in social media. In\nour work, we also aim at classifying entire news\nmedia outlets, but we do so using user attention\ncycles in YouTube along with text.\nUsing Temporal Attention Data for Disinforma-\ntion Detection We focus on the analysis of tem-\nporal patterns associated with news outlets of differ-\nent type and quality. Previous studies (Ruchansky\net al., 2017; Nguyen et al., 2020) have suggested\nthat a combination of temporal, content-based, and\nuser-based features is promising for disinforma-\ntion detection. As the dynamics of the viral spread\nis often associated with successful junk news, we\nlook at studies focusing on modelling virality, such\nas (Hoang et al., 2011) for tweets. Similar fea-\ntures are well-suited for our task, as described in\nSection 4.1.2, but ( i) we model the user behavior\ndifferently, and ( ii) we focus on data collected from\nthe YouTube channels of the target news media.\nUsing YouTube Data for Classi\ufb01cation The\nYouTube platform contains information that is still\nunderexplored for the purposes of disinformation\ndetection. Dinkov et al. (2019) looked into detect-\ning the left-centre-right political bias of YouTube\nchannels. Baly et al. (2020) included features from\nthe news source\u2019s YouTube channel, derived from\nboth sound and user pro\ufb01les. The above work uses\nraw statistics about the number of views, likes, dis-\nlikes, and comments per video. We, instead, use\nmuch richer temporality features in combination\nwith the textual representation of the videos.\n3 Data\nWe started from a corpus of news media outlets,\nwhose reliability has been evaluated by Media\nBias/Fact Check4(MBFC). Lead by a team of in-\ndependent journalists and researchers, MBFC has\nanalyzed close to 4,000 news outlets over the past\nsix years. For each news outlet, they provide a de-\ntailed analysis summarized by a \u2018factuality\u2019 score\nchosen among: Very High ,High ,Mostly Factual ,\nMixed ,Low, and Very Low .\n4http://mediabiasfactcheck.com\nFactuality Channels Videos\n# % # %\nHigh 308 63.0 22,932 61.7\nMixed 153 31.3 12,125 32.7\nLow 28 5.7 2,091 5.6\nTotal 489 100.0 37,148 100.0\nTable 1: Statistics about the dataset, showing the distri-\nbution of the channels and of the videos for each level\nof factuality of reporting.\nWe searched the YouTube channels of news out-\nlets in the MBFC corpus and we monitored all\nthe videos they published from February\u20192020 to\nAugust\u20192020. Using the YouTube Data API,5we\ncollected the number of views, likes, dislikes and\ncomments collected during the \ufb01rst seven days af-\nter the publication of each video. We also stored\nits title and its description.\nWe observed that the percentage of media chan-\nnels labelled with the Very High and the Very Low\ncategories was 3.1% and 1%, respectively. We thus\nmerged Very High with High ;Mostly Factual with\nMixed ; and Very Low with Low, ending up with a\n3-way labelling: High ,Mixed , and Low.\nFinally, for the sake of data balancing, we ex-\ncluded the channels with fewer than 20 videos, and\nwe capped the most proli\ufb01c channels at the newest\n100 videos. The \ufb01nal distribution of channels and\ntheir corresponding videos for each level of factu-\nality is shown in Table 1.\n4 Method\nOur system is composed of two main components\nfocusing on ( i) data preparation, and on ( ii) sequen-\ntial classi\ufb01cation, respectively. Below, we describe\nthe representation we use for video-level and also\nfor channel-level classi\ufb01cation.\n4.1 Representation\nThe data preparation component transforms the\nYouTube source data and produces representations\n(or features) for our model. We generate represen-\ntations both for the textual content of the videos\nand for the user attention data, for which we in-\ntroduce a number of novel features, presented in\nSection 4.1.2.\n5http://developers.google.com/youtube/v3/docs/videos4.1.1 Textual Features\nWe gathered the title and the description of each\nvideo. We extracted Sentence BERT embeddings\n(768 features) for each title and description. These\nembeddings are derived from a modi\ufb01cation of\nthe pretrained BERT model, which yields semanti-\ncally meaningful sentence embeddings of size 768,\nwhich are trained to be readily comparable using\ncosine similarity (Reimers and Gurevych, 2019).\n4.1.2 Attention Features\nWe hypothesize that the attention received over\ntime by the videos in a YouTube channel can be\nused to predict the quality of its contribution to\nthe online public debate, as captured (albeit impre-\ncisely) by the factuality score assigned by MBFC.\nWe generate a set of features that model the user\nattention cycles by looking at the temporal varia-\ntion of the number of user actions (Views, Likes,\nDislikes, and Comments) in the \ufb01rst week after a\nvideo has been published. We aim to model the\nfollowing:\n\u2022How are user actions distributed hourly/daily?\n\u2022How much are the user actions concentrated\nin peak hours?\n\u2022At what moment in time does the peak hour\nfor each user action type occur?\n\u2022How steep is the time series in terms of the\ndistribution of hourly user actions?\nWe de\ufb01ne UAdias the total number of user ac-\ntions that occurred by the end of day i. In our case,\niranges inf1;2; : : : ; 7g. Similarly, UAhjis the\nnumber of user actions occurring by the end of\nhour jafter the publication of the video. We gener-\nate a set of attention features for each user action,\nwhich we group into the following categories:\n1.User actions daily percentage (Ddi), or the\nfraction of user actions out of the total that\noccurred on day i, where 1\u0014i\u00147, and\nUAd0= 0(7 features per user action):\nDdi=UAdi\u0000UAdi\u00001\nUAd7\n2.User actions daily cumulative percentage\n(DCdi), or the fraction of user actions out of\nthe total by the end of day i, where 1\u0014i\u00147\n(7 features per user action):\nDCdi=UAdi\nUAd7\n3.User actions daily increase (DIdi), or the\nproportion of increase in the number of user\nactions on day icompared to day i\u00001, where\n2\u0014i\u00147(6 features per user action):\nDIdi=UAdi\u0000UAdi\u00001\nUAdi\u00001\n4.User actions hourly increase (HIhj), or the\nproportion of increase in the number of user\nactions during hour jcompared to those dur-\ning hour j\u00001, where 2\u0014j\u0014168(167\nfeatures per user action):\nHIhj=UAhj\u0000UAhj\u00001\nUAhj\u00001\n5.User actions average hourly increase per\nday(AHI di), or the average hourly increase\nin the number of user actions on day i, where\n1\u0014i\u00147(7 features per user action):\nAHI di=Pn=i\u000324\nj=(i\u00001)\u000324+1HIhj\n24\n6.User actions majority interval length\n(MIT), or the number of hours containing\nthe majority of the user actions\nMIT= min\n1\u0014i<j\u0014168\u001a\nj\u0000i\f\f\f\fUAhj\u0000UAhi\nUAh168\u0015T\u001b\nwhere T(one off0.5, 0.7, 0.9g) is the major-\nityshare (3 features per user action).\n7.User actions peak delay interval (PDI ),\nor the number of hours leading to the hour\nwith the highest concentration of user actions\n(1 feature per user action):\nPDI = argmax\nfij2\u0014i\u0014168gUAhi\u0000UAhi\u00001\n8.User actions alive interval length (AI), or\nthe hour up to which user actions were\nrecorded (1 feature per user action):\nAI= min\n1\u0014p\u0014167(pjUAhi\u0000UAhi\u00001= 0;\n8i2fp+ 1; : : : ; 168g)\n9.User actions peak share (PS), or the num-\nber of user actions during the peak hour di-\nvided by the total (1 feature per user action):\nPS= max\n2\u0014i\u0014168\u001aUAhi\u0000UAhi\u00001\nUAh168\u001b\nFigure 1: First day breakdown into periods\nMost of the attention received by the videos in\nour corpus is concentrated in the \ufb01rst day after a\nvideo has been published, and thus we monitor the\nattention during this period more closely. Besides\ndaily andhourly , we look at six additional periods\nduring the \ufb01rst day, as depicted on Figure 1. We ex-\ntract the following features: ( i)Percentage of User\nActions per Period , (ii)User Actions per Period\nIncrease , and ( iii)User Actions Average Hourly\nIncrease for a Period . This yields 18 additional\nfeatures and a total of 218 features per user action.\nTo model the opinion of the users regarding the\nvideos, we also use features derived by ratios of\ndifferent user action types:\n\u2022Positive Reactions : the ratio between the num-\nber of likes and the number of views;\n\u2022Negative Reactions : the ratio between the\nnumber of dislikes and the number of views;\n\u2022Engagement : the ratio between the number of\ncomments and the number of views;\n\u2022Controversiality : the ratio between the num-\nber of likes and the sum of the number of likes\nand of dislikes.\nFor each of these ratios, we calculate a set of\nfeatures that show how the numbers change daily,\nsimilarly to the User Actions Daily Percentages\n(7 features per ratio) and the User Actions Daily\nCumulative Percentages (6 features per ratio) fea-\ntures. We further generate ratio features for the\nmore granular \ufb01rst day periods (6 features per ra-\ntio), which yields a total of 19 ratio-driven features.\nOverall, we have 952 attention features per video.\n4.2 Models\nOur architecture contains two consecutive clas-\nsi\ufb01cations: ( i) for YouTube videos, and ( ii) for\nYouTube channels. As we want to make use of\nthe features derived from the YouTube videos, we\nlabelled each video with the factuality score of the\nchannel that published it, using distant supervision.\nThus, the video classi\ufb01cation learns to predict\nthe factuality labels that are projected from the\ncorresponding channels. Naturally, not all videos\npublished by a low-factuality channel necessarily\ncontain disinformation. Yet, this is not a problem\nsince we do not aim at classifying correctly indi-\nvidual videos, but at detecting factuality-related\npatterns, which would then be used at the channel\nlevel: our channel classi\ufb01er uses the predictions of\nthe video-level classi\ufb01er to predict the factuality of\nchannels.\n4.2.1 Video Modelling\nFor each video, we have 768 features from the\nsentence-level BERT representation. We calculated\nthese features once for the title and once for the\ndescription of the video, obtaining a total of 1,536\ntextual features.\nWe further have 952 attention-driven features per\nvideo. To validate the relevance of these features\nwith respect to our classi\ufb01cation task, we apply a\nset of feature selection methods over the training\nsplit of our dataset, namely ANOV A, Pearson cor-\nrelation, and Spearman correlation. According to\nthese methods, the ratio features turn out to be the\nmost relevant ones. We selected the best 100 fea-\ntures from each method or 124 attention video-level\nfeatures, which we used in our classi\ufb01cation exper-\niments. Combined with the 1,536 textual features,\nthis yielded a total of 1,660 features per video.\n4.2.2 Channel Modelling\nFor the second classi\ufb01er, at the channel level, we\ngenerate the following groups of features:\n1. YouTube statistics (total of 13 features):\n\u2022Popularity (7 features): number of sub-\nscribers, average number of hourly, daily,\nand weekly views and comments;\n\u2022Activity (5 features): number of videos,\naverage number of videos published\nhourly, daily, and weekly, and average\nnumber of videos per channel subscriber;\n\u2022Attention concentration (1 feature): Gini\nindex measuring the concentration of\nvideo views within a channel.\n2.Averaged videos features (1,660) features):\naverage values of the features for the videos\npublished by the channel. For each video\nfeature , we have a corresponding aggregated\nchannel feature .3.Aggregated video-level classi\ufb01er predictions\nfeatures (9 features): for each channel, we\naggregate the predictions of the video-level\nclassi\ufb01er for the videos in that channel. We\nuse three types of aggregations:\n\u2022maximum probability across the videos\nfor each factuality label;\n\u2022average probability across the videos for\neach factuality label;\n\u2022factuality distributions percents: for each\nfactuality, this is the percent of videos\npredicted to have that factuality.\n5 Experiments and Evaluation\nWe train two subsequent classi\ufb01ers for factuality\nprediction: for videos and for channels. We con-\nduct experiments with different models and we\ncompare them to a majority-class baseline. We\nevaluate the models in terms of accuracy, balanced\naccuracy, and mean absolute error (MAE). MAE is\na more relevant measure in our case as it takes into\naccount the ordering of the labels: confusing high\nfactuality with mixed factuality is a smaller error\nthan confusing it with low factuality.\nWhile our ultimate goal is to classify channels,\nwe start with video classi\ufb01cation, then we aggre-\ngate the predictions, and we use them to make\npredictions at the channel level.\nWe divide the dataset into training, develop-\nment, and test split at the channel level. Then,\nfor the video-level experiments, we use for train-\ning/development/testing the videos for the respec-\ntive channels. Note that this guarantees that all\nvideos for a given channel go into the same split.\n5.1 Video-Level Classi\ufb01cation\nBelow, we report results using Gradient Boosted\nDecision Trees (GBDT). We also experimented\nwith logistic regression, ordinal logistic regression,\nand SVM with various kernels, but they performed\nworse. We trained separate models (a) using the\ntextual representation, and (b) using the user atten-\ntion cycles. Table 2 shows the evaluation results.\n# Experiment Acc. Bal. Acc. MAE\n0 Baseline 61.72 33.33 0.4391\n1 BERT 67.54 45.89 0.3692\n2 User attention 64.93 55.12 0.3979\nTable 2: Video-level experiments with GBDT.\nGroup # Experiment Dim. Acc. MAE\nBaseline 0 Majority class - 63.08 0.4308\nText1 BERT averaged 1,536 73.85 0.3077\n2 BERT aggregated predictions 9 75.38 0.3077\n3 BERT all 1,545 73.85 0.3538\nUser Attention4 User attention averaged 124 75.38 0.2769\n5 User attention channel statistics 13 63.08 0.4000\n6 User attention aggregated predictions 9 67.69 0.3846\n7 User attention all 146 70.77 0.3231\nEnsemble 8 BERT all + User attention averaged 1,669 76.92 0.2615\nTable 3: Channel-level experiments.\nNote that our datasets are not well-balanced and\nhave very few examples of low-factuality videos\nand channels. To mitigate this, we apply over-\nsampling using SMOTE (Chawla et al., 2002),\nwhich generates additional synthetic examples.\nMoreover, it is important that the video classi\ufb01er\ngenerates predictions for the low-factuality and the\nmixed-factuality classes; otherwise, the predictions\nfor these classes could be lost when aggregating for\nthe channel classi\ufb01cation. Thus, we also report bal-\nanced accuracy , as it is important when choosing\nwhich video experiments to select for aggregation.\n5.2 Channel-Level Classi\ufb01cation\nWe experimented with several approaches for chan-\nnel classi\ufb01cation:\n\u2022using aggregated video-level features to ob-\ntain channel-level representation;\n\u2022using the posterior probabilities of video-level\nclassi\ufb01ers;\n\u2022 using the previous two together;\n\u2022ensemble of different channel-level classi\ufb01ers.\nFor the ensemble aggregation, we experimented\nwith three methods for choosing the most likely\nclass for a given channel:\n\u2022after averaging the predictions from the vari-\nous models (mean);\n\u2022after getting the maximum probability predic-\ntion from the various models (max);\n\u2022after getting the minimum probability predic-\ntion from the various models (min) \u2013 tells us\nwhich class is least likely to be wrong.The results are shown in Table 3. All experi-\nments use GBDT, except for experiment 8, which\nuses ordinal logistic regression.\n6 Discussion\nBelow, we analyze the results and we perform an\nablation study.\n6.1 Result Analysis\nWe can see in Table 3 that all models improve over\nthe majority class baseline by a sizable margin. We\nfurther see that using average information on user\nattention cycles (line 4) performs better than using\ntextual features (lines 1\u20133). Moreover, combining\nthe two yields the best result (line 8). The other two\nsets of user attention features: channel statistics\nand aggregated video-classi\ufb01er predictions, do not\ncontribute to the combined user attentions model\n(compare line 4 to line 6).\nThe relative improvements over the majority\nclass baseline in terms of accuracy are generally\nsmaller than those for MAE, which can be ex-\nplained by class imbalance. To improve accuracy,\nthe models need to learn to assign mixed andlow\nfactuality labels properly (as the majority class is\nhigh factuality). Most of the trained models un-\ndervalue the unrepresented classes. If some of the\nbalancing techniques are applied, the models rec-\nognize better the lowand the mixed examples, but\nat the cost of false positives for these classes from\nthehigh-factuality examples, which decreases the\noverall accuracy. In contrast, MAE rewards mod-\nels that can improve the small classes even given\nthe risk of introducing some errors for the majority\nclass.\nFeature Group Acc. Bal. Acc. MAE\nBaseline 63.08 33.33 0.4308\nViews (V) 66.15 41.79 0.3692\nDislikes (D) 64.62 39.27 0.4000\nComments (C) 56.92 32.64 0.4615\nLikes (L) 64.62 37.56 0.3846\nV + L + C 67.69 49.27 0.3385\nV + L + D + C 69.23 44.27 0.3385\nEngagement 63.08 44.27 0.4000\nControversiality 70.77 48.33 0.3231\nPositive reactions 70.77 48.33 0.3231\nContr + Eng 63.08 45.12 0.4000\nContr + Pos 67.69 46.71 0.3538\nPos + Eng 64.62 46.79 0.3846\nChannel statistics 63.08 39.31 0.400\nAggregated 67.69 60.20 0.3846\nAll 70.77 56.02 0.3231\nTable 4: Ablation study (channel-level classi\ufb01cation)\nusing various user attention features.\n6.2 Ablation Study\nAs our focus is on attention cycles, we performed\nan ablation study for these features against the com-\nbined user attention channel model. The results\nare shown in Table 4. We can see that ratio fea-\ntures such as controversiality andpositive reactions\nalone yield the best accuracy and MAE. Using the\npredictions of the video-level classi\ufb01er as features\nyields the best balanced accuracy. This con\ufb01rms\nthe importance of having accurate low- and mixed -\nfactuality predictions for the video classi\ufb01er prior\nto the aggregation. Finally, the overall best results,\nwhen considering all measures, are achieved when\ncombining all features.\n7 Conclusion and Future Work\nWe proposed a novel framework for predicting the\nfactuality of reporting of news outlets by study-\ning the user attention cycles in their respective\nYouTube channels. We further designed a rich set\nof features derived from the temporal evolution\nof the number of views, likes, dislikes, and com-\nments for a video, which we then aggregated at the\nchannel level. Our experiments demonstrated both\ncomplementarity and sizable improvements over\nstate-of-the-art textual representations.We further developed and released a dataset\ncontaining observations about user attention on\nYouTube channels for 489 news media. We hope\nthat this will enable future research on using data\nfrom video sharing platforms.\nIn future work, we plan to improve the class im-\nbalance of the dataset by extending it with more\nexamples. We further want to integrate additional\nfeatures based on the comments for the videos\nand other information sources such as Twitter and\nWikipedia. Finally, we plan to study the utility of\nuser attention cycles for other related tasks such as\npolitical ideology detection for news media.\nEthics and Broader Impact\nData Collection Our dataset was collected from\nYouTube, using their public API.\nUser Privacy Our dataset contains aggregated\nattention statistics without any user data.\nBiases Any biases found in the dataset are unin-\ntentional, and we do not intend to do harm to any\ngroup or individual.\nIntended Use and Misuse Potential Our\ndataset and the proposed model can enable the\ndevelopment of systems for automatic detection\nof reliable/unreliable YouTube channels, which\ncould support media literacy, as well as analysis\nand decision making for the public good. However,\nthey could also be misused by malicious actors.\nEnvironmental Impact. Finally, we would also\nlike to warn that the use of large-scale Transform-\ners requires a lot of computations and the use\nof GPUs/TPUs for training, which contributes to\nglobal warming (Strubell et al., 2019).\nAcknowledgments\nThis research is part of the Tanbih mega-project,6\ndeveloped at the Qatar Computing Research In-\nstitute, HBKU, which aims to limit the impact of\n\u201cfake news\u201d, propaganda, and media bias by making\nusers aware of what they are reading, thus promot-\ning media literacy and critical thinking.\nThis research is also partially supported\nby Project UNITe BG05M2OP001-1.001-0004\nfunded by the OP \u201cScience and Education for\nSmart Growth\u201d and co-funded by the EU through\nthe ESI Funds.\n6http://tanbih.qcri.org\nReferences\nRamy Baly, Georgi Karadzhov, Dimitar Alexandrov,\nJames Glass, and Preslav Nakov. 2018. Predict-\ning factuality of reporting and bias of news media\nsources. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing ,\nEMNLP \u201918, pages 3528\u20133539, Brussels, Belgium.\nRamy Baly, Georgi Karadzhov, Jisun An, Haewoon\nKwak, Yoan Dinkov, Ahmed Ali, James Glass, and\nPreslav Nakov. 2020. What was written vs. who\nread it: News media pro\ufb01ling using text analysis and\nsocial media context. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics , ACL \u201920, pages 3364\u20133374.\nNitesh V . Chawla, Kevin W. Bowyer, Lawrence O. Hall,\nand W. Philip Kegelmeyer. 2002. SMOTE: synthetic\nminority over-sampling technique. J. Artif. Intell.\nRes., 16:321\u2013357.\nYoan Dinkov, Ahmed Ali, Ivan Koychev, and Preslav\nNakov. 2019. Predicting the leading political ide-\nology of YouTube channels using acoustic, textual,\nand metadata information. In Proceedings of the\n20th Annual Conference of the International Speech\nCommunication Association , INTERSPEECH \u201919,\npages 501\u2013505, Graz, Austria.\nAnthony Downs. 1972. Up and down with ecology:\nThe \u201cissue-attention cycle\u201d. The Public Interest ,\n28:38\u201350.\nAnastasia Giachanou, Paolo Rosso, and Fabio Crestani.\n2019. Leveraging emotional signals for credibility\ndetection. In Proceedings of the 42nd International\nACM SIGIR Conference on Research and Develop-\nment in Information Retrieval , SIGIR \u201919, pages\n877\u2013880, Paris, France.\nGenevieve Gorrell, Ian Roberts, Mark A. Greenwood,\nMehmet E. Bakir, Benedetta Iavarone, and Kalina\nBontcheva. 2018. Quantifying media in\ufb02uence and\npartisan attention on Twitter during the UK EU ref-\nerendum. In Social Informatics , pages 274\u2013290,\nCham.\nNir Grinberg, Kenneth Joseph, Lisa Friedland, Briony\nSwire-Thompson, and David Lazer. 2019. Fake\nnews on Twitter during the 2016 U.S. presidential\nelection. Science , 363(6425):374\u2013378.\nTuan-Anh Hoang, Ee-Peng Lim, Palakorn Achananu-\nparp, Jing Jiang, and Feida Zhu. 2011. On modeling\nvirality of Twitter content. In Digital Libraries: For\nCultural Heritage, Knowledge Dissemination, and\nFuture Creation , pages 212\u2013221. Springer Berlin\nHeidelberg.\nBenjamin D. Horne and Sibel Adal\u0131. 2017. This just in:\nFake news packs a lot in title, uses simpler, repetitive\ncontent in text body, more similar to satire than real\nnews. ArXiv 1703.09398 .Jure Leskovec, Lars Backstrom, and Jon Kleinberg.\n2009. Meme-tracking and the dynamics of the news\ncycle. In Proceedings of the 15th ACM SIGKDD\nInternational Conference on Knowledge Discovery\nand Data Mining , KDD \u201909, pages 497\u2013506, Paris,\nFrance.\nMarshall McLuhan. 1964. Understanding media: The\nextensions of man . McGraw-Hill, New York.\nLuke Munn. 2019. Alt-right pipeline: Individual jour-\nneys to extremism online. First Monday .\nVan-Hoang Nguyen, Kazunari Sugiyama, Preslav\nNakov, and Min-Yen Kan. 2020. FANG: Leveraging\nsocial context for fake news detection using graph\nrepresentation. In Proceedings of the 29th ACM In-\nternational Conference on Information and Knowl-\nedge Management , CIKM \u201920, pages 1165\u20131174.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Nat-\nural Language Processing , EMNLP-IJCNLP \u201919,\npages 3982\u20133992, Hong Kong, China.\nManoel Horta Ribeiro, Raphael Ottoni, Robert West,\nVirg\u00b4\u0131lio AF Almeida, and Wagner Meira Jr. 2020.\nAuditing radicalization pathways on YouTube. In\nProceedings of the 2020 Conference on Fairness, Ac-\ncountability, and Transparency , pages 131\u2013141.\nVictoria Rubin, Niall Conroy, Yimin Chen, and Sarah\nCornwell. 2016. Fake news or truth? Using satir-\nical cues to detect potentially misleading news. In\nProceedings of the Second Workshop on Computa-\ntional Approaches to Deception Detection , pages 7\u2013\n17, San Diego, CA, USA.\nNatali Ruchansky, Sungyong Seo, and Yan Liu. 2017.\nCSI: A hybrid deep model for fake news detec-\ntion. In Proceedings of the 2017 ACM on Confer-\nence on Information and Knowledge Management ,\nCIKM \u201917, page 797\u2013806, Singapore.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics , ACL \u201919, pages 3645\u20133650, Flo-\nrence, Italy.\nTommaso Venturini. 2019. From fake to junk news, the\ndata politics of online virality. In Didier Bigo, En-\ngin Isin, and Evelyn Ruppert, editors, Data Politics:\nWorlds, Subjects, Rights . Routledge, London.\nZhe Zhao, Paul Resnick, and Qiaozhu Mei. 2015. En-\nquiring minds: Early detection of rumors in social\nmedia from enquiry posts. In Proceedings of the\n24th International Conference on World Wide Web ,\nWWW \u201915, pages 1395\u20131405, Geneva, Switzerland.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Predicting the factuality of reporting of news media using observations about user attention in their YouTube channels", "author": ["K Bozhanova", "Y Dinkov", "I Koychev", "M Castaldo"], "pub_year": "2021", "venue": "arXiv preprint arXiv \u2026", "abstract": "We propose a novel framework for predicting the factuality of reporting of news media outlets  by studying the user attention cycles in their YouTube channels. In particular, we design a"}, "filled": false, "gsrank": 322, "pub_url": "https://arxiv.org/abs/2108.12519", "author_id": ["", "1W4SUg4AAAAJ", "o5YAI9wAAAAJ", "stHFAHsAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:hRnnDJdSaCIJ:scholar.google.com/&output=cite&scirp=321&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D320%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=hRnnDJdSaCIJ&ei=PrWsaIaUD7TWieoP1pCJ2AY&json=", "num_citations": 5, "citedby_url": "/scholar?cites=2479322403577469317&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:hRnnDJdSaCIJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2108.12519"}}, {"title": "Biased news data influence on classifying social media posts", "year": "2019", "pdf_data": "Biased News Data In\ruence on\nClassifying Social Media Posts\nMarija Stanojevic, Jumanah Alshehri, Eduard Dragut, Zoran Obradovic\nCenter for Data Analytics and Biomedical Informatics (DABI)\nTemple University\nPhiladelphia, Pennsylvania, USA\nfmarija.stanojevic, jumanah.alshehri, edragut, zoran.obradovic g@temple.edu\nAbstract\nA common task among social scientists is to\nmine and interpret public opinion using social\nmedia data. Scientists tend to employ o\u000b-the-\nshelf state-of-the-art short-text classi\fcation\nmodels. Those algorithms, however, require a\nlarge amount of labeled data. Recent e\u000borts\naim to decrease the compulsory number of\nlabeled data via self-supervised learning and\n\fne-tuning. In this work, we explore the use\nof news data on a speci\fc topic in \fne-tuning\nopinion mining models learned from social me-\ndia data, such as Twitter. Particularly, we in-\nvestigate the in\ruence of biased news data on\nmodels trained on Twitter data by consider-\ning both the balanced and unbalanced cases.\nResults demonstrate that tuning with biased\nnews data of di\u000berent properties changes the\nclassi\fcation accuracy up to 9:5%. The ex-\nperimental studies reveal that the character-\nistics of the text of the tuning dataset, such\nas bias, vocabulary diversity and writing style,\nare essential for the \fnal classi\fcation results,\nwhile the size of the data is less consequen-\ntial. Moreover, a state-of-the-art algorithm\nis not robust on unbalanced twitter dataset,\nand it exaggerates when predicting the most\nfrequent label.\nCopyright c\r2019 for the individual papers by the papers' au-\nthors. Copying permitted for private and academic purposes.\nThis volume is published and copyrighted by its editors.\nIn: A. Aker, D. Albakour, A. Barr\u0013 on-Cede~ no, S. Dori-Hacohen,\nM. Martinez, J. Stray, S. Tippmann (eds.): Proceedings of the\nNewsIR'19 Workshop at SIGIR, Paris, France, 25-July-2019,\npublished at http://ceur-ws.org1 Introduction\nIn recent years, social media platforms have be-\ncome leading channels for the exchange of knowl-\nedge, debates, and product or opinion advertising\n[PP10, WD07, Gly18, SKB12]. Social scientists rou-\ntinely use data from social media platforms to sur-\nvey public opinion on speci\fc topics [Mos13, CSPR16,\nHBK+17, BM18] and computer scientists use the data\nto improve the performance of state-of-the-art natu-\nral language processing (NLP) algorithms [CXHW17,\nACCF16, GPCR18, ZWWL18].\nSocial media data, while abundant, pose many chal-\nlenges in usage: 1) user demographics are rarely avail-\nable; 2) posts are short and sometimes hard to un-\nderstand without context, and 3) it is challenging to\nlabel millions of posts manually in short time. One\nmay overcome the \frst challenge by selecting only in-\nformation from users where demographic information\nis available using multiple social platforms. However,\nthis may bias the data. In order to solve the other\ntwo problems, we need systems that classify data into\ndi\u000berent opinion classes with limited human involve-\nment.\nFigure 1: ULMFiT model training-\row overview\nNumerous algorithms have been proposed to cope\nwith large amounts of short text [ZZL15, ZQZ+16,\nLQH16, XC16, CSBL16, YYD+16, MGB+18]. All\nthese algorithms are supervised in nature, and there-\nfore, require hundreds of thousands of labels in or-\nder to achieve adequate performance levels. In the\nlast two years, algorithms such as CoVe [MBXS17],\nELMo [PNI+18], ULMFiT [HR18] and OpenAI GPT\nFigure 2: ULMFiT model training details\n[RNSS18] have been proposed to minimize the need\nfor labeled data and increase the performance by clev-\nerly utilizing text characteristics. Those methods start\nfrom word-vectors pre-trained on general documents\nand \fne-tune them on domain-speci\fc documents by\nemploying self-supervised learning. In the Universal\nLanguage Model Fine-tuning for Text Classi\fcation\n(ULMFiT) [HR18] self-supervised process predicts the\nnext word based on the previous words in the context.\nAfter the \fne-tuning step, we additionally train the\nmodel with a small number of manually labeled text\ninstances (Figure 1).\nMost of the datasets (e.g., AGNews, DBPedia, Ya-\nhoo Answers) [ZZL15] used for testing text classi\fca-\ntion algorithms are balanced and on average contain\nmuch longer texts than social media posts. On the\nother hand, social media data retrieved with a pur-\npose to model opinion is usually unbalanced. The\ngoal of this paper is to investigate the performance of\nULMFiT model on classifying social media posts for\ndi\u000berent settings of \fne-tuning and labeled datasets.\nWe test balanced and imbalanced labeled social me-\ndia datasets and \fne-tuning news texts with di\u000berent\ncharacteristics (e.g., size, bias, writing style).\nExperiments utilize Twitter data related to USA\nmidterm elections from 2018 and news data from the\nUSA elections 2016. The news data is collected from\nsix major outlets which are considered to have a bias\ntowards the left or right political spectrum1. We test\nhow \fne-tuning with articles from di\u000berent news out-\nlets in\ruences the accuracy of social media posts clas-\nsi\fcation. The hypothesis is that \fne-tuning with ap-\npropriate topic-related text from news can help im-\nprove classi\fcation, but bias in news articles can also\n1Information about outlet bias is taken from:\nhttps://mediabiasfactcheck.comhurt the performance. We test the hypothesis on\nULMFiT algorithm described in the next section.2\n2 Methods\nThe ULMFiT model [HR18] consists of three train-\ning components (Figure 2). Each component is based\non the language model AWD-LSTM [MKS17] and\nconsists of a word-embedding (input) layer, multiple\nLSTM-layers, and a softmax layer used to predict the\noutput. Experimental results in literature prove that\nmultiple LSTM-layers can learn more complex con-\ntexts [MBXS17, PNI+18, HR18] than single LSTM-\nlayer models.\nIn the \frst part of ULMFiT, words and contexts\nembedding is learned from general texts (such as\nWikipedia). In the second part, they are updated\n(\fne-tuned) with topic-related data to learn domain-\nspeci\fc words and phrases. The third part is trained\non labeled domain-speci\fc examples so it can predict\nlabels for the new examples. The output of each part\nis the input in the next step.\nEven though the ULMFiT model is complex, it can\nbe trained e\u000eciently on GPUs when smartly imple-\nmented. Once trained, the \frst part of the model does\nnot change, so we use WT103 pre-trained vectors to\nreduce training time.3\nIn order to speed up \fne-tuning (Figure 2b), we\nuse di\u000berent learning rates for each LSTM layer. The\ntop layer, which calculates softmax, has the largest\nlearning rate, \u0011L. Learning rates for remaining layers\nare set tonl\u00001=nl=2:6 forl2(1;L) as suggested\nin a prior study[HR18]. Instead of having a constant\n2The latest text classi\fcation progress:\nhttp://nlpprogress.com/english/text classi\fcation.html\n3WT103 word-vectors can be found here:\nhttp://\fles.fast.ai/models/wt103\nlearning rate, slanted triangular learning rates are used\nfor every layer to improve the accuracy of the model\n[HR18]. First, the learning rate sharply linearly in-\ncreases so that the model can learn fast from the \frst\nexamples. Once learning rate achieves the \u0011L, it slowly\nlinearly declines as shown in the top-right corner of\nFigure 2.\nIn the third step (Figure 2c), layers are trained\ngradually. First, only the top layer is trained with la-\nbeled data for one epoch while other layers are frozen.\nIn each new epoch, the next frozen layer from the top\nis added to the training.\n3 Experiments\nExperiments are conducted using Twitter data on\nUSA midterm elections 2018 and news data from USA\nelections 2016.\nTwitter data is collected by searching for posts\npublished between November 4th and 7th 2018 which\nhave one of the hashtags: \"#vote\", \"#trump\",\n\"#election\", \"#midtermelection\", \"#democrats\",\n\"#republicans\" and \"#2018midterms\". In total, we\naccrue 936,462 tweets. Most of the posts are retweets,\nwhich appear multiple times in the corpus. After\nretweets removal, 244,320 distinct posts remained, and\nwe pre-process their text by removing all characters,\nexcept alphanumerics.\nOut of those posts, we label 1,526 examples with 0,\n1 or 2. Label 0 is assigned to examples that support\nor promote the left political spectrum or denounce the\nright point of view. Label 1 is given to politically neu-\ntral posts (e.g., posts that encouraged voting). Label\n2 is assigned to examples that support or advertise\nthe right political spectrum or condemn the left point\nof view. We discard 500 examples ( \u001825% of posts)\nbecause they are unrelated to elections.\nNews data is collected from six outlets that are\nperceived to have di\u000berent political partisanship, rang-\ning from the left-oriented to right-oriented outlets\nbased on media bias fact check website (Table 1). Ar-\nticles published between October 2015 and May 2017\nthat contain words \"election\", \"ballot\", \"republican\",\n\"GOP\", or \"democrat\" are selected. The news ar-\nticles di\u000ber substantially in writing style, content di-\nversity, bias, number of articles and number of words\n(Table 1). As with the tweets, news articles do not\nalways discuss the U.S. elections. Sometimes, they\ndebate Brexit or elections in France and other coun-\ntries worldwide. In pre-processing, we remove all non-\nalphanumeric characters from news articles.\nExperiments settings. We use the pre-trained\nWT103 token-vectors in the \frst ULMFiT step.\nWT103 has 103 million tokens from Wikipedia texts\nfor training, 217K tokens for validation and 245K to-Table 1: Outlets\nOutlet Bias #Words\nCNN News (CNN) left 426,778\nWashington Post (WP) left-center 9,229,176\nBBC News (BBC) neutral-left 1,247,437\nMarketWatch (MW) neutral-right 1,505,107\nWall Street Journal (WSJ) right-center 547,548\nFoxNews (FN) right 3,082,912\nkens for testing [MXBS16]. Our system is trained us-\ning the architecture in Figure 2a. The vocabulary has\n267K unique tokens. In this paper word and token\nhave interchangeable meanings.\nFor the \fne-tuning step, we explore ten di\u000berent\nsettings: 1) \"all news\" text with the data from all\noutlets + tweets text; 2) only the tweets; 3) text\nfrom \"left-biased\" outlets + tweets text; 4) text from\n\"right-biased\" outlets + tweets text. Remaining six\nexperiments contain text from one outlet and tweets\ntext. We randomly permute examples in a \fne-tuning\ndataset before usage.\nIn the third step, experiments test two settings of\nlabeled Twitter data. Mix 1 (balanced mix) contains\n380 examples with label 0 (left), 323 examples with la-\nbel 1 (neutral) and 323 examples with label 2 (right).\nMix 2 (unbalanced mix) contains 380 examples with\nlabel 0 (left), 823 examples with label 1 (neutral) and\n323 examples with label 2 (right). We randomly split\nlabeled data into three disjoint parts: test (200 ex-\namples), validation (200 examples) and training (626\nexamples in Mix 1 and 1126 examples in Mix 2). Each\nexperiment is repeated four times and accuracy mean,\nand the standard deviation is reported for each of the\nten settings.\nWe do not clean Twitter, and news data of non-\nrelevant examples in order to emulate the real-world\nsituation. The data retrieval process is intention-\nally simple to mirror the information extraction pro-\ncess often used in research papers [Mos13, CSPR16,\nHBK+17, BM18]. Those experiments test the robust-\nness of the model to the bias and noise in data and\nrobustness to the unbalanced classes.\n4 Results and discussion\nWe repeat each experiment four times, and we report\nthe accuracy mean and standard deviation in Table\n2. High standard deviation (1:2 \u00005:3%) indicates the\nmodel's sensitivity to the order of examples in the \fne-\ntuning data and a need for more labeled examples.\nResults provide evidence that the model is not ro-\nbust to unbalanced datasets. When Mix 1 and Mix 2\nresults are compared, the model always achieved bet-\nter results for Mix 2 (Table 2) which has 54% of neu-\ntral labels as compared to 31.5% of neutral labels in\nTable 2: Classi\fcation results\nNews sources included Mix 1 Mix 2\n(Left : Neutral : Right) (380 : 323 : 323) (380 : 823 : 323)\nAll news 53:2\u00063% 59:4\u00063:7%\nNo news 56\u00065:3% 66:6\u00062:5%\nLeft-biased (CNN+WP+BBC) 49:2 \u00062:9% 61:1\u00063:3%\nRight-biased (MW+WSJ+FN) 51:7 \u00063:8% 63:0\u00063:2%\nCNN 58:7\u00061:2% 62:7\u00063:0%\nWashington Post (WP) 55:6 \u00063:0% 60:7\u00061:4%\nBBC 55:1\u00063:1% 64:1\u00062:7%\nMarketWatch (MW) 56:5\u00062:6% 64:2\u00061:8%\nWall Street Journal (WSJ) 57 :7\u00063:7% 60:0\u00064:3%\nFoxNews (FN) 53:2\u00062:9% 61:9\u00063:3%\nFigure 3: Balanced Twitter Dataset: Percent of pre-\ndicted labels from each class when \fne-tuned with ten\ndi\u000berent combinations of news outlets texts\nMix 1. As evident from Figure 4, 80 \u000090% of pre-\ndicted labels for Mix 2 are neutral. Therefore, better\nresults for Mix 2 are achieved because the algorithm\nexaggerates the most frequent (neutral) label in the\nimbalanced dataset (which contains 54% of examples\nof that class).\nThe classi\fcation accuracy di\u000berence between Mix\n1 and 2 is the largest (11.9%) when \"left-biased news\"\nis used for \fne-tuning. In this case, the accuracy on\nboth Mix 1 and Mix 2 decreases compared to when\n\"No news\" is present. However, outlet bias has more\nin\ruence on the accuracy of Mix 1.\nFigure 3 reveals that using \"all news\" data for \fne-\ntuning achieves the best balance among predicted la-\nbels for Mix 1. However, almost half of predicted labels\nare wrong, so accuracy is low.\nLabeled Twitter data demonstrate diversity among\nposts with label \"left\". They often talk only about one\nparticular issue and have fewer hashtags that support\nthe left political spectrum. Additionally, the diversity\nof people and entities mentioned is more prominent in\nthe posts labeled as \"left\" than those labeled \"right\"\n(which mainly mention president Trump). Hence, the\nbest performance for Mix 1 is achieved when \fne-\nFigure 4: Unbalanced Twitter Dataset: Percent of pre-\ndicted labels from each class when \fne-tuned with ten\ndi\u000berent combinations of news outlets texts\ntuning with \"CNN\" data because the model is trained\nto focus more on left-relevant contexts.\nThe next best results for Mix 1 are achieved when\n\fne-tuning with news articles from The Wall Street\nJournal because its articles often discuss both sides in\ndetail (sometimes even in the same sentence). Hence,\nwhen the model is trained with data from this out-\nlet, it understands relevant phrases and predicts \"left\"\nand \"right\" labels with higher accuracy. On the other\nhand, \"The Wall Street Journal\" \fne-tuned experi-\nment predicts much more often \"right\" label for \"left-\nlabeled\" example than the other experiments.\nThe confusion matrices created for each experiment\nand Figure 4 reveal that the algorithm recognizes the\nright label easier than the left label in Mix 2. A better\nunderstanding of the right label can be explained with\nthe di\u000berent writing style of left-labeled tweets, which\nre\rects a more diverse set of topics and entities as\ndiscussed above. The best accuracy score for Mix 2\nis achieved when \"no news\" data is used for the \fne-\ntuning process. Most of the labels are neutral, and\nnews data is mainly left or right oriented/biased, so it\nin\ruences the accuracy negatively.\nAs hypothesized, results demonstrate that \fne-\ntuning with biased news datasets can in\ruence accu-\nracy in contrasting ways. Di\u000berent in\ruence of bi-\nased news is particularly visible in the results of Mix\n1 where the di\u000berence between the best and the worst\naccuracy for di\u000berent \fne-tuning settings is 9 :5%. In\nMix 2 this di\u000berence is also notable, 7 :2%. In\ruence of\nthe bias is not uniform. While \fne-tuning with \"left-\nbiased news\" gives the worst result for Mix 1, its per-\nformance for Mix 2 is average when compared to other\nexperiments. On the other hand, \fne-tuning with \"all\nnews\" gives the worst results for Mix 2 and average\nresults for Mix 1.\nThe size of the \fne-tuning data does not seem to in-\n\ruence the results. \"Washington Post\" has the largest\namount of words, but it achieves average results in\nboth mixes. \"CNN\" is the smallest dataset, but it\nachieves the best result for Mix 1. It is interesting\nto notice that \"all news\" achieves worse results than\n\"no news\" \fne-tuning for both Mix 1 and Mix 2, even\nthough in literature, training with more data often\ncontributes to better results. This result suggests that\nthe content (bias) of the \fne-tuning dataset is more\nimportant than its size.\nAccuracy behavior in many experiments requires\nfurther analysis in order to better understand the in-\n\ruence of \fne-tuning text characteristics on the per-\nformance. Additionally, the e\u000bect of non-relevant text\non the accuracy should be further tested since its fre-\nquency is high in both news and Twitter data. Since\nresults clearly show that this model is not robust on\nbias and noise, other novel methods should be tested\nsimilarly. It is essential to create unbalanced and bi-\nased datasets for \fne-tuning and testing of the future\nmodels to create robust methods that would be bene-\n\fcial to the real-world applications.\n5 Conclusion\nIn this work we have shown that bias, noise and text\nproperties need to be accounted for when constructing\ndata for \fne-tuning language models. Text size does\nnot seem to be an important dimension. We performed\nexperiments with data collected from Twitter and six\nnews outlets using ULMFiT language model. Results\nshow that the algorithm is not robust to noise in data,\nto bias in the \fne-tuning dataset, or to the dataset\nimbalance.\nWhile conducted experiments show weaknesses of\nthe existing system, further work is needed to un-\nderstand better the relationship between properties of\n\fne-tuning data and speci\fc tasks. Additionally, bet-\nter models are required that are more robust to bias\nand noise in order to be able to solve challenging real-\nworld problems.6 Acknowledgements\nThis research was supported in part by the NSF grants\nIIS-1842183.\nReferences\n[ACCF16] Orestes Appel, Francisco Chiclana, Jenny\nCarter, and Hamido Fujita. A hybrid ap-\nproach to the sentiment analysis problem\nat the sentence level. Knowledge-Based\nSystems, 108:110{124, 2016.\n[BM18] Marco Bastos and Dan Mercea.\nParametrizing brexit: mapping twit-\nter political space to parliamentary\nconstituencies. Information, Com-\nmunication & Society , 21(7):921{939,\n2018.\n[CSBL16] Alexis Conneau, Holger Schwenk, Lo\u007f \u0010c\nBarrault, and Yann Lecun. Very deep con-\nvolutional networks for text classi\fcation.\narXiv preprint arXiv:1606.01781, 2016.\n[CSPR16] Fabio Celli, Evgeny Stepanov, Massimo\nPoesio, and Giuseppe Riccardi. Predict-\ning brexit: Classifying agreement is better\nthan sentiment and pollsters. In Proceed-\nings of the Workshop on Computational\nModeling of Peoples Opinions, Personal-\nity, and Emotions in Social Media (PEO-\nPLES), pages 110{118, 2016.\n[CXHW17] Tao Chen, Ruifeng Xu, Yulan He, and\nXuan Wang. Improving sentiment anal-\nysis via sentence type classi\fcation using\nbilstm-crf and cnn. Expert Systems with\nApplications, 72:221{230, 2017.\n[Gly18] Carroll J Glynn. Public opinion . Rout-\nledge, 2018.\n[GPCR18] Aitor Garc\u0013 \u0010a-Pablos, Montse Cuadros,\nand German Rigau. W2vlda: almost un-\nsupervised system for aspect based senti-\nment analysis. Expert Systems with Ap-\nplications, 91:127{137, 2018.\n[HBK+17] Philip N Howard, Gillian Bolsover, Bence\nKollanyi, Samantha Bradshaw, and Lisa-\nMaria Neudert. Junk news and bots dur-\ning the us election: What were michi-\ngan voters sharing over twitter. Com-\nputational Propaganda Research Project,\nOxford Internet Institute, Data Memo , 1,\n2017.\n[HR18] Jeremy Howard and Sebastian Ruder.\nUniversal language model \fne-tuning\nfor text classi\fcation. arXiv preprint\narXiv:1801.06146, 2018.\n[LQH16] Pengfei Liu, Xipeng Qiu, and Xuanjing\nHuang. Recurrent neural network for\ntext classi\fcation with multi-task learn-\ning. arXiv preprint arXiv:1605.05101,\n2016.\n[MBXS17] Bryan McCann, James Bradbury, Caim-\ning Xiong, and Richard Socher. Learned\nin translation: Contextualized word vec-\ntors. In Advances in Neural Informa-\ntion Processing Systems, pages 6294{6305,\n2017.\n[MGB+18] Tomas Mikolov, Edouard Grave, Piotr\nBojanowski, Christian Puhrsch, and Ar-\nmand Joulin. Advances in pre-training\ndistributed word representations. In Pro-\nceedings of the International Conference\non Language Resources and Evaluation\n(LREC 2018) , 2018.\n[MKS17] Stephen Merity, Nitish Shirish Keskar,\nand Richard Socher. Regularizing and\noptimizing lstm language models. arXiv\npreprint arXiv:1708.02182 , 2017.\n[Mos13] Mohamed M Mostafa. More than words:\nSocial networks text mining for consumer\nbrand sentiments. Expert Systems with\nApplications, 40(10):4241{4251, 2013.\n[MXBS16] Stephen Merity, Caiming Xiong, James\nBradbury, and Richard Socher. Pointer\nsentinel mixture models. arXiv preprint\narXiv:1609.07843, 2016.\n[PNI+18] Matthew E Peters, Mark Neumann, Mo-\nhit Iyyer, Matt Gardner, Christopher\nClark, Kenton Lee, and Luke Zettlemoyer.\nDeep contextualized word representa-\ntions. arXiv preprint arXiv:1802.05365,\n2018.\n[PP10] Alexander Pak and Patrick Paroubek.\nTwitter as a corpus for sentiment analysis\nand opinion mining. In LREc, volume 10,\npages 1320{1326, 2010.\n[RNSS18] Alec Radford, Karthik Narasimhan, Tim\nSalimans, and Ilya Sutskever. Improving\nlanguage understanding by generative\npre-training. URL https://s3-us-west-2.amazonaws. com/openai-assets/research-\ncovers/languageunsupervised/language\nunderstanding paper. pdf , 2018.\n[SKB12] Pawel Sobkowicz, Michael Kaschesky, and\nGuillaume Bouchard. Opinion mining\nin social media: Modeling, simulating,\nand forecasting political opinions in the\nweb. Government Information Quarterly,\n29(4):470{479, 2012.\n[WD07] Duncan J Watts and Peter Sheridan\nDodds. In\ruentials, networks, and public\nopinion formation. Journal of consumer\nresearch , 34(4):441{458, 2007.\n[XC16] Yijun Xiao and Kyunghyun Cho. E\u000ecient\ncharacter-level document classi\fcation by\ncombining convolution and recurrent lay-\ners. arXiv preprint arXiv:1602.00367,\n2016.\n[YYD+16] Zichao Yang, Diyi Yang, Chris Dyer,\nXiaodong He, Alex Smola, and Eduard\nHovy. Hierarchical attention networks for\ndocument classi\fcation. In Proceedings of\nthe 2016 Conference of the North Ameri-\ncan Chapter of the Association for Com-\nputational Linguistics: Human Language\nTechnologies , pages 1480{1489, 2016.\n[ZQZ+16] Peng Zhou, Zhenyu Qi, Suncong Zheng,\nJiaming Xu, Hongyun Bao, and Bo Xu.\nText classi\fcation improved by inte-\ngrating bidirectional lstm with two-\ndimensional max pooling. arXiv preprint\narXiv:1611.06639, 2016.\n[ZWWL18] Shunxiang Zhang, Zhongliang Wei, Yin\nWang, and Tao Liao. Sentiment analy-\nsis of chinese micro-blog text based on ex-\ntended sentiment dictionary. Future Gen-\neration Computer Systems, 81:395{403,\n2018.\n[ZZL15] Xiang Zhang, Junbo Zhao, and Yann Le-\nCun. Character-level convolutional net-\nworks for text classi\fcation. In Advances\nin neural information processing systems ,\npages 649{657, 2015.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Biased news data influence on classifying social media posts", "author": ["M Stanojevic"], "pub_year": "2019", "venue": "3rd Int'l Workshop on Recent Trends in News \u2026", "abstract": "A common task among social scientists is to mine and interpret public opinion using social  media data. Scientists tend to employ off-theshelf state-of-the-art short-text classification"}, "filled": false, "gsrank": 323, "pub_url": "https://par.nsf.gov/servlets/purl/10113547", "author_id": ["pAyfhIkAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:EoOrShH7_lIJ:scholar.google.com/&output=cite&scirp=322&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D320%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=EoOrShH7_lIJ&ei=PrWsaIaUD7TWieoP1pCJ2AY&json=", "num_citations": 11, "citedby_url": "/scholar?cites=5980493406880367378&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:EoOrShH7_lIJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://par.nsf.gov/servlets/purl/10113547"}}, {"title": "Fine-grained classification of political bias in German news: A data set and initial experiments", "year": "2021", "pdf_data": "Proceedings of the Fifth Workshop on Online Abuse and Harms , pages 121\u2013131\nAugust 6, 2021. \u00a92021 Association for Computational Linguistics121Fine-grained Classi\ufb01cation of Political Bias in German News:\nA Data Set and Initial Experiments\nDmitrii Aksenov, Peter Bourgonje, Karolina Zaczynska,\nMalte Ostendorff, Juli\u00e1n Moreno-Schneider, Georg Rehm\nDFKI GmbH, Berlin, Germany\nfirstname.lastname@dfki.de\nAbstract\nWe1present a data set consisting of German\nnews articles labeled for political bias on a \ufb01ve-\npoint scale in a semi-supervised way. While\nearlier work on hyperpartisan news detection\nuses binary classi\ufb01cation (i. e., hyperpartisan\nor not) and English data, we argue for a more\n\ufb01ne-grained classi\ufb01cation, covering the full\npolitical spectrum (i. e., far-left, left, centre,\nright, far-right) and for extending research to\nGerman data. Understanding political bias\nhelps in accurately detecting hate speech and\nonline abuse. We experiment with different\nclassi\ufb01cation methods for political bias detec-\ntion. Their comparatively low performance (a\nmacro-F 1of 43 for our best setup, compared to\na macro-F 1of 79 for the binary classi\ufb01cation\ntask) underlines the need for more (balanced)\ndata annotated in a \ufb01ne-grained way.\n1 Introduction\nThe social web and social media networks have re-\nceived an ever-increasing amount of attention since\ntheir emergence 15-20 years ago. Their popular-\nity among billions of users has had a signi\ufb01cant\neffect on the way people consume information in\ngeneral, and news in particular (Newman et al.,\n2016). This development is accompanied by a\nnumber of challenges, which resulted in various\nNLP tasks that deal with information quality (Der-\nczynski and Bontcheva, 2014; Dale, 2017; Saquete\net al., 2020). Due to the data-driven nature of these\ntasks, they are often evaluated under the umbrella\nof (un)shared tasks, on topics such as rumour detec-\ntion or veri\ufb01cation (Derczynski et al., 2017; Gorrell\net al., 2019), offensive language and hate speech\ndetection (Zampieri et al., 2019; Basile et al., 2019;\n1This work was done while all co-authors were at DFKI.\nThe new af\ufb01liations of the \ufb01rst two authors are ambeRoad\nTech GmbH, Aachen, Germany (dmitrii@amberoad.de) and\nMorningsun Technology GmbH, Saarbr\u00fccken, Germany\n(peter.bourgonje@morningsun-technology.com).Stru\u00df et al., 2019; Waseem et al., 2017; Fi\u0161er et al.,\n2018; Roberts et al., 2019; Akiwowo et al., 2020)\nor fake news and fact-checking (Hanselowski et al.,\n2018; Thorne et al., 2019; Mihaylova et al., 2019).\nSeveral shared tasks concentrate on stance (Mo-\nhammad et al., 2016) and hyper-partisan news de-\ntection (Kiesel et al., 2019), which predict either\nthe stance of the author towards the topic of a\nnews piece, or whether or not they exhibit alle-\ngiance to a particular party or cause. We argue\nthat transparency and de-centralisation (i. e., mov-\ning away from a single, objective \u201ctruth\u201d and a\nsingle institution, organisation or algorithm that\ndecides on this) are essential in the analysis and\ndissemination of online information (Rehm, 2018).\nThe prediction of political bias was recently exam-\nined by the 2019 Hyperpartisan News Detection\ntask (Kiesel et al., 2019) with 42 teams submitting\nvalid runs, resulting in over 30 publications. This\ntask\u2019s test/evaluation data comprised English news\narticles and used labels obtained by Vincent and\nMestre (2018), but their \ufb01ve-point scale was bina-\nrised so the challenge was to label articles as being\neither hyperpartisan ornot hyperpartisan .\nWe follow Wich et al. (2020) in claiming that,\nin order to better understand online abuse and hate\nspeech, biases in data sets and trained classi\ufb01ers\nshould be made transparent, as what can be consid-\nered hateful or abusive depends on many factors\n(relating to both sender and recipient), including\nrace (Vidgen et al., 2020; Davidson et al., 2019),\ngender (Brooke, 2019; Clarke and Grieve, 2017),\nand political orientation (Vidgen and Derczynski,\n2021; Jiang et al., 2020). This paper contributes\nto the detection of online abuse by attempting to\nuncover political bias in content.\nWe describe the creation of a new data set of\nGerman news articles labeled for political bias. For\nannotation, we adopt the semi-supervised strategy\nof Kiesel et al. (2019) who label (English) articles\n122according to their publisher. In addition to opening\nup this line of research to a new language, we use\na more \ufb01ne-grained set of labels. We argue that, in\naddition to knowing whether content is hyperpar-\ntisan, the direction of bias (i. e., left-wing or right-\nwing) is important for end user transparency and\noverall credibility assessment. As our labels are not\njust about hyperpartisanism as a binary feature, we\nrefer to this task as political bias classi\ufb01cation . We\napply and evaluate various classi\ufb01cation models\nto the data set. We also provide suggestions for\nimproving performance on this challenging task.\nThe rest of this paper is structured as follows. Sec-\ntion 2 discusses related work on bias and hyper-\npartisanism. Section 3 describes the data set and\nprovides basic statistics. Section 4 explains the\nmethods we apply to the 2019 Hyperpartisan News\nDetection task data (for evaluation and benchmark-\ning purposes) and to our own data set. Sections 5\nand 6 evaluate and discuss the results. Section 7\nsums up our main \ufb01ndings.\n2 Related Work\n2.1 Data sets\nFor benchmarking purposes, we run our system on\nthe data from Kiesel et al. (2019). They introduce a\nsmall number of articles (1,273) manually labeled\nby content, and a large number of articles (754,000)\nlabeled by publisher via distant supervision, using\nlabels from BuzzFeed news2and Media Bias Fact\nCheck3. Due to the lack of article-level labels for\nGerman media, we adopt the strategy of labeling\narticles by publisher.\nSeveral studies use the data from allsides.com4,\nwhich provides annotations on political ideology\nfor individual articles in English. Using this data,\nBaly et al. (2020) introduce adversarial domain\nadaptation and triplet loss pre-training that prevents\nover-\ufb01tting to the style of a speci\ufb01c news medium,\nKulkarni et al. (2018) demonstrate the importance\nof the article\u2019s title and link structure for bias pre-\ndiction and Li and Goldwasser (2019) explore how\nsocial content can be used to improve bias predic-\ntion by leveraging Graph Convolutional Networks\nto encode a social network graph.\nZhou et al. (2021) analysed several unreliable\nnews data sets and showed that heterogeneity of the\n2https://github.com/BuzzFeedNews/2017-08-partisan-sit\nes-and-facebook-pages\n3https://mediabiasfactcheck.com\n4https://www.allsides.com/media-biasnews sources is crucial for the prevention of source-\nrelated bias. We adopt their strategy of splitting\nthe sources into two disjoint sets used for building\ntrain and test data sets respectively.\nGangula et al. (2019) work on detecting bias in\nnews articles in the Indian language Telugu. They\nannotate 1,329 articles concentrating on headlines,\nwhich they \ufb01nd to be indicative of political bias. In\ncontrast to Kiesel et al. (2019), but similar to our\napproach, Gangula et al. (2019) treat bias detection\nas a multi-class classi\ufb01cation problem. They use\nthe \ufb01ve main political parties present in the Telugu-\nspeaking region as their classi\ufb01cation labels, but do\nnot position these parties on the political spectrum.\nTaking into account the political orientation of\nthe author, SemEval 2016 Task 6 (Mohammad\net al., 2016) worked on stance detection, where\nsub-task A comprised a set of tweets, the target\nentity or issue (e. g., \u201cHillary Clinton\u201d, or \u201cClimate\nChange\u201d) and a label (one of favour ,against ,nei-\nther). The tweet-target-stance triples were split\ninto training and test data. Sub-task B had a simi-\nlar setup, but covered a target not included in the\ntargets of task A, and presented the tweet-target-\nstance triples as test data only (i. e., without any\ntraining data for this target). While (political)\nstance of the author is at the core of this challenge,\nit differs from the problems we tackle in two impor-\ntant ways: 1) The task dealt with tweets, whereas\nwe process news articles, which are considerably\nlonger (on average 650 words per text for both\ncorpora combined, see Section 3, compared to the\n140-character limit5enforced by Twitter) and are\nwritten by professional authors and edited before\nposted. And 2) unlike the shared task setup, we\nhave no target entity or issue and aim to predict the\npolitical stance, bias or orientation (in the context\nof this paper, we consider these three words synony-\nmous and use the phrase political bias throughout\nthe rest of this paper) from the text, irrespective of\na particular topic, entity or issue.\nOne of the key challenges acknowledged in the\nliterature is cross-target or cross-topic performance\nof stance detection systems (K\u00fc\u00e7\u00fck and Can, 2020).\nTrained for a speci\ufb01c target or topic (Sobhani et al.,\n2017), performance is considerably lower when\nthese systems are applied to new targets. Vamvas\nand Sennrich (2020) address this issue by annotat-\ning and publishing a multilingual (standard Swiss\n5The shared task took place before Twitter increased the\ncharacter limit of one tweet from 140 to 280 in 2017.\n123German, French, Italian) stance detection corpus\nthat covers a considerably higher number of targets\n(over 150, compared to six in Mohammad et al.,\n2016). Vamvas and Sennrich (2020) work with\ncomments, which are longer than tweets (on av-\nerage 26 words), but still shorter than our news\narticles. Similar to Mohammad et al. (2016) but un-\nlike our approach, the data is annotated for stance\ntoward a particular target.\nEarlier work on political stance is represented by\nThomas et al. (2006), who work on a corpus of US\ncongressional debates, which is labeled for stance\nwith regard to a particular issue (i. e., a proposed\nlegislation) and which uses binary labels for sup-\nporting or opposing the proposed legislation. From\nthis, political bias could potentially be deduced,\nif information on the party of the person that pro-\nposed the legislation is available. However, \ufb01rst of\nall this correlation is not necessarily present, and\nsecond, it results in a binary (republican vs. demo-\ncratic) labeling scheme, whereas we use a larger\nset of labels covering the political spectrum from\nleft-wing to right-wing (see Section 3).\nA comprehensive review of media bias in news\narticles, especially attempting to cover insights\nfrom social sciences (representing a more theoreti-\ncal, rational approach) and computer science (rep-\nresenting a more practical, empiricist approach), is\nprovided by Hamborg et al. (2018). The authors\nobserve a lack of inter-disciplinary work, and al-\nthough our work is mainly empirical, we agree\nthat using a more diverse range of corpora and lan-\nguages is one way to move away from \u201ctoo simplis-\ntic (models)\u201d (Hamborg et al., 2018, p. 410) that\nare currently in use. In this respect, we would like\nto stress that, unlike Kulkarni et al. (2018); Baly\net al. (2020); Li and Goldwasser (2019), who all\neither work on or contribute data sets (or both) to\npolitical bias classi\ufb01cation in English, we strongly\nbelieve that a sub-discipline dealing with bias de-\ntection bene\ufb01ts especially from a wide range of\ndifferent data sets, ideally from as many different\nlanguages and cultural backgrounds as possible.\nWe contribute to this cause by publishing and work-\ning with a German data set.\n2.2 Models\nWith regard to the system architecture, Bie\u00dfmann\n(2016) use similar techniques as we do (bag-of-\nwords and a Logistic Regression classi\ufb01er, though\nwe do not use these two in combination), but workon the domain of German parliament speeches, at-\ntempting to predict the speaker\u2019s af\ufb01liation based\non their speech. Iyyer et al. (2014) use a bag-of-\nwords and Logistic Regression system as well, but\nimprove over this with a Recursive Neural Network\nsetup, working on the Convote data set (Thomas\net al., 2006) and the Ideological Book Corpus6.\nHamborg et al. (2020) use BERT for sentiment\nanalysis after \ufb01nding Named Entities \ufb01rst, in order\nto \ufb01nd descriptions of entities that suggest either\na left-wing or a right-wing bias (e. g., using either\n\u201cfreedom \ufb01ghters\u201d or \u201cterrorists\u201d to denote the same\ntarget entity or group). Salminen et al. (2020) work\non hate speech classi\ufb01cation. We adopt their idea\nof evaluating several methods (features and models,\nsee Sections 4.1 and 4.2) on the same data and also\nadopt their strategy of integrating BERT represen-\ntations with different classi\ufb01cation algorithms.\n3 Data Collection and Processing\nWe obtain our German data through two differ-\nent crawling processes, described in Sections 3.1\nand 3.2, which also explain how we assign labels\nthat re\ufb02ect the political bias of the crawled, Ger-\nman news articles. Since the 2019 shared task\ndata which we use for benchmarking purposes is\ndownloaded and used as is, we refer to Kiesel et al.\n(2019) for more information on this data set.\n3.1 News-Streaming Data\nThis work on political bias classi\ufb01cation is carried\nout in the context of a project on content curation\n(Rehm et al., 2020).7One of the project partners8\nprovided us with access to a news streaming ser-\nvice that delivers a cleaned and augmented stream\nof content from a wide range of media outlets, con-\ntaining the text of the web page (without advertise-\nments, HTML elements or other non-informative\npieces of text) and various metadata, such as pub-\nlisher, publication date, recognised named entities\nand sentiment value. We collected German news\narticles published between February 2020 and Au-\ngust 2020. Filtering these for publishers for which\nwe have a label (Section 3.4) resulted in 28,954\narticles from 35 publishers. The average length of\nan article is 741 words, compared to 618 words\nfor the 2019 Hyperpartisan News Detection shared\ntask data (for the by-publisher data set).\n6https://people.cs.umass.edu/~miyyer/ibc/index.html\n7https://qurator.ai\n8https://www.ubermetrics-technologies.com\n124Data set Type Far-left Centre-left Centre Centre-right Far-right General Regional Overall\nTrainingNum. publishers 2 3 11 8 2 23 3 26\nNum. articles 1,146 11,958 11,714 15,624 1,772 41,175 1,039 42,214\nTestNum. publishers 1 3 3 2 1 8 2 10\nNum. articles 215 1,159 1,349 1,754 671 3,597 1,551 5,148\nTable 1: Basic statistics of our data set.\n3.2 Crawled Data\nTo further augment the data set described in Sec-\ntion 3.1, we used the open-source news crawler\nnews-please9. Given a root URL, the crawler ex-\ntracts text from a website, together with metadata\nsuch as author name, title and publication date.\nWe used the 40 German news outlets for which\nwe have bias labels (Section 3.4) as root URLs to\nextract news articles. We applied regular expres-\nsion patterns to skip sections of websites unlikely to\ncontain indications of political bias10. This resulted\nin over 60,000 articles from 15 different publishers.\n3.3 Data Cleaning\nAfter collecting the data, we \ufb01ltered and cleaned\nthe two data sets. First, we removed duplicates in\neach collection. Because the two crawling methods\nstart from different perspectives \u2013 with the \ufb01rst one\ncollecting large volumes and \ufb01ltering for particular\npublishers later, and the second one targeting these\nparticular publishers right from the beginning \u2013 but\noverlap temporally, we also checked for duplicates\nin the two collections. While we found no exact\nduplicates (probably due to differences in the im-\nplementation of the crawlers), we checked articles\nwith identical headlines and manually examined\nthe text, to \ufb01nd irrelevant crawling output.\nSecond, we removed non-news articles (e. g.,\npersonal pages of authors, pages related to legal or\ncontact information, or lists of headlines). This step\nwas mostly based on article headlines and URLs.\nBecause the vast majority of data collected was\npublished after 2018, we \ufb01ltered out all texts pub-\nlished earlier, fearing too severe data sparsity issues\nwith the older articles. Due to the low number of\narticles, a model may associate particular events\nthat happened before 2018 with a speci\ufb01c label\nonly because this was the only available label for\narticles covering that speci\ufb01c event.\n9https://github.com/fhamborg/news-please\n10For some websites, the URL was indicative of the cate-\ngory, like domain.com/politics/ or domain.com/sports/. These\nare \ufb01ltered out through regular expressions.Finally, we inspected our collection trying to\ndetect and delete pieces of texts that are not part\nof the articles (such as imprints, advertisements or\nsubscription requests). This process was based on\nkeyword search, after which particular articles or\nsections of articles were removed manually.\nThis procedure resulted in 26,235 articles from\n34 publishers and 21,127 articles from 15 pub-\nlishers11in our two collections respectively. We\ncombined these collections, resulting in a set of\n47,362 articles from 34 different publishers. For\nour experiments on this data, we created a 90-10\ntraining-test data split. Because initial experiments\nshowed that models quickly over-\ufb01t on publisher\nidentity (through section names, stylistic features\nor other implicit identity-related information left\nafter cleaning), we ensured that none of the publish-\ners in the test set appear in the training data. Due\nto the low number of publishers for certain classes,\nthis requirement could not be met in combination\nwith 10-fold cross-validation, which is why we re-\nfrain from 10-fold cross-validation and use a single,\nstatic training and test data split (see Table 1).\n3.4 Label Assignment\nTo assign political bias labels to our news articles,\nwe follow the semi-supervised strategy of Kiesel\net al. (2019), who use the identity of the publisher\nto label (the largest part of) their data set. The\nvalues for our labels are based on a survey carried\nout by Medienkompass.org, in which subjects were\nasked to rate 40 different German media outlets\non a scale of partiality and quality. For partiality,\na range from 1 to 7 was used with the following\nlabels: 1 \u2013 left-wing extremism (fake news and con-\nspiracy theories) ,2 \u2013 left-wing mission (question-\nable journalistic values) ,3 \u2013 tendentiously left ,4 \u2013\nminimal partisan tendency ,5 \u2013 tendentiously right ,\n6 \u2013 right-wing mission (questionable journalistic\nvalues) ,7 \u2013 right-wing extremism (fake news and\nconspiracy theories) . For quality, a range from 1 to\n11For 25 out of the 40 root URLs, we have been unable to\nextract anything using the news-please crawler.\n1255 was used: 1 \u2013 click bait ,2 \u2013 basic information ,3\n\u2013 meets high standards ,4 \u2013 analytical ,5 \u2013 complex .\nA total of 1,065 respondents positioned these\n40 news outlets between (an averaged) 2.1 (indy-\nmedia) and 5.9 (Compact) for partiality, and be-\ntween 1.3 (BILD) and 3.5 (Die Zeit, Deutschland-\nfunk) for quality. We used the result of this survey,\navailable online12, to \ufb01lter and annotate our news\narticles for political bias based on their publisher.\nIn this paper we use the bias labels for classi\ufb01cation\nand leave quality classi\ufb01cation for further research.\nBecause 60-way classi\ufb01cation for partiality (1\nto 7 with decimals coming from averaging respon-\ndents\u2019 answers) results in very sparsely populated\n(or even empty) classes for many labels, and even\nrounding off to the nearest natural number (i. e., 7-\nway classi\ufb01cation) leads to some empty classes, we\nconverted the 7-point scale to a 5-point scale, using\nthe following boundaries: 1-2.5 \u2013 far-left, 2.5-3.5 \u2013\ncentre-left, 3.5-4.5 \u2013 centre, 4.5-5.5 \u2013 centre-right,\n5.5-7 \u2013 far-right. We favoured this equal distribu-\ntion over the scale of the survey over class size\nbalance (there are more far-right articles than far-\nleft articles, for example). The distribution of our\ndata over this 5-point scale is shown in Table 1.\n3.5 Topic Detection\nTo get an overview of the topics and domains cov-\nered in the data set, we applied a topic detection\nmodel, which was trained on a multilingual data set\nfor stance detection (Vamvas and Sennrich, 2020)\nwhere, in addition to stance, items are classi\ufb01ed as\nbelonging to one of 12 different news topics. We\ntrained a multinomial Naive Bayes model on the\nBOW representation of all German items (just un-\nder 50k in total) in this multilingual data set, achiev-\ning an accuracy of 79% and a macro-averaged F 1-\nscore of 78. We applied this model to our own data\nset. The results are shown in Table 2. Note that\nthis is just to provide an impression of the distribu-\ntion and variance of topics. Vamvas and Sennrich\n(2020) work on question-answer/comment pairs,\nand the extent to which a topic detection model\ntrained on such answers or comments is eligible\nfor transfer to pure news articles is a question we\nleave for future work.\nSince the majority of articles was published in\n2020, a year massively impacted by the COVID-\n19 pandemic, we applied simple keyword-based\nheuristics, resulting in the estimate that approxi-\n12https://medienkompass.org/deutsche-medienlandschaft/Topic Training set Test set\nDigitisation 53 6\nEconomy 4,843 628\nEducation 1,379 126\nFinances 1,309 79\nForeign Policy 8,638 969\nHealthcare 925 79\nImmigration 3,881 455\nInfrastructure & Environment 3,132 473\nPolitical System 5,087 563\nSecurity 7,175 883\nSociety 4,077 709\nWelfare 1,715 178\nAbout COVID-19 16,994 2,414\nNot about COVID-19 25,220 2,734\nTable 2: Predicted topics of the articles\nmately 40% of all articles are about COVID-19, as\nillustrated in the bottom rows of Table 2.\nWe publish the data set as a list of URLs and\ncorresponding labels. Due to copyright issues, we\nare unable to make available the full texts.\n4 Methodology\nIn this section we describe the different (feature)\nrepresentations of the data we use to train different\nclassi\ufb01cation models on as well as our attempts to\nalleviate the class imbalance problem (Table 1).\n4.1 Features\nBag-Of-Words Bag-of-Words (BOW) repre-\nsents the text sequence as a vector of jVjfeatures\nwithVbeing the vocabulary size. Each feature\nvalue contains the frequency of the word associ-\nated with the position in the vector in the input text.\nThe vocabulary is based on the training data.\nTF-IDF Term-Frequency times Inverse-\nDocument-Frequency (TF-IDF) differs from BOW\nin that it takes into account the frequency of terms\nin the entire corpus (the training data, in our case).\nIn addition to its popularity in all kinds of IR and\nNLP tasks, TF-IDF has recently been used in hate\nspeech detection tasks (Salminen et al., 2019).\nBERT Since its introduction, BERT (Devlin\net al., 2019), has been used in many NLP tasks. We\nuse the German BERT base model from the Hug-\nging Face Transformers library13. We adopt the\n\ufb01ne-tuning strategy from (Salminen et al., 2020):\n\ufb01rst, we \ufb01ne-tune the BertForSequenceClassi\ufb01ca-\ntion model, consisting of BERT\u2019s model and a lin-\near softmax activation layer. After training, we\n13https://huggingface.co/bert-base-german-cased\n126drop the softmax activation layer and use BERT\u2019s\nhidden state as the feature vector, which we then\nuse as input for different classi\ufb01cation algorithms.\n4.2 Models\nLogistic Regression We use logistic regression\nas our \ufb01rst and relatively straightforward method,\nmotivated by its popularity for text classi\ufb01cation.\nWe add L2 regularization to the cross-Entropy loss\nand optimize it using Stochastic Average Gradient\n(SAGA) (Defazio et al., 2014).\nNaive Bayes Equally popular in text classi\ufb01ca-\ntion, Naive Bayes is based on the conditional in-\ndependence assumption. We model BOW and TF-\nIDF features as random variables distributed ac-\ncording to the multinomial distribution with Lid-\nstone smoothing. BERT features are modeled as\nGaussian random variables.\nRandom Forest Random Forest is an ensemble\nalgorithm using decision tree models. The random\nselection of features and instances allows reduc-\ntion of the model\u2019s variance and co-adaptation of\nthe models. To handle class imbalance we use\nthe Weighted Random Forest method (Chen and\nBreiman, 2004). This changes the weights assigned\nto each class when calculating the impurity score\nat the split point, penalises mis-classi\ufb01cation of the\nminority classes and reduces the majority bias.\nEasyEnsemble EasyEnsemble is another ensem-\nble method targeting the class imbalance problem\n(Liu et al., 2009). It creates balanced training sam-\nples by taking all examples from the minority class\nand randomly selecting examples from the major-\nity class, after which AdaBoost (Schapire, 1999) is\napplied to the re-sampled data.\n5 Evaluation\n5.1 Hyperpartisan News Detection Data\nFor benchmarking purposes, we \ufb01rst apply our\nmodels to the 2019 Hyperpartisan News Detection\ntask. This data set uses binary labels as opposed\nto our 5-point scale. Since the 2019 shared task\nused TIRA (Potthast et al., 2019), the organisers\nrequested submission of functioning code and ran\nthe evaluation on a dedicated machine to which the\nshared task participants did not have access. The\ntest set used in the shared task was notpublished\nand even after submission deadline has not been\nmade publicly available. As a consequence, weuse the validation set to produce our scores on the\ndata. This renders a direct comparison impossi-\nble. To provide an estimate of our performance,\nwe include Table 3, which lists the top 3 systems\nparticipating in the task. As illustrated by the row\nTF-IDF+Naive Bayes (our best-performing setup\non this data set), we achieve a considerably lower\naccuracy score, but a comparable macro F 1-score.\nThe performance of the other setups is shown in\nTable 3. BERT+Logistic Regression scored just\nslightly worse than TF-IDF+Naive Bayes, with a\nprecision score that is one point lower.\n5.2 German Data Set\nWe apply the models to our own data. The results\nare shown in Table 5 for accuracy and in Table 6\nfor macro-averaged F 1-score. The per-class per-\nformance is shown in Table 7, which, in addition,\ncontains performance when binarising our labels\n(the last three rows) to compare this to the 2019\nshared task data and to provide an idea of the differ-\nence in performance when using more \ufb01ne-grained\nlabels. We assume articles with the labels Far-left\nand Far-right to be hyperpartisan, and label all other\narticles as non-hyperpartisan. The accuracy for bi-\nnary classi\ufb01cation (not listed in Table 7) was 86%,\ncompared to 43% (Naive Bayes+BOW in Table 5)\nfor 5-class classi\ufb01cation.\nFrom the results we can conclude the follow-\ning. First, class imbalance poses a serious problem,\nthough some setups suffer from this more than oth-\ners. Linear Regression, on all different features,\nperformed poorly on the Far-left articles. We as-\nsume this is due to the small number of Far-left\narticles (215 in the test set, 1,146 in the training\nset) and publishers (one in the test set, two in the\ntraining set). Despite the high degree of class imbal-\nance, the EasyEnsemble method, designed to target\nthis problem particularly, does not outperform the\nothers with any of the different feature sets. Sec-\nond, BERT features scored surprisingly low with\nall classi\ufb01cation models. Overall, we can conclude\nthat the two best-performing setups that show both\nhigh accuracy and F 1-score are BOW+Naive Bayes\nand TF-IDF+Random Forest features. Table 7 in-\ncludes the scores for TF-IDF+Random Forest, our\nbest-performing setup.\n6 Discussion\nIn many NLP tasks, the strategy of using BERT as\na language model that is \ufb01ne-tuned to a speci\ufb01c\n127Team Rank Accuracy Precision Recall F 1\ntintin 1 0.70 0.74 0.63 0.68\njoseph-rouletabille 2 0.68 0.64 0.83 0.72\nbrenda-starr 3 0.66 0.63 0.81 0.71\nTF-IDF + Naive Bayes (ours) n. a. 0.58 0.55 0.84 0.67\nTable 3: Our best performing setup (TF-IDF + Naive Bayes) on the 2019 Hyperpartisan News Detection validation\nset compared to the top 3 systems of the 2019 Hyperpartisan News Detection task on the by-publisher test set.\nModel Accuracy Precision Recall F 1\nBOW + Random Forest 0.51 0.51 0.59 0.55\nBOW + Naive Bayes 0.57 0.54 0.81 0.65\nTF-IDF + Random Forest 0.52 0.51 0.59 0.55\nTF-IDF + Naive Bayes 0.58 0.55 0.85 0.67\nBERT + Logistic Regression 0.58 0.55 0.84 0.66\nBERT + Logistic Regression (10%) 0.56 0.54 0.85 0.66\nTable 4: Results of our setups on the 2019 Hyperpartisan News Detection task (by-publisher validation set).\nModel BOW TF-IDF BERT\nLogistic Regression 0.4289 0.4472 0.4202\nNaive Bayes 0.4304 0.4021 0.4188\nRandom Forest 0.3980 0.4258 0.4320\nEasyEnsemble 0.3811 0.3798 0.3646\nTable 5: Accuracy for different features and classi\ufb01ca-\ntion methods\nModel BOW TF-IDF BERT\nLogistic Regression 0.3132 0.2621 0.3389\nNaive Bayes 0.4243 0.2234 0.3637\nRandom Forest 0.4007 0.4303 0.3836\nEasyEnsemble 0.4197 0.4070 0.3432\nTable 6: Macro-averaged F 1-measure for different fea-\ntures and classi\ufb01cation methods\ntask, has recently been shown to exhibit signi\ufb01-\ncant improvements over previously used methods\nand models, such as Naive Bayes and Random\nForest. To determine why our BERT-based setups\ndid not outperform the others, we investigated the\nimpact of training data volume. We trained the\nBERT+Logistic Regression setup on only 10% of\nthe original training data of the 2019 setup ex-\nplained earlier and evaluated it on the same test\nsetup (i. e., the validation set in the 2019 shared\ntask). As illustrated by the last row in Table 4,\nthe accuracy dropped by only 2% and F 1-score re-\nmained the same, suggesting that data volume has\nrelatively little impact.\nTo further analyse our results, we examined the\nattention scores of the \ufb01rst BERT layer and selected\nthe ten tokens BERT paid most attention to for ev-Class Precision Recall F 1Support\nFar-left 0.59 0.40 0.48 215\nCentre-left 0.34 0.38 0.36 1,159\nCentre 0.31 0.23 0.27 1,349\nCentre-right 0.51 0.55 0.53 1,754\nFar-right 0.46 0.58 0.51 671\nTotal 0.44 0.43 0.43 5,148\nHyperpartisan 0.56 0.81 0.66 886\nNon-hyperpartisan 0.96 0.87 0.87 4262\nTotal 0.76 0.84 0.79 5,148\nTable 7: Experimental results for TF-IDF+Random\nForest, per class for political bias and hyperpartisan\nclassi\ufb01cation.\nery article. We then combined adjacent tokens and\n\ufb01nished non-complete words (with their most likely\ncandidate) to determine the key phrases of the text\nthat the model used for classi\ufb01cation. We repeated\nthis procedure on all hyperpartisan articles (i. e.,\nFar-left and Far-right) and derived a list of words\nand phrases that the model paid most attention to.\nThe result is shown in Table 8.\nThe question whether or not attention can be\nused for explaining a model\u2019s prediction is still un-\nder discussion (Jain and Wallace, 2019; Wiegreffe\nand Pinter, 2019). Note that with Table 8, we at-\ntempt to gain insight into how words are used to\nconstruct BERT embeddings, and not necessarily\nwhich words are used for prediction.\nThe lists of words show that the majority of\nwords for the Far-left classi\ufb01cation are neither ex-\nclusively nor mainly used by left-wing news media\nin general, e. g., wirkt (works), seither (since) or\nGeliebte (beloved, lover). An exception is antisemi-\n128Far-left Far-right\nwirkt Checklisten\nneunziger Willkommenskultur\nHungernden Wohlverhaltensvorschriften\nantisemitische Alltagsgebrauch\nSeither Tichys [Einblick]\nGeliebte Witz\nPlausch Islam\nbiologistischen Gutmenschen\nSahelzone korrekte\nundurchsichtige Diversity\nTable 8: The top ten words most indicative of Far-left or\nFar-right content according to BERT\u2019s attention scores.\ntische (anti-semitic), with anti-semitism in society\nbeing a common topic in left-wing media. Other\nhighlighted words are likely to be related to the\ntopic of refugee migration and its causes, such as\nHungernden (hungry people) and Sahelzone (Sa-\nhel), an area known for its con\ufb02icts and current\nsocietal challenges. In contrast to the words we\nidenti\ufb01ed for the Far-left, we found most of the\nwords we identi\ufb01ed for the Far-right to be more\ndescriptive of this side of the political spectrum.\nNearly all words listed under Far-right in Table 8\nare typically either used sarcastically or in a highly\ncritical manner in typical right-wing media out-\nlets. For example, Willkommenskultur (welcoming\nculture) is a German compound describing a wel-\ncoming and positive attitude towards immigrants,\nwhich is often mocked and criticised by the far\nright. Another example is Gutmensch (of which\nGutmenschen is the plural), a term mainly used by\nthe right as an ironic or contemptuous denigration\nof individuals or groups that strive to be \u2018politi-\ncally correct\u2019. Another word in the right column\nof Table 8 is Tichys , referring to the blog and print\nmagazine Tichys Einblick . This news magazine\ncalls itself a platform for authors of the liberal and\nconservative spectrum but is considered by some\nobservers to be a highly controversial right-wing\nmagazine with neo-liberal tendencies.14Since we\nmade sure that the training data publishers and test\ndata publishers are disjoint sets, this cannot be a\ncase of publisher identity still being present in the\ntext and the model over-\ufb01tting to this. Upon closer\ninvestigation, we found15that indeed, many other\npublishers refer to Tichy\u2019s Einblick , and these were\npredominantly publishers with the Far-right label.\n14https://www.politico.eu/article/new-conservative-magaz\nine-takes-on-angela-merkel-and-the-media-roland-tichy-ti\nchys-einblick/ (last visited: March 21, 2021).\n15Through simple string search on \u201cTichy\u201d in the articles.Generally, entries in Table 8 (for both the Far-left\nand Far-right columns) in italics are those we con-\nsider indicative of their particular position on the\npolitical spectrum. Some words on the right side\nare in themselves neutral but often used by right-\nwing media with a negative connotation, which is\nwhy we italicised them, too (e. g., Islam ,Diversity ).\n7 Conclusion and Future Work\nWe present a collection of German news articles\nlabeled for political bias in a semi-supervised way,\nby exploiting the results of a survey on the politi-\ncal af\ufb01liation of a list of prominent German news\noutlets.16This data set extends on earlier work\non political bias classi\ufb01cation by including a more\n\ufb01ne-grained set of labels, and by allowing for re-\nsearch on political bias in German articles. We\npropose various classi\ufb01cation setups that we eval-\nuate on existing data for benchmarking purposes,\nand then apply to our own data set. Our results\nshow that political bias classi\ufb01cation is very chal-\nlenging, especially when assuming a non-binary\nset of labels. When using a more \ufb01ne-grained label\nset, we demonstrate that performance drops by 36\npoints in accuracy, from 79 in the binary case to 43\nin the more \ufb01ne-grained setup.\nPolitical orientation plays a role in the detection\nof hate speech and online abuse (along with other\ndimensions, such as gender and race). By making\navailable more data sets, in different languages, and\nusing as many different publishers as possible (our\nresults validate earlier \ufb01ndings that models quickly\nover-\ufb01t to particular publisher identity features),\nwe contribute to uncovering and making transpar-\nent political bias of online content, which in turn\ncontributes to the cause of detecting hate speech\nand abusive language (Bourgonje et al., 2018).\nWhile labeling articles by publisher has the ob-\nvious advantage of producing a larger number of\nlabeled instances more quickly, critical investiga-\ntion and large-scale labeling of individual articles\nmust be an important direction of future work.\nAcknowledgments\nThis work has received funding from the Ger-\nman Federal Ministry of Education and Re-\nsearch (BMBF) through the projects QURATOR\n(no. 03WKDA1A, https://qurator.ai) and PAN-\nQURA (no. 03COV03E).\n16The URLs of the documents in our data set and the labels\ncan be found at https://github.com/axenov/politik-news.\n129References\nSeyi Akiwowo, Bertie Vidgen, Vinodkumar Prab-\nhakaran, and Zeerak Waseem, editors. 2020. Pro-\nceedings of the Fourth Workshop on Online Abuse\nand Harms . Association for Computational Linguis-\ntics, Online.\nRamy Baly, Giovanni Da San Martino, James Glass,\nand Preslav Nakov. 2020. We can detect your bias:\nPredicting the political ideology of news articles. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) ,\npages 4982\u20134991, Online. Association for Computa-\ntional Linguistics.\nValerio Basile, Cristina Bosco, Elisabetta Fersini,\nDebora Nozza, Viviana Patti, Francisco Manuel\nRangel Pardo, Paolo Rosso, and Manuela San-\nguinetti. 2019. SemEval-2019 task 5: Multilin-\ngual detection of hate speech against immigrants and\nwomen in Twitter. In Proceedings of the 13th Inter-\nnational Workshop on Semantic Evaluation , pages\n54\u201363, Minneapolis, Minnesota, USA. Association\nfor Computational Linguistics.\nFelix Bie\u00dfmann. 2016. Automating political bias pre-\ndiction. CoRR , abs/1608.02195.\nPeter Bourgonje, Juli\u00e1n Moreno Schneider, and Georg\nRehm. 2018. Automatic Classi\ufb01cation of Abusive\nLanguage and Personal Attacks in Various Forms of\nOnline Communication. In Language Technologies\nfor the Challenges of the Digital Age: 27th Inter-\nnational Conference, GSCL 2017, Berlin, Germany,\nSeptember 13-14, 2017, Proceedings , number 10713\nin Lecture Notes in Arti\ufb01cial Intelligence (LNAI),\npages 180\u2013191, Cham, Switzerland. Gesellschaft\nf\u00fcr Sprachtechnologie und Computerlinguistik e.V .,\nSpringer. 13/14 September 2017.\nSian Brooke. 2019. \u201ccondescending, rude, assholes\u201d:\nFraming gender and hostility on Stack Over\ufb02ow. In\nProceedings of the Third Workshop on Abusive Lan-\nguage Online , pages 172\u2013180, Florence, Italy. Asso-\nciation for Computational Linguistics.\nChao Chen and Leo Breiman. 2004. Using random for-\nest to learn imbalanced data. University of Califor-\nnia, Berkeley .\nIsobelle Clarke and Jack Grieve. 2017. Dimensions of\nabusive language on Twitter. In Proceedings of the\nFirst Workshop on Abusive Language Online , pages\n1\u201310, Vancouver, BC, Canada. Association for Com-\nputational Linguistics.\nRobert Dale. 2017. NLP in a post-truth world. Natural\nLanguage Engineering , 23(2):319\u2013324.\nThomas Davidson, Debasmita Bhattacharya, and Ing-\nmar Weber. 2019. Racial bias in hate speech and\nabusive language detection datasets. In Proceedings\nof the Third Workshop on Abusive Language Online ,\npages 25\u201335, Florence, Italy. Association for Com-\nputational Linguistics.Aaron Defazio, Francis Bach, and Simon Lacoste-\nJulien. 2014. Saga: A fast incremental gradient\nmethod with support for non-strongly convex com-\nposite objectives.\nLeon Derczynski and Kalina Bontcheva. 2014. Pheme:\nVeracity in digital social networks. In Posters, De-\nmos, Late-breaking Results and Workshop Proceed-\nings of the 22nd Conference on User Modeling,\nAdaptation, and Personalization co-located with the\n22nd Conference on User Modeling, Adaptation,\nand Personalization (UMAP2014), Aalborg, Den-\nmark, July 7-11, 2014 , volume 1181 of CEUR Work-\nshop Proceedings . CEUR-WS.org.\nLeon Derczynski, Kalina Bontcheva, Maria Liakata,\nRob Procter, Geraldine Wong Sak Hoi, and Arkaitz\nZubiaga. 2017. SemEval-2017 task 8: RumourEval:\nDetermining rumour veracity and support for ru-\nmours. In Proceedings of the 11th International\nWorkshop on Semantic Evaluation (SemEval-2017) ,\npages 69\u201376, Vancouver, Canada. Association for\nComputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171\u20134186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nDarja Fi\u0161er, Ruihong Huang, Vinodkumar Prabhakaran,\nRob V oigt, Zeerak Waseem, and Jacqueline Werni-\nmont, editors. 2018. Proceedings of the 2nd Work-\nshop on Abusive Language Online (ALW2) . Associ-\nation for Computational Linguistics, Brussels, Bel-\ngium.\nRama Rohit Reddy Gangula, Suma Reddy Duggen-\npudi, and Radhika Mamidi. 2019. Detecting polit-\nical bias in news articles using headline attention.\nInProceedings of the 2019 ACL Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 77\u201384, Florence, Italy. Asso-\nciation for Computational Linguistics.\nGenevieve Gorrell, Elena Kochkina, Maria Liakata,\nAhmet Aker, Arkaitz Zubiaga, Kalina Bontcheva,\nand Leon Derczynski. 2019. SemEval-2019 task 7:\nRumourEval, determining rumour veracity and sup-\nport for rumours. In Proceedings of the 13th Inter-\nnational Workshop on Semantic Evaluation , pages\n845\u2013854, Minneapolis, Minnesota, USA. Associa-\ntion for Computational Linguistics.\nFelix Hamborg, Karsten Donnay, and Bela Gipp. 2018.\nAutomated identi\ufb01cation of media bias in news ar-\nticles: An interdisciplinary literature review. Inter-\nnational Journal on Digital Libraries (IJDL) , pages\n391\u2013415.\nFelix Hamborg, Anastasia Zhukova, Karsten Donnay,\nand Bela Gipp. 2020. Newsalyze: Enabling news\n130consumers to understand media bias. In Proceed-\nings of the ACM/IEEE Joint Conference on Digital\nLibraries in 2020 , JCDL \u201920, page 455\u2013456, New\nYork, NY , USA. Association for Computing Machin-\nery.\nAndreas Hanselowski, Avinesh PVS, Benjamin\nSchiller, Felix Caspelherr, Debanjan Chaudhuri,\nChristian M. Meyer, and Iryna Gurevych. 2018. A\nretrospective analysis of the fake news challenge\nstance-detection task. In Proceedings of the 27th\nInternational Conference on Computational Lin-\nguistics , pages 1859\u20131874, Santa Fe, New Mexico,\nUSA. Association for Computational Linguistics.\nMohit Iyyer, Peter Enns, Jordan Boyd-Graber, and\nPhilip Resnik. 2014. Political ideology detection us-\ning recursive neural networks. In Proceedings of the\n52nd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) , pages\n1113\u20131122, Baltimore, Maryland. Association for\nComputational Linguistics.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers) , pages 3543\u20133556, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nShan Jiang, Ronald E. Robertson, and Christo Wilson.\n2020. Reasoning about political bias in content mod-\neration. Proceedings of the AAAI Conference on Ar-\nti\ufb01cial Intelligence , 34(09):13669\u201313672.\nJohannes Kiesel, Maria Mestre, Rishabh Shukla, Em-\nmanuel Vincent, Payam Adineh, David Corney,\nBenno Stein, and Martin Potthast. 2019. SemEval-\n2019 task 4: Hyperpartisan news detection. In\nProceedings of the 13th International Workshop on\nSemantic Evaluation , pages 829\u2013839, Minneapo-\nlis, Minnesota, USA. Association for Computational\nLinguistics.\nDilek K\u00fc\u00e7\u00fck and Fazli Can. 2020. Stance detection: A\nsurvey. ACM Computing Surveys , 53(1).\nVivek Kulkarni, Junting Ye, Steve Skiena, and\nWilliam Yang Wang. 2018. Multi-view models for\npolitical ideology detection of news articles. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 3518\u2013\n3527, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nChang Li and Dan Goldwasser. 2019. Encoding so-\ncial information with graph convolutional networks\nforPolitical perspective detection in news media.\nInProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n2594\u20132604, Florence, Italy. Association for Compu-\ntational Linguistics.X. Liu, J. Wu, and Z. Zhou. 2009. Exploratory under-\nsampling for class-imbalance learning. IEEE Trans-\nactions on Systems, Man, and Cybernetics, Part B\n(Cybernetics) , 39(2):539\u2013550.\nTsvetomila Mihaylova, Georgi Karadzhov, Pepa\nAtanasova, Ramy Baly, Mitra Mohtarami, and\nPreslav Nakov. 2019. SemEval-2019 task 8: Fact\nchecking in community question answering forums.\nInProceedings of the 13th International Workshop\non Semantic Evaluation , pages 860\u2013869, Minneapo-\nlis, Minnesota, USA. Association for Computational\nLinguistics.\nSaif Mohammad, Svetlana Kiritchenko, Parinaz Sob-\nhani, Xiaodan Zhu, and Colin Cherry. 2016.\nSemEval-2016 task 6: Detecting stance in tweets.\nInProceedings of the 10th International Workshop\non Semantic Evaluation (SemEval-2016) , pages 31\u2013\n41, San Diego, California. Association for Computa-\ntional Linguistics.\nNic Newman, Richard Fletcher, David A. L. Levy, and\nRasmus Kleis Nielsen. 2016. Reuters institute digi-\ntal news report.\nMartin Potthast, Tim Gollub, Matti Wiegmann, and\nBenno Stein. 2019. TIRA integrated research archi-\ntecture. In Nicola Ferro and Carol Peters, editors, In-\nformation Retrieval Evaluation in a Changing World\n- Lessons Learned from 20 Years of CLEF , volume 41\nofThe Information Retrieval Series , pages 123\u2013160.\nSpringer.\nGeorg Rehm. 2018. An Infrastructure for Empowering\nInternet Users to handle Fake News and other On-\nline Media Phenomena. In Language Technologies\nfor the Challenges of the Digital Age: 27th Inter-\nnational Conference, GSCL 2017, Berlin, Germany,\nSeptember 13-14, 2017, Proceedings , number 10713\nin Lecture Notes in Arti\ufb01cial Intelligence (LNAI),\npages 216\u2013231, Cham, Switzerland. Gesellschaft\nf\u00fcr Sprachtechnologie und Computerlinguistik e.V .,\nSpringer. 13/14 September 2017.\nGeorg Rehm, Peter Bourgonje, Stefanie Hegele, Flo-\nrian Kintzel, Juli\u00e1n Moreno Schneider, Malte Os-\ntendorff, Karolina Zaczynska, Armin Berger, Ste-\nfan Grill, S\u00f6ren R\u00e4uchle, Jens Rauenbusch, Lisa\nRutenburg, Andr\u00e9 Schmidt, Mikka Wild, Henry\nHoffmann, Julian Fink, Sarah Schulz, Jurica Seva,\nJoachim Quantz, Joachim B\u00f6ttger, Jose\ufb01ne Matthey,\nRolf Fricke, Jan Thomsen, Adrian Paschke, Ja-\nmal Al Qundus, Thomas Hoppe, Naouel Karam,\nFrauke Weichhardt, Christian Fillies, Clemens\nNeudecker, Mike Gerber, Kai Labusch, Vahid\nRezanezhad, Robin Schaefer, David Zellh\u00f6fer,\nDaniel Siewert, Patrick Bunk, Lydia Pintscher,\nElena Aleynikova, and Franziska Heine. 2020.\nQURATOR: Innovative Technologies for Content\nand Data Curation. In Proceedings of QURATOR\n2020 \u2013 The conference for intelligent content solu-\ntions , Berlin, Germany. CEUR Workshop Proceed-\nings, V olume 2535. 20/21 January 2020.\n131Sarah T. Roberts, Joel Tetreault, Vinodkumar Prab-\nhakaran, and Zeerak Waseem, editors. 2019. Pro-\nceedings of the Third Workshop on Abusive Lan-\nguage Online . Association for Computational Lin-\nguistics, Florence, Italy.\nJoni Salminen, Hind Almerekhi, Milica Milenkovic,\nSoon-Gyo Jung, Jisun An, Haewoon Kwak, and Jim\nJansen. 2019. Anatomy of online hate: Developing\na taxonomy and machine learning models for identi-\nfying and classifying hate in online news media.\nJoni Salminen, Maximilian Hopf, S. A. Chowdhury,\nSoon-Gyo Jung, H. Almerekhi, and Bernard J.\nJansen. 2020. Developing an online hate classi\ufb01er\nfor multiple social media platforms. Human-centric\nComputing and Information Sciences , 10:1\u201334.\nEstela Saquete, David Tom\u00e1s, Paloma Moreda, Patricio\nMart\u00ednez-Barco, and Manuel Palomar. 2020. Fight-\ning post-truth using natural language processing: A\nreview and open challenges. Expert Systems with\nApplications , 141:112943.\nRobert E. Schapire. 1999. A brief introduction to boost-\ning. In Proceedings of the 16th International Joint\nConference on Arti\ufb01cial Intelligence - Volume 2 , IJ-\nCAI\u201999, page 1401\u20131406, San Francisco, CA, USA.\nMorgan Kaufmann Publishers Inc.\nParinaz Sobhani, Diana Inkpen, and Xiaodan Zhu.\n2017. A dataset for multi-target stance detection. In\nProceedings of the 15th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Volume 2, Short Papers , pages 551\u2013557,\nValencia, Spain. Association for Computational Lin-\nguistics.\nJulia Maria Stru\u00df, Melanie Siegel, Josef Ruppenhofer,\nMichael Wiegand, and Manfred Klenner. 2019.\nOverview of germeval task 2, 2019 shared task\non the identi\ufb01cation of offensive language. Pre-\nliminary proceedings of the 15th Conference on\nNatural Language Processing (KONVENS 2019),\nOctober 9 \u2013 11, 2019 at Friedrich-Alexander-\nUniversit\u00e4t Erlangen-N\u00fcrnberg, pages 352 \u2013 363,\nM\u00fcnchen [u.a.]. German Society for Computational\nLinguistics & Language Technology und Friedrich-\nAlexander-Universit\u00e4t Erlangen-N\u00fcrnberg.\nMatt Thomas, Bo Pang, and Lillian Lee. 2006. Get out\nthe vote: Determining support or opposition from\ncongressional \ufb02oor-debate transcripts. In Proceed-\nings of the 2006 Conference on Empirical Methods\nin Natural Language Processing , pages 327\u2013335,\nSydney, Australia. Association for Computational\nLinguistics.\nJames Thorne, Andreas Vlachos, Oana Cocarascu,\nChristos Christodoulopoulos, and Arpit Mittal. 2019.\nThe FEVER2.0 shared task. In Proceedings of the\nSecond Workshop on Fact Extraction and VERi\ufb01ca-\ntion (FEVER) , pages 1\u20136, Hong Kong, China. Asso-\nciation for Computational Linguistics.Jannis Vamvas and Rico Sennrich. 2020. X-stance: A\nmultilingual multi-target dataset for stance detection.\nCoRR , abs/2003.08385.\nBertie Vidgen and Leon Derczynski. 2021. Direc-\ntions in abusive language training data, a system-\natic review: Garbage in, garbage out. PLOS ONE ,\n15(12):1\u201332.\nBertie Vidgen, Scott Hale, Ella Guest, Helen Mar-\ngetts, David Broniatowski, Zeerak Waseem, Austin\nBotelho, Matthew Hall, and Rebekah Tromble. 2020.\nDetecting East Asian prejudice on social media.\nInProceedings of the Fourth Workshop on Online\nAbuse and Harms , pages 162\u2013172, Online. Associa-\ntion for Computational Linguistics.\nEmmanuel Vincent and Maria Mestre. 2018. Crowd-\nsourced measure of news articles bias: Assessing\ncontributors\u2019 reliability. In Proceedings of the 1st\nWorkshop on Subjectivity, Ambiguity and Disagree-\nment in Crowdsourcing, and Short Paper Proceed-\nings of the 1st Workshop on Disentangling the Rela-\ntion Between Crowdsourcing and Bias Management\n(SAD 2018 and CrowdBias 2018) co-located the\n6th AAAI Conference on Human Computation and\nCrowdsourcing (HCOMP 2018), Z\u00fcrich, Switzer-\nland, July 5, 2018 , volume 2276 of CEUR Workshop\nProceedings , pages 1\u201310. CEUR-WS.org.\nZeerak Waseem, Wendy Hui Kyong Chung, Dirk Hovy,\nand Joel Tetreault, editors. 2017. Proceedings of\nthe First Workshop on Abusive Language Online . As-\nsociation for Computational Linguistics, Vancouver,\nBC, Canada.\nMaximilian Wich, Jan Bauer, and Georg Groh. 2020.\nImpact of politically biased data on hate speech clas-\nsi\ufb01cation. In Proceedings of the Fourth Workshop\non Online Abuse and Harms , pages 54\u201364, Online.\nAssociation for Computational Linguistics.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is\nnot not explanation. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP) , pages 11\u201320, Hong Kong, China. Associ-\nation for Computational Linguistics.\nMarcos Zampieri, Shervin Malmasi, Preslav Nakov,\nSara Rosenthal, Noura Farra, and Ritesh Kumar.\n2019. SemEval-2019 task 6: Identifying and catego-\nrizing offensive language in social media (OffensE-\nval). In Proceedings of the 13th International Work-\nshop on Semantic Evaluation , pages 75\u201386, Min-\nneapolis, Minnesota, USA. Association for Compu-\ntational Linguistics.\nXiang Zhou, Heba Elfardy, Christos Christodoulopou-\nlos, Thomas Butler, and Mohit Bansal. 2021. Hid-\nden biases in unreliable news detection datasets. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 2482\u20132492, Online.\nAssociation for Computational Linguistics.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Fine-grained classification of political bias in German news: A data set and initial experiments", "author": ["D Aksenov", "P Bourgonje", "K Zaczynska"], "pub_year": "2021", "venue": "Proceedings of the \u2026", "abstract": "We present a data set consisting of German news articles labeled for political bias on a five-point  scale in a semi-supervised way. While earlier work on hyperpartisan news detection"}, "filled": false, "gsrank": 324, "pub_url": "https://aclanthology.org/2021.woah-1.13/", "author_id": ["2Y-S87oAAAAJ", "kcfd6_0AAAAJ", "enTq0qQAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:up1jvCCWC0kJ:scholar.google.com/&output=cite&scirp=323&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D320%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=up1jvCCWC0kJ&ei=PrWsaIaUD7TWieoP1pCJ2AY&json=", "num_citations": 33, "citedby_url": "/scholar?cites=5263465656856321466&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:up1jvCCWC0kJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://aclanthology.org/2021.woah-1.13.pdf"}}, {"title": "Language-Agnostic Modeling of Source Reliability on Wikipedia", "year": "2024", "pdf_data": "Language-Agnostic Modeling of Source Reliability on Wikipedia\nJacopo D\u2019Ignazi\nISI Foundation\nTurin, Italy\nUniversitat Pompeu Fabra\nBarcelona, Spain\njacopo.dignazi@upf.eduAndreas Kaltenbrunner\nUniversitat Oberta de Catalunya\nBarcelona, Spain\nISI Foundation\nTurin, ItalyYelena Mejova\nMichele Tizzani\nKyriaki Kalimeri\nISI Foundation\nTurin, Italy\nMariano Beir\u00f3\nUniversidad de San Andr\u00e9s\nCONICET\nBuenos Aires, ArgentinaPablo Arag\u00f3n\nUniversitat Pompeu Fabra\nBarcelona, Spain\nABSTRACT\nOver the last few years, content verification through reliable sources\nhas become a fundamental need to combat disinformation. Here, we\npresent a language-agnostic model designed to assess the reliability\nof sources across multiple language editions of Wikipedia. Utiliz-\ning editorial activity data, the model evaluates source reliability\nwithin different articles of varying controversiality such as Climate\nChange, COVID-19, History, Media, and Biology topics. Crafting\nfeatures that express domain usage across articles, the model effec-\ntively predicts source reliability, achieving an F1 Macro score of\napproximately 0.80 for English and other high-resource languages.\nFor mid-resource languages, we achieve 0.65 while the performance\nof low-resource languages varies; in all cases, the time the domain\nremains present in the articles (which we dub as permanence ) is\none of the most predictive features. We highlight the challenge\nof maintaining consistent model performance across languages of\nvarying resource levels and demonstrate that adapting models from\nhigher-resource languages can improve performance. This work\ncontributes not only to Wikipedia\u2019s efforts in ensuring content ver-\nifiability but in ensuring reliability across diverse user-generated\ncontent in various language communities.\nCCS CONCEPTS\n\u2022Information systems \u2192Wikis ;\u2022Computing methodologies\n\u2192Machine learning algorithms .\nKEYWORDS\nWikipedia, knowledge equity, knowledge integrity, information\nreliability\n1 INTRODUCTION\nThe proliferation of fake news in recent years is deteriorating the\nreliability and trustworthiness of the online information ecosys-\ntem [ 11,36]. To mitigate their spread and impact in society, re-\nsearchers on web and data mining are devoting a great deal of\neffort in generating resources to detect untrue content on social\nmedia platforms [2, 15, 32, 40\u201343].\nIn popular web platforms like Wikipedia1, disinformation is also\na main threat to knowledge integrity [ 1]. Wikipedia\u2019s conception\n1Wikipedia is among the top visited websites worldwide in 2024: https://www.semrush.\ncom/trending-websites/global/all.of knowledge as a service requires content to be verifiable in order\nto preserve its reliability and integrity2. Thus, the platform\u2019s policy\non verifiability implies that readers should be able to check that all\nthe information in Wikipedia articles comes from reliable sources3.\nWhile a classical patrolling technique in this platform is to detect\nand remove statements that violate basic core content policies [ 22],\nsome Wikipedia editors have proven more effective in tackling\ndisinformation by first identifying unreliable sources [ 9]. Recent\nresearch revealed the positive impact of the community-curated\nlist of perennial sources in English Wikipedia [ 3]. However, that\nlist is limited or even non-existent in other language editions [4].\nTo address this challenge, we present our research on source\nreliability, based on language-agnostic approaches and edit activity\ndata from multiple Wikipedia language editions. We aim to sup-\nport Wikipedia editors, including the ones in medium and small\nprojects that often miss advanced tools, in identifying sources with\npatterns associated with low reliability, and in monitoring their\nprevalence across language editions. To meet this goal, we address\nthe following research questions:\n\u2022RQ1 . What are the most predictive language-agnostic fea-\ntures when modeling source reliability in Wikipedia, consid-\nering the Climate Change topic in English as a case study?\n\u2022RQ2 . To what extent can model performance be related to\nthe topic and size of a language edition of Wikipedia?\n\u2022RQ3 . Is it possible to adapt these models across topics and/or\nlanguages?\nOur methods to answer these questions are inspired by exist-\ning language-agnostic approaches to measure the controversial-\nity of linked elements in a given Wikipedia article [ 6]. Results\nwith datasets from different topics and language editions suggest a\npromising scenario. Given the several limitations outlined in the\nfollowing sections, we pay attention to model performance not only\nfor the largest editions but also in mid- and low-resource settings,\nwhere adaptation of the models may provide the most benefit. To\nour knowledge, this is the first work modeling source reliability on\nWikipedia using only language-agnostic features.\n2https://research.wikimedia.org/knowledge-integrity.html\n3https://en.wikipedia.org/wiki/Wikipedia:Verifiability.arXiv:2410.18803v2  [cs.SI]  14 Jan 2025\n2 RELATED WORK\nSeveral research works have assessed the verifiability of Wikipedia\ncontent and the reliability of the sources by analysing, e.g., the\nquality of references, the appropriateness of links between articles,\nand the consensus between editors regarding this content.\nThe Contropedia project [ 6], for example, aimed at identifying\nand visualizing controversial links between Wikipedia articles, i.e.,\nlinks that point to other articles in the platform and whose collabo-\nrative edit history shows signs of dispute between editors before\nreaching consensus. Furthermore, several tools provide interactive\nvisualizations that help understand the path to consensus for a\nWikipedia article [ 6,16]. Cultural differences in the controversiality\nof specific topics have been shown to exist (e.g., [13]).\nThe Citation Detective tool [ 7] leveraged machine learning mod-\nels [28] to assess the evolution of reference quality between 2010\nand 2020, in terms of the ratio of sentences missing a required ci-\ntation, and the ratio of citations that reference non-authoritative\nsources. The authors found out that, thanks to the efforts of the com-\nmunity, the ratio of sentences missing a required citation was signif-\nicantly reduced, while the ratio of references to non-authoritative\nsources has remained below 1%. More recently, [ 27] has proposed\na neural-network-based system to improve the verifiability of con-\ntent.\nThe observed improvements in reference quality coincide with\nthe introduction of the perennial source list [38], a curated, non-\nexhaustive list of sources whose reliability and use on Wikipedia\nare frequently discussed. Though the perennial source list has been\nused by several works to assess the authoritativeness of sources,\nonly a few language versions of Wikipedia contain lists of peren-\nnial sources and, as the reliability of a source is not static, these\nlists might evolve through time. Moreover, [ 4] showed that the\nreliability of references might be different across cultures, such\nthat unreliable references in one language might persist across edi-\ntions in another language. This makes it difficult to train general\nmodels for source controversiality and, given that some language\neditions of Wikipedia are still small, achieving reference quality\nacross all languages calls for language-agnostic approaches as the\none presented in this work. Beyond the perennial source list, other\ndomain quality rankings for news sources exist, such as the Iffy\nindex of unreliable news sources [ 5] based on the Media Bias/Fact-\nCheck [ 46]. However, we show that the latter corresponds only\nloosely with the perennial source list, pointing to the necessity of\nWikipedia-specific models.\nAnother area of research around the platform is related to helping\nreaders assess the information quality. For example, the Wikipedia\ncommunity has developed user-oriented quality ratings like the\n\u201cGood Article\u201d or \u201cFeatured Article\u201d marks [ 37] that help readers\nunderstand the quality and consensus level of the article they are\nreading. Modeling approaches have been built using these labels as a\nground-truth [ 10]. Some works have also assessed the performance\nof more complex visual trust indicators [ 23]. In addition, the design\nof fact-checking API\u2019s based on Wikipedia [ 35] has shown the\npotential of Wikipedia data to combat misinformation.\nThe multilingual nature of Wikipedia calls for specific model-\ning techniques. Models such as multilingual BERT have been used\nto detect vandalism [ 35] or political bias [ 33], while other workshave built rankings of relative quality in multiple language ver-\nsions of the platform [ 24,25]. For some other tasks, and due to the\nlimitations found in languages with smaller Wikipedia datasets,\nlanguage-agnostic approaches have been used, e.g., to assess arti-\ncle quality [ 10], or to perform automatic entity-linking [ 14] and\ntopic classification [19]. Language-agnostic models using features\nextracted from the network structure or user behavior have also\nbeen used in the more general context of detecting misinformation\nor low-quality information in social media. In this line, Shu et al .\n[31] constructed embeddings of users and news and used diffusion\nmodels to trace the paths through which fake news propagates,\nwhile Zhao et al . [49] extracted features from the user interaction\nnetwork to help identify misinformation in online health communi-\nties. We show that, indeed, user-behavior related language-agnostic\nsignals are useful for modeling source reliability, and that they\ncan be adapted across languages to improve performance in low-\nresource settings.\n3 DATA AND METHODS\n3.1 Perennial Sources\nWe use the English Wikipedia perennial source list [ 38] as ground\ntruth for modeling source reliability. It was collected on March 30,\n2023. Although the list exists for 12 other languages, we only use\nthe English version as it is by far the most extensive (see [ 4] for\ndetails on coverage). In English, it consists of a collection of web\ndomains in five categories: blacklisted (569), deprecated (125), gen-\nerally unreliable (178), no consensus (137), and generally reliable\n(184). To build a binary classifier, we use the generally reliable do-\nmains as positive labels, merge the domains blacklisted, deprecated,\nand generally unreliable into a single unreliable category (i.e., the\nnegative label) and ignore the no consensus domains.\n3.2 Language Groups\nAs we aim to test the proposed model on a variety of languages, we\ncollect data on all 326available language editions of Wikipedia. We\nthen discard all languages which have fewer then two reliable and\nunreliable sources according to the English perennial source list.\nAmong the remaining ones, we distinguish between low, middle,\nand high-resource languages by examining the number of \u201cactive\nusers\u201d, defined as \u201cregistered users who have made at least one\nedit in the last 30days.\u201d4We consider the top 5% of the languages\nhaving the most active users as high-resource, the subsequent 25%\nas mid-resource, and the remaining 70% as low-resource languages.\n3.3 Article Collection\n3.3.1 Topic Selection. In this study, we have focused on articles\nfrom the following five topic datasets: Climate Change, COVID-\n19, Biology, History, and Media. Climate Change and COVID-19\nwere selected as they are both controversial but for a different time\nduration. COVID-19 appeared at the end of 2019, while Climate\nChange has been a controversial topic for decades [ 21]. Both have a\nWikiproject community of editors5who label articles as relevant to\nthe topic with special guidelines. For comparison, we also selected\n4https://meta.wikimedia.org/wiki/List_of_Wikipedias (accessed on 01-03-2024)\n5https://en.wikipedia.org/wiki/Wikipedia:WikiProject\n2\nTable 1: Statistics of the topic-language datasets used in the study grouped by high, mid, and low resource languages. All\nnumbers are averages (apart from the number of languages). Perennial Domains refers to the number of domains for the\nperennial source list in English found in the different datasets.\nTopic # Langs # Articles Revs Revs/Article URLs URLs/Article Domains Perennial DomainsHighClimate Change 7 1378 372 133 269 58 671 49 15 363 203\nCOVID-19 7 898 233 905 260 60 168 89 8289 206\nBiology Sample 7 13 607 586 171 43 59 763 6 11 713 148\nHistory Sample 7 5710 624 428 109 52 702 12 12 121 166\nMedia Sample 7 5577 819 541 146 108 583 22 21 290 259MidClimate Change 36 395 28 364 71 6224 19 2599 94\nCOVID-19 37 272 18 523 68 8984 39 1788 102\nBiology 37 4153 83 599 20 8691 3 2022 61\nHistory 37 1680 83 258 49 6274 7 2054 67\nMedia 37 1018 46 957 46 7932 10 2416 123LowClimate Change 56 115 3430 29 1079 12 570 35\nCOVID-19 75 32 916 28 568 22 242 32\nBiology 41 593 21 096 35 1159 4 460 16\nHistory 55 511 13 501 26 952 6 406 22\nMedia 70 159 2262 14 528 7 264 30\nthree topics that are not as controversial, but which varied in terms\nof reference quality based on the \u201creference risk\u201d metric (which\nis also based on the usage of perennial sources) from previous\nwork [ 3]. We choose one example from each risk category: Biology\n(low risk), History (medium risk), and Media (high risk).\n3.3.2 Data Download. For the first two topics, we use the articles\nlisted by the corresponding topic-based WikiProjects. For the latter\nthree, we use the topics identified using the Wikipedia Objective\nRevision Evaluation Service (ORES) machine learning system [ 18].\nTo keep the sizes of datasets comparable and manageable, we sample\nthe articles identified by ORES to be in Biology and History topics\nby 10% (each), and in the Media topic by 2%. These two approaches\nallow us to find articles in the English Wikipedia for each of the five\ntopics, resulting in a total of 84 027 articles. To find the versions of\nthese articles in other languages, we use the MediaWiki API, which\nresults in another 335 809 of articles in all of the 326 languages.\nWe collect the articles and their complete edit history using the\nMediaWiki API in between December 2023 and February 2024.\n3.3.3 Extraction of Source Domains. Citations may appear in the\nbody of the article text and as references at the bottom of the article,\neach of which has standard templates. Each revision that involves\na citation contains the author of the revision, position within the\ndocument of the citation, text of the citation, and meta information,\nsuch as whether it is a DOI, URL, ISBN, ISSN, etc. or a raw URL.\nWe standardize these revisions to create a list of \u201csource edits\u201d\ncontaining the metadata about each insertion, deletion, or edit that\ninvolves a source. We resolve redirects, apply rules to clean and\nstandardize the URLs (including DOI references redirecting to a\nresource) and merge revisions when the same user edits an article in\nsuccession (regardless of the time). Finally, we extract the domains\nfrom the URLs, obtaining 376 566 unique source domains.\n3.3.4 Dataset Statistics. The summary statistics about the topic-\nlanguage datasets (or simply \u201cdatasets\u201d throughout this paper), forthe five topics grouped into low-, mid- and high-resource languages\ncan be seen in Table 1. Recall that we only use datasets that have\npassed the threshold of having at least 2 reliable and 2 unreliable\nperennial sources.\nThe smallest topic in terms of the number of articles is COVID-\n19, while the highest numbers of revisions per article are found\naround Climate Change and COVID-19 in high-resource languages.\nOn the contrary, biology has the largest number of articles, but\nthese are revised on average the fewest number of times in high-\nand mid-resource languages. High-resource languages have much\nbetter coverage of the English perennial sources, with an average\nof 150 to 260 sources per topic. These values decrease to an average\nof 15 to 35 sources for low-resource languages, making the training\nand evaluation more challenging.\n3.4 Feature Definition\nUnlike previous studies measuring Wikipedia edit quality that did\nnot consider certain types of edits and reverts [ 34], we propose\nto focus on the entire edit history concerning each domain. Using\nedit metadata, we define 52 features capturing the popularity of\nthe domain, the \u201cpermanence\u201d of the domain across edits, and the\nnumber and type of users involved in edits adding or removing\nthat domain. As the features are computed for each domain within\neach dataset separately (recall, by \u201cdataset\u201d we mean a collection\nof articles on a certain topic, in a certain language) of varying sizes\nand editorial frequencies, we explore different ways of normalizing\nthese statistics w.r.t. the first appearance of the domain, age of the\ndataset, and in terms of time duration or number of revisions.\n3.4.1 Popularity features.\n\u2022\ud835\udc41articles ,\ud835\udc41\ud835\udc4e\ud835\udc5f\ud835\udc61\ud835\udc56\ud835\udc50\ud835\udc59\ud835\udc52\ud835\udc60 : Number of articles a domain has appeared in\n(sensitive to the size of the dataset), and its normalized version\n(by the number of articles in the dataset).\n3\n\u2022\ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc5f\ud835\udc41 \ud835\udc4e\ud835\udc5f\ud835\udc61\ud835\udc56\ud835\udc50\ud835\udc59\ud835\udc52\ud835\udc60 ,\ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc5f\ud835\udc41 \ud835\udc4e\ud835\udc5f\ud835\udc61\ud835\udc56\ud835\udc50\ud835\udc59\ud835\udc52\ud835\udc60 : Number of articles a domain is used\nin at collection time, and normalized by the total number of\narticles in the dataset.\n3.4.2 Permanence features.\n\u2022\u03a3\ud835\udc43\ud835\udc52\ud835\udc5f\ud835\udc5a \ud835\udc51,\u03a3\ud835\udc43\ud835\udc52\ud835\udc5f\ud835\udc5a \ud835\udc5f: Permanence, how long a domain has been used\nin an article (not necessarily consecutively), summed across all\narticles and measured in days (subscript \ud835\udc51) or by this number of\nrevisions (subscript \ud835\udc5f).\n\u2022\u03a3\ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc5f\ud835\udc43\ud835\udc52\ud835\udc5f\ud835\udc5a \ud835\udc51,\u03a3\ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc5f\ud835\udc43\ud835\udc52\ud835\udc5f\ud835\udc5a \ud835\udc5f: Same as above, but considering only\nthe articles where the domain is currently used.\n\u2022\u03a3\ud835\udc43\ud835\udc52\ud835\udc5f\ud835\udc5a \ud835\udc51,\u03a3\ud835\udc43\ud835\udc52\ud835\udc5f\ud835\udc5a \ud835\udc5f: Sum of all permanences for a domain in all ar-\nticles, normalized by the sum of the ages of all articles (measured\nby the number of days or the number of revisions that an article\nexisted).\n\u2022\u27e8\ud835\udc43\ud835\udc52\ud835\udc5f\ud835\udc5a \ud835\udc51\u27e9,\u27e8\ud835\udc43\ud835\udc52\ud835\udc5f\ud835\udc5a \ud835\udc5f\u27e9: The average permanence of a domain (over\nall articles where it was used).\n\u2022\u27e8\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc53\ud835\udc43\ud835\udc52\ud835\udc5f\ud835\udc5a \ud835\udc51\u27e9,\u27e8\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc53\ud835\udc43\ud835\udc52\ud835\udc5f\ud835\udc5a \ud835\udc5f\u27e9: Self-permanence: permanence di-\nvided by the number of days (or revisions) since the first time the\ndomains were added to the article, then averaged article-wise.\n\u2022\u03a3\ud835\udc4e\ud835\udc54\ud835\udc52\ud835\udc51,\u03a3\ud835\udc4e\ud835\udc54\ud835\udc52\ud835\udc5f: The sum of ages of the domain over all the articles\nin a dataset, where the age of a domain in an article is the number\nof days or revisions since a URL from that domain has been first\nadded to the article.\n\u2022\u27e8\ud835\udc4e\ud835\udc54\ud835\udc52\ud835\udc51\u27e9,\u27e8\ud835\udc4e\ud835\udc54\ud835\udc52\ud835\udc5f\u27e9: The average age of a domain over all the articles\nit appears at least once.\n3.4.3 User-based features.\n\u2022\ud835\udc48\ud835\udc4e\ud835\udc51\ud835\udc51,\ud835\udc48\ud835\udc5f\ud835\udc52\ud835\udc5a: Number of users that have added or removed a do-\nmain.\n\u2022\ud835\udc45\ud835\udc4e\ud835\udc51\ud835\udc51,\ud835\udc45\ud835\udc5f\ud835\udc52\ud835\udc5a: Same as above, but only counting registered users.\n\u2022\ud835\udc48\ud835\udc4e\ud835\udc51\ud835\udc51,\ud835\udc48\ud835\udc5f\ud835\udc52\ud835\udc5a,\ud835\udc45\ud835\udc4e\ud835\udc51\ud835\udc51,\ud835\udc45\ud835\udc5f\ud835\udc52\ud835\udc5a: The above four features, normalized by\nthe total number of unique users in the dataset.\n\u2022\u27e8\ud835\udc48\ud835\udc4e\ud835\udc51\ud835\udc51\u27e9,\u27e8\ud835\udc48\ud835\udc5f\ud835\udc52\ud835\udc5a\u27e9,\u27e8\ud835\udc45\ud835\udc4e\ud835\udc51\ud835\udc51\u27e9,\u27e8\ud835\udc45\ud835\udc5f\ud835\udc52\ud835\udc5a\u27e9: The above four features, mea-\nsured as an average per article.\n\u2022\ud835\udc45\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c(\ud835\udc45\ud835\udc4e\ud835\udc51\ud835\udc51/\ud835\udc48\ud835\udc4e\ud835\udc51\ud835\udc51),\ud835\udc45\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c(\ud835\udc45\ud835\udc5f\ud835\udc52\ud835\udc5a/\ud835\udc48\ud835\udc5f\ud835\udc52\ud835\udc5a)The proportion of regis-\ntered vs. all users that ever added or removed a domain.\n\u2022\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc4e(\ud835\udc45\ud835\udc4e\ud835\udc51\ud835\udc51),\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc4e(\ud835\udc45\ud835\udc5f\ud835\udc52\ud835\udc5a)Probability that, when a domain is\nadded or removed, this is done by the registered users. Contrary\nto the ratio here we take into account the number of revisions\nper user.\n\u2022For all user-based features listed above, we compute a version\nwherein instead of counting any instance of adding a domain, we\nonly count instances of adding it for the first time on an article\n(\u201cstarting\u201d), and similarly counting only the instances that the\ndomain was removed for the last time (\u201cending\u201d). For example,\n\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc4e(\ud835\udc45\ud835\udc4e\ud835\udc51\ud835\udc51), and\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc4e(\ud835\udc45\ud835\udc5f\ud835\udc52\ud835\udc5a)would have a corresponding ver-\nsion\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc4e(\ud835\udc45\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61)and\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc4e(\ud835\udc45\ud835\udc52\ud835\udc5b\ud835\udc51)(similarly for the related\nfeatures).\nWhen we aggregate the various statistics over articles for a\ndomain, we have considered the average and the median and found\nthe best performance with the average, which we use in the models\nin the following sections.6.\n6We have also experimented with versions of the above features that use the aggrega-\ntion over all URLs in each dataset, instead of domains. However, the resulting features\nwere performing very similarly to those aggregated over domains, and we exclude\nthese from the analysis.3.5 Model Training and Evaluation\nUsing the above features, we train an XGBoost7model to classify\nthe domains as reliable or unreliable. To account for class imbalance,\nwe use weights (e.g., the weight for the positive class is the fraction\nof negative/positive). To limit overfitting, we set the maximum\ndepth of the classifier to 1 (we have tested the classifier with depths\nup to 5, but found no increase in leave-one-out (LOO) validation\nperformance, and the learning rate to \ud835\udf02=0.1(default is 0.3).\nWhen evaluating in the \u201cnative\u201d condition, wherein the training\nand target data are from the same dataset, we employ LOO cross-\nvalidation. In the \u201ccross-language adaptation\u201d condition, wherein\nthe model is trained on one language and tested on another (keeping\nthe topic the same), we simply compute the evaluation metrics on\nthe test language. Similarly, for cross-topic adaptation, we keep\nthe language consistent and test on a target topic. In the \u201cmixed\u201d\ncondition, wherein the model is trained on a set of languages and\ntested on one of these languages, we again employ LOO cross-\nvalidation where a domain of test language is left out at a time.\nTo compute a confidence interval for a metric, we employ boot-\nstrapping wherein we resample with replacement the output of\nthe classifier and recompute the metrics \ud835\udc5b=100times. The main\nmetric we compute is the F1 macro score, which is the unweighted\naverage of the F1 scores of the negative and positive classes.\n4 RESULTS\n4.1 RQ1: Case study: Climate Change in English\nWe begin by studying the behavior of our model for the Wikipedia\narticles in English around the Climate Change topic. The model\nperformance is shown in the first row of Table 2, which lists the F1\nMacro, precision and recall for each class. We find that, out of the\nunreliable domains, the classifier correctly identifies as unreliable\n83% of them (recall), and out of the domains the classifier guessed\nas unreliable, 83% are truly unreliable (precision). Both precision\nand recall for the reliable class are slightly worse at 80%. Note that\nthe classes are fairly balanced in this setting, with 46% of the dataset\nhaving the reliable label.\nFigure 1a shows the \u201cbee swarm\u201d plot of SHAP (SHapley Ad-\nditive exPlanations) values that illustrate the importance of each\nfeature in the classifier [ 26]. The top 6 most predictive features (and\nan aggregation of the rest) are shown. The SHAP values for the\ntop feature\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc4e(\ud835\udc45\ud835\udc52\ud835\udc5b\ud835\udc51)\u2013 the probability that, when a domain is\nremoved, it is done by a registered editor \u2013 reveal that high values\nof this feature are associated with a negative impact in the domain\nreliability prediction. In other words, when a registered editor re-\nmoves a domain, it is more likely to be unreliable. The opposite\ncan be seen for the \u27e8\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc53\ud835\udc43\ud835\udc52\ud835\udc5f\ud835\udc5a \ud835\udc51\u27e9\u2013average proportion of days the\ndomain remains cited since it was first added\u2013 and \u27e8\ud835\udc43\ud835\udc52\ud835\udc5f\ud835\udc5a \ud835\udc5f\u27e9\u2013the\naverage number of revisions that the domain remains on the article\u2013\n: higher values of these features are associated with more reliable\nsources. \u03a3\ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc5f\ud835\udc43\ud835\udc52\ud835\udc5f\ud835\udc5a \ud835\udc5f\u2013the aggregated number of revisions through\nwhich the domain has existed across all articles\u2013 also correlates\npositively with the reliable category, so does as well as the sum of\nthe domain ages across articles, \u03a3\ud835\udc4e\ud835\udc54\ud835\udc52\ud835\udc5f. Finally, \u201cyounger\u201d domains\n\u2013 those that have been added to an article more recently, \u27e8\ud835\udc4e\ud835\udc54\ud835\udc52\ud835\udc51\u27e9\u2013\n7We use Python package XGBoost V1.6.2 https://pypi.org/project/xgboost/1.6.2/.\n4\nTable 2: Leave-one-out validation F1 macro, and precision and recall metrics for both classes for the model trained and tested\nin the English Wikipedia on each topic, and for all topics combined (standard deviations specified with \u00b1). Class balance in\nterms of the number of reliable (rel) and unreliable (unr) domains.\nTopic F1 Macro Precision (rel) Recall (rel) Precision (unr) Recall (unr) # rel # unr\nClimate Change 0.81\u00b10.02 0.80\u00b10.03 0.80\u00b10.04 0.83\u00b10.04 0.83\u00b10.03 129 152\nCOVID-19 0.88\u00b10.02 0.87\u00b10.03 0.91\u00b10.03 0.89\u00b10.03 0.85\u00b10.03 136 125\nBiology 0.80\u00b10.03 0.81\u00b10.03 0.81\u00b10.03 0.80\u00b10.03 0.80\u00b10.03 124 120\nHistory 0.75\u00b10.03 0.70\u00b10.04 0.83\u00b10.04 0.82\u00b10.03 0.69\u00b10.05 120 139\nMedia 0.83\u00b10.02 0.81\u00b10.03 0.82\u00b10.03 0.85\u00b10.03 0.84\u00b10.03 146 176\nAll topics 0.83\u00b10.02 0.76\u00b10.04 0.85\u00b10.03 0.89\u00b10.02 0.82\u00b10.03 159 234\n0.75\n 0.50\n 0.25\n 0.00 0.25 0.50 0.75\nSHAP value (impact on model output)Sum of 46 other featuresaged\nager\nCurrPermr\nPermr\nSelfPermd\nProba(Rend)\nBeeswarm for Climate change English dataset\nLowHigh\nFeature value\n(a) Feature Importance\n2.0\n 1.5\n 1.0\n 0.5\n 0.047 other features0.876 = Ratio(R/Uadd)118.577 = Permr\n0.559 = SelfPermd\n0.512 = Proba(Rend)17357 = CurrPermr\n47 other features Ratio(R/Uadd) Permr\n SelfPermd\n Proba(Rend) CurrPermr\n +1.06\n+0.270.76\n0.5\n0.32\n0.25\ntwitter.com True label: Unreliable, predicted label: Unreliable\nE[f(X)]=0.161\nf(x)=0.67\n (b) twitter.com\n0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.7547 other features219 = CurrNpages0.399 = Proba(Rend)116.255 = Permr\n0.712 = SelfPermd\n29085 = CurrPermr\n47 other features CurrNpages Proba(Rend) Permr\n SelfPermd\n CurrPermr\n +1.06\n+0.52\n+0.32\n+0.19\n+0.180.32\nresearchgate.net True label: Unreliable, predicted label: Reliable\nE[f(X)]=0.161\nf(x)=1.777 (c) researchgate.net\nFigure 1: Beeswarm analysis of an XGBoost model trained on Climate Change English dataset (class encoding is 1: reliable and\n0: unreliable) and two example classification explanations (score shown is the log-odds ratio).\nare more likely to be reliable. This may be a sign of the increased\nfocus of the platform on the quality of sources [3].\nIn Figures 1b and 1c we illustrate the performance of the model\nfor two domains of special interest: twitter.com , correctly identified\nas unreliable, and researchgate.net , incorrectly identified as reliable.\nIn the case of twitter.com , despite being added/present frequently on\narticles (high \u03a3\ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc5f\ud835\udc43\ud835\udc52\ud835\udc5f\ud835\udc5a \ud835\udc5f), it is soon removed by registered editors\n(\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc4e(\ud835\udc45\ud835\udc52\ud835\udc5b\ud835\udc51)and also SelfPermanence, \u27e8\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc53\ud835\udc43\ud835\udc52\ud835\udc5f\ud835\udc5a \ud835\udc51\u27e9is low). In the\ncase of researchgate.net , most of the permanence features point to it\nbeing used as a reliable source. This points out that, although this\ndomain is considered unreliable in the perennial source list because\nthe platform does not perform fact-checking or peer reviewing and\nis considered a self-published source, the Wikipedia community\noften uses this domain as if it were reliable. In fact, this platform\ncould be used as a way to provide an open-access link to peer-\nreviewed references when the official version is behind a paywall.\nTherefore, this specific example shows that our methodology could\nbe used to identify cases where the use of domains is not in line\nwith the established guidelines.\n4.2 RQ2: Topic & Language Generalizability\nWe continue by evaluating the performance of our language-agnostic\nmodeling approach in other topical and linguistic settings. Table 2\nshows the F1 macro and class-specific precision and recall for these\ntopics, in the English Wikipedia. We find that, overall, the perfor-\nmance is consistent, with the lowest performance observed for the\nHistory dataset, with a recall of 0.69 for the unreliable class. The\nbest performance is achieved on the COVID-19 dataset, possiblydue to the ample data on the edits and the more recent nature of\nthe corresponding articles. We find that the amount of edited data\nis important for the classifier\u2019s performance, as we show later in\nthis section. Finally, we combine all English-language datasets into\none, and perform training and LOO validation; the performance of\nthis model is shown in Table 2 under \u201call topics\u201d. Such a combined\nmodel performs on par with other topical models, with especially\nhigh precision for the unreliable class (0.89), suggesting that com-\nbining knowledge about different topics does not overall degrade\nthe performance of the classifier.\nNext, we explore the applicability of the proposed model to other\nlanguages. In these experiments, we first keep the topic constant\n\u2013 Climate Change \u2013 while training and evaluating models in dif-\nferent languages. Figure 2 shows the LOO F1 macro scores for\nselect languages form the high-, mid-, and low-resource groups, in\ncomparison to the performance of a random baseline model. Specif-\nically, we show the average performance \u00b1one standard deviation\nas shaded areas. To estimate the performance of a random model,\nwe assign a random binary classification for the domains and per-\nform bootstrapping ( \ud835\udc5b=100) similar to the evaluation of all other\nmodels, as discussed in Section 3.5. The performance is highest for\nhigh-resource languages and degrades for mid- and low-resource\nones. The standard deviations of the scores also increase with lower\nresource languages (both for our model and the baselines), pointing\nto an increased lack of information for computing the score.\nAdditionally, we show aggregate statistics of the model perfor-\nmance on the languages in each category in Figure 3. In particular,\nwe are interested in whether our model outperforms the random\n5\nen\n281fr\n160de\n154pt\n138ru\n130ja\n101it\n119fa\n87hi\n56eu\n52id\n101uz\n43cy\n60kn\n43gu\n28mr\n16lv\n15rm\n4\nLanguage ISO code \n And number of perennial domains found0.10.20.30.40.50.60.70.80.9F1 performancesHigh resource                        Mid resource                         Low resource\nLOO validation\nRandom modelFigure 2: Performances (mean \u00b1stdv) of models trained and\nvalidated in languages other than English on the Climate\nChange datasets. The three batches of languages, separated\nby a vertical black line, show results on a sample of high\n(left), mid (centre), and low (right) resource languages.\nLanguage resourceness50%100%% lang.Climate change\nCOVID-19Biology\nHistoryMedia\nAll topics\nhigh mid low\nClass0.00.20.40.60.81.0F1 macro\nFigure 3: Native model performance per topic for different\nlanguage types. Top panel: % of models that perform better\nthan a random classifier. Bottom panel: Violin plots of corre-\nsponding distributions of F1 macro performances.\nclassifier in terms of F1 macro score (top panel of the figure): we\nperform this comparison using Mann-Whitney scores and adjust\nthe significance level of 0.05 using the Bonferroni correction for\nmultiple comparisons. For instance, in the Climate Change topic\nin 100% of the high-resource languages, the models outperform\nthe random baseline, but instead in only 44% of the low-resource\nlanguages. The performance on low-resource languages tends to\nvary widely for different topics. The brown bars show the mod-\nels for which articles for all topics within a language are used for\ntraining and testing. These models perform markedly better than\ntopic-specific ones, echoing our findings for the English language.\nEffect of dataset size and introduction of perennial sources on model\nperformance. The worse performance of our model in the lower-\nresource languages may be mainly (among others) due to two rea-\nsons: the datasets available for training the model are too small,\nor the behaviour of the users of these languages towards domains\nis intrinsically difficult to model (their treatment of reliable and\n103104105106\n# revisions0.10.20.30.40.50.60.70.8F1 macroF1 score in relation to dataset size\nEnglish\nOther languages\nEnglish (pre Perennial)Figure 4: Average model F1 macro (line) and standard dev.\n(shaded area) versus the size of the training dataset in terms\nof the number of revisions. In English (red), all topics are com-\nbined and the data is sampled at regular intervals. The same\nexperiment is repeated, considering only revisions before\nthe implementation of the perennial sources on 2018-07-01\n(grey). The performance of models in all other languages\n(also all-topic) is shown in blue, without sub-sampling.\nunreliable domains is difficult to distinguish). We design two ex-\nperiments to address these two reasons, starting with the model\nsensitivity to the amount of training data.\nThe red line in Figure 4 shows the performance of the model\ntrained on different dataset sizes. In particular, we consider the\nEnglish (all-topic) dataset and sample the data at log-regular in-\ntervals (starting at 103, then at 103.25etc. until 106.75), resulting\nin an increasing number of revisions available for training. We re-\npeat this exercise 10 times (mean \u00b1stdv. shown in the Figure). For\ncomparison, we depict the performance of the models trained on\nthe all-topic datasets in all other languages (shown in blue). Note\nthat in these cases there is no sampling, and for every language, all\navailable data is used. We find that the performance of the English-\nlanguage model is comparable to that of the other languages when\nthere is little data available (up until about 105revisions) and is\nabout 10% higher than the other languages when almost all data\nis used for its training. At least in the English-language case, we\nfind that for stable model performance around 10,000 revisions\nare necessary. At the higher range of data, we find that the clas-\nsifier\u2019s performance has not levelled off, suggesting that adding\nmore articles (possibly other topics) may improve the performance\nfurther.\nTurning to editorial behaviour, the introduction of the perennial\nsources list provides a \u201cnatural experiment\u201d to measure a possible\neditorial behaviour change in response to these additional guide-\nlines. We only examine the impact of this change in the English-\nlanguage datasets, as it is not clear whether editors from other lan-\nguage editions of Wikipedia used the English perennial sources list.\nThus, we created a version of our English-language dataset using\nonly revisions up to 2018-06-30 (i.e., before these guidelines were\nintroduced). To control for the dataset size (which as shown above,\nmay affect model performance), we perform again sub-sampling\nsimilar to what we did with the full English-language dataset. The\nblack dash-dotted line and the grey area in Figure 4 show the model\n6\nhigh mid low\nLanguage resourceness0.00.51.0% lang.\nhigh mid low0.00.20.40.60.81.0F1 macroSame language, cross-topic Cross-language, same topic Same language, same topicFigure 5: Model performance in two adaptation scenarios:\ncross-language setting (trained on a language, tested on dif-\nferent language from the same class, in the same topic, in\nred) and a cross-topic setting (trained on a topic, tested on\na different topic, in the same language, in green). Results\naggregated by language resourcefulness and compared to\nsame-topic and language native models (in blue).\nperformance based on this limited data. We observe its performance\nto be notably lower than with more recent data (red line and area),\nindicating that the editorial behaviour before the introduction of\nthe perennial sources list was less predictive of reliable/unreliable\ndomain classes than afterwards. In this case, the performance is sim-\nilar to that of other languages, showing that that editor behaviour\nallows extracting meaningful and predictive signals for source re-\nliability even when the editors did not have an official guideline\nfor source reliability. Furthermore, these findings support previous\nwork which shows that the quality of the Wikipedia references\nimproved after the introduction of the perennial sources [3].\n4.3 RQ3: Model Adaptation across Topics &\nLanguages\nThe multilingual nature of Wikipedia presents an opportunity to\nsupplement information available in lower-resource languages with\nthat in the larger ones. Considering our datasets, two directions\nof adaptation are possible: across languages and across topics. We\nconsider both scenarios in Figure 5, which shows the performance\nof the models adapted in these two ways: in a cross-language set-\nting (trained on a language and afterwards tested on a different\nlanguage from the same resourcefulness class, in the same topic)\nand a cross-topic setting (trained on a topic, tested on a different\ntopic, in the same language). We find that, in both cases, the per-\nformance of the adapted models decreases compared to the native\nperformance. The decrease in performance is about the same in\ncross-language and cross-topic settings. This suggests that the edi-\ntor\u2019s behaviour around references may be non-identical in different\ntopics, as well as languages. However, again, we see the importance\nof resourcefulness on the performance: the models adapted across\nhigh-resource languages perform better than those across mid and\nlow-resource ones.\nFinally, we consider all topics together and analyse the perfor-\nmance of our model when trained with different training strategies\nand tested on each language. In particular, we test whether mod-\nels trained on higher-resource data would be helpful to those in\nLanguage resourceness50%100%vs RandomAll topics - english\nAll topics - high res. lang.All topics - all lang.\nAll topics - all lang. norm.All topics - all lang. norm. tree depth 3\nNative models\nLanguage resourceness50%100%vs NativeWorse Comparable Better\nhigh mid low0.00.20.40.60.81.0F1 macroFigure 6: Combined Model performances for different train-\ning datasets and strategies. Top panel: % of models that per-\nform better, same as, or worse than a random classifier; Cen-\nter: same, but compared to the native model; Bottom: violin\nplots of the corresponding distributions of the F1 macro per-\nformances.\nlower-resource settings. In Figure 6 we aggregate the corresponding\nresults by languages of similar resourcefulness: the top panel shows\nhow for many languages the different models perform significantly\nbetter, same as, or worse than a random model (according to Mann-\nWhitney tests with Bonferroni correction), the center panel shows\nthe same, but compared to the native model, and the bottom panel\nshows the distribution of F1 macro scores. We find that F1 scores\nare generally lower when training a model on the English dataset\nand applying it to low-resource languages, confirming that cross-\nlanguage adaptation can be problematic, even if a high-resource\nlanguage such as English is used. However, when training on all\nhigh-resource languages or all languages together, the performance\nfor low-resource languages significantly increases. This suggests\nthat including a plurality of languages in training data may capture\na variety of user behaviours that are transferable to lower-resource\nlanguages.\nNonetheless, each language-specific dataset has its own biases\nconnected to the amount of data available, temporal peculiarities of\nthe articles and their edits, and the volume of editorial activity. To\nalleviate potential difficulties in adapting feature values between\nlanguages, we normalize each dataset using quantile normalization\n(converting feature values to ranks). This approach further improves\nthe performance of the all-language model adaptation, even on\naverage performing better than the native models. Finally, since\nthe size of the dataset increases with the combination of topics, we\ntrain the classifier with the maximum tree depth of 3 instead of 1\nbut do not find a substantial increase in performance.\n4.4 Qualitative Insights on Source Usage\nThe above quantitative evaluation of the proposed model abstracts\nthe rich insight that its output provides. When examining the\ndomains that attain the highest reliability score by the classifier,\nwe find interesting signals of the editorial treatments of different\n7\nsources, in each topical sphere (here, we focus on English data). For\ninstance, among the domains predicted (incorrectly) as reliable in\nthe Climate Change domain is YouTube (which appeared in 644\narticles in that dataset) with a positive (reliable) class probability of\n\ud835\udc43\ud835\udc5f=0.64, as well as Internet Movie Database (IMDB) (appearing\nin 56 articles, \ud835\udc43\ud835\udc5f=0.56). This points to the importance of popular\nor mainstream culture in this topic. When considering the History\ndataset, we find the classifier incorrectly labels as reliable domains\npointing to sources that the perennial sources label as having parti-\nsan bias, such as the Jewish Virtual Library ( \ud835\udc43\ud835\udc5f=0.81), Sixth Tone\n(\ud835\udc43\ud835\udc5f=0.74), and Daily Sabah ( \ud835\udc43\ud835\udc5f=0.69).\nOn the other hand, one can use the output of this classifier to\nfind sources that different communities of editors consider inappro-\npriate for their topical domain. For instance, in the Biology dataset,\nScience-Based Medicine, which is considered generally reliable and\nhas \u201ca credible editorial board\u201d according to the perennial source\nlist, is treated by the editors more similarly to unreliable domains\n(\ud835\udc43\ud835\udc5f=0.20). Similarly, the Huffington Post, rated as \u201cfairly reliable\nfor factual reporting on non-political topics\u201d, scores low in the\nCOVID dataset ( \ud835\udc43\ud835\udc5f=0.07), possibly due to the politically charged\nand serious nature of the rhetoric around the pandemic.\n4.5 Alternative Sources of Reliability\nOne limiting factor of this study was the availability of labels for\nthe quality of references. We consider an alternative: Media Bias /\nFact Check (MBFC), an \u201cindependent website\u201d that provides ratings\nof \u201cthe bias, factual accuracy, and credibility of media sources\u201d8,\nwhich has been widely used in misinformation research and by\nother indexes [ 5,8,45]. We use this credibility metric and consider\nvery high andhigh as the positive class, and lowandvery low as\nnegative. With the same features and model as above and LOO\ncross-validation, we gauge its performance on MBFC. Although the\nclass-average F1 macro is 0.61, the classifier performs especially\npoorly on the negative class, with a precision of 0.32 and recall of\n0.55, suggesting that the features do not capture this metric. Indeed,\nthere may be several reasons why such resources are not suitable\nfor the analysis of Wikipedia references. First, there is little overlap\nbetween the sources noted in MBFC and in the English Wikipedia\nperennial sources list (out of 1156 perennial sources, 272 are in\nMBFC). Second, the MBFC credibility metric is weakly aligned with\nthe perennial source reliability one. Third, MBFC itself is listed as\ngenerally unreliable by the English Wikipedia perennial sources list.\n5 DISCUSSION\nIn this study, we have proposed and assessed editing behaviour-\nbased features for modeling the reliability of web domains used as\nreferences on Wikipedia. Editing behaviour is not the only source\nof useful information for language-agnostic modeling of source re-\nliability. Links between articles (already used for topical clustering\nof Wikipedia articles [ 19]) may provide semantically meaningful\ncontext for references, whereas users\u2019 reading sessions (used for\nentity linking [ 14]) may give controversiality signals from the per-\nspective of the audience. Future work will also take a page from\nthe social media literature and build editor-resource networks [ 31]\n8https://mediabiasfactcheck.com/about/that would capture structural peculiarities of topics, editors, and\nresources associated with low-quality content.\nApart from finding that the proposed features capture editorial\nbehaviour within a dataset spanning one topic and language, we\nalso observe that they retain useful information during the model\nadaptation across languages. Although English has been the lingua\nfranca in NLP, having the best-developed tools and resources, we\nshow that adaptation from the English-language dataset to other\nlanguages is largely inferior to using a model that combines in-\nformation from many languages. This improvement is especially\nnoticeable in performance on low-resource languages. These find-\nings are aligned with limitations posed by Das et al . [10] suggesting\nthat, although the features are language-agnostic, limiting data\nselection to one language may limit the breadth of the captured\nbehaviours and their applicability to new settings. As multilingual\nembedding models have gained popularity [ 47], it remains to be\nseen whether the latest developments in NLP will benefit the low-\nresource languages [ 48], and what the impact will be on Wikipedia\nspecifically.9The need to monitor possible changes in the editorial\nstandards and accompanying quality of new references calls for\nfurther development of features reflecting these behaviors, such as\nthose described in the present study.\nAs ground truth, we have used the list of perennial sources [ 38]\nfrom the English Wikipedia, which is part of Wikipedia\u2019s efforts to\nprovide clear editorial instructions to its community. We compare\nthis resource with another popular source of reliability measure, the\nMBFC [ 46], and find significant differences between them, pointing\nto the necessity of Wikipedia-specific models like ours. Previous\nwork comparing the perennial sources lists in different languages\nshowed disagreement on whether some domains are indeed reli-\nable [ 4]. Therefore, the system we propose could be used to expand\nlists of perennial sources and lists derived from them10, or even to\nassess cultural biases in Wikipedia sources [44, 50].\nLimitations\nThe present study has multiple limitations. Using language-agnostic\napproaches may be seen as a limitation, as it does not take advantage\nof the predictive power of language-dependent features. However,\ndespite the significant advances with multilingual NLP resources\nsuch as mBERT [ 12], the motivation for our model was precisely\nto be ready for use in the 300+ language editions of Wikipedia,\nespecially those that are less privileged and under-resourced.\nSecond, while the dataset contains thousands of articles and cu-\nmulatively millions of revisions, it spans only a fraction of Wikipedia.\nDespite choosing topical foci such that different reference quality\nand time scales, they may not capture the entire diversity of subject\nmatter of this massive collaborative effort. For instance, differen-\ntiating topics by whether the topic is currently controversial may\nreveal shifts in editorial handling of sources.\nThird, the selection of the English perennial sources list as the\ngold standard disadvantaged other languages. Unfortunately, only\na few other languages have a similar resource and they are sig-\nnificantly less extensive and conclusive [ 4]. Given the global role\n9https://en.wikipedia.org/wiki/Wikipedia:Why_you_shouldn%27t_write_articles_\nwith_ChatGPT,_according_to_ChatGPT\n10https://science.feedback.org/consensus-credibility-scores-comprehensive-dataset-\nweb-domains-credibility/\n8\nplayed historically by the English edition of Wikipedia [ 17], our\nselection provided the best choice to reach the maximum potential\ncoverage across other language editions. In the future, we aim to\nuse the scores computed by the adapted models as initial input for\nfuture collaboration with the editors of these language editions to\ncreate language-specific source quality guidelines.\nFinally, we show that the method is sensitive to the amount of\ndata available for training (and testing), possibly resulting in in-\nsignificant results in some low-resource language settings. Further\nresearch will be needed to extend the amount of available data in\nthese contexts.\nEthical Considerations\nThis study, and online content moderation in general, have im-\nportant ethical aspects that should be kept in mind both by the\nplatforms and the misinformation researchers.\nFirst, this study has a direct impact on the access equity to ency-\nclopedic resources in low-resource languages. Currently, the lack\nof resources such as the perennial source list for these versions of\nWikipedia limits the extent to which our model is able to reflect the\nstandards of that linguistic community. The domain scores provided\nby the models adapted from other (higher resource) languages that\nwe propose here could be a starting point for bootstrapping the\neditorial discussion of the quality of sources used in low-resource\nlanguages.\nSecond, the process of determining the quality of sources bears\nscrutiny. In the news domain, the quality of information is often\ndetermined by professional fact-checkers. However, on Wikipedia,\nthe editors from the community define the status of the sources via\na deliberation process, guided by the community standards. Any\npotential biases in the judgment of these editors or failings in the\ndeliberative process would be reflected in the quality of these labels\n(for instance, according to Kharazian et al . [20] , some Wikipedia edi-\ntions were dominated for a time by a \u201csmall group of administrators\nwho introduced far-right bias and outright disinformation\u201d). Gener-\nally, in 2014 Shaw and Hill [30]find that the Wikipedia contributors\nself-organize into oligarchic structures, though in 2021 Rijshouwer\net al. [29] find \u201cstrong counter-tendencies\u201d to editorial power con-\ncentration. Our proposed method may reveal community behaviors\nwhich are not necessarily explicitly stated, but which shape the\nplatform\u2019s use of outside sources, potentially aiding the community\nto self-monitor the use of sources.\nWe believe that the code and resources we make available to the\ncommunity11(compiled in accordance with FAIR (Findable, Acces-\nsible, Interoperable, and Reusable principles [ 39]) will be useful to\nassist future research efforts and to further evaluate and extend the\nfeatures and modeling we proposed.\nREFERENCES\n[1]Pablo Arag\u00f3n and Diego S\u00e1ez-Trumper. 2021. A preliminary approach to knowl-\nedge integrity risk assessment in Wikipedia projects. CoRR abs/2106.15940 (2021),\n1\u20134. arXiv:2106.15940 https://arxiv.org/abs/2106.15940 (Cited on 1)\n[2]Joshua Ashkinaze, Eric Gilbert, and Ceren Budak. 2024. The Dynamics of (Not)\nUnfollowing Misinformation Spreaders. In Proceedings of the ACM on Web Con-\nference 2024 . 1115\u20131125. (Cited on 1)\n11https://github.com/JacopoDignazi/Wiscom[3]Aitolkyn Baigutanova, Jaehyeon Myung, Diego Saez-Trumper, Ai-Jou Chou,\nMiriam Redi, Changwook Jung, and Meeyoung Cha. 2023. Longitudinal As-\nsessment of Reference Quality on Wikipedia. In Proceedings of the ACM Web\nConference 2023 (WWW \u201923) . ACM, New York, NY, USA, 2831\u20132839. https:\n//doi.org/10.1145/3543507.3583218 (Cited on 1, 3, 5, 7)\n[4]Aitolkyn Baigutanova, Diego Saez-Trumper, Miriam Redi, Meeyoung Cha, and\nPablo Arag\u00f3n. 2023. A Comparative Study of Reference Reliability in Multiple\nLanguage Editions of Wikipedia. In Proceedings of the 32nd ACM International\nConference on Information and Knowledge Management (CIKM \u201923) . ACM, New\nYork, NY, USA, 3743\u20133747. https://doi.org/10.1145/3583780.3615254 (Cited on 1,\n2, 8)\n[5]Golding Barret. 2021. Iffy index of unreliable sources. (2021). https://iffy.news/\nindex/ [accessed 2024 April 3]. (Cited on 2, 8)\n[6]Erik Borra, Esther Weltevrede, Paolo Ciuccarelli, Andreas Kaltenbrunner, David\nLaniado, Giovanni Magni, Michele Mauri, Richard Rogers, and Tommaso Ven-\nturini. 2015. Societal Controversies in Wikipedia Articles. In Proceedings of\nthe 33rd Annual ACM Conference on Human Factors in Computing Systems\n(CHI \u201915) . Association for Computing Machinery, New York, NY, USA, 193\u2013196.\nhttps://doi.org/10.1145/2702123.2702436 (Cited on 1, 2)\n[7]Ai-Jou Chou, Guilherme Gon\u00e7alves, Sam Walton, and Miriam Redi. 2020. Citation\ndetective: a public dataset to improve and quantify wikipedia citation quality at\nscale. Wiki Workshop. (Cited on 2)\n[8]Rajdipa Chowdhury, Sriram Srinivasan, and Lise Getoor. 2020. Joint Estimation of\nUser And Publisher Credibility for Fake News Detection. In Proceedings of the 29th\nACM International Conference on Information & Knowledge Management (CIKM\n\u201920). Association for Computing Machinery, New York, NY, USA, 1993\u20131996.\nhttps://doi.org/10.1145/3340531.3412066 (Cited on 8)\n[9]Noam Cohen. 2021. One Woman\u2019s Mission to Rewrite Nazi History on Wikipedia.\nWIRED. [Online; accessed 15-Apr-2024]. (Cited on 1)\n[10] Paramita Das, Isaac Johnson, Diego Saez-Trumper, and Pablo Arag\u00f3n. 2024.\nLanguage-Agnostic Modeling of Wikipedia Articles for Content Quality Assess-\nment across Languages. Proceedings of the International AAAI Conference on Web\nand Social Media 18, 01 (2024), 1\u201311. (Cited on 2, 8)\n[11] Michela Del Vicario, Alessandro Bessi, Fabiana Zollo, Fabio Petroni, Antonio\nScala, Guido Caldarelli, H Eugene Stanley, and Walter Quattrociocchi. 2016. The\nspreading of misinformation online. Proceedings of the national academy of\nSciences 113, 3 (2016), 554\u2013559. (Cited on 1)\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 (2018). (Cited on 8)\n[13] Mengyuan Fu, Kunhao Yang, and Yuko Fujigaki. 2023. Introducing an \u201cinvisible\nenemy\u201d: a case study of knowledge construction regarding microplastics in\nJapanese Wikipedia. New Media & Society 0, 0 (2023), 14614448221149747. (Cited\non 2)\n[14] Martin Gerlach, Marshall Miller, Rita Ho, Kosta Harlan, and Djellel Difallah. 2021.\nMultilingual entity linking system for Wikipedia with a machine-in-the-loop\napproach. In Proceedings of the 30th ACM International Conference on Information\n& Knowledge Management . ACM, New York, NY, USA, 3818\u20133827. (Cited on 2, 8)\n[15] Hao Guo, Weixin Zeng, Jiuyang Tang, and Xiang Zhao. 2023. Interpretable\nFake News Detection with Graph Evidence. In Proceedings of the 32nd ACM\nInternational Conference on Information and Knowledge Management (CIKM \u201923) .\nAssociation for Computing Machinery, New York, NY, USA, 659\u2013668. https:\n//doi.org/10.1145/3583780.3614936 (Cited on 1)\n[16] Yuhan Guo, Qin Han, Yuke Lou, Yiming Wang, Can Liu, and Xiaoru Yuan. 2023.\nEdit-History Vis: An Interactive Visual Exploration and Analysis on Wikipedia\nEdit History. In 2023 IEEE 16th Pacific Visualization Symposium (PacificVis) . IEEE,\nNew York, NY, USA, 157\u2013166. (Cited on 2)\n[17] Scott A Hale. 2014. Multilinguals and Wikipedia editing. In Proceedings of the\n2014 ACM conference on Web science . 99\u2013108. (Cited on 9)\n[18] Aaron Halfaker and R Stuart Geiger. 2020. ORES: Lowering barriers with participa-\ntory machine learning in wikipedia. Proceedings of the ACM on Human-Computer\nInteraction 4, CSCW2 (2020), 1\u201337. (Cited on 3)\n[19] Isaac Johnson, Martin Gerlach, and Diego S\u00e1ez-Trumper. 2021. Language-agnostic\nTopic Classification for Wikipedia. In Companion Proceedings of the Web Con-\nference 2021 (WWW \u201921) . Association for Computing Machinery, New York, NY,\nUSA, 594\u2013601. https://doi.org/10.1145/3442442.3452347 (Cited on 2, 8)\n[20] Zarine Kharazian, Kate Starbird, and Benjamin Mako Hill. 2023. Governance\nCapture in a Self-Governing Community: A Qualitative Comparison of the Serbo-\nCroatian Wikipedias. arXiv preprint arXiv:2311.03616 (2023). (Cited on 9)\n[21] Jasper W. Korte, Sabine Bartsch, Rasmus Beckmann, Roxanne El Baff, Andreas\nHamm, and Tobias Hecking. 2023. From causes to consequences, from chat to\ncrisis. The different climate changes of science and Wikipedia. Environmental\nScience & Policy 148 (2023), 103553. https://doi.org/10.1016/j.envsci.2023.103553\n(Cited on 2)\n[22] Srijan Kumar, Robert West, and Jure Leskovec. 2016. Disinformation on the Web:\nImpact, Characteristics, and Detection of Wikipedia Hoaxes. In Proceedings of\nthe 25th International Conference on World Wide Web (WWW \u201916) . International\n9\nWorld Wide Web Conferences Steering Committee, Republic and Canton of\nGeneva, CHE, 591\u2013602. https://doi.org/10.1145/2872427.2883085 (Cited on 1)\n[23] Andrew Kuznetsov, Margeigh Novotny, Jessica Klein, Diego Saez-Trumper, and\nAniket Kittur. 2022. Templates and Trust-o-meters: Towards a widely deployable\nindicator of trust in Wikipedia. In Proceedings of the 2022 CHI Conference on\nHuman Factors in Computing Systems . ACM, New York, NY, USA, 1\u201317. (Cited\non 2)\n[24] W\u0142odzimierz Lewoniewski, Krzysztof W\u0119cel, and Witold Abramowicz. 2019.\nMultilingual ranking of Wikipedia articles with quality and popularity assessment\nin different topics. Computers 8, 3 (2019), 60. (Cited on 2)\n[25] W\u0142odzimierz Lewoniewski, Krzysztof W\u0119cel, and Witold Abramowicz. 2020. Mod-\neling popularity and reliability of sources in multilingual Wikipedia. Information\n11, 5 (2020), 263. (Cited on 2)\n[26] Scott M Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model\nPredictions. In Advances in Neural Information Processing Systems 30 , I. Guyon,\nU. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett\n(Eds.). Curran Associates, Inc., Red Hook, NY, USA, 4765\u20134774. http://papers.nips.\ncc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf (Cited\non 4)\n[27] Fabio Petroni, Samuel Broscheit, Aleksandra Piktus, Patrick Lewis, Gautier Izac-\nard, Lucas Hosseini, Jane Dwivedi-Yu, Maria Lomeli, Timo Schick, Michele\nBevilacqua, et al .2023. Improving wikipedia verifiability with ai. Nature Machine\nIntelligence 5, 10 (2023), 1142\u20131148. (Cited on 2)\n[28] Miriam Redi, Besnik Fetahu, Jonathan Morgan, and Dario Taraborelli. 2019. Cita-\ntion Needed: A Taxonomy and Algorithmic Assessment of Wikipedia\u2019s Verifiabil-\nity. In The World Wide Web Conference (WWW \u201919) . Association for Computing\nMachinery, New York, NY, USA, 1567\u20131578. https://doi.org/10.1145/3308558.\n3313618 (Cited on 2)\n[29] Emiel Rijshouwer, Justus Uitermark, and Willem de Koster. 2023. Wikipedia: a\nself-organizing bureaucracy. Information, Communication & Society 26, 7 (2023),\n1285\u20131302. (Cited on 9)\n[30] Aaron Shaw and Benjamin M Hill. 2014. Laboratories of oligarchy? How the iron\nlaw extends to peer production. Journal of Communication 64, 2 (2014), 215\u2013238.\n(Cited on 9)\n[31] Kai Shu, H. Russell Bernard, and Huan Liu. 2019. Studying Fake News via\nNetwork Analysis: Detection and Mitigation. In Emerging Research Challenges\nand Opportunities in Computational Social Network Analysis and Mining , Nitin\nAgarwal, Nima Dokoohaki, and Serpil Tokdemir (Eds.). Springer International\nPublishing, Cham, 43\u201365. https://doi.org/10.1007/978-3-319-94105-9_3 (Cited\non 2, 8)\n[32] Xing Su, Jian Yang, Jia Wu, and Yuchen Zhang. 2023. Mining user-aware multi-\nrelations for fake news detection in large scale online social networks. In Pro-\nceedings of the sixteenth ACM international conference on web search and data\nmining . 51\u201359. (Cited on 1)\n[33] Swati Swati, Adrian Mladeni\u0107 Grobelnik, Dunja Mladeni\u0107, and Marko Grobelnik.\n2023. A commonsense-infused language-agnostic learning framework for enhanc-\ning prediction of political bias in multilingual news headlines. Knowledge-Based\nSystems 277 (2023), 110838. (Cited on 2)\n[34] Mykola Trokhymovych, Muniza Aslam, Ai-Jou Chou, Ricardo Baeza-Yates, and\nDiego Saez-Trumper. 2023. Fair Multilingual Vandalism Detection System for\nWikipedia. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining (KDD \u201923) . Association for Computing Machinery,\nNew York, NY, USA, 4981\u20134990. https://doi.org/10.1145/3580305.3599823 (Cited\non 3)\n[35] Mykola Trokhymovych and Diego Saez-Trumper. 2021. WikiCheck: An End-\nto-end Open Source Automatic Fact-Checking API based on Wikipedia. In Pro-\nceedings of the 30th ACM International Conference on Information & Knowledge\nManagement (CIKM \u201921) . Association for Computing Machinery, New York, NY,\nUSA, 4155\u20134164. https://doi.org/10.1145/3459637.3481961 (Cited on 2)\n[36] Soroush Vosoughi, Deb Roy, and Sinan Aral. 2018. The spread of true and false\nnews online. science 359, 6380 (2018), 1146\u20131151. (Cited on 1)\n[37] Krzysztof W\u0119cel and W\u0142odzimierz Lewoniewski. 2015. Modelling the quality of\nattributes in Wikipedia infoboxes. In Business Information Systems Workshops:\nBIS 2015 International Workshops, Pozna\u0144, Poland, June 24-26, 2015, Revised Papers\n18. Springer International Publishing, Cham, 308\u2013320. (Cited on 2)\n[38] Wikipedia contributors. 2024. English Wikipedia. 2024. Wikipedia:Reliable\nsources/Perennial sources. https://en.wikipedia.org/wiki/Wikipedia:Reliable_\nsources/Perennial_sources [Online; accessed 12-April-2024]. (Cited on 2, 8)\n[39] Mark D Wilkinson, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Apple-\nton, Myles Axton, Arie Baak, Niklas Blomberg, Jan-Willem Boiten, Luiz Bonino\nda Silva Santos, Philip E Bourne, et al .2016. The FAIR Guiding Principles for\nscientific data management and stewardship. Scientific data 3, 1 (2016), 1\u20139.\n(Cited on 9)\n[40] Jiaying Wu, Shen Li, Ailin Deng, Miao Xiong, and Bryan Hooi. 2023. Prompt-\nand-Align: Prompt-Based Social Alignment for Few-Shot Fake News Detection.\nInProceedings of the 32nd ACM International Conference on Information and\nKnowledge Management (CIKM \u201923) . Association for Computing Machinery, New\nYork, NY, USA, 2726\u20132736. https://doi.org/10.1145/3583780.3615015 (Cited on 1)[41] Liang Xiao, Qi Zhang, Chongyang Shi, Shoujin Wang, Usman Naseem, and Liang\nHu. 2024. MSynFD: Multi-hop Syntax aware Fake News Detection. In Proceedings\nof the ACM on Web Conference 2024 . 4128\u20134137.\n[42] Tianshu Xiao, Sichang Guo, Jingcheng Huang, Riccardo Spolaor, and Xiuzhen\nCheng. 2023. HiPo: Detecting Fake News via Historical and Multi-Modal Analyses\nof Social Media Posts. In Proceedings of the 32nd ACM International Conference on\nInformation and Knowledge Management (CIKM \u201923) . Association for Computing\nMachinery, New York, NY, USA, 2805\u20132815. https://doi.org/10.1145/3583780.\n3614914\n[43] Xiaofei Xu, Ke Deng, and Xiuzhen Zhang. 2022. Identifying cost-effective de-\nbunkers for multi-stage fake news mitigation campaigns. In Proceedings of the\nFifteenth ACM International Conference on Web Search and Data Mining . 1206\u2013\n1214. (Cited on 1)\n[44] Puyu Yang and Giovanni Colavizza. 2024. Polarization and reliability of news\nsources in Wikipedia. Online Information Review ahead-of-print, ahead-of-print\n(2024), 1\u201318. (Cited on 8)\n[45] Junting Ye and Steven Skiena. 2019. MediaRank: Computational Ranking of\nOnline News Sources. In Proceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining (KDD \u201919) . Association for\nComputing Machinery, New York, NY, USA, 2469\u20132477. https://doi.org/10.1145/\n3292500.3330709 (Cited on 8)\n[46] Dave Van Zandt. 2024. Media Bias/Fact-Check. (2024). https://mediabiasfactcheck.\ncom/ [accessed 2024 April 23]. (Cited on 2, 8)\n[47] Wenxuan Zhang, Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong\nBing. 2023. M3Exam: A Multilingual, Multimodal, Multilevel Benchmark\nfor Examining Large Language Models. In Advances in Neural Informa-\ntion Processing Systems , A. Oh, T. Naumann, A. Globerson, K. Saenko,\nM. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates, Inc., New York,\n5484\u20135505. https://proceedings.neurips.cc/paper_files/paper/2023/file/\n117c5c8622b0d539f74f6d1fb082a2e9-Paper-Datasets_and_Benchmarks.pdf\n(Cited on 8)\n[48] Xiang Zhang, Senyu Li, Bradley Hauer, Ning Shi, and Grzegorz Kondrak. 2023.\nDon\u2019t trust ChatGPT when your question is not in English: A study of mul-\ntilingual abilities and types of LLMs. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing . Association for Computational\nLinguistics, Singapore, 7915\u20137927. (Cited on 8)\n[49] Yuehua Zhao, Jingwei Da, and Jiaqi Yan. 2021. Detecting health misinformation\nin online health communities: Incorporating behavioral features into machine\nlearning based approaches. Information Processing & Management 58, 1 (2021),\n102390. (Cited on 2)\n[50] Xiang Zheng, Jiajing Chen, Erjia Yan, and Chaoqun Ni. 2023. Gender and country\nbiases in Wikipedia citations to scholarly publications. Journal of the Association\nfor Information Science and Technology 74, 2 (2023), 219\u2013233. (Cited on 8)\n10", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Language-Agnostic Modeling of Source Reliability on Wikipedia", "author": ["J D'Ignazi", "A Kaltenbrunner", "Y Mejova"], "pub_year": "2024", "venue": "arXiv preprint arXiv \u2026", "abstract": "Over the last few years, content verification through reliable sources has become a fundamental  need to combat disinformation. Here, we present a language-agnostic model designed"}, "filled": false, "gsrank": 326, "pub_url": "https://arxiv.org/abs/2410.18803", "author_id": ["", "e7cIlFAAAAAJ", "iXWFZMQAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:ETog3ZmtQt4J:scholar.google.com/&output=cite&scirp=325&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D320%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=ETog3ZmtQt4J&ei=PrWsaIaUD7TWieoP1pCJ2AY&json=", "num_citations": 1, "citedby_url": "/scholar?cites=16015554101234383377&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:ETog3ZmtQt4J:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2410.18803"}}, {"title": "Tanbih: Get to know what you are reading", "year": "2019", "pdf_data": "Tanbih: Get To Know What You Are Reading\nYifan Zhang1, Giovanni Da San Martino1, Alberto Barr \u00b4on-Cede \u02dcno2, Salvatore Romeo1,\nJisun An1, Haewoon Kwak1, Todor Staykovski3, Israa Jaradat4, Georgi Karadzhov5,\nRamy Baly6, Kareem Darwish1, James Glass6, Preslav Nakov1\n1Qatar Computing Research Institute, HBKU,2Universit `a di Bologna, Forl `\u0131, Italy\n3So\ufb01a University4University of Texas at Arlington,5SiteGround Hosting EOOD\n6MIT Computer Science and Arti\ufb01cial Intelligence Laboratory\nfyzhang,gmartino,sromeo,jan,hkwak,kdarwish,pnakov g@hbku.edu.qa\na.barron@unibo.it ,israa.jaradat@mavs.uta.edu ,fbaly,glassg@mit.edu\nAbstract\nWe introduce Tanbih , a news aggregator with\nintelligent analysis tools to help readers un-\nderstanding what\u2019s behind a news story. Our\nsystem displays news grouped into events and\ngenerates media pro\ufb01les that show the gen-\neral factuality of reporting, the degree of pro-\npagandistic content, hyper-partisanship, lead-\ning political ideology, general frame of report-\ning, and stance with respect to various claims\nand topics of a news outlet. In addition, we\nautomatically analyse each article to detect\nwhether it is propagandistic and to determine\nits stance with respect to a number of contro-\nversial topics.\n1 Introduction\nNowadays, more and more readers consume news\nonline. The reduced costs and, generally speak-\ning, less strict regulations with respect to standard\npress, have led to a proliferation of the number of\nonline sources. However, that does not necessar-\nily entail that readers are exposed to a plurality of\nviewpoints. News consumed via social networks\nare known to reinforce the bias of the user (Flax-\nman et al., 2016). On the other hand, visiting\nmultiple websites to gather a more comprehensive\nanalysis of an event might be too time consuming\nfor an average reader.\nNews aggregators \u2014such as Flipboard1, News\nLens2and Google News3\u2014, gather news from dif-\nferent sources and, in the case of the latter two,\ncluster them into events. In addition, News Lens\ndisplays all articles about an event in a timeline\nand provides additional information, such as sum-\nmary of the event and a description for each entity\nmentioned in an article.\n1https://flipboard.com\n2https://newslens.berkeley.edu\n3https://news.google.com .While these news aggregators help readers to get a\nmore comprehensive coverage of an event, some\nof the sources might be unknown to the user,\ntherefore he/she could naturally question the va-\nlidity and trustworthiness of the information pro-\nvided. Deep analysis of the content published by\nnews outlets has been performed by expert jour-\nnalists. For example, Media Bias/Fact Check4pro-\nvides reports on the bias and factuality of report-\ning of entire news outlets, whereas Snopes5and\nFactCheck6are popular fact checking websites.\nAll these manual efforts cannot cope with the rate\nat which news are produced.\nWe propose Tanbih , a news platform that, in ad-\ndition to displaying news grouped into events, pro-\nvides additional information about the articles and\ntheir media source in order to develop the media\nliteracy of users. Our system automatically gener-\nates media pro\ufb01les with reports on the factuality,\nleading political ideology, hyper-partisanship, use\nof propaganda and bias of a news outlet. Further-\nmore, Tanbih automatically categorizes articles in\nEnglish and Arabic, \ufb02ags potentially propagandis-\ntic ones, and examines framing bias.\n2 System Architecture\nThe architecture of Tanbih is sketched in Figure 1.\nThe system consists of three main components: an\nonline streaming processing pipeline for data col-\nlection and article level analysis, of\ufb02ine process-\ning for event and media source level analysis and\na website for delivering news to consumers. The\nonline streaming processing pipeline continuously\nretrieves articles in English and Arabic. Trans-\nlation, categorization, general frame of reporting\nclassi\ufb01cation and propaganda detection are per-\nformed for each article.\n4http://mediabiasfactcheck.com\n5http://www.snopes.com/\n6http://www.factcheck.org/arXiv:1910.02028v1  [cs.CL]  4 Oct 2019\nFigure 1: Architecture of Tanbih .\nClustering is performed on the articles that are col-\nlected every 30 minutes. Of\ufb02ine processing in-\ncludes factuality prediction, leading political ide-\nology prediction, audience reach and twitter user\nbased bias prediction on source level and stance\ndetection, aggregation of statistics at article level,\ne.g. propaganda index (see Section 2.3), for each\nmedium. Of\ufb02ine processing does not have strict\ntime requirements, therefore the choice of the\nmodels we develop will favour accuracy of the re-\nsults over speed.\nIn order to run everything in a streaming and\nscalable fashion, we use KAFKA7as messaging\nqueue and Kubernetes8to manage scalability and\nfault-tolerant deployment. In the following we de-\nscribe each component of the system. We have\nopen sourced the code for some of those, we will\nrelease the remaining ones upon acceptance of the\ncorresponding research papers.\n2.1 Crawlers and Translation\nOur crawlers collect articles from an on-growing\nlist of sources9, which currently includes 155\nRSS feeds, 82 twitter accounts and 2 websites.\nOnce a link to an article is obtained from any\nof these sources, we rely on the Newspaper3k\nPython library to retrieve its content.10After de-\nduplication, crawlers currently download 7k-10k\narticles every day. Currently we have more than\n700k articles stored in our database. In order to\ndisplay news both in English and in Arabic, we\nuse QCRI Machine Translation (Dalvi et al., 2017)\nto translate English content into Arabic and vice\nversa. Since translation is performed of\ufb02ine, we\nselect the most accurate system in Dalvi et al.\n(2017), i.e. the Neural-based one.\n7https://kafka.apache.org\n8https://kubernetes.io\n9https://www.tanbih.org/about\n10https://newspaper.readthedocs.io2.2 Section Categorization\nWe build a model to classify an article into one\nof six news sections: Entertainment, Sports, Busi-\nness, Technology, Politics, and Health. We build a\ncorpus using the New York Times articles from the\nFakeNews dataset11published between Jan. 1st,\n2000 and Dec. 31st, 2017. We extract the news\nsection information embedded in the article URL\nand in total we use 538k articles for training our\nmodels on TF-IDF representations of the contents.\nOn a test set of 107k articles, the best-performing\nmodel is built based on Logistic Regression with\nF1=0:82;0:58;0:8;and0:9for Sports, Business,\nTechnology, and Politics, the sections used in our\nsystem, respectively. The baseline F 1is 0.497.\n2.3 Propaganda Detection\nWe developed a propaganda detection component\nto \ufb02ag articles that potentially could be propagan-\ndistic, i.e. purposefully biased to in\ufb02uence its\nreaders and ultimately pursue a speci\ufb01c agenda.\nGiven a corpus of news, binary labelled as pro-\npagandistic/non propagandistic (Barr \u00b4on-Cede \u02dcno\net al., 2019), we train a maximum entropy classi-\n\ufb01er trained on 51k articles, represented with var-\nious style-related features, such as character n-\ngrams and a number of vocabulary richness and\nreadability measures, and obtain state-of-the-art\nF1=82:89on a separate test set of 10k articles.\nWe refer to the score p2[0;1]of the classi\ufb01er\naspropaganda index and we de\ufb01ne the following\npropaganda labels which we\u2019ll use to \ufb02ag articles\n(see Figure 2): very unlikely ( p <0:2), unlikely\n(0:2\u0014p <0:4), somehow ( 0:4\u0014p <0:6), likely\n(0:6\u0014p <0:8), and very likely ( p\u00150:8).\n11https://github.com/several27/FakeNewsCorpus\n2.4 Framing Bias Detection\nFraming is a central concept in political commu-\nnication, which intentionally emphasizes (or ig-\nnores) certain dimensions of an issue (Entman,\n1993). In Tanbih , we infer frames of news articles\nto make it transparent. We use the Media Frames\nCorpus (MFC) (Card et al., 2015) for training our\nmodel to detect topic-agnostic media frames.\nWe \ufb01ne-tuned the BERT-based model with our\ntraining data using a small learning rate, 0.0002,\na maximum sequence number of 128, and a batch\nsize of 32. The performance of the model, trained\non 11k articles in MFC, is an accuracy of 66.7% on\na test set of 1,138 articles, which is better than the\nreported state-of-the-art (58.4%) from the subset\nof MFC (Ji and Smith, 2017).\n2.5 Factuality of Reporting and Leading\nPolitical Ideology of a Source\nVerifying the reliability of the source is one of the\nbasic tools used by investigative journalists to ver-\nify information reliability. To tacke this issue, we\nincorporated \ufb01ndings from our recent research on\nclassifying the political bias and factuality of re-\nporting of a news media (Baly et al., 2018) into\nTanbih . In order to predict the factuality and\nthe bias for a given news medium, we considered:\na representation for a typical article of a medium\nby averaging linguistic and semantic features of all\narticles of the medium; features extracted from the\nWikipedia page of the source and from the meta-\ndata of the Twitter account, the structure of the\nmedium\u2019s URL to identify malicious patterns (Ma\net al., 2009) and web traf\ufb01c through the Alexa\nRank12.\nIn order to collect gold labels for training our\nsupervised models, we used the data from the Me-\ndia Bias/Fact Check (MBFC) website,13which\ncontains reliable annotations of factuality, bias and\nother aspects for over 2,000 news media. We\ntrain a Support Vector Machine (SVM) classi\ufb01er\nto predict factuality or bias using the representa-\ntions above. Factuality of reporting was mod-\neled at a 3-point scale ( low,mixed andhigh), and\nthe model achieved a 65% accuracy. On the other\nhand, political ideology was modeled on a left-to-\nright scale, and the model achieved a 69% accu-\nracy.\n12http://www.alexa.com/\n13https://mediabiasfactcheck.com2.6 Stance Detection\nStance detection aims to identify the relative per-\nspective of a piece of text with respect to a claim,\ntypically modeled using labels such as agree ,dis-\nagree ,discuss , and unrelated . An interesting ap-\nplication of stance detection is medium pro\ufb01ling\nwith respect to controversial topics. In this setting,\ngiven a particular medium, the stance for each arti-\ncle is computed with respect to a set of prede\ufb01ned\nclaims. The stance of a medium is then obtained\nby aggregating the stance at article level. In Tan-\nbihthe stance is used to pro\ufb01le media sources.\nWe implemented our stance detection by \ufb01ne-\ntuning the BERT classi\ufb01er on the FNC-1 dataset\nfrom the Fake News Challenge14. Our model out-\nperforms the best submitted system (Hanselowski\net al., 2018). In particular, our system obtained\nF1macro = 75 :30and F 1= 69 :61;49:76;83:01;\nand98:81foragree ,disagree ,discuss , and unre-\nlated classes, respectively.\n2.7 Audience Reach\nUser interactions on Facebook enables the plat-\nform to generate comprehensive user pro\ufb01les such\nas gender, age, income bracket, and political pref-\nerences. After marketers have determined a set\nof criteria for their target audience, Facebook can\nthen provide them with an estimate of the size of\nthis audience on its platform To illustrate, there\nare an estimated 160K Facebook users that are 20-\nyear-old, very liberal females with an interest in\nThe New York Times. In our system, we ex-\nploit the demographic composition, the political\nleaning in particular, of Facebook users who fol-\nlow news media as a means to improve media bias\nprediction.\nTo get the audience of each news medium,\nwe use Facebook\u2019s Marketing API to identify the\nmedium\u2019s \u201cInterest ID.\u201d Using this ID, we then ex-\ntract the demographic data of the medium\u2019s audi-\nence with a focus on audience members who re-\nside in the US and their political leanings (ideolo-\ngies), which we categorize according to \ufb01ve classi-\n\ufb01cations: ( Very Conservative, Conservative, Mod-\nerate, Liberal, and Very Liberal )15.\n14http://www.fakenewschallenge.org/\n15Political leaning information is only available for US-\nbased Facebook users\n2.8 Twitter User-Based Bias Classi\ufb01cation\nControversial social and political issues may spur\nsocial media users to express their opinion through\nsharing supporting newspaper articles. Our intu-\nition is that the bias (or ideological leaning) of\nnews sources can be inferred based on the bias of\nusers. For example, if articles from a news source\nare strictly shared by left or right leaning users,\nthen the source is likely far-left or far-right lean-\ning respectively. Similarly, if it is being cited by\nboth groups, then it is likely closer to the cen-\nter. We used an unsupervised user-based stance\ndetection method on different controversial topics\nto \ufb01nd core groups of right and left-leaning users\n(Darwish et al., 2019). Given that the stance de-\ntection produces clusters with nearly perfect purity\n(>97% purity), we used the identi\ufb01ed core users\nto train a deep learning-based classi\ufb01er, fastText\n(Joulin et al., 2016), using the accounts that they\nretweeted as features to further tag more users.\nNext, we computed the so-called valence score\nfor each news source for each topic. The valence\nscores ranges between -1 and 1, with higher ab-\nsolute values indicating being cited with greater\nproportion by one group as opposed to the other.\nThe score is calculated as follows (Conover et al.,\n2011): V(u) = 2tf(u;C0)\ntotal (C0)\ntf(u;C0)\ntotal (C0)+tf(u;C1)\ntotal (C1)\u00001, where\ntf(u; C 0)is the number of times (term frequency)\nitemuis cited by group C0, and total(C0)is the\nsum of the term frequencies of all items cited by\nC0.tf(u; C 1)andtotal(C1)are de\ufb01ned in a sim-\nilar fashion. We subdivided the range between -1\nand 1 into 5 equal size ranges and assigned the la-\nbelsfar-left ,left,center ,right , and far-right to the\nranges.\n2.9 Event Identi\ufb01cation / Clustering\nThe clustering module aggregates news articles\ninto stories. The pipeline is divided in two stages:\n(i) local topics identi\ufb01cation and (ii) long-term\ntopics matching for story generation.\nFor step (i), We represent each article as a tf\u0000\n\u0000id fvector, built from the title and the body con-\ncatenated. The pre-processing consists of case-\nfolding, lemmatization, punctuation removal, and\nstopwording. In order to obtain the preliminary\nclusters, in stage (i) we compute the cosine simi-\nlarity between all article pairs in a prede\ufb01ned time\nwindow. We set n= 6 as the number of days\nwithing a window with an overlap of 3 days.\nFigure 2: Tanbih website home page.\nThe resulting matrix of similarities for each\nwindow is then used to build a graph G=\n(V; E)where Vis the set of vertices \u2014the news\narticles\u2014 and Eis the set of edges. An edge be-\ntween two articles fdi; djg2Vis drawn only if\nsim(di; dj)\u0015T1, with T1= 0:31. We select all\nparameters empirically on the training part of the\ncorpus from (Miranda et al., 2018).\nThe sequence of overlapping local graphs is\nmerged in order of their creation, thus generat-\ning stories from topics. After merging, a commu-\nnity detection algorithm is used in order to \ufb01nd\nthe correct assignment of the nodes into clusters.\nWe use one of the fastest modularity-based algo-\nrithms: the Louvain method (Blondel et al., 2008).\nFor step (ii), the topics created from the pre-\nceding stage are merged if the cosine similarity\nsim(ti; tj)\u0015T2, where ti(tj) is the mean of all\nvectors belonging to topic i(j), with T2= 0:8.\nThe model has state-of-the-art performance, on\nthe test partition of the corpus from Miranda\net al. (2018): F1= 98:11and F 1BCubed = 94:41\n(F1BCubed is an evaluation measure speci\ufb01cally de-\nsigned to evaluate clustering algorithms (Amig \u00b4o\net al., 2009)). As a comparison, the best model\nin (Miranda et al., 2018) has F1= 94:1(see\nStaykovski et al. (2019) for further details).\n3 Interface\nThe home page of Tanbih16displays news arti-\ncles grouped into stories, i.e., clusters of articles\n(see the screenshot in Figure 2). Each story is dis-\nplayed as a card. Users can go back and forth be-\ntween the articles of an event by clicking on the\nleft/right arrows below the title of the article. The\npropaganda label is displayed if the article is pro-\npagandistic.\n16http://www.tanbih.org\n(a)\n (b)\n(c)\nFigure 3: A partial screenshot of the media pro\ufb01le page for CNN in Tanbih .\nFigure 4: A partial screenshot of the topic page for\nKhashoggi Murder in Tanbih .\nIn Figure 2 the article from the Sputnik is\n\ufb02agged as likely to be propagandistic by our sys-\ntem. The source of each article is displayed with\nthe logo or the avatar of the respective news orga-\nnization, and it links to a pro\ufb01le page of this orga-\nnization (see Figure 3). On the top-left of the home\npage, Tanbih provides language selection buttons,\ncurrently English and Arabic only, to switch the\nlanguage the news are display in. A search box in\nthe top-right corner is also provided allowing the\nuser to \ufb01nd the pro\ufb01le page of a particular news\nmedium of interest.On the media pro\ufb01le page (Figure 3a), a short\nextract from the Wikipedia page of the medium is\ndisplayed on top, with recently-published articles\non the right-hand side. The pro\ufb01le page includes\na number of statistics automatically derived from\nthe models in Section 2. We use as an example\nFigure 3 which shows screenshots of the pro\ufb01le\nof CNN17. The \ufb01rst two charts in Figure 3a show\ncentrality and hyper-partisanship (in the example,\nCNN is reported as fairly central and low hyper-\npartisan) and the distribution of propagandistic ar-\nticles (CNN publishes mostly non-propagandistic\narticles). Figure 3b shows the overall framing bias\ndistribution for the medium (CNN focuses mostly\non cultural identity and politics), factuality of re-\nporting (CNN is mostly factual). The pro\ufb01le also\nshows the leading political ideology distribution\nof the medium. Figure 3c shows audience reach\nof the medium and the bias classi\ufb01cation accord-\ning to users\u2019 retweets (see Section 2.8): CNN is\npopular among readers with any political view, al-\nthough it tends to have a left-leaning ideology on\nthe topics listed. The pro\ufb01le also features reports\non the stance of CNN on a number of topics.\nUsing the topic search box on the Tanbih home\npage, user can \ufb01nd the dedicated page of a topic,\nfor example Brexit or the Khashoggi\u2019s murder.\nThe top of the Khashoggi\u2019s murder event page is\nshown in Figure 4.\n17CNN full pro\ufb01le page is available at https://www.\ntanbih.org/media/1\nRecent stories in this topic will be listed on the top\nof the page, followed by statistics such as num-\nber of countries, number of articles and number of\nmedia. A map showing how much reporting on\nthis event by each country allows users to have an\noverview of how important this topic is for these\ncountries. The page also has two sets of charts\nshowing i)the top countries in terms of coverage\nof the event, both by absolute numbers and by ra-\ntio with respect to the total number of articles pub-\nlished; ii)the media sources that have most propa-\ngandistic content on the topic, again both in ab-\nsolute terms and by ratio with respect to the total\nnumber of articles published by the medium on the\ntopic. The pro\ufb01le page also features plots equiv-\nalent to the ones in Figure 3b, showing the distri-\nbution of propagandistic articles and the framing\nbias on a topic.\n4 Conclusions and Future Work\nWe have introduced Tanbih , our news aggregator\nwhich automatically computes media level and ar-\nticle level analyses to help the user in better un-\nderstanding what they are reading. Tanbih features\nfactuality prediction, propaganda detection, stance\ndetection, translation, leading political ideology\nanalysis, media framing bias detection, and event\nclustering. The architecture of Tanbih is \ufb02exible,\nfault-tolerant and it is able to scale to handle thou-\nsands of sources.\nAs future work, we plan to expand the system to\ninclude many more sources, especially from non-\nEnglish speaking regions and to add interactive\ncomponents, for example letting users ask ques-\ntions about a topic.\nReferences\nEnrique Amig \u00b4o, Julio Gonzalo, Javier Artiles, and\nFelisa Verdejo. 2009. A comparison of extrinsic\nclustering evaluation metrics based on formal con-\nstraints. Inf. Retr. , 12(4):461\u2013486.\nRamy Baly, Georgi Karadzhov, Dimitar Alexandrov,\nJames Glass, and Preslav Nakov. 2018. Predict-\ning factuality of reporting and bias of news media\nsources. In Proc. of EMNLP\u201918 , pages 3528\u20133539.\nAlberto Barr \u00b4on-Cede \u02dcno, Giovanni Da San Martino, Is-\nraa Jaradat, and Preslav Nakov. 2019. Proppy: Or-\nganizing news coverage on the basis of their propa-\ngandistic content. Information Processing and Man-\nagement , 56(5):1849\u20131864.Vincent D Blondel, Jean-Loup Guillaume, Renaud\nLambiotte, and Etienne Lefebvre. 2008. Fast un-\nfolding of communities in large networks. Journal\nof Statistical Mechanics: Theory and Experiment ,\n2008(10):P10008.\nDallas Card, Amber E Boydstun, Justin H Gross, Philip\nResnik, and Noah A Smith. 2015. The media frames\ncorpus: Annotations of frames across issues. In\nProc. of ACL \u201915 , pages 438\u2013444.\nMichael Conover, Jacob Ratkiewicz, Matthew R Fran-\ncisco, Bruno Gonc \u00b8alves, Filippo Menczer, and\nAlessandro Flammini. 2011. Political polarization\non Twitter. In Proc. of ICWSM\u201911 , pages 89\u201396.\nFahim Dalvi, Yifan Zhang, Sameer Khurana, Nadir\nDurrani, Hassan Sajjad, Ahmed Abdelali, Hamdy\nMubarak, Ahmed Ali, and Stephan V ogel. 2017.\nQCRI live speech translation system. In Proc. of\nEACL\u201917 , pages 61\u201364.\nKareem Darwish, Peter Stefanov, Micha \u00a8el J Aupetit,\nand Preslav Nakov. 2019. Unsupervised user stance\ndetection on Twitter. In Proc. of ICWSM\u201920 .\nRobert M Entman. 1993. Framing: Toward clari\ufb01ca-\ntion of a fractured paradigm. Journal of communi-\ncation , 43(4):51\u201358.\nSeth Flaxman, Sharad Goel, and Justin M Rao. 2016.\nFilter bubbles, echo chambers, and online news con-\nsumption. Public opinion quarterly , 80(S1):298\u2013\n320.\nAndreas Hanselowski, Avinesh P.V .S., Benjamin\nSchiller, Felix Caspelherr, Debanjan Chaudhuri,\nChristian M. Meyer, and Iryna Gurevych. 2018. A\nretrospective analysis of the fake news challenge\nstance-detection task. In Proc. of COLING\u201918 ,\npages 1859\u20131874.\nYangfeng Ji and Noah A. Smith. 2017. Neural dis-\ncourse structure for text categorization. In Proc.\nACL\u201917 , pages 996\u20131005.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\nTomas Mikolov. 2016. Bag of tricks for ef\ufb01cient text\nclassi\ufb01cation. arXiv preprint arXiv:1607.01759 .\nJustin Ma, Lawrence K. Saul, Stefan Savage, and Ge-\noffrey M. V oelker. 2009. Identifying suspicious\nURLs: An application of large-scale online learning.\nInProc. of the 26th ICML , pages 681\u2013688.\nSebasti \u02dcao Miranda, Arturs Znotins, Shay B. Cohen,\nand Guntis Barzdins. 2018. Multilingual clustering\nof streaming news. In Proc. of EMNLP\u201918 , pages\n4535\u20134544.\nTodor Staykovski, Alberto Barr \u00b4on-Cede \u02dcno, Giovanni\nDa San Martino, and Preslav Nakov. 2019. Dense\nvs. sparse representations for news stream cluster-\ning. In Proc. of Text2story\u201919 , pages 47\u201352.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Tanbih: Get to know what you are reading", "author": ["Y Zhang", "GDS Martino", "A Barr\u00f3n-Cedeno"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We introduce Tanbih, a news aggregator with intelligent analysis tools to help readers  understanding what's behind a news story. Our system displays news grouped into events and"}, "filled": false, "gsrank": 329, "pub_url": "https://arxiv.org/abs/1910.02028", "author_id": ["", "URABLy0AAAAJ", "0q0QVG4AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:9XJ54cWnDyIJ:scholar.google.com/&output=cite&scirp=328&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D320%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=9XJ54cWnDyIJ&ei=PrWsaIaUD7TWieoP1pCJ2AY&json=", "num_citations": 7, "citedby_url": "/scholar?cites=2454364790273438453&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:9XJ54cWnDyIJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/1910.02028"}}, {"title": "Supporting verification of news articles with automated search for semantically similar articles", "year": "2021", "pdf_data": "Supporting verification of news articles with\nautomated search for semantically similar articles\nVishwani Guptaa, Katharina Beckha,b, Sven Giesselbacha,b, Dennis Wegeneraand\nTim Wirtza,c\naFraunhofer IAIS\nbCompetence Center for Machine Learning Rhine-Ruhr\ncFraunhofer Center for Machine Learnning\nAbstract\nFake information poses one of the major threats for society in the 21st century. Identifying misinformation\nhas become a key challenge due to the amount of fake news that is published daily. Yet, no approach\nis established that addresses the dynamics and versatility of fake news editorials . Instead of classifying\ncontent, we propose an evidence retrieval approach to handle fake news. The learning task is formulated\nas an unsupervised machine learning problem. For validation purpose, we provide the user with a set of\nnews articles from reliable news sources supporting the hypothesis of the news article in query and the\nfinal decision is left to the user. Technically we propose a two-step process: (i) Aggregation -step: With\ninformation extracted from the given text we query for similar content from reliable news sources. (ii)\nRefining -step: We narrow the supporting evidence down by measuring the semantic distance of the text\nwith the collection from step (i). The distance is calculated based on Word2Vec and the Word Mover\u2019s\nDistance. In our experiments, only content that is below a certain distance threshold is considered as\nsupporting evidence. We find that our approach is agnostic to concept drifts, i.e. the machine learning\ntask is independent of the hypotheses in a text. This makes it highly adaptable in times where fake news\nis as diverse as classical news is. Our pipeline offers the possibility for further analysis in the future,\nsuch as investigating bias and differences in news reporting.\nKeywords\nfake news, document similarity, word mover\u2019s distance, news verification\n1. Introduction\nAlthough its negative influence and its weaponizing usage is known for ages [ 1],fake news\n(in non-political context also known as false news [2]) and its negative impact was globally\nrecognized, during the U.S. elections in 2016, as one of the major challenges for the society of the\n21st century [ 3,4]. Importantly, it was used to promote both political campaigns in the election.\nBeside the promotion of political campaigns [ 5,6], fake news occurs with various purposes or\ndue to various circumstances, e.g. to destabilize governments in third countries, accidentally\nROMCIR 2021: Workshop on Reducing Online Misinformation through Credible Information Retrieval, held as part of\nECIR 2021: the 43rd European Conference on Information Retrieval, March 28 \u2013 April 1, 2021, Lucca, Italy (Online Event)\n/envelope-openvishwani.gupta@iais.fraunhofer.de (V. Gupta); katharina.beckh@iais.fraunhofer.de (K. Beckh);\nsven.giesselbach@iais.fraunhofer.de (S. Giesselbach); dennis.wegener@iais.fraunhofer.de (D. Wegener);\ntim.wirtz@iais.fraunhofer.de (T. Wirtz)\n/globehttp://www.iais.fraunhofer.de/nlu (S. Giesselbach); http://www.iais.fraunhofer.de/mlops (D. Wegener)\n\u00a92020 Copyright for this paper by its authors.\nUse permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\nCEUR\nWorkshop\nProceedingshttp://ceur-ws.org\nISSN 1613-0073\nCEUR Workshop Proceedings (CEUR-WS.org)\n1arXiv:2103.15581v1  [cs.IR]  29 Mar 2021\nVishwani Gupta et al. CEUR Workshop Proceedings 1\u201313\ndue to unconscious misinterpretation of facts [ 7] or as a worthwhile revenue stream based on\nadvertisement [8, 9, 10].\nFollowing [ 7] the term fake news is used twice (a) to discredit and downgrade media and\njournalism; and (b) to summarize various forms of wrong, misguided, or fabricated information.\nThroughout this manuscript we are speaking about (b) when discussing fake news. Fake news\narticles, as just described, are to a large extend published, maintained, circulated and promoted\nin social media [ 11,12,13,14]. On a high level, two strategies of potential interventions have\nbeen highlighted [ 15], (i) empowering of individuals to evaluate and assess fake news and\n(ii) structural changes preventing exposure of fake news to individuals. Most likely, machine\nlearning based intervention strategies can be categorized into the second class of strategies.\nHowever, our intent is to propose an approach, while mainly based on state-of-the-art machine\nlearning methodology, that can be categorized into the first class. Because we believe that\nthe most sustainable strategy to fight the impact of fake news is to empower individuals to\nevaluate and assess fake news, we propose to assess content with evidence from reliable news\nsources supporting the hypotheses in the articles. We leave the final decision to the user\nwhich helps to improve acceptance because no actual censorship is happening. However, a\nquantitative statistical evaluation is still possible by simply adding a threshold from cross-\nvalidation experiments on top of the mechanism.\nIn summary our contribution can be structured into the following aspects:\n\u2022Modular system for the comparison of news articles from various sources.\n\u2022Unsupervised approach for verification of a queried article and its content.\n\u2022Automatic querying for supporting articles using News API.\n\u2022An intuitive user interface which allows to individualize the collection of reliable sources\nand to receive visual feedback for the queried article.\nThe outline of the paper is as follows: Section 2 summarizes the main related work, high-\nlighting prior approaches towards verification. In section 3, we outline the relevant machine\nlearning building blocks of our approach. Section 4 introduces our system for news verification\nand describes the workflow, architecture and user interface. The deployment of the solution\narchitecture is described in section 5 as well as a discussion of the approach in general and its\nadvantages in section 6. Finally, in section 7 we summarize the approach.\n2. Related Work\nFollowing the line of argumentation of [ 16], approaches to identify fake news can be structured\ninto four major categories: knowledge-based, style-based, propagation-based and source-based\nPropagation-based analyses are concerned with how fake news spread online which is mostly\nformulated as a binary classification problem. The input can be either a news cascade [ 17] or\na self-defined graph [ 18]. For style-based analysis, the writing style is assessed according to\nmalicious intent. Perez et al. [ 19] point out stylistic biases that exists in text in order to automate\nfake news detection. Source-based approaches assess the credibility of a news source [ 20,21]\nwhile knowledge-based approaches compare news content with known facts [22].\n2\nVishwani Gupta et al. CEUR Workshop Proceedings 1\u201313\nAccording to the scheme from Zhou et al. [ 16], our proposed approach combines two cat-\negories of fake news detection: source-based and knowledge-based analysis. For both, we\nhighlight prior work.\nThe most prevalent source-based approach is to rate news sources on their credibility. Tradi-\ntional source-based approaches are Web ranking algorithms which rely on website credibility to\nimprove search results for user queries [ 23]. Two current resources for news publisher credibility\nare MediaBias/FactCheck [ 24] and NewsGuard [ 20], a browser extension that displays ratings of\nnews websites. The ratings are manually curated by journalists. Since the ratings go through a\nmanual review process the list of rated websites is prone to be incomplete and quickly outdated.\nTherefore, recent efforts aim for automating source reliability ratings. Based on expert- features\nincluding for example web-traffic, the existence of a verified Twitter account or textual infor-\nmation, the authors of [ 21] classify the news sources in a supervised manner using a Support\nVector Machine. Another approach to evaluate credibility of the knowledge is proposed by\nEsteves et al. [ 25]. The proposed approaches are based on supervised learning to automatically\nextract source reputation cues and to compute a credibility factor. A further approach that also\ntaps into style-based methods is to analyse text and metadata in the article. Rashkin et al. [ 26]\nassess the reliability of entire news articles by predicting whether the document originates\nfrom a website classified as hoax, satire or propaganda by comparing the language of real news\nwith those three categories to find linguistic characteristics of untrustworthy text. Wang et al.\nshowed that significant improvements can be achieved for fine-grained fake news detection\nwhen meta-data is combined with text [27].\nKnowledge-based approaches mostly tackle the process of fact-checking. Several fact-check-\ning organizations such as CORRECTIV [ 28], PolitiFact [ 29] and Snopes [ 30] operate by manually\nverifying claims (see [ 16] for more expert-based fact-checking websites). A drawback of manual\nverification is that it may reach readers too late. An approach tackling this issue was recently\npublished [ 22] where the authors approached fact-checking with machine learning methods\nand focuse on claim verification.\nAnother related approach is presented in [ 31] where the authors introduce a Fact Extraction\nand VERification (FEVER) Shared Task. The aim is to classify whether a claim is factual or not\nby retrieving evidence from Wikipedia. Both works treat the task as a classification problem,\nand a critical challenge with this approach is that we can not guarantee that the system is able to\ngive suggestions to very recent claims. An approach geared towards misinformation detection\nfor social media treating exactly this challenge is presented in [ 32] by including a retrieval\nstep. While the authors still include classification as a second step, i.e. for stance detection, we\ncompletely omit any supervised task and focus on retrieval and an expert-knowledge-based\nscoring.\n3. Building Blocks of the Approach\nAs presented in more detail in section 4 we propose an evidence retrieval approach to handle fake\nnews instead of classifying content. The learning task is formulated as an unsupervised machine\nlearning problem. The evidence supporting the hypothesis of the queried article is gathered\nfrom a collection of reliable news sources which we provide to the user. It is individualized\n3\nVishwani Gupta et al. CEUR Workshop Proceedings 1\u201313\nby selecting an arbitrary number of sources out of a curated list of reliable news sources for\nevidence-gathering purposes. Technically, we propose a two-step process:\n1.Aggregation -step: Extract information from the given article and query for similar content\nfrom reliable sources\n2.Refining -step: Narrow the supporting evidence down by calculating the semantic distance\nof the text with the collection that was retrieved in step 1.\nIn the following subsection we briefly introduce the most relevant machine learning concepts,\nforming the basis of the proposed approach. To calculate the semantic distance of news articles\nwe rely on distributed word embeddings and the Word Mover\u2019s distance.\n3.1. Word Embedding\nMikolov et al. [ 33] proposed the Word2Vec algorithm to learn vector representations of words.\nThe method is based on the distributional hypothesis [ 34,35] that words get their meaning\nfrom the context in which they appear. Mikolov et al. propose two different variations of the\nWord2Vec algorithm, both typically trained on large text corpora. The Continuous Bag-of-\nWords Model (CBOW) and the Continuous Skip-gram Model (skip-gram) which predict target\nwords from source context words and source context words from target words, respectively.\nSpecifically, they propose a shallow neural network architecture, which trains continuous word\nvectors representations to maximize the log probability of neighboring words in a corpus. For a\ngiven sequence of words \ud835\udc641, \ud835\udc642, ..., \ud835\udc64 \ud835\udc41, it models the probability of this particular sequence as\nfollows\n1\n\ud835\udc41\ud835\udc41\u2211\ufe01\n\ud835\udc5b=1\u2211\ufe01\n\ud835\udc57\u2208\ud835\udc5b\ud835\udc4f(\ud835\udc5b)log\ud835\udc5d(\ud835\udc64\ud835\udc57|\ud835\udc64\ud835\udc5b) (1)\nHere, \ud835\udc5b\ud835\udc4f(\ud835\udc5b)is the set of neighboring words of the word \ud835\udc64\ud835\udc5b. The unsupervised training is done\nby optimizing the maximum likelihood of a corpus of sentences (sequences of words) such that\nthe word embeddings capture the semantic information of words and relations between them,\ngiven a particular context. In their original work [ 33], the authors approximated the objective\nabove by more efficiently trainable objectives.\nA flaw of Word2Vec is its inability to infer continuous representations for words not seen\nduring training. Especially in domains such as news, new vocabulary can emerge rapidly. A\nsimple way to account for that is to incorporate morphological information about words in the\ntext representations. Bojanowski et al. [ 36] proposed fastText , an extension of the skip-gram\nmodel, which learns word representations by including sub-word information. This is achieved\nby not only representing words with vectors but also the sub-word parts they consist of, bag\nof character n-grams. Word vector representations are built as the sum of their sub-word and\ntheir own representation.\nIn this work, we experimented with two embedding models, Word2Vec and fastText embed-\ndings.\nAlthough there are by far more than two approaches available in the literature (also more\nadvanced approaches like Transformers [ 37]), see [ 38,39] for comprehensive reviews, we focus\n4\nVishwani Gupta et al. CEUR Workshop Proceedings 1\u201313\non those because they can be efficiently implemented on standard hardware and are well-\nestablished in the NLP community. Nevertheless, there is freedom in experimenting with other\nword embeddings as it only requires a change of the distance threshold. Hence, the approach\ncan be easily adapted to support news verification for different languages.\n3.2. Word Mover\u2019s Distance\nEarth mover\u2019s distance (EMD), also known as the Wasserstein distance, is a distance measure\nbetween two probability distribution. Kusner et al. [ 40] proposed a version of EMD applicable\nto language models, the Word mover\u2019s distance (WMD) which evaluates the distance between\ntwo documents represented in a continuous space using word embeddings such as the afore-\nmentioned Word2Vec and fastText embeddings. For any two documents AandB, WMD is\ndefined as the minimum cost of transforming document Ainto document B. Each document is\nrepresented by the relative frequencies of its words relative to the total number of words of the\ndocument, i.e., for the jth word in the document,\n\ud835\udc51\ud835\udc34,\ud835\udc57=\ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc61 (\ud835\udc57)/|\ud835\udc34| (2)\nwhere|\ud835\udc34|is the total word count of document A and \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc61 (\ud835\udc57)is number of occurrences\nof the word with vocabulary index \ud835\udc57. The jth word is represented by its corresponding word\nembedding, say v\ud835\udc57\u2208R\ud835\udc5b. The \ud835\udc5b-dimensional word embeddings are obtained from a pre-trained\nmodel, e.g. Word2Vec or fastText. The distance between two words can easily be measured\nusing Euclidean distance,\n\ud835\udeff(\ud835\udc56, \ud835\udc57) =\u2016v\ud835\udc56\u2212v\ud835\udc57\u2016 (3)\nBased on this choice, the Word mover\u2019s distance is defined to be the solution of the following\nlinear program,\n\ud835\udc4a\ud835\udc40\ud835\udc37 (\ud835\udc34, \ud835\udc35) = min\nT\u22650\ud835\udc49\u2211\ufe01\n\ud835\udc56=1\ud835\udc49\u2211\ufe01\n\ud835\udc57=1T\ud835\udc56,\ud835\udc57\ud835\udeff(\ud835\udc56, \ud835\udc57)\nsuch that\ud835\udc49\u2211\ufe01\n\ud835\udc56=1T\ud835\udc56,\ud835\udc57=\ud835\udc51\ud835\udc34,\ud835\udc57\nand\ud835\udc49\u2211\ufe01\n\ud835\udc57=1T\ud835\udc56,\ud835\udc57=\ud835\udc51\ud835\udc34,\ud835\udc56(4)\nHere, T\u2208R\ud835\udc49\u00d7\ud835\udc49is a non-negative matrix, where T\ud835\udc56,\ud835\udc57denotes how much of word iin document\nAis assigned to tokens of word jin document B. Empirically, WMD has reported improved\nperformance on many real world classification tasks as demonstrated in [ 40]. The WMD\nhas intriguing properties. The distance between two documents can be broken down and\nrepresented as the sparse distances between few individual words. The distance metric is\nalso hyper-parameter free. The most important feature is that it incorporates the semantic\ninformation encoded in the word embedding space and is agnostic to arbitrary word embedding\nmodels.\n5\nVishwani Gupta et al. CEUR Workshop Proceedings 1\u201313\nAggregation StepArticle to\nverifyCrawler Preprocessing GoogleNews API\nPotentially \nMatching Articles\nNo article\nfound\nRefining StepArticle Similarities\nWord\nembeddingsPotentially\nMatching ArticlesList of matching \narticles\nArticle to verify \n(pre-processed)\nFigure 1: Overview of the system\u2019s workflow with aggregation and refining step.\n4. A Retrieval-Based Approach Supporting Fake News\nIdentification Methods\nWe constructed a pipelined system which helps in extracting semantically similar articles from\nreliable news sources. Its core is the analysis of the credibility of news articles based on the\noverall evidence collected from a set of automatically retrieved articles published by reliable\nnews sources. Figure 1 gives an overview of the components and workflow. The system consists\nof three components: a news content extractor, a search engine query and a content analyzer.\nAll of these components can easily be exchanged and extended depending on the language and\nthe list of reliable news sources.\n4.1. Article Extractor\nGiven a link to an article that should be verified the article extractor component extracts\ninformation from the link such as the publication date, the article title, its authors and the\ntextual content. For most of the news sources, the python library Newspaper3k1is suitable\nand manages to extract all of the above information. However, at least for some sources,\nwe built our own article extractors by parsing the HTML page of the article and extracting\ncertain tags. We extract relevant keywords and entities from the title and body of the article\n1https://newspaper.readthedocs.io/en/latest/\n6\nVishwani Gupta et al. CEUR Workshop Proceedings 1\u201313\nusing the Newspaper API keyword extractor. The goal of this step is to get as much relevant\ninformation characterizing the article as possible. This extracted information is used in building\nan automated query.\n4.2. Querying the Google News API\nTo obtain news articles from reliable news sources we use a query component which queries the\nGoogle News API. Using the keywords and entities we obtained in the step before, we construct\na query. We structure the query so that we can filter the articles based on date, number of\nrequested news articles from the sources, location and language. The API returns ten article\nlinks based on the search criterion from every source selected. The number of articles returned\nby the API can be changed based on individual requirements and computation power. The\narticle extractor component extracts the content of the articles obtained from the search API.\nThe system offers six news sources and we can easily add new sources or remove existing ones\nfrom the list. Automatic querying used here is different from manual news search using search\nengines as we aggregate news based on dates, keywords extracted from the article, and selected\nreliable sources.\n4.3. Content Analysis: Semantic Distance Analysis\nThe content analysis component computes the semantic distance between the query article\nand the articles returned by the query component. Before computing the distance score, we\nclean the article titles and bodies by removing stop words and special symbols and computing\ntheir bag of n-grams representations. The semantic distance of articles is calculated using\nword embeddings and the WMD. For the word embeddings, we experimented with different\nword embeddings such as fastText and the pre-trained Google news embeddings. The quality\nof the word embeddings depends on the size of training data, thus, we use pre-trained word\nembeddings.\nSince the original WMD is computationally expensive, we approximate the distance by using\nthe Regularized Wasserstein distance proposed by [ 41] and only keep the five closest articles.\nThe five articles with the least distance are then selected for computation with the original\nWMD. The WMD returns a distance score for each remaining article from the individual sources.\nThe smaller the distance, the more related the articles are. Only articles that are below a\npredefined threshold are considered as similar to the given article. We set the distance threshold\nby empirically checking the distances of a couple of articles. Similar news articles, i.e. articles\nthat fall below the distance threshold, are then displayed with a message that closely related\narticles were found. If the system does not return similar articles the reader is informed that\nthe given article is potentially fake.\nOur prototype was exemplary tested on a small set of articles. A systematic evaluation with\na self-curated dataset and the FakeNewsNet [ 42] dataset is planned. The semantic distance\nanalysis in our approach is based on unsupervised models which in turn make the system highly\nadaptable to different languages. We just need to replace the word embeddings and adapt the\nthreshold.\n7\nVishwani Gupta et al. CEUR Workshop Proceedings 1\u201313\nFigure 2: The technical architecture of the Fake News Detector with three docker components.\nFurthermore, the unsupervised nature renders the approach agnostic to concept drifts which\nmeans that the machine learning task is independent of the hypotheses in a text.\n5. Architecture and Deployment\nTo showcase our approach we build a \"Fake News Detector\" system. The Fake News Detector\nsystem consists of a few technical components. Its technical architecture is based on a set of\ndocker components (see Figure 2). In detail, there are the following three docker components:\n1.A container with simple django running the python code of our application and serving\nthe frontend.\n2. A container serving the data for the backend - the model container.\n3. A container that includes data pre-processed by several NLTK functions.\nThe Fake News Detector application can be accessed via web UI (see Figure 3). In the UI, we\ncan insert a link of a news article to be verified, in this example, we want to verify an article\ntitled Gatorade banned and fined $300k for bad-mouthing water . Next, we select the news sources\nto check and match against. By clicking on the verification button the analysis process is started\nat the backend running all the components. After the analysis, the results are shown as a list of\npotentially matching articles. If no matching articles are found after the analysis, a message\nis displayed that the article might be potentially fake. In the example shown in Figure 3, we\nselected all six sources. The system queried against all sources and analysed the potentially\nmatching articles using semantic similarity and found that CNN has published a similar news\narticle during the same time frame.\n6. Discussion and Future Work\nIn the previous sections, we have addressed various benefits of following such an unsupervised\napproach. These benefits are the plasticity of the system, its modularity and user-driven decision\nmaking. In this section, we discuss several challenges and insights for the future work.\n8\nVishwani Gupta et al. CEUR Workshop Proceedings 1\u201313\nFigure 3: The User Interface of the Fake News Detector. In this example, the query article is titled\nGatorade banned and fined $300k for bad-mouthing water . The user pastes the URL into the text box,\nselects sources and receives a matching article from the source CNN.\nOne challenge for the demonstrator is to deal with very recent news. The system will not\nbe able to collect semantically similar articles from other reliable news sources that might not\nhave published yet on the subject. Here, date as well as publishing time become important. For\nfuture work, we propose to highlight this fact to the user to avoid perceiving recent news as\npotentially fake.\nAnother challenge in analyzing news articles is novel words which are out-of-vocabulary.\nFor these words no word vectors exist which in turn affects the WMD score. The system can\nhandle this scenario but the performance may be impaired. To overcome this we are crawling\nnews from major news sources every hour to build a text dataset. We propose to update word\nembedding models frequently to avoid an increased number of out-of-vocabulary words in\nrecent news articles.\nRecently, Yokoi et al. [ 43], proposed an improvement on Word Mover\u2019s Distance, namely\nWord Rotator\u2019s Distance (WRD) which measures the degree of semantic overlap between two\ntexts using word alignment. This approach is designed such that the norm (a proxy for the\n9\nVishwani Gupta et al. CEUR Workshop Proceedings 1\u201313\nimportance of word) and angle of word vectors (a proxy for dissimilarity between words)\ncorrespond to the probability mass and transportation cost in EMD, respectively. This approach\noutperforms the WMD in several semantic textual similarity tasks. This alternative might help\nin improving the document distance threshold and is subject for our planned evaluation.\nWe also propose to use the approach of document similarity for related use cases where\nwe see potential in two directions. The first direction is helping fact-checkers by providing\nadequate evidence to verify hypotheses. By providing them similar content, e.g. evidence in the\nform of news but also scientific articles, the system can support their task. Second, reviewers in\nseveral domains, e.g. medical health news review, need to determine how comprehensible a\ntext document is. By comparing a text to scientific or more simple language it is possible to\nprovide a comprehensibility score.\nSince the system alone does not guarantee news verification or falsification we recommend\nconsidering combining it with fact-checking methods.\n7. Conclusion\nWe presented a system to find semantically similar articles to a given news article from selected\nreliable sources. For the system, we propose an evidence retrieval approach to handle fake news\ninstead of treating it as a classification task. This way, we aid the users in finding supporting\nevidence and, thus, manual search work can be reduced.\nThe benefits of our system are that (i) it is unsupervised and therefore agnostic to concept\ndrifts, (ii) it gives the user decision power and (iii) it is modular, i.e. the system can be easily\nadapted to other languages, extended and improved with further components.\nAcknowledgments\nThis research has been partly funded by the Federal Ministry of Education and Research of\nGermany as part of the Competence Center for Machine Learning ML2R (01|S18038B). T. Wirtz\ncontributed as part of the Fraunhofer Center for Machine Learning within the Fraunhofer\nCluster for Cognitive Internet Technologies.\nReferences\n[1]The long and brutal history of fake news, https://www.politico.com/magazine/story/2016/\n12/fake-news-history-long-violent-214535, 2016.\n[2]C. Wardle, H. Derakhshan, Information disorder: Toward an interdisciplinary framework\nfor research and policy making, Council of Europe report 27 (2017).\n[3]A. R. Activities, Intentions in recent us elections, Intelligence Community Assessment,\nOffice of the Director of National Intelligence 6 (2017).\n[4]L. Howell, et al., Digital wildfires in a hyperconnected world, WEF report 3 (2013) 15\u201394.\n[5] D. Barstow, Behind tv analysts, pentagon\u2019s hidden hand, New York Times 20 (2008) A1.\n[6]H. Allcott, M. Gentzkow, Social media and fake news in the 2016 election, Journal of\neconomic perspectives 31 (2017) 211\u201336.\n10\nVishwani Gupta et al. CEUR Workshop Proceedings 1\u201313\n[7]T. Quandt, L. Frischlich, S. Boberg, T. Schatto-Eckrodt, Fake news, The international\nencyclopedia of Journalism Studies (2019) 1\u20136.\n[8]M. M. Waldrop, News feature: The genuine problem of fake news, Proceedings of the\nNational Academy of Sciences 114 (2017) 12631\u201312634.\n[9] N. Kshetri, J. Voas, The economics of \u201cfake news\u201d, IT Professional 19 (2017) 8\u201312.\n[10] J. A. Braun, J. L. Eklund, Fake news, real money: Ad tech platforms, profit-driven hoaxes,\nand the business of journalism, Digital Journalism 7 (2019) 1\u201321.\n[11] N. Wingfield, M. Isaac, K. Benner, Google and facebook take aim at fake news sites, The\nNew York Times 11 (2016) 12.\n[12] M. Koohikamali, A. Sidorova, Information re-sharing on social network sites in the age of\nfake news., Informing Science 20 (2017).\n[13] A. Bovet, H. A. Makse, Influence of fake news in twitter during the 2016 us presidential\nelection, Nature communications 10 (2019) 1\u201314.\n[14] K. Shu, H. R. Bernard, H. Liu, Studying fake news via network analysis: detection and\nmitigation, in: Emerging Research Challenges and Opportunities in Computational Social\nNetwork Analysis and Mining, Springer, 2019, pp. 43\u201365.\n[15] D. M. Lazer, M. A. Baum, Y. Benkler, A. J. Berinsky, K. M. Greenhill, F. Menczer, M. J.\nMetzger, B. Nyhan, G. Pennycook, D. Rothschild, et al., The science of fake news, Science\n359 (2018) 1094\u20131096.\n[16] X. Zhou, R. Zafarani, A survey of fake news: Fundamental theories, detection methods,\nand opportunities, ACM Computing Surveys (CSUR) 53 (2020) 1\u201340.\n[17] C. Castillo, M. Mendoza, B. Poblete, Information credibility on twitter, in: Proceedings of\nthe 20th international conference on World wide web, 2011, pp. 675\u2013684.\n[18] Z. Jin, J. Cao, Y. Zhang, J. Luo, News verification by exploiting conflicting social viewpoints\nin microblogs, in: Proceedings of the AAAI Conference on Artificial Intelligence, volume 30,\n2016.\n[19] V. P\u00e9rez-Rosas, B. Kleinberg, A. Lefevre, R. Mihalcea, Automatic detection of fake news,\nin: Proceedings of the 27th International Conference on Computational Linguistics, Asso-\nciation for Computational Linguistics, 2018.\n[20] Newsguard: The internet trust tool, https://www.newsguardtech.com, Accessed:\n20.12.2020.\n[21] R. Baly, G. Karadzhov, D. Alexandrov, J. Glass, P. Nakov, Predicting factuality of reporting\nand bias of news media sources, in: Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, Association for Computational Linguistics,\nBrussels, Belgium, 2018, pp. 3528\u20133539. URL: https://www.aclweb.org/anthology/D18-1389.\ndoi:10.18653/v1/D18-1389 .\n[22] B. Botnevik, E. Sakariassen, V. Setty, Brenda: Browser extension for fake news detection,\nin: Proceedings of the 43rd International ACM SIGIR Conference on Research and Devel-\nopment in Information Retrieval, SIGIR \u201920, Association for Computing Machinery, New\nYork, NY, USA, 2020, p. 2117\u20132120.\n[23] L. Page, S. Brin, R. Motwani, T. Winograd, The PageRank citation ranking: Bringing order\nto the web., Technical Report, Stanford InfoLab, 1999.\n[24] Mediabiasfactcheck, https://mediabiasfactcheck.com/, Accessed: 26.02.2021.\n[25] D. Esteves, A. J. Reddy, P. Chawla, J. Lehmann, Belittling the source: Trustworthiness\n11\nVishwani Gupta et al. CEUR Workshop Proceedings 1\u201313\nindicators to obfuscate fake news on the web, in: Proceedings of the First Workshop\non Fact Extraction and VERification (FEVER), Association for Computational Linguistics,\n2018.\n[26] H. Rashkin, E. Choi, J. Y. Jang, S. Volkova, Y. Choi, Truth of varying shades: Analyzing\nlanguage in fake news and political fact-checking, in: Proceedings of the 2017 Conference\non Empirical Methods in Natural Language Processing, Association for Computational\nLinguistics, 2017.\n[27] W. Y. Wang, \u201cliar, liar pants on fire\u201d: A new benchmark dataset for fake news detection, in:\nProceedings of the 55th Annual Meeting of the Association for Computational Linguistics\n(Volume 2: Short Papers), Association for Computational Linguistics, 2017.\n[28] Correctiv: Investigations in the public interest, https://correctiv.org, Accessed: 20.12.2020.\n[29] T. P. Institute, Politifact, https://www.politifact.com, Accessed: 20.12.2020.\n[30] Snopes, https://www.snopes.com, Accessed: 20.12.2020.\n[31] J. Thorne, A. Vlachos, O. Cocarascu, C. Christodoulopoulos, A. Mittal, The fact extraction\nand VERification (FEVER) shared task, in: Proceedings of the First Workshop on Fact\nExtraction and VERification (FEVER), Association for Computational Linguistics, 2018, pp.\n1\u20139.\n[32] T. Hossain, R. L. Logan IV, A. Ugarte, Y. Matsubara, S. Young, S. Singh, COVIDLies:\nDetecting COVID-19 misinformation on social media, in: Proceedings of the 1st Workshop\non NLP for COVID-19 (Part 2) at EMNLP 2020, Association for Computational Linguistics,\nOnline, 2020.\n[33] T. Mikolov, K. Chen, G. Corrado, J. Dean, Efficient estimation of word representations\nin vector space, in: Y. Bengio, Y. LeCun (Eds.), 1st International Conference on Learning\nRepresentations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track\nProceedings, 2013.\n[34] Z. Harris, Distributional structure, Word 10 (1954) 146\u2013162. URL: https://link.springer.\ncom/chapter/10.1007/978-94-009-8467-7_1. doi: 10.1007/978-94-009-8467-7_1 .\n[35] J. R. Firth, Papers in Linguistics, 1934-1951, Oxford University Press, London, 1957.\n[36] P. Bojanowski, E. Grave, A. Joulin, T. Mikolov, Enriching word vectors with subword\ninformation, Transactions of the Association for Computational Linguistics (2017) 135\u2013146.\n[37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, I. Polo-\nsukhin, Attention is all you need, in: Advances in neural information processing systems,\n2017, pp. 5998\u20136008.\n[38] Y. Li, T. Yang, Word embedding for understanding natural language: a survey, in: Guide\nto Big Data Applications, Springer, 2018, pp. 83\u2013104.\n[39] F. Almeida, G. Xex\u00e9o, Word embeddings: A survey, arXiv preprint arXiv:1901.09069 (2019).\n[40] M. Kusner, Y. Sun, N. Kolkin, K. Weinberger, From word embeddings to document distances,\nin: International conference on machine learning, 2015, pp. 957\u2013966.\n[41] G. Balikas, C. Laclau, I. Redko, M.-R. Amini, Cross-lingual document retrieval using\nregularized wasserstein distance, in: Proceedings of the 40th European Conference ECIR\nconference on Information Retrieval, ECIR 2018, Grenoble, France, March 26-29, 2018,\n2018.\n[42] K. Shu, D. Mahudeswaran, S. Wang, D. Lee, H. Liu, Fakenewsnet: A data repository with\nnews content, social context, and spatiotemporal information for studying fake news on\n12\nVishwani Gupta et al. CEUR Workshop Proceedings 1\u201313\nsocial media, Big Data (2020).\n[43] S. Yokoi, R. Takahashi, R. Akama, J. Suzuki, K. Inui, Word rotator\u2019s distance, in: Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),\n2020, pp. 2944\u20132960.\n13", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Supporting verification of news articles with automated search for semantically similar articles", "author": ["V Gupta", "K Beckh", "S Giesselbach", "D Wegener"], "pub_year": "2021", "venue": "arXiv preprint arXiv \u2026", "abstract": "Fake information poses one of the major threats for society in the 21st century. Identifying  misinformation has become a key challenge due to the amount of fake news that is published"}, "filled": false, "gsrank": 330, "pub_url": "https://arxiv.org/abs/2103.15581", "author_id": ["KAPUlSAAAAAJ", "P24VptUAAAAJ", "2NHUjNkAAAAJ", "P9p1pYkAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:1wC4Jxv9EAsJ:scholar.google.com/&output=cite&scirp=329&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D320%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=1wC4Jxv9EAsJ&ei=PrWsaIaUD7TWieoP1pCJ2AY&json=", "num_citations": 7, "citedby_url": "/scholar?cites=797415427116892375&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:1wC4Jxv9EAsJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2103.15581"}}, {"title": "News source credibility assessment: A Reddit case study", "year": "2025", "pdf_data": "News Source Credibility Assessment: A Reddit Case Study\nArash Amini, Yigit Ege Bayiz, Ashwin Ram, Radu Marculescu, and Ufuk Topcu\nThe University of Texas at Austin\n{a.amini, egebayiz, ashwin.ram, radum, utopcu}@utexas.edu\nAbstract\nWe present a transformer-based model for credibility as-\nsessment, CREDiBERT (CREDibility assessment using Bi-\ndirectional Encoder Representations from Transformers),\nfine-tuned for Reddit submissions focusing on political dis-\ncourse. We adopt a semi-supervised training approach for\nCREDiBERT, leveraging the community structure of Reddit.\nBy encoding submission content using CREDiBERT and in-\ntegrating it with a classification neural network, we improve\nthe credibility assessment for Reddit submission by 3%in F1\nscore compared to existing methods. Additionally, we intro-\nduce a new version of the post-to-post network in Reddit that\nefficiently encodes user interactions to enhance the credibil-\nity assessment task by 8%in the F1score. We demonstrate\nCREDiBERT\u2019s applicability by evaluating the susceptibility\nof Reddit communities to different topics and assessing the\ncredibility score of unseen sources.\nIntroduction\nSocial media and online news platforms have drastically al-\ntered the landscape of news consumption. Individuals now\nencounter a diverse array of news through social media plat-\nforms, compared to the time when people relied on news-\npapers and television channels. While the digital age offers\nunprecedented personalization and speed in news delivery, it\nalso presents a significant risk of receiving inaccurate infor-\nmation. Discerning fake news on social media platforms is\na formidable challenge, necessitating sophisticated method-\nologies to distinguish it from authentic reporting.\nRecent advances in natural language processing have\nshown great potential for identifying fake news. Through\ndetailed analysis of language patterns and textual features in\nnews content, natural language processing techniques have\ndemonstrated high precision in differentiating fake news\nfrom legitimate reports (Raza and Ding 2022). This break-\nthrough has catalyzed new research avenues, thus empower-\ning scientists to delve deeper into the characteristics of fake\nnews and devise robust countermeasures.\nDespite advances in fake news detection algorithms\nthrough content, the reputation of the article\u2019s source is still\none of the key components in assessing the trustworthiness\nof the article. As evidenced by Pehlivanoglu et al. (2021),\nCopyright \u00a9 2025, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.historical patterns in reporting accuracy often correlate to\npotential misinformation or propagandistic intents.\nThis paper diverges from the predominant focus on identi-\nfying fake news in the literature. Instead, we concentrate on\ndiscerning the credibility of news sources shared to combat\nthe spread of misinformation. Our objective is to ascertain\nthe credibility of the sources rather than categorically label-\ning specific news as true or fake. Although there is no di-\nrect, infallible correlation between a source\u2019s credibility and\nnews veracity, the news source historical track record can\noffer valuable insights into its general reliability and edito-\nrial practices. Such an analysis, especially when conducted\nover an extended period of time, can reveal patterns and ten-\ndencies that indicate the source commitment to accuracy and\njournalistic integrity.\nSource credibility refers to the perceived believability of\na source, which significantly influences how receivers pro-\ncessed the messages it shares. It is primarily comprised of\nperceptions of the source\u2019s trustworthiness (the confidence\nin the source\u2019s intent to communicate valid information) and\nexpertise (the perception that the source is capable of provid-\ning valid assertions). Credibility is fundamentally a percep-\ntual judgment, distinct from, but related to, the factual ac-\ncuracy of the information presented (Hocevar, Metzger, and\nFlanagin 2017).\nThroughout this paper, we define the credibility of a news\nsource as a numerical score that sums up the reliability and\ntrustworthiness of the information it disseminates. Political\nbias and credibility are crucial indicators of how individu-\nals perceive a news source, significantly impacting how peo-\nple engage with the media. The rise of social networks has\nchallenged the traditional dominance of conventional media\nover the information ecosystem, with content creators, influ-\nencers, and political elites now having a more direct impact\non public opinion than ever. Unlike conventional media out-\nlets, assessing the credibility of digital content is challeng-\ning due to the wide variety and volume of online content. To\naddress this, we train a language model to detect features re-\nlated to credibility in textual information. This framework\nautomates the assessment of credibility and improves our\nunderstanding of the quality of information that communi-\nties receive on various subjects.\nMisinformation and fake news can be ambiguous and\nhave different meanings depending on the context. For con-\nProceedings of the Nineteenth International AAAI Conference on Web and Social Media (ICWSM 2025)\n68\nsistency, we define fake news and misinformation as any\nnews or article contaminated by fake stories or false narra-\ntives intended to influence public opinion. Note that a high\ncredibility score for a media source does not imply it never\ndisseminates unreliable information. Rather, it indicates that\nthe likelihood of information shared from this source being\ncontaminated by misinformation is low.\nReddit, a prominent social news website, exemplifies the\nchallenges inherent in news source verification. Reddit hosts\nvarious subreddits, each focused on a specific topic. Users\nfrequently post submissions in political subreddits, includ-\ning links to news articles for discussion. Reddit provides a\ndistinctive platform where users can anonymously post con-\ntent in various topic-specific communities, providing a con-\nducive environment for disseminating fake news. The sub-\nstantial proliferation of misinformation about COVID-19 in\nReddit and QAnon incidents underscores the importance of\nmitigating misinformation across Reddit before it spreads to\nother platforms.\nWe call a website (or platform), source, when it frequently\ndisseminates information; thus, in this manuscript, the def-\ninition of a source includes conventional media outlets and\nsmall content creators such as YouTube channels or Twit-\nter profiles.A source on Reddit is verifiable if its informa-\ntion history, like that of conventional news outlets, can be\ntracked. The credibility score of a verifiable source is known,\nwhile an unverifiable source has an unknown credibility\nscore. We denote the credibility of a submission by the cred-\nibility score for the source of the submission in question.\nExpanding news sources to social media platforms such\nas Twitter and YouTube made verifying the trustworthiness\nof the sources of news shared often challenging. Although\nsome submissions link to verifiable sources, such as ma-\njor news websites, many do not. Our analysis indicates that\nmore than 60% of submissions posted in major political sub-\nreddits do nothave a verifiable source.\nRecognizing that identical news stories are often reported\nwith distinct biases across various communities, we lever-\nage Reddit\u2019s community-based structure to generate a com-\nprehensive dataset. This dataset comprises over 1.5million\npairs of Reddit submissions, each pair referencing the same\nevent but sourced from different outlets, thus having differ-\nent credibility scores. To this end, we employ sentence\ntransformers (Thakur et al. 2020) to assemble a dataset of\nsubmission pairs referencing the same event based on their\ntextual similarity. We train a bi-encoder, CREDiBERT, using\nthis dataset to capture credibility-related features by aligning\nthe textual similarity of submission pairs with their credi-\nbility score discrepancies. An artificial neural network clas-\nsifies the submissions as credible or non-credible by their\nCREDiBERT-generated embeddings. The proposed frame-\nwork outperforms BERT-based sentence classifiers by over\n3%in the F1 score in the submission credibility assessment\ntask.\nAs the last contribution, we encode the user interaction\ninformation to enhance the credibility assessment. We in-\ntroduce a weighted post-to-post network (Hurtado, Ray, and\nMarculescu 2019) that efficiently represents the social inter-\nactions among Reddit users. In the original post-to-post net-\nFigure 1: Credibility score with respect to Political Bias for\n223 news sources. The data is normalized and sourced to the\nAd-Font media website.\nwork, submissions are represented as nodes in a graph, with\nedges between submissions if they share users who com-\nment on them. We enhance this network by introducing a\nweighted version of the post-to-post network that incorpo-\nrates user reactions to the comments, providing additional\ncontext and depth to the interactions between submissions.\nOf note, this network does notrely on user profiling, which\noften raises privacy and security concerns. We then employ\na Graph Convolutional Network to infer the credibility of\nsubmissions. This approach ensures a balance between in-\nsightful analysis and user privacy, thus paving the way for\nmore ethical credibility assessment in social media.\nWe cross-validate our results with human evaluations in\ntwo stages. Given the large data requirements for training\na bi-encoder, we automate the pairing process and validate\nits accuracy by having humans manually evaluate random\nsamples selected from the results of the automated pairing\nalgorithm. Additionally, we validate the credibility assess-\nment task by comparing the results with human evaluations\nfor100samples.\nThis paper is structured as follows: It starts with a review\nof related works. Next, we describe our dataset, which com-\nprises approximately 1.2million Reddit submissions and\nover12million pairs of submissions. In the first half of the\nMethods section, we present the CREDiBERT and discuss\nthe direct credibility assessment task. The second half in-\ntroduces a new post-2-post network, incorporating user re-\nactions from comments to enhance the results further. Fol-\nlowing this, we compare our framework with other baselines\nin the Results section and discuss its applications and limi-\ntations in the Discussion section. The paper concludes by\nhighlighting our main contributions.\nRelated Work\nOver the last decade, the issue of fake news detection has\ngarnered substantial interest, particularly highlighted by its\n69\nSubmission content Subreddit Credibility Score Overall Score\nPoll: Majority of Republicans would support Trump in 2024 r/politics 0.74 78\nFor everyone who thinks he is going away: Majority of\nRepublicans would support Trump in 2024r/Republican 0.73 85\nTrump remains the most popular Republican in the country\nand the leading candidate for the 2024 GOP nominationr/Conservative 0.24 963\nTable 1: Example of submissions referencing the same event with various representations in different subreddits\nimplications during major events such as the 2016 U.S. pres-\nidential election. Initial strides in the domain were made by\nCastillo et al. (2013), who focus on assessing the credibil-\nity of Twitter content during crises, thus laying the ground-\nwork for understanding digital misinformation. The mo-\nmentum was significantly amplified post-2016 by attract-\ning a wide array of research, including social impact assess-\nments by Allcott and Gentzkow (2017) and broader misin-\nformation detection techniques by Shu et al. (2017). Chan\nand Donovan (2017) further extended this realm into strate-\ngies for countering misinformation. A persistent challenge\nnoted across studies, particularly by Torabi Asr and Taboada\n(2019), is the scarcity of high-quality labeled data, which\nimpedes the development of robust detection mechanisms.\nAutomated Fact Checking\nAutomated fact-checking has significantly evolved with the\nadvancement of artificial intelligence, increasingly relying\non the intricate features of news content and social context\nto discern authenticity. Notably, the introduction of Bidirec-\ntional Encoder Representations from Transformers (BERT)\nmarked a substantial advancement in natural language pro-\ncessing, enhancing the capability to process and understand\nnews content. Devlin et al. (2018) spearheaded this develop-\nment, which was later built upon by Jwa et al. (2019), who\nproposed exBAKE, a BERT-based architecture specifically\ntailored for fake news identification. This model notably\nimproved detection efficacy, as measured by the F1 score.\nHowever, the journey of refinement continued as Przyby\u0142a\n(2020) critically examined the text style role in automated\nfact-checking, revealing that while BERT-based methods\nare potent, they tend to overfit and might underperform in\nstyle capturing tasks of large texts compared to Stylome-\ntry and BiLSTM. Addressing some of these concerns, Raza\nand Ding (2022) proposed an enhanced transformer model\nthat leverages both content and social media traces, pushing\nthe boundary further in automated fact-checking. This on-\ngoing evolution, including the rise of pre-trained large lan-\nguage models, continually presents new opportunities and\nchallenges in the field, as evidenced by the work of Chen\nand Shu (2024) and Leite et al. (2023), who explore these\ncutting-edge developments in combating misinformation.\nCredibility Assessment\nThe quest to understand and counteract misinformation has\nled researchers to examine the role of source credibilityclosely. Pehlivanoglu et al. (2021) suggest that historical pat-\nterns in a source reporting accuracy can indicate misinfor-\nmation or propagandistic content, setting a foundation for\nfurther studies. Spezzano et al. (2021) explore how users\nrecognize and react to news from low-credibility sources\non social media, while Mosleh and Rand (2022) shift focus\ntowards measuring user exposure to misinformation rather\nthan identifying false news directly. Measuring user expo-\nsure to misinformation necessitates a robust method to as-\nsess media quality, a controversial and challenging task.\nBachmann et al. (2022) contribute to this by proposing a\nquantitative measure for news media source quality. Fur-\nther technological advancements are employed by Chiang et\nal. (2022), who apply BERT and artificial neural networks\nto automate the identification of source credibility. Lastly,\nChipidza et al. (2022) provide an interesting angle by in-\nvestigating the interplay between the ideological stance and\nperceived source credibility, particularly in the context of\nCOVID-19 discussions on Reddit. These research studies\nilluminate various facets of source credibility, highlighting\nits multifaceted nature and the diverse methodologies em-\nployed to understand and evaluate it.\nMisinformation on Reddit\nReddit\u2019s unique combination of anonymity and community-\ndriven content, exemplified by incidents like QAnon, fos-\nters an environment conducive to spreading fake news. Un-\nderstanding the mechanics of this spread is crucial for ef-\nfective detection and mitigation. Glenski et al. (2018) shed\nlight on this issue by highlighting how user engagement met-\nrics and subreddit-specific norms significantly influence the\nperceived credibility of posts on Reddit. Their findings un-\nderscore the importance of contextual and community fac-\ntors in content credibility. Building upon the need for robust\ndata in studying these dynamics, Sakketou et al. (2022) in-\ntroduced the FACTOID dataset, a comprehensive collection\nof Reddit submissions with annotated credibility and bias in-\nformation. This dataset facilitates a deeper analysis of how\nmisinformation spreaders operate within the Reddit ecosys-\ntem. Bond and Garret\u2019s (2023) study paved the way for more\nnuanced research into users\u2019 temporal and contextual inter-\nactions with both true and false news submissions on the\nplatform, contributing to the broader understanding of mis-\ninformation propagation in online communities.\nThis paper distinguishes itself from the existing body of\nliterature by introducing two significant innovations: First\nwe leverage Reddit\u2019s intricate community structure to auto-\n70\nmatically curate a novel dataset comprising pairs of submis-\nsions referencing identical events, providing a unique van-\ntage point for understanding source credibility. Secondly,\nwe introduce the CREDiBERT model, a semi-supervised ap-\nproach utilizing a Siamese network architecture specifically\ndesigned to mitigate the overfitting issues prevalent in stan-\ndard BERT-based models for this task (Przybyla 2020). This\nadaptation allows for a more robust and generalizable source\ncredibility assessment, addressing a critical challenge in the\nfield.\nData\nWe compile a comprehensive dataset from five major polit-\nical subreddits: r/politics, r/Conservative, r/Republican, and\nr/democrats. Our dataset, covering the period from January\n2016 to December 2022, includes detailed information such\nas submission text, author IDs, source domains, submission\ntimes, associated subreddits, overall submission scores, and\ncomment counts for each submission1. We then collect all\navailable comments from these submissions. We select these\nspecific subreddits based on their activity volume to ensure\na diverse range of political discourse and viewpoints. In col-\nlecting this data, we adhere to ethical guidelines for research\ninvolving online communities, ensuring the anonymity and\nprivacy of the users.\nWe utilize a dataset from the Ad Fontes Media website2,\nwhich provides political bias and credibility scores for 223\nmajor news sources. Ad Fontes Media assigns credibility\nscores and political bias ratings to media outlets, with credi-\nbility scores ranging from 0(least credible) to 64(most cred-\nible), and political bias ratings from \u221242(extreme left) to 42\n(extreme right). In this paper, we normalize the credibility\nand bias scores by dividing them to 64and42, respectively.\nFigure 1 depicts the credibility scores concerning political\nbias for 223media sources. We note that the curve is asym-\nmetric, indicating that left-leaning sources are perceived as\nmore credible than right-leaning ones.\nWe ensure the reliability of this data by cross-validating\nthe scores with additional datasets from FACTOID date set\n(Sakketou et al. 2022) and Media-Bias-Fact-Check website3\nfor the common sources. We consider a news source as ver-\nified if one can access the history of information shared by\nthe news source in question. While the Ad-front media does\nnot share details on the establishment of credibility scores,\nthey state that the credibility score reflects how frequently\ninformation shared by news outlets contains misinformation\nor deceptive narratives.\nAfter thorough data cleaning, the dataset comprises a to-\ntal of 1,312,853submissions. We organize comments based\non their reply level: first-tier comments are direct responses\nto submissions, second-tier comments are replies to first-tier\ncomments, and so on. We limit our analysis to comments\nup to the fourth tier, as we observed that comments beyond\nthis level often diverge significantly from the original sub-\nmission content. To maintain data quality, we exclude com-\n1All Reddit data collected from https://pushshift.io\n2Details available in https://adfontesmedia.com/\n3Accessible here https://mediabiasfactcheck.com/\nFigure 2: The distribution of credibility score discrepan-\ncies with respect to cosine similarity between embeddings\nprovided by CREDiBERRT (left) and S-BERT (right) for\n300,000pairs of submissions from 2022. Pairs are selected\nto have at least 0.6similarity score for S-BERT embedding,\nthus only parts with data are depicted.\nments when the author\u2019s account has been deleted, removed,\nor if the comment text is unavailable. The experiments uti-\nlize data collected from the 2022, which includes 85,291\nsubmissions. This temporal split helps to evaluate the model\nperformance on recent, previously unseen data, providing a\nmore reliable assessment of its predictive capabilities.\nMethod\nWe develop CREDiBERT based on the premise that a text\nencoder capable of capturing features related to credibility\nshould map submissions referencing the same event with\nsignificant credibility discrepancies far apart. Inspired by the\neffectiveness of bi-encoders (Reimers and Gurevych 2019;\nThakur et al. 2020) in producing sentence embeddings that\nsurpass BERT-based models, we train CREDiBERT as a bi-\nencoder that aligns textual similarities of submissions with\ntheir credit discrepancies. The initial step in training CRED-\niBERT involves identifying pairs of submissions referenc-\ning the same event. To this end, we employ Sentence Trans-\nformers to assess textual similarity between pairs and iden-\ntify submissions referencing the same event from different\nsources. After training CREDiBERT, we use the embed-\ndings it generates to train another artificial neural network\nfor credibility assessment task. It is important to note that\nwhen we discuss the credibility score for a submission, we\nspecifically refer to the credibility score for the source of\nthat submission.\nSubmission Pairing Moderators play a pivotal role in\nshaping the content of each subreddit, thus ensuring that\nsubmissions align with the intended narrative and bias. This\nmoderation influences the diversity of news sources and per-\nspectives presented within their domains and provides us\nwith submissions that reference the same event from vari-\nous sources. We identify submissions referencing the same\nevent through their textual similarity measured by sentence\ntransformer (S-BERT). Table 1 shows an example of a sub-\n71\nmission referencing the same event with different credibility\nscores.\nWhile the BERT-based model excels at creating context-\naware word embeddings through training with a masked lan-\nguage model and next-sentence prediction, it falls short in\nefficiently embedding entire sentences in the latent space.\nTo overcome this challenge, Reimers et al. 2019 introduced\nsentence transformers, which use a Siamese network archi-\ntecture to enrich sentence-level embedding. The resulting S-\nBERT model outperforms traditional BERT-based models in\nsentence embedding tasks. In this study, we employ the pre-\ntrained \u201call-distilroberta-v1\u201d model from Huggingface web-\nsite.\nLet us define the complete set of collected submissions as\nS:={s1, s2,\u00b7\u00b7\u00b7, sn}, where nrepresents the total number\nof submissions. For each submission si, we denote its text,\ncredit score, and posting time as pi,ci, and ti, respectively.\nTo determine the similarity between two news submissions,\nwe encode the text pithrough a sentence transformer, de-\nnoted by T. This model maps the content of submission si\nto the embedding ei, byei=T(pi), where eiis a vector of\nreal numbers with 768dimensions. We utilize a bi-encoder\nto identify similar submissions, which assesses the textual\nsimilarity between two submissions, siandsj, by the cosine\nsimilarity of their corresponding embeddings. We calculate\nthe similarity by\ncos(ei, ej) =\u27e8ei, ej\u27e9\n\u2225ei\u2225\u2225ej\u2225, (1)\nwhere \u2225 \u00b7 \u2225 and\u27e8\u00b7\u27e9denote the Euclidean norm and the in-\nner product, respectively. This method allows us to quanti-\ntatively compare submissions based on their textual content,\nenabling a more precise identification of similar news stories\nacross different subreddits.\nWe opt for bi-encoders over cross-encoders to efficiently\ncompare the vast number of submission pairs, primarily due\nto their faster processing capabilities. Reimers and Gurevych\n(Reimers and Gurevych 2019) note that comparing 10,000\npair of sentences using cross-encoders would take about\n65hours while generating embeddings and computing co-\nsine similarities would take less than 6second. Given that\nour dataset encompasses over one million submissions, this\nchoice significantly reduces the computational burden.\nRecognizing that submissions referencing the same event\nare likely posted in close temporal proximity, we implement\na time constraint, denoted as \u2206, to refine our search. We let\n\u2206equal to two days across this manuscript unless otherwise\nstated. This constraint means we only consider pairing sub-\nmissions if their posting times, represented by tij=|ti\u2212tj|,\nare within \u2206.\nTwo submissions are similar if they meet the temporal cri-\nteria and their similarity score exceeds the similarity thresh-\nold\u00afe= 0.60. For each qualified pair of similar submis-\nsion(si, sj), we then calculate the credit score difference\ncij=|ci\u2212cj|, which is the absolute difference in their\ncredit scores. This approach streamlines our process, ensur-\ning we focus on our analysis\u2019s most relevant and temporally\naligned submission pairs. We define the pool of all pairs ofsimilar submissions, P, by\nP:={(si, sj, cij)|\u00afe <cos(ei, ej),|ti\u2212tj| \u2264\u2206}. (2)\nThis approach effectively expands the dataset from 1.2mil-\nlion individual submissions to a comprehensive set of 12\nmillion uniquely paired submissions. Automated submis-\nsion pairing facilitates the substantial dataset expansion and\nallows us to train a bi-encoder, CREDiBERT, that captures\ncredibility-related features. This process also enables regu-\nlar, unsupervised training of the bi-encoder, ensuring it stays\ncurrent with recent trends.\nTo ensure a balanced training set, we meticulously se-\nlected 1,383,385 samples from the pool of 12,171,894\npairs. We balance the data set by dividing the set of all pairs\nbased on their credibility score discrepancies into K= 5\ngroups. If a pair (si, sj, cij)has their credibility discrepancy\nsatisfiesk\u22121\nK< \u03f1c ij<k\nK, then this pair belongs to group\nk\u2208 {1,\u00b7\u00b7\u00b7, K}and\u03f1= 2is the scaling factor. We balance\nthe dataset by limiting the maximum number of samples se-\nlected from each group to 300,000. If a group has a popula-\ntion smaller than the ideal number of samples, we select all\nthe populations of that group. We perform this selection to\nachieve representative and diverse training data, enhancing\nthe reliability and generalizability of the CREDiBERT.\nWe validated the automated pairing by randomly select-\ning1,000pair samples from the submission pair pool, P\u03c4,\nand having two humans assess whether pairs reference the\nsame event. The results indicate that the proposed algorithm\naccurately predicted 92.7%of the pairings, with the pairs\nalmost always covering the same subject. For transparency,\nwe include the data and the results of expert evaluations in\nthe code files.\nCREDiBERT To generate embeddings that reflect proper-\nties associated with the credibility scores of submissions, we\ntrain a bi-encoder, CREDiBERT, that aligns the embeddings\nof submission pairs based on their credibility score discrep-\nancies. We subsequently use these embeddings to train a sec-\nondary artificial neural network for credibility assessment\ntasks. The underlying objective is to align the embeddings\nfrom CREDiBERT such that cosine similarity between pairs\nwith minor credibility score differences have values close to\n1, indicating similarity, while those with significant discrep-\nancies are marked as dissimilar.\nTo train the CREDiBERT model, we adopt a Siamese ar-\nchitecture similar to S-BERT training. This choice is mo-\ntivated by the architecture effectiveness in processing and\ncomparing text embeddings when pairs of labeled sentences\nare available. The training is guided by the following objec-\ntive function,\nX\n(i,j)\u2208P\u03c4|cos(C(pi),C(pj))\u2212\u0000\n1\u2212\u03f1cij\u0001\n|2, (3)\nwhere C(\u00b7)denotes the embedding generated by CREDiB-\nERT,P\u03c4represent the set of training pairs and \u03f1= 2is the\nscaling factor. The objective function aims to minimize the\ndiscrepancy between the cosine similarity of the embeddings\nand the credit score differences. We trained the CREDiB-\nERT model using the \u2019all-distilroberta-v1\u2019 S-BERT model\n72\nModel Type Model F1\nNon-Credible Credible Overall\nRandomRandom 0.482 0. 513 0. 504\nMajority 0.000 1. 000 0. 583\nWord Embeddingword2vec-google-news-300 0.591\u00b10.002 0. 735\u00b10.002 0. 674\u00b10.001\nfasttext-wiki-news-300 0.597\u00b10.003 0. 739\u00b10.001 0.679\u00b10.001\nBERT-based Text ClassifierBERT-base-uncased 0.710\u00b10.005 0. 806\u00b10.006 0.762\u00b10.006\nDistilBERT-base-uncased 0.704\u00b10.008 0. 805\u00b10.004 0. 758\u00b10.006\nSentence TransformerS-BERT: all-distilroberta-v1 0.637\u00b10.002 0. 766\u00b10.001 0. 712\u00b10.001\nCREDiBERT (Ours) 0.735\u00b10.002 0. 824\u00b10.001 0.791\u00b10.001\nTable 2: CREDiBERT surpasses other text classification models in binary credibility classification of submissions, as evidenced\nby cross-validation results. We train and validate all models using 56,491and12,088samples and reporter results on 12,088\ntest samples. We show the best F1 performance in each class in bold.\nas the starting point with a dataset of 1,312,853pairs of\nsubmissions. The entire training process took nearly 8 hours\non a single NVIDIA A100 GPU.\nFigure 2 illustrates the distribution of credibility score dis-\ncrepancies of submission pairs with respect to the cosine\nsimilarity of embeddings generated by CREDiBERT and S-\nBERT. Recall that we chose the minimum similarity score of\n\u00afe= 0.60. Therefore, the cosine similarity of all the S-BERT-\ngenerated embeddings for the selected submission pairs is\ngreater than 0.60. It is evident from Figure 2 that the embed-\ndings from CREDiBERT demonstrate a strong correlation\nwith the credibility discrepancies between the submissions\ncompared to S-BERT.\nCredibility Assessment We train an artificial neural net-\nwork equipped with a three-layer perceptron and a softmax\noutput layer to assess the credibility of submissions. We give\nthe classifier network CREDiBERT-generated embeddings\nof submissions and train it to predict the corresponding sub-\nmission labels. To ensure a fair comparison with baselines,\nwe maintain temporal separation between the data used to\ntrain CREDiBERT and the data for the credibility assess-\nment task.\nPost-to-Post Network User interaction with submission\nprovides rich information regarding the submission con-\ntents. However, user monitoring can potentially leads to pri-\nvacy violations and erode the trust between the community\nand the platform. We develop a post-to-post network based\non user interaction patterns to overcome this challenge and\nenhance the result of the credibility assessment task. The un-\nderlying premise is that if users exhibit similar reactions to\ntwo submissions, these submissions imply some similarity.\nIdeally, one could encode these reactions alongside text em-\nbeddings and analyze them through a model to discern subtle\nsimilarities. However, this approach poses a significant chal-\nlenge due to the sheer volume of user engagement, with tens\nof thousands of active authors commenting on submissions.\nTo address this issue, we explore an alternative method to\ndistill meaningful patterns from user interactions by focus-\ning on key indicators that reflect the essence of users reac-\ntions without getting overwhelmed by the data volume.The post-to-post network is based on the premise that sub-\nmissions with similar commenting patterns, particularly re-\ngarding their authors, are likely to be more closely related.\nWhile the original network introduced by (Hurtado, Ray,\nand Marculescu 2019) considers submissions connected if\nthey share at least one commenter, Bond and Garret (2023)\nsuggest that authors may exhibit different reactions depend-\ning on the news authenticity. We employ S-BERT to en-\ncode the comment content to estimate the users reactions\ntoward the submissions. For instance, if a comment qi\nkon\nsubmission siresponds to a parent comment qi\nl, which has\nsimilarity toward the post by \u03b1i\nl, we can measure the sim-\nilarity between these comments by the cosine similarity of\ntheir embeddings, cos(T(qi\nl),T(qi\nk)), recalling T(\u00b7)repre-\nsent the S-BERT encoding model. We can then calculate the\nsubmission-similarity for comment qi\nktoward the submis-\nsionsiby\n\u03b1i\nk:=\u03b1i\nl\u00d7cos(T(qi\nl),T(qi\nk)), (4)\nwhere qi\nlis the parent comment for qi\nkand\u03b1i\n0= 1. This\nmethod enriches the analysis by considering the depth of\nuser engagement and its impact on the perceived similarity\nbetween various submissions.\nWe initiate the analysis with first-tier comments, assign-\ning a submission-similarity score based on the cosine simi-\nlarity between the submission and the comment, calculated\nascos(T(pi),T(qi))for some first tier comment qi, posted\non submission si. This process is then iteratively applied to\nsecond-tier comments and subsequent tiers, allowing us to\ndetermine the post-similarity score for all comments across\ndifferent levels. To encapsulate the spectrum of user reac-\ntions for each post, we construct a vector, ri, which aggre-\ngates the reactions of all users within the set A. For an au-\nthorawho has commented on submission si, the reaction is\nquantified in the ath element of the vector riby\n{ri}a=1\n|Cia|X\nk\u2208Ci\na\u03b1i\nk, (5)\nwhere Ci\nais the set of all comments author aposted in re-\nply to submission si. We denote the cardinality of the set Ci\na,\n73\nthe number of distinct elements in the set, by |Ci\na|. In cases\nwhere an author has not commented on submission si, we\nset{ri}a= 0. Given the large number of authors, repre-\nsented by |A|, we navigate the challenge of combining the\nembedding vector and the reaction vector for each submis-\nsion by introducing a weighted version of the post-to-post\nnetwork.\nTo better capture the intricate interplay between user in-\nteractions and submissions, we define the post-to-post graph\nG:= (V,E,W). In this graph, Vdenotes the nodes (submis-\nsions), with EandWrepresenting the edges and weights,\nrespectively. We consider the set of users who commented\non post i\u2208 V asAi. An edge is established between sub-\nmissions iandj\u2208 V if they share more than mcommon\nusers, leading to the edge set E. Thus Eis defined as\nE:={(si, sj)| |A i\u2229 Aj|> m}. (6)\nThis structure allows us to analyze the relationships and\nsimilarities between submissions based on user engagement\npatterns. While the original post-to-post network captures\nthe basic structure and offers insights into the connections\nbetween posts, it does not differentiate between positive\nand negative user reactions to submissions. We enhance the\nnetwork by assigning weights to its edges to address this.\nSpecifically, for an edge (i, j)\u2208 E, we calculate the weight\nwijby\nwij=\u27e8ri, rj\u27e9, (7)\nwhere \u27e8\u00b7,\u00b7\u27e9denote the inner product between two vectors.\nThis weight reflects the degree of similarity in user reactions\nto both submissions. The weights are then stored in W.\nAssigning edge weights to incorporate user responses en-\nriches the network\u2019s analytical depth. For instance, if a user\nexhibits negative and positive reactions to two distinct sub-\nmissions, our model interprets this as indicating a strong\nseparation between these submissions, while the original\nnetwork presented by Hurtado, Ray, and Marculescu (2019)\nconsiders them strongly connected. This nuanced analysis\nallows us to map the network of submissions more accu-\nrately, considering the presence of user interactions and their\nqualitative nature. On the other hand, this method avoids\nprofiling individual users, thus upholding user privacy and\nsafety.\nResult\nWe conduct four distinct experiments to test the accuracy,\nreliability, and applicability of the proposed framework. We\nstudy the credibility assessment task in the first experiment\nand cross-validate results with baseline models and human\nassessment. In the second experiment, we investigate the ef-\nfect of integrating the post-to-post network on the credibil-\nity assessment task. We demonstrate the applicability of the\nproposed framework for various tasks in the third and fourth\nexperiments. We challenge the framework to estimate the\ncredibility of six unseen sources for the third task. All the\ninformation linked to these sources was removed from the\ntraining data at all stages. Finally, we perform a case study\nto detect the susceptibility of communities to misinforma-\ntion with respect to different topics. In the rest of the paper,\nFigure 3: The post-to-post network for 4,371submissions.\nThe network shows a strong separation between different\ncommunities. For brevity, only edges with weights over 0.2\nare shown.\nwe pair two submissions if they have a cosine similarity of\nat least \u00afe= 0.6and a posting time difference of no more\nthan\u2206 = 2 days.\nCredibility Assessment\nWe assess submission credibility by predicting their la-\nbels using the proposed framework. The experiments in-\nclude scenarios where credibility is binary (credible or non-\ncredible) and where multiple levels of credibility can be as-\nsigned to a submission. We perform a sensitivity analysis for\nthe credibility threshold for binary classification tasks and\ncross-validate the binary classification results with baselines\nand human assessments.\nBinary Classification Task For binary classification,\nwe call a source credible if it has a credibility score\ngreater than a certain threshold \u03a5. To study how differ-\nent choices for credibility threshold would affect the re-\nsults of binary classification, we experiment with five dif-\nferent values of \u03a5. Figure 4 shows the results of the bi-\nnary classification corresponding to the values of \u03a5 =\n{0.40,0.50,0.55,0.60,0.7}. It is evident that the task\nbecomes more challenging for \u03a5 ={0.55,0.60}because\nthe labels are distributed evenly for these thresholds. The\nS-BERT-based model performed close to the majority for\n\u03a5 ={0.40,0.70}, indicating that high and low threshold\nvalues make the data unbalanced. However, CREDiBERT\nmaintains its superior performance for these threshold val-\nues, demonstrating its ability to distinguish between differ-\nent levels of credibility. We set the credibility threshold for\nthe rest of the study to \u03a5 = 0 .55. This decision would ensure\n74\nWeighted Average F1 Score\nCredibility ThresholdFigure 4: Average weighted F1 score for binary classifica-\ntion task with various credibility thresholds.\na balanced data set, which is important in learning baseline\nmodels. We did not choose a higher threshold since the dis-\ntribution of sources with a credibility score greater than 0.6\nbecomes extremely dense, as depicted in Figure 1\nCross Validation We focus on comparing the proposed\nmodel, CREDiBERT, against the word2vec embedding\nmodel (Mikolov et al. 2013), standard BERT text classifi-\ncation model (Devlin et al. 2018), and the S-BERT embed-\nding model (Reimers and Gurevych 2019). We emphasize\nthat submission texts are often short and include only one\nsentence. Thus, methods such as bag-of-words and stylom-\netry, which require medium to large text corpus, are incom-\npatible with this task (Przybyla 2020). The experiment data\nset is collected exclusively from 2022, comprising a total of\n80,248submissions, divided into training sets (70%), vali-\ndation ( 15%), and testing (50%) sets. All models are trained\nover the identical training data set and the results are re-\nported over the test data set. Notably, we train CREDiBERT\non submission pairs posted before 2022, ensuring that our\ntraining and test data sets are novel and independent. The\ndetails of each baseline model are available in Appendix A.\nWe obtain the confidence interval by repeating the training\nof each model 20time.\nTable 2 presents the cross-validation results for the binary\nclassification task. The proposed framework outperforms the\nother baselines in terms of the F1 score by 4%. Although\nBERT-based text classifiers outperformed other baselines,\ntraining BERT requires significantly more computational\npower than alternative models. As expected, word2vec-\nbased models cannot perform properly, as they disregard the\nstyle and tone of the text.\nHuman Assessment We cross-validate the results with\nhuman assessment. To do this, we randomly selected 100\nsubmissions from February 2022 and employed four hu-\nmans to assess the credibility label of the submissions. To\nensure that human credibility annotation is consistent, we\nselected a submission 20from the same time period and\ntrained annotators by evaluating the learning dataset. Af-\nter the training phase, each human assesses the credibilityof the selected submissions. Table 3 shows the human as-\nsessment of the credibility results for \u03a5 = 0 .55, against\nthe models trained for binary classification. Expert evalu-\nation results in an accuracy of 0.633\u00b10.17. The overall\nagreement of expert was 66.7%, which is defined by the\naverage proportion of all possible pair of raters that agree\non an item. We emphasize that this task is difficult for hu-\nmans because most submission texts are news headlines\nand sources with low credibility. Although CREDiBERT\noutperforms human assessments by more than 14%, other\nmodels perform close to experts. We emphasize that dis-\ntinguishing credible from non-credible news is challenging\nfor humans due to several factors highlighted in the litera-\nture(Horne, Khedr, and Adali 2018; Horne, N\u00f8rregaard, and\nAdali 2019). The sheer volume and speed of the information\nonline often overwhelm traditional vetting processes and in-\ndividual capacity for evaluation. People frequently consume\nnews passively via social media, leading them to rely on\nmental shortcuts rather than careful analysis, making them\nsusceptible to engaging but misleading content character-\nized by simpler language or negativity. Furthermore, echo\nchambers can normalize hyper-partisan or even false narra-\ntives, blurring the lines between legitimate, biased, and unre-\nliable sources. This difficulty is compounded by tactics em-\nployed by some sources, such as strategically mixing true\nand false information or copying content from credible out-\nlets, which can obscure the source\u2019s true nature and intent.\nMulti-Class Classification The Ad-Font recognizes a\nsource as low credible if it has a normalized credibility score\nof less than 0.4, mixed if its credibility score falls between\n0.4and0.6, and credible if it has a score greater than 0.6. We\ndemonstrate the strengths and limitations of the proposed\nmodel by examining the framework performance on multi-\nclass classification task. We first divided the credibility score\ninto3classes identical to the Ad-Font website annotations.\nFigure 5 shows the results for this task. It is evident that\nCREDiBERT outperforms other models in all classes; while\nall models struggle with low credibility class, the CRED-\niBERT performs significantly better than other models in\nthe detection of low credibility submissions, further show-\ncasing the ability of CREDiBERT in encoding credibility-\nrelated features. The results obtained from CREDiBERT\nhave tighter confidence bounds, indicating robustness, espe-\ncially on low-credible submissions. We emphasize that the\naverage weighted F1 score for this task is significantly lower\nthan the binary classification task. This phenomenon indi-\ncates that predicting credibility labels with high resolution\nis challenging even for CREDiBERT-based models.\nPost-to-Post Network\nWe integrate the post-to-post network with a Graph Convo-\nlutional Network (GCN) (Kipf and Welling 2016) for the\nbinary classification task. To this end, we feed the textual\nembedding of the submissions to the GCN as the node fea-\ntures along with the network information. We compare the\nCREDiBERT model against the node2vec embedding and\nbaseline models discussed in the previous section. The ex-\nperiment details involving node2vec are available in Ap-\n75\nBERTCREDiBERT\nS-BERT\nLow Medium High\nCredibilityF1 ScoreFigure 5: F1 score for multi-class classification task. The\npredictions based on CREDiBERT embeddings outperform\nother models in detecting low credibility submissions.\npendix A. We remark that the node2vec experiment does\nnot have access to textual information and asses credibility\nsolely through post-to-post network information. We focus\non submissions from 2022 across three specific subreddits:\nr/democrats, r/Libertarian, and r/Republican. The reason be-\nhind this selection is that r/politics and r/conservative have\na substantial size, posing significant computational chal-\nlenges.\nFigure 3 illustrates the resulting post-to-post network for\nthe4,371submissions encoding information of more than\n100,000comments. Figure 3 representation allows us to ob-\nserve how well a post-to-post network can distinguish be-\ntween communities and reveal the submissions interaction.\nTo select submissions, we first choose the authors with more\nthan5comments during 2022 and then select the posts with\nat least 2comments from the selected authors to ensure\nmeaningful interaction. After cleaning data, we are left with\n4,371submissions.\nNode2vec Node2vec, introduced by Grover and Leskove-\ncis (2016), is a graph embedding technique. It effectively\nrepresents graph nodes as vectors in a latent space, capturing\nboth the structural characteristics and homophilic tendencies\nof the graph. We pass the graph embeddings to a classifier\nlayer for binary classification of the submissions. We em-\nphasize that in this technique, we do notutilize the submis-\nsion content to demonstrate the post-to-post graph represen-\ntativeness.\nTable 5 provides the cross-validation results for the differ-\nent baseline models. We divide the data into the train (80%),\nvalidation (10%), and test (10%) sets. All models undergo\na validation process, and we report the results for the test\nset. Combining CREDiBERT with the Graph Convolutional\nNetwork (GCN) demonstrates superior performance com-\npared to the baseline models. We compare the results with\nthe binary classification models discussed in the previous\nsection to ensure comparable results. The results underscore\nmore than 9%improvement in the F1 score compared to the\nBERT-classification model, illustrating the effectiveness ofModel Accuracy\nMajority 0.543\nExpert Assessment 0.633\nS-BERT + Classifier 0.650\nBERT Classifier 0.683\nCREDiBERT + Classifier (Ours) 0.775\nTable 3: Credibility classification results compared with ex-\npert assessment for 100random samples.\nthe post-to-post network in encoding user interactions with\nsubmissions. It also emphasizes that BERT-classifiers strug-\ngle with unseen data, while CREDiBERT-based classifiers\nperform similarly. Remarkably, Node2vec achieves an F1\nscore of 0.747without textual embeddings, indicating that\nthe post-to-post network is proficient in encoding user reac-\ntions to detect low-credibility submissions.\nUnseen Source Credibility Estimation\nIn this experiment, we show that the proposed framework\nis not limited to Reddit submissions and can estimate the\ncredibility of unseen sources. We use the binary classifiers\ndeveloped in the previous section to estimate the credibil-\nity score of unseen sources. To this end, we first predict\nall submission labels referencing the source in question and\nthen estimate the score by the ratio of submissions labeled\nas credible to all submissions. In other words, we measure\nthe frequency at which a source disseminates non-credible\ninformation, which aligns with the definition of credibility\nprovided earlier.\nTable 4 shows the results of this experiment. While the\nS-BERT classifier struggles, the CREDiBERT-based and\nBERT classifiers can completely distinguish between cred-\nible and non-credible sources. The results provided by\nCREDiBERT show more accuracy and robustness in esti-\nmating source credibility scores.\nThis experiment confirms that the proposed framework\ncan effectively detect the credibility of unverified sources\nand can be used to detect low-credibility information across\nReddit.\nTopic-based Susceptibility Analysis\nTo demonstrate the capabilities of the CREDiBERT, we con-\nduct a case study on the susceptibility of different subred-\ndit communities toward low-credible sources with respect\nto different topics. By analyzing the credibility of submis-\nsions in these communities, we aim to gauge their exposure\nto low credible information (Mosleh and Rand 2022). Red-\ndit voting system, where users upvote or downvote submis-\nsions, offers insights into community responses to different\ninformation sources. We use submission scores, which re-\nflect the average of upvotes and downvotes and the diverse\nsubmissions covering major events in each subreddit, to de-\ntermine each community\u2019s susceptibility to specific topics.\nFor this study, we identify the major topics discussed in the\nsubmissions for all the subreddits (verified and unverified)\n76\nSource Score CREDi. BERT S-BERT\npatch.com 0.76 \u22120.13\u22120.16 +0.08\nusnews.com 0.71 +0. 01 +0 .08 +0 .20\nvice.com 0.60 +0. 03 +0 .10 +0 .26\nrt.com 0.48 \u22120.10 +0 .05 +0 .31\nlifesitenews.com 0.34 \u22120.06\u22120.04 +0 .28\nspectator.com 0.29 \u22120.07 +0 .09 \u22120.12\nTable 4: Credibility score prediction for six unseen sources.\nThe \u2019Score\u2019 column shows the base credibility score. The\ncolumns CREDi., BERT, and S-BERT show the difference\nbetween the original predictions of CREDiBERT, BERT\nClassifier, and S-BERT, respectively, and the base Score.\nModel Acc. F1\nRandom 0.505 0. 506\nMajority 0.543 0. 352\nNode2vec 0.754 0. 747\nBinary S-BERT 0.673 0. 604\nBinary BERT 0.729 0. 730\nBinary CREDiBERT (Ours) 0.771 0. 770\nCREDiBERT + GCN (Ours) 0.818 0. 817\nTable 5: Cross-validation report for binary classification re-\nsults of 4,371submissions in three major political subred-\ndits.\nposted during January 2022 in five major subreddits and uti-\nlize the BERTopic model, a method developed by Grooten-\ndorst 2022 for topic modeling.\nLet us consider topic hand the set of submissions ref-\nerencing hposted in subreddit zbySh,z. We measure the\nexposure of users in subreddit zto low credible information,\n\u03b3h,z, by averaging the credibility label of the submissions\ngiven by\n\u03b3h,z=1\n|Sh,z|X\ns\u2208Sh,z\u03b3s, (8)\nwhere |Sh,z|denote the number of submissions addressing\ntopic hin subreddit z, and \u03b3s\u2208 {0, 1}stands for binary\ncredibility label for submission sgenerated through CRED-\niBERT. To estimate the reaction of the subreddit user to topic\nh, we calculate the weighted average of the credibility labels\nof submission in Sh,zwith respect to the submission score,\ngiven by\n\u03c1h,z=P\ns\u2208Sh,z\u03b9s\u03bbsP\ns\u2208Sh,z\u03bbs, (9)\nwhere \u03bbs, and \u03c1h,zdenotes the submission score and reac-\ntion score. The reaction score, \u03c1h,z, reflects the credibility of\nthe sources users promoted regarding the topic in question.\nFigure 6 illustrates the exposure and reaction scores for\nsix topics across four subreddits: r/Conservative, r/Repub-\nlican, r/Libertarian, and r/politics. We select topics with a\nminimum of four submissions in each subreddit and ensure a\nbalanced representation across the political spectrum. To as-sess a community susceptibility to misinformation, we con-\nsider two factors: the reaction score \u03c1h,z, which reflects the\ncredibility of sources favored by users, and the exposure\nscore \u03b3h,z, reflecting the sources available to the commu-\nnity, predominantly controlled by subreddit moderators. A\nlower exposure score than a reaction score in a subreddit\nsuggests a preference for more credible sources than those\nprovided by moderators. Therefore, the reaction score (\u03c1 z,h)\nand the difference between the reaction and exposure scores\n(\u03c1z,h\u2212\u03b3z,h) are crucial indicators of a community suscep-\ntibility to specific topics.\nWe observe that subreddits generally identified as right-\nleaning, such as r/Conservative, show lower exposure scores\nthan those identified as left-leaning. Specifically, in r/Con-\nservative, there is a notable trend: users tend to promote\nsubmissions from more credible sources when the topics\nalign with right-leaning perspectives (second row of Fig-\nure 6). Conversely, users favor less credible sources more\nfrequently when topics contradict their biases. This pattern\nhighlights the influence of political biases on the selection of\ninformation sources within these online communities. How-\never, in order to arrive at any concrete conclusions, conduct-\ning a more comprehensive study is necessary.\nSubreddit r/politics shows the highest exposure to credi-\nble information among the subreddits in this study. However,\nwe observe a trend where users chose to promote sources\nwith lower credibility in discussions on topics like \u2018High-\nest inflation rate since 1982\u2019 and \u2018Supreme Court ruling on\nvaccine mandate\u2019. This suggests that, for certain topics, the\ncommunity\u2019s exposure to credible sources does not neces-\nsarily prevent the selection of less reliable information. This\nphenomenon indicates a susceptibility within r/politics to fa-\nvor less credible sources when discussing specific, perhaps\nmore controversial, topics. While these observations from\nr/politics and r/conservative do not establish a direct correla-\ntion with susceptibility to fake news, they suggest a potential\nfor greater susceptibility to these topics compared to other\ncommunities. This is particularly notable in cases where\nusers, despite being exposed to low-credibility sources, pre-\ndominantly vote in favor of more credible ones.\nDiscussion\nThe framework presented in this paper demonstrates a no-\ntable efficacy in identifying low-credibility submissions on\nReddit, outperforming conventional text classification meth-\nods. While currently optimized for Reddit data, its design\nholds potential for adaptation to other social media plat-\nforms and news websites, broadening its applicability. Fur-\nthermore, the automated pairing of submissions in CRED-\niBERT facilitates ongoing model refinement, enabling it to\nstay current with evolving trends and patterns in online mis-\ninformation. Our findings indicate that even in communities\nwith access to reliable sources, there is a tendency to fa-\nvor less credible information for certain topics. This insight\nopens up avenues for using CREDiBERT to understand and\npotentially mitigate topic-specific misinformation.\nThe weighted post-to-post network offers a novel method\nto analyze user interactions without delving into personal\nuser data. This method encodes extensive user interactions\n77\nTopic: Energy\nTopic: Congressional redistricting-\n--\n-\n-\n--\n-\nr/conservative r/Libertarian r/Republican r/politicsr/conservative r/Libertarian r/Republican r/politics\n-\n--\n-Topic: Abortion\nTopic: Vaccine mandate-\n--\n-\n-\n--\n-\nr/conservative r/Libertarian r/Republican r/politicsr/conservative r/Libertarian r/Republican r/politics-\n--\n-Topic: Biden first year progress\nTopic: Inflation\n-\n0.20.40.60.8\n--\n-\nr/conservative r/Libertarian r/Republican r/politicsr/conservative r/Libertarian r/Republican r/politics0.20.40.60.8Exposure Score\nReaction ScoreFigure 6: The exposure (green) and reaction (yellow) score of 6 topics in r/Conservative, r/Republican, r/Libertarian, and\nr/politics. r/politics has the highest exposure score among all subreddits, while r/Conservative and r/Republican have the lowest\nexposure score. While r/Libertarian shows extreme susceptibility to certain topics for others, it has identical exposure and\nreaction scores.\nin a weighted graph, enabling comprehensive analysis while\nrespecting user privacy. While platforms like Twitter and\nFacebook present different challenges due to their content\nstructure, adapting the post-to-post methodology could be a\npromising path forward.\nLimitations A major limitation of CREDiBERT lies in its\nability to assess only the credibility of news sources rather\nthan the veracity of the content within the articles. This\nmeans that while it can effectively identify patterns of mis-\ninformation from generally unreliable sources over time, it\nmay not detect false information disseminated through oth-\nerwise credible outlets. This gap highlights the challenge of\ndiscerning nuanced misinformation.\nUnreliable news sources often attempt to mimic the\nstyle of credible ones, particularly in their headlines. This\npresents a second challenge for us: as these distinctions blur\nover time, text classification can struggle to differentiate be-\ntween them. However, incorporating a post-to-post network\ncould potentially mitigate this issue. By analyzing users re-\nactions, we can glean additional insights into the credibility\nof posts, as demonstrated earlier.\nData Biases The current data set is constructed from five\nmajor political subreddits. However, nearly 70% of data be-\nlongs to r/politics. The analysis indicates that sources refer-\nenced to r/politics have a tendency to be left-leaning; thus,\nthe data is skewed toward the left. We emphasize that the ex-\ntreme right and extreme left have significantly lower credi-\nbility scores than unbiased media outlets, depicted in Figure\n1. The data in Figure 1 shows that the left-leaning sources\nhas a higher credibility score compared to the right-leaning\nones. The implications of this skew are best demonstrated in\nFigure 6. While subjects are chosen to be equally favored\nby both sides of the political spectrum, the right-leaningsubreddits generally show a lower credibility score, creat-\ning a perception that right-leaning has a tendency to engage\nwith low-credible sources more often. While the credibil-\nity score of submission sources can be used to estimate the\naverage susceptibility of the communities, it is essential to\nconsider other factors, such as engagement with such con-\ntent, to make any conclusion about the susceptibility of the\ncommunity.\nConclusion\nIn this study, we introduced CREDiBERT, an innovative\nsentence-level embedding model designed to assess credi-\nbility in Reddit submissions. According to our evaluations,\nCREDiBERT-generated embeddings can outperform exist-\ning text classification models over the credibility assessment\ntask. We also developed a weighted post-to-post network to\nencode Reddit user interactions efficiently without requir-\ning user profiling. When integrated with CREDiBERT, this\nnetwork enhances the detection of credible sources. By ap-\nplying CREDiBERT to recent Reddit submissions, we have\nrevealed its capability to estimate community susceptibility\nto low-credible information on various topics. Exploring the\napplication of CREDiBERT in other social media contexts\nand refining its methodology to address its current limita-\ntions present exciting avenues for future research.\nReferences\nAllcott, H.; and Gentzkow, M. 2017. Social media and fake\nnews in the 2016 election. Journal of Economic Perspec-\ntives, 31(2): 211\u2013236.\nBachmann, P.; Eisenegger, M.; and Ingenhoff, D. 2022.\nDefining and measuring news media quality: Comparing the\n78\ncontent perspective and the audience perspective. The Inter-\nnational Journal of Press/Politics, 27(1): 9\u201337.\nBond, R. M.; and Garrett, R. K. 2023. Engagement with\nfact-checked posts on Reddit. PNAS nexus, 2(3): pgad018.\nCastillo, C.; Mendoza, M.; and Poblete, B. 2013. Predicting\ninformation credibility in time-sensitive social media. Inter-\nnet Research, 23(5): 560\u2013588.\nChan, M.-p. S.; Jones, C. R.; Hall Jamieson, K.; and Al-\nbarrac \u00b4\u0131n, D. 2017. Debunking: A meta-analysis of the psy-\nchological efficacy of messages countering misinformation.\nPsychological Science, 28(11): 1531\u20131546.\nChen, C.; and Shu, K. 2024. Combating misinformation in\nthe age of llms: Opportunities and challenges. AI Magazine,\n45(3): 354\u2013368.\nChiang, T. H.; Liao, C.-S.; and Wang, W.-C. 2022. In-\nvestigating the Difference of Fake News Source Credibility\nRecognition between ANN and BERT Algorithms in Artifi-\ncial Intelligence. Applied Sciences, 12(15): 7725.\nChipidza, W.; Krewson, C.; Gatto, N.; Akbaripourdibazar,\nE.; and Gwanzura, T. 2022. Ideological variation in\npreferred content and source credibility on Reddit dur-\ning the COVID-19 pandemic. Big Data & Society, 9(1):\n20539517221076486.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nGlenski, M.; Weninger, T.; and V olkova, S. 2018. Identify-\ning and understanding user reactions to deceptive and trusted\nsocial news sources. arXiv preprint arXiv:1805.12032.\nGrootendorst, M. 2022. BERTopic: Neural topic model-\ning with a class-based TF-IDF procedure. arXiv preprint\narXiv:2203.05794.\nGrover, A.; and Leskovec, J. 2016. node2vec: Scalable fea-\nture learning for networks. In Proceedings of the 22nd\nACM International Conference on Knowledge Discovery\nand Data Mining, 855\u2013864.\nHocevar, K. P.; Metzger, M.; and Flanagin, A. J. 2017.\nSource credibility, expertise, and trust in health and risk\nmessaging. In Oxford Research Encyclopedia of Commu-\nnication.\nHorne, B.; Khedr, S.; and Adali, S. 2018. Sampling the news\nproducers: A large news and feature data set for the study of\nthe complex media landscape. In Proceedings of the Inter-\nnational AAAI Conference on Web and Social Media, vol-\nume 12.\nHorne, B. D.; N\u00f8rregaard, J.; and Adali, S. 2019. Robust\nfake news detection over time and attack. ACM Transactions\non Intelligent Systems and Technology (TIST), 11(1): 1\u201323.\nHurtado, S.; Ray, P.; and Marculescu, R. 2019. Bot detec-\ntion in reddit political discussion. In Fourth International\nWorkshop on Social Sensing, 30\u201335.\nJwa, H.; Oh, D.; Park, K.; Kang, J. M.; and Lim, H. 2019.\nexbake: Automatic fake news detection model based on bidi-\nrectional encoder representations from transformers (bert).\nApplied Sciences, 9(19): 4062.Kipf, T. N.; and Welling, M. 2016. Semi-supervised classi-\nfication with graph convolutional networks. arXiv preprint\narXiv:1609.02907.\nLeite, J. A.; Razuvayevskaya, O.; Bontcheva, K.; and Scar-\nton, C. 2023. Detecting misinformation with llm-predicted\ncredibility signals and weak supervision. arXiv preprint\narXiv:2309.07601.\nMikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Ef-\nficient estimation of word representations in vector space.\narXiv preprint arXiv:1301.3781.\nMosleh, M.; and Rand, D. G. 2022. Measuring exposure\nto misinformation from political elites on Twitter. Nature\nCommunications, 13(1): 7144.\nPehlivanoglu, D.; Lin, T.; Deceus, F.; Heemskerk, A.; Ebner,\nN. C.; and Cahill, B. S. 2021. The role of analytical rea-\nsoning and source credibility on the evaluation of real and\nfake full-length news articles. Cognitive Research: Princi-\nples and Implications, 6(1): 1\u201312.\nPrzybyla, P. 2020. Capturing the style of fake news. In Pro-\nceedings of the AAAI conference on artificial intelligence ,\nvolume 34, 490\u2013497.\nRaza, S.; and Ding, C. 2022. Fake news detection based on\nnews content and social contexts: a transformer-based ap-\nproach. International Journal of Data Science and Analyt-\nics, 13(4): 335\u2013362.\nReimers, N.; and Gurevych, I. 2019. Sentence-bert: Sen-\ntence embeddings using siamese bert-networks. arXiv\npreprint arXiv:1908.10084.\nSakketou, F.; Plepi, J.; Cervero, R.; Geiss, H.-J.; Rosso, P.;\nand Flek, L. 2022. Factoid: A new dataset for identifying\nmisinformation spreaders and political bias. arXiv preprint\narXiv:2205.06181.\nShu, K.; Sliva, A.; Wang, S.; Tang, J.; and Liu, H. 2017. Fake\nnews detection on social media: A data mining perspective.\nACM SIGKDD explorations newsletter, 19(1): 22\u201336.\nSpezzano, F.; Shrestha, A.; Fails, J.; and Stone, B. 2021.\nThat\u2019s fake news! Investigating how readers identify the re-\nliability of news when provided title, image, source bias, and\nfull articles. ACM, Human Computer Interaction journal, 5.\nThakur, N.; Reimers, N.; Daxenberger, J.; and Gurevych, I.\n2020. Augmented SBERT: Data augmentation method for\nimproving bi-encoders for pairwise sentence scoring tasks.\narXiv preprint arXiv:2010.08240.\nTorabi Asr, F.; and Taboada, M. 2019. Big Data and quality\ndata for fake news and misinformation detection. Big Data\n& Society, 6(1): 2053951719843310.\n79\nPaper Checklist\n1. For most authors...\n(a) Would answering this research question advance sci-\nence without violating social contracts, such as violat-\ning privacy norms, perpetuating unfair profiling, exac-\nerbating the socio-economic divide, or implying disre-\nspect to societies or cultures?\nYes.\n(b) Do your main claims in the abstract and introduction\naccurately reflect the paper\u2019s contributions and scope?\nYes.\n(c) Do you clarify how the proposed methodological ap-\nproach is appropriate for the claims made? Yes.\n(d) Do you clarify what are possible artifacts in the data\nused, given population-specific distributions? Yes.\n(e) Did you describe the limitations of your work? Yes.\n(f) Did you discuss any potential negative societal im-\npacts of your work? Yes.\n(g) Did you discuss any potential misuse of your work?\nYes.\n(h) Did you describe steps taken to prevent or mitigate po-\ntential negative outcomes of the research, such as data\nand model documentation, data anonymization, re-\nsponsible release, access control, and the reproducibil-\nity of findings? Yes.\n(i) Have you read the ethics review guidelines and en-\nsured that your paper conforms to them? Yes.\n2. Additionally, if your study involves hypotheses testing...\n(a) Did you clearly state the assumptions underlying all\ntheoretical results? NA\n(b) Have you provided justifications for all theoretical re-\nsults? NA\n(c) Did you discuss competing hypotheses or theories that\nmight challenge or complement your theoretical re-\nsults? NA\n(d) Have you considered alternative mechanisms or expla-\nnations that might account for the same outcomes ob-\nserved in your study? NA\n(e) Did you address potential biases or limitations in your\ntheoretical framework? NA\n(f) Have you related your theoretical results to the existing\nliterature in social science? NA\n(g) Did you discuss the implications of your theoretical\nresults for policy, practice, or further research in the\nsocial science domain? NA\n3. Additionally, if you are including theoretical proofs...\n(a) Did you state the full set of assumptions of all theoret-\nical results? NA\n(b) Did you include complete proofs of all theoretical re-\nsults? NA\n4. Additionally, if you ran machine learning experiments...\n(a) Did you include the code, data, and instructions\nneeded to reproduce the main experimental results (ei-\nther in the supplemental material or as a URL)? Yes,we have uploaded the CREDiBERT model, the data\nset we employed for training it, and the training and\ntest data set in an anonymous Google drive. Codes are\navailable in the Appendix B.\n(b) Did you specify all the training details (e.g., data splits,\nhyperparameters, how they were chosen)? Yes.\n(c) Did you report error bars (e.g., with respect to the ran-\ndom seed after running experiments multiple times)?\nYes\n(d) Did you include the total amount of compute and the\ntype of resources used (e.g., type of GPUs, internal\ncluster, or cloud provider)? Yes.\n(e) Do you justify how the proposed evaluation is suffi-\ncient and appropriate to the claims made? Yes.\n(f) Do you discuss what is \u201cthe cost\u201c of misclassifica-\ntion and fault (in)tolerance? No, however, we address\nour method limitation and blindness toward individual\nnews articles varsity in Discussion and Introduction.\n5. Additionally, if you are using existing assets (e.g., code,\ndata, models) or curating/releasing new assets, without\ncompromising anonymity...\n(a) If your work uses existing assets, did you cite the cre-\nators? Yes\n(b) Did you mention the license of the assets? Yes.\n(c) Did you include any new assets in the supplemental\nmaterial or as a URL? No\n(d) Did you discuss whether and how consent was ob-\ntained from people whose data you\u2019re using/curating?\nNo, because all the data we use are publicly available,\nand we did not creat a new data set.\n(e) Did you discuss whether the data you are using/curat-\ning contains personally identifiable information or of-\nfensive content? Yes please see Ethics Statement and\nData\n(f) If you are curating or releasing new datasets, did you\ndiscuss how you intend to make your datasets FAIR ?\nNA\n(g) If you are curating or releasing new datasets, did you\ncreate a Datasheet for the Dataset? NA\n6. Additionally, if you used crowdsourcing or conducted\nresearch with human subjects, without compromising\nanonymity...\n(a) Did you include the full text of instructions given to\nparticipants and screenshots? NA\n(b) Did you describe any potential participant risks, with\nmentions of Institutional Review Board (IRB) ap-\nprovals? NA\n(c) Did you include the estimated hourly wage paid to\nparticipants and the total amount spent on participant\ncompensation? NA\n(d) Did you discuss how data is stored, shared, and dei-\ndentified? NA\nEthics Statement\nThe research presented in this paper strictly utilizes publicly\navailable data from Reddit. In line with ethical standards, our\n80\nmethodology prioritizes user anonymity; we do not track or\nprofile any users. The post-to-post framework is designed to\nanalyze user interactions while fully respecting user privacy,\navoiding the inclusion of any personally identifiable infor-\nmation. It is crucial to address the ethical concerns related\nto the data used to evaluate credibility, specifically the bias-\ncredibility curve we use throughout this paper. A skewed\ncurve could substantially affect the definition of credibility\nand favor one party over another as non-credible. Therefore,\nwe recommend users of CREDiBERT to disclose this infor-\nmation along with the model.\nWe recognize the potential for misusing the proposed\nframework. For example, sources with low credibility might\nattempt to leverage CREDiBERT to modify their content,\nmaking it appear more credible. To mitigate such risks, we\npropose a set of specific ethical guidelines for the use of\nCREDiBERT and similar tools.\nTransparency: Users of CREDiBERT-like tools should be\nrequired to publish transparency reports that detail their use\nof the tools, the nature of the data processed, and the pur-\nposes for which it is used.\nIndependent audit: We advocate for regular independent\naudits of these tools to ensure that the predictions and oper-\nations remain unbiased.\nStakeholder engagement: Developing the guidelines\nshould involve a broad spectrum of stakeholders from both\nparties. This would also motivate such organizations to in-\ncrease their credibility and reputation.\n81\nAppendix A\nEvaluation Methods\nWord2Vec Word2Vec was developed as a technique to\nestimate the vector representation of words efficiently\n(Mikolov et al. 2013). We compute the average word em-\nbeddings of the submission text and then feed the embed-\nding to a classification layer to assess the credibility of the\nsubmissions.\nBERT-based Text Classifier The BERT-based text clas-\nsifiers fine-tune the underlying transformer for the classi-\nfication task and utilize the first token as the aggregate\nsentence representation for classification, allowing them to\nover-perform naive classification of average embeddings\npooling generated by BERT-based models.\nSentence Transformers The S-BERT model was devel-\noped to address the challenges the BERT-based model faces\nin sentence-level embedding in sentence comparison tasks.\nWe integrate the sentence-level embedding with two dif-\nferent classification models for better comparison. We em-\nphasize that while the CREDiBERT is primarily created\nfor classification tasks, the model is trained to estimate\nCredibility score discrepancies; thus, it is considered a sen-\ntence transformer model. For the first approach, we integrate\nsentence-level embeddings with a classification layer for bi-\nnary classification.\nNode2vec Node2vec, introduced by Grover and Leskove-\ncis (2016), is a graph embedding technique. It effectively\nrepresents graph nodes as vectors in a latent space, capturing\nboth the structural characteristics and homophilic tendencies\nof the graph. We pass the graph embeddings to a classifier\nlayer for binary classification of the submissions. We em-\nphasize that in this technique, we do notutilize the submis-\nsion content to demonstrate the post-to-post graph represen-\ntativeness.\nAppendix B\nCode and Data Access\nThe code and data for training and evaluating CREDiBERT\nare accessible through:\nhttps://drive.google.com/drive/folders/1hxA5eaz-\nzz88ftq6RbvS73DODl69YnbU?usp=sharing\n82", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "News source credibility assessment: A Reddit case study", "author": ["A Amini", "YE Bayiz", "A Ram", "R Marculescu"], "pub_year": "2025", "venue": "Proceedings of the \u2026", "abstract": "We present a transformer-based model for credibility assessment, CREDiBERT (CREDibility  assessment using Bi-directional Encoder Representations from Transformers), fine-tuned"}, "filled": false, "gsrank": 332, "pub_url": "https://ojs.aaai.org/index.php/ICWSM/article/view/35804", "author_id": ["0n97ZL8AAAAJ", "CZMjmzIAAAAJ", "8ZxVHtcAAAAJ", "ZCmYP5cAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:ZnhXgDR2DZIJ:scholar.google.com/&output=cite&scirp=331&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D330%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=ZnhXgDR2DZIJ&ei=QLWsaMqsGZXUieoPmrax2A8&json=", "num_citations": 5, "citedby_url": "/scholar?cites=10524197872098310246&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:ZnhXgDR2DZIJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://ojs.aaai.org/index.php/ICWSM/article/download/35804/37958"}}, {"title": "Images, emotions, and credibility: Effect of emotional facial expressions on perceptions of news content bias and source credibility in social media", "year": "2023", "pdf_data": "Images, Emotions, and Credibility: Effect of Emotional Facial Expressions on\nPerceptions of News Content Bias and Source Credibility in Social Media\nAlireza Karduni1, Ryan Wesslen2, Douglas Markant2, Wenwen Dou2\n1IDEO\n2University of North Carolina at Charlotte\nalireza116@gmail.com, rwesslen@gmail.com, dmarkant@uncc.edu, wdou1@uncc.edu\nAbstract\nImages are an indispensable part of the news we consume.\nHighly emotional images from mainstream and misinforma-\ntion sources can greatly influence our trust in the news. We\npresent two studies on the effects of emotional facial images\non users\u2019 perception of bias in news content and the cred-\nibility of sources. In study 1, we investigate the impact of\nrepeated exposure to content with images containing positive\nor negative facial expressions on users\u2019 judgments of source\ncredibility and bias. In study 2, we focus on sources\u2019 sys-\ntematic emotional portrayal of specific politicians. Our re-\nsults show that the presence of negative (angry) facial emo-\ntions can lead to perceptions of higher bias in content. We\nalso find that consistent negative portrayal of different politi-\ncians leads to lower perceptions of source credibility. These\nresults highlight how implicit visual propositions manifested\nby emotions in facial expressions may substantially affect our\ntrust in news.\nIntroduction\nIn the aftermath of the US 2020 presidential election, some\nrioters who did not trust the official results of the election\nstormed the capitol building. Around the same time, when\nthe first Covid-19 vaccines were introduced, many did not\ntrust the news about the safety of the vaccines. Observing\nsuch incidents, what causes news consumers not to trust\ncertain mainstream news sources while believing misinfor-\nmation published by others? Recent research suggests one\nanswer may be cognitive factors, such as the propensity\nfor analytical reasoning affecting belief in the accuracy of\nnews content (Pennycook and Rand 2021). However, our\njudgments of news source credibility is also an important\nbut often neglected factor shaping our judgments of news\nand misinformation (Pehlivanoglu et al. 2021; Wallace, We-\ngener, and Petty 2020b). Indeed, prominent misinformation\nscholars advocate for increasing focus on behaviors of news\nsources rather than individual stories since \u201cthe defining ele-\nment of fake news is the intents and processes of a publisher\u201d\n(Lazer et al. 2018). In this research, we tackle the inter-\nactions between news sources and consumers by exploring\nhow social media users\u2019 evaluations of the news content in-\nfluence their judgments of news source credibility. We posit\nCopyright \u00a9 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.that identifying factors that influence consumers\u2019 judgments\nof news sources is essential, not only to help detect and com-\nbat misinformation but also to identify and combat practices\nthat reduce consumers\u2019 trust in trustworthy sources.\nNews sources often frame their content with specific\nstyles, tones, and emotions. Both mainstream news media\nand misinformation sources use highly emotionalized and\nsensational content to influence their audiences (Arsenault\nand Castells 2006; Wardle and Derakhshan 2017; V olkova\net al. 2017; Richards 2007; Peng 2018; Rathje, Van Bavel,\nand van der Linden 2021). Various studies have shown that\nsuch content strategies are often effective. People are more\nlikely to be drawn to highly emotionalized news, click on ar-\nticles associated with extreme emotions, and share headlines\nwith more negative sentiments (Bowman and Cohen 2020;\nReis et al. 2015; Berger and Milkman 2012). In addition to\nemotional verbal content, news sources take advantage of\nsocial media\u2019s highly visual nature by choosing images that\namplify the persuasive power of the verbal content (Wardle\nand Derakhshan 2017; Grabe and Bucy 2009). Examples of\nsuch visual content include images that communicate racist\nconcepts not present within the text (Messaris and Abraham\n2001), emotional facial expressions of news-casters (Mullen\net al. 1986), and partisan media producing ideological bias\nin their visual coverage of political candidates with features\nincluding facial expressions and face size (Peng 2018).\nIn this work, we tackle a salient aspect of visual infor-\nmation, facial expressions, in social media posts on users\u2019\njudgments: Do users perceive a source as less credible when\nthe source repeatedly publishes tweets with angry facial ex-\npressions? What if a source systematically portrays a neg-\native visual bias against a politician (e.g. Bernie Sanders\nor Donald Trump) by repeatedly publishing news with an-\ngry images of those politicians? We explore these questions\nby considering source credibility, a primary factor in the\npersuasiveness of news sources (Hovland, Janis, and Kel-\nley 1953; Wallace, Wegener, and Petty 2020b); and content\nbias, defined as \u201chaving a perspective that is skewed,\u201d (Wal-\nlace, Wegener, and Petty 2020b) as two primary elements\nof users\u2019 judgments of news sources. We investigate how\nrepeated usage of images with positive (happy) or negative\n(angry) emotional facial expressions in social media posts\naffects users\u2019 judgments about source credibility andcon-\ntent bias.\nProceedings of the Seventeenth International AAAI Conference on Web and Social Media (ICWSM 2023)\n470\nWe seek to answer the following questions: 1) How does\nrepeated exposure to news content accompanied by images\nwith positive (happy) or negative (angry) facial expressions\nimpact users\u2019 judgments about a source\u2019s credibility?; and\n2) How sources\u2019 repeated positive or negative visual por-\ntrayal of different politicians interacts with users\u2019 prior atti-\ntudes (favorability and familiarity) towards those politicians\nto shape their judgments of source credibility. To answer\nthese questions, we conducted two consecutive preregistered\ncontrolled experiments:\n1. In study 1, we examined how users\u2019 credibility judg-\nments are impacted by news sources that show a pat-\ntern of using highly emotionalized visual images.1More\nspecifically, we investigated how angry or happy fa-\ncial expressions in tweet images affect users\u2019 judgments\nabout eight anonymized right/left-leaning and main-\nstream/misinformation news accounts.\n2. Leveraging findings from study 1, in study 2, we focused\non sources with repeated emotionalized visual coverage\nof influential politicians.2We examined how users\u2019 judg-\nments are impacted by sources that consistently portray\nspecific politicians\u2019 emotions as negative or positive. In\nstudy 2, we also investigated how users\u2019 prior attitudes\ntowards each politician interact with sources\u2019 negative or\npositive emotional portrayals of those politicians.\nIn study 1, we observe that users find tweets with neg-\native (angry) facial expressions as more biased. However,\nwe found that negative facial expressions might not lead\nto lower perceived source credibility and that this effect is\nlikely heavily moderated by textual content and tone of the\nnews coverage (e.g., misinformation sources were deemed\nas less credible). In study 2, we found that focusing on main-\nstream sources\u2019 repeated negative visual portrayal of spe-\ncific politicians, users are more likely to perceive the content\nas more biased and the source as less credible. Across both\nstudies, in addition to quantitative analysis of the experimen-\ntal treatments, we qualitatively analyzed users\u2019 comments\nexplaining the reasons behind their judgments. Our qualita-\ntive analysis demonstrated that users also relied on several\ncues such as opinionated language, political bias, and satire,\nto guide their credibility judgments. These studies highlight\nthe extent to which highly emotionalized visual coverage\nmight impact users\u2019 trust in news sources.\nRelated Work\nSource Credibility and Content Bias. False, inaccurate\nor \u201cfake\u201d news is a serious problem for our societies and\ndemocracies (Lazer et al. 2018; Karduni et al. 2019; Wardle\nand Derakhshan 2017). In their work on decision-making\nabout misinformation, Pennycook and Rand highlight differ-\nent aspects of why users fall for \u201cfake news.\u201d They provide\nevidence that \u201ccontradict the common narrative that par-\ntisanship and politically motivated reasoning explain why\npeople fall for fake news\u201d ((Pennycook and Rand 2021) pg.\n1) Furthermore, they find that the propensity to engage in\n1Pre-registration: https://aspredicted.org/blind.php?x=9rn6i7\n2Pre-registration: https://aspredicted.org/blind.php?x=9js6d5reasoning is a more important factor in users\u2019 ability to de-\ntect false news.\nIn addition to users\u2019 ability to detect instances of inac-\ncurate and false information, perceptions of source credibil-\nity influence how users interact with and are persuaded by\nthe news (Pornpitakpan 2004). Perceptions of trustworthi-\nness and expertise are noted as two main factors in credibil-\nity judgments (Petty, Wegener, and Fabrigar 1997; Wallace,\nWegener, and Petty 2020b). Research on credibility percep-\ntions on social media has highlighted that when faced with\nshort content (e.g., Twitter), users might have different cred-\nibility perceptions for different topics (e.g. science versus\nentertainment) and might rely on different heuristics such\nas username style to judge source credibility (Morris et al.\n2012). Recent work by Laura Wallace suggests that percep-\ntions of source bias and untrustworthiness can have different\neffects on perceptions of source credibility. For instance, in\ncertain cases, users might believe sources are biased but hon-\nest in their reporting and therefore credible (Wallace, We-\ngener, and Petty 2020b,a).\nImages, Emotions, and Users\u2019 Judgments of News. Vi-\nsual information impacts how users judge the credibility of\ninformation sources in several ways. Wobbrock et al. show\nthat visual appearances like font size or inclusion of images\nmay impact users\u2019 perceptions of credibility (Wobbrock\net al. 2021). Different images may have varying effects on\nusers\u2019 judgments. For example, after seeing an image of a\nbrain, users are more likely to believe claims in certain sci-\nentific articles (Schweitzer, Baker, and Risko 2013; McCabe\nand Castel 2008) while images of smoking increases belief\nin messages in warning signals (Shi et al. 2017). Articles\naccompanied by alarming images (Knobloch et al. 2003)\nor ones that depict victimization (Zillmann, Knobloch, and\nYu 2001) increase users\u2019 selective interaction with the arti-\ncles. Furthermore, when exposed to false versus true news\nheadlines, participants\u2019 self-reported experience of height-\nened emotion (users who report feeling more angry or sad)\nis linked to an increase in perceived accuracy of false news\nbut not factually correct news (Martel, Pennycook, and Rand\n2020).\nRecent work in social psychology and media bias has\nhighlighted the interpersonal effect of emotions on users\u2019\ndecisions (Van Kleef, De Dreu, and Manstead 2010). For ex-\nample, displays of angry (but not happy) facial expressions\nmight reduce the likelihood of our decisions to trust oth-\ners (Campellone and Kring 2013). In the context of charity\nadvertisements, images with sad facial expressions can in-\ncrease the likelihood of donating (Small and Verrochi 2009).\nMoreover, displays of different emotions can impact per-\nceptions of cooperativeness and appraisal (e.g., happy agent\nperceived to be more likely to cooperate and angry/sad the\nopposite) (de Melo et al. 2012). Andalibi and Buss assert\nthat people find emotions as \u201cinsights to behavior, prone to\nmanipulation, intimate, vulnerable, and complex\u201d (Andalibi\nand Buss 2020). Emotional content in images also impacts\nthe likelihood of people believing a statement. For example,\nexposure to highly negative emotional images about differ-\nent phenomena increases belief in the accuracy of news con-\n471\ntent (Vlasceanu, Goebel, and Coman 2020). In comparison\nto positive imagery, being exposed to negative imagery has\nbeen associated with greater perceived accuracy of false in-\nformation (Porter et al. 2010). Melodramatic animations are\nalso shown to likely increase users\u2019 perceptions of the cred-\nibility of a news report (Lo and Cheng 2017). From the per-\nspective of visual media bias, (Grabe and Bucy 2009) called\nfor more attention on studying the visual framing of politi-\ncians in the news due to the increase in visual (image) por-\ntrayals in the news. A study by (Peng 2018) showed that,\namong other features, happy facial images increase the per-\nceived favorability of a political candidate and perceived\ntrustworthiness, but angry facial expressions decrease both.\nAnother study by Masch and Gabriel suggests that depend-\ning on the political context, positive and negative depictions\nof politicians in videos might impact users\u2019 attitudes towards\nthose politicians (Masch and Gabriel 2020).\nCurrent research on misinformation focuses on percep-\ntions of content accuracy and users\u2019 ability to distinguish\nfalse and true information (Pennycook and Rand 2021)\nbased on various cues, including emotions in text (Kar-\nduni et al. 2018) or images (Vlasceanu, Goebel, and Coman\n2020). Research on persuasion has shown users\u2019 judgments\nof source credibility and content bias, and the certainty of\nthose judgments, are important factors in news sources\u2019 per-\nsuasive power (Wallace, Wegener, and Petty 2020b; Tormala\nand Petty 2004). Inspired by the mentioned work, this paper\nstudies an important but, to our knowledge, overlooked as-\npect of our judgments of news sources: the effect of sources\u2019\nrepeated and potentially systematic usage of highly emo-\ntional facial images on users\u2019 judgments of source credibil-\nity and content bias.\nStudy Design and Implementation\nConsidering perception of source credibility as a product of\nthe news content that users consume, we formulate the over-\nall structure of the studies3as 1) Users first view curated so-\ncial media posts from an anonymized news source; 2) Users\nevaluate bias for multiple posts from the source; 3) After de-\nciding they have viewed and evaluated enough posts from a\nsource (with a hard minimum of five tweets), users are in-\nstructed to provide a credibility rating of the source. Users\nrepeated these steps for eight different new sources.\nThis structure applies to both studies 1 and 2. The ran-\ndomized treatments were applied by altering the content\nusers see from each source (i.e., content with positive or neg-\native images or no images as a control condition).\nAs opposed to limiting users\u2019 judgments to binary out-\ncomes, we elicited users\u2019 bias and credibility judgments on\ncontinuous response scales. We elicited perceived bias of\na tweet as a continuous number between 0 (unbiased) and\n1 (biased). Similarly, we elicited perceived credibility of a\nsource as a number between 0 (not credible) and 1 (credi-\nble). As it has been argued that user attitudes with higher\ncertainties have important consequences, including increas-\ning resistance to persuasion and being persistent over time\n(Tormala and Petty 2004; Tormala and Rucker 2007), we\n3Approved by our institutions\u2019 Internal Review Board (IRB)elicited users\u2019 certainty around their judgments as a confi-\ndence interval within each respective domain. We adopted\na modified version of the Line + Cone technique (Karduni\net al. 2020) to elicit users\u2019 judgments. The visual technique\n(called Line + Range; See Figure 7) enabled us to elicit\njudgments and uncertainty using a visual technique. To ex-\nplore users\u2019 self-described reasons behind their judgments,\nwe also asked users to provide verbal comments using an\nopen text box.\nDataset and Study Stimuli\nThe tweets dataset was collected from the Twitter streaming\nAPI from October 25th to January 25th, 2018, from multi-\nple news accounts labeled as misinformation or mainstream\nby Karduni et al. (Karduni et al. 2019, 2018). After down-\nloading all images from the dataset, we used the python\nface-recognition library4to extract images with faces and\ncropped the images only to include faces. To identify images\nof different politicians, we used Google\u2019s FECNet (Schroff,\nKalenichenko, and Philbin 2015) to extract feature vectors\nof cropped faces and used HDBScan (McInnes, Healy, and\nAstels 2017) to cluster faces and manually identified influ-\nential politicians. We chose to include only cropped faces in\nour study to remove potential confounding factors such as\nbody language and surrounding scenes, as studies show that\nrecognition of facial expressions is influenced by both(Kret\net al. 2013; Kret and Gelder 2012, 2013).\nTo extract emotion predictions from faces, we used two\nseparate python libraries, EmoPy5and deepface6, which\nprovide the ability to sort the tweets based on emotion pre-\ndictions and model confidence. After generating a candidate\ndataset, we qualitatively excluded low-resolution images, in-\naccurately assigned to a cluster, or had wrong emotion pre-\ndictions, All qualitative coding to produce the final datasets\nfor both studies was done by the authors. The first author\nfirst created a larger candidate dataset. Using the study inter-\nface, the other authors iteratively evaluated each data point.\nDisagreements were resolved in team meetings.\nStudy 1 & 2 Models\nOur experimental design for both studies consisted of re-\npeated measures for each user within two levels of re-\nsponses, including source-level responses (credibility) and\ntweet-level responses (bias). Users\u2019 responses were continu-\nous variables bounded between 0 and 1. For both studies, we\nused mixed-effects beta regressions with glmmTMB to ad-\ndress the hierarchical design of our studies and the bounded\ndependent variables. Within the text, model coefficients are\nreported as log-odds ratios with corresponding confidence\nintervals. In the model figures, we transformed the log-odds\nratios to odds ratios for ease of interpretation. Odds ratios\nshow the direction and strength of how each independent\nvariable impacts the dependent variables. We used the nor-\nmal approximation to calculate p-values of fixed effects and\nt-values produced by lme4.\n4https://pypi.org/project/face-recognition/\n5https://github.com/thoughtworksarts/EmoPy\n6https://pypi.org/project/deepface/\n472\nConsentDemographic\nQuestionnaire\npostquestionnaireInstructions\nHappy\n4 accountsno images\n4 accountswith images4 accountsno images\n4 accountswith images4 accountsno images\n4 accountswith imagesAngry Mixed\nDebrie/f_ingEach user randomly assigned toFigure 1: Study 1 conditions and process\nStudy 1\nIn Study 1, we examined how positive (happy) and nega-\ntive (angry) emotions in images influence perceived content\nbias (tweets) and source credibility (accounts). Participants\ncompleted a task in which they observed a series of tweets\nfrom eight different accounts. To provide a variety of dif-\nferent political orientations and content types, the accounts\nwere either categorized as mainstream or known to produce\nmisinformation. The accounts were also categorized as be-\ning right-leaning or left-leaning (see Table 1).7Given these\ntwo dimensions of mainstream/misinformation and right/left\npolitical orientation, users evaluate a total of eight accounts\nwith more than 90 tweets in the final dataset. The content\nshown to users was manipulated in a between-subjects man-\nner. Users were randomly assigned to one of three condi-\ntions: happy, angry, and mixed (see Figure 1). The happy\ncondition contained tweets with the highest happiness scores\nfor the images. The angry condition included tweets contain-\ning images with the highest angry image scores. The mixed\n(control) condition alternates between happy and angry im-\nages. Furthermore, one account from each category (right,\nleft, misinformation, mainstream) was randomly selected to\ninclude no images. For example, a participant assigned to a\nhappy condition, viewed tweets with the highest-rated happy\nimages from four of the eight accounts but did not see im-\nages from the other four accounts.\nFor each account, users responded based on their judg-\nments about the bias of each tweet. By clicking on \u201cView\nmore tweets\u201d, participants viewed additional tweets until\nthey decided to rate the source\u2019s credibility (a minimum of\n5 tweets was enforced for each account). By clicking on\n\u201cMake a decision,\u201d users saw a pop-up view in which they\nentered their credibility judgments. Users also had the option\nto use a text box to describe what influenced their decisions.\nHypotheses\nWe hypothesize that in comparison to tweets with happy\nimages, users are more likely to assess tweets with angry\nimages as biased. Furthermore, we also hypothesize that as\n7Right or left-leaning labels were confirmed with https://\nmediabiasfactcheck.com/ and https://www.allsides.com/Source name Type Orientation\n@veteranstoday misinformation left\n@amlookout misinformation right\n@opednews misinformation left\n@InvestWatchBlog misinformation right\n@MotherJones mainstream left\n@Jeresulem Post mainstream right\n@cnnPolitics mainstream left\n@nypost mainstream right\nTable 1: 8 anonymized Twitter accounts in Study 1\ncompared to the mixed condition, users in the angry condi-\ntion are more likely to assess sources as less credible. How-\never, since it is likely for users\u2019 judgment to be influenced\nby the inherent differences in the nature of sources, we also\nexplore the effects of political orientation (right vs. left) and\nsource type (mainstream vs. misinformation) on their judg-\nments.\nDependent & Independent Variables\nWe considered four total dependent variables (DV): (1) con-\ntent bias (bounded value between [0,1]), (2) uncertainty\nrange around content bias (bounded value between [0,1]),\n(3) source credibility (bounded value between [0,1]), (4) un-\ncertainty around source credibility (bounded value between\n[0,1]). For our independent variables (IV), we included the\nimage emotion condition (angry, happy, or mixed), image\nshown (true or false), as well as political orientation (right\nor left), and source type (mainstream or misinformation).\nFor models built based on tweet-level responses, i.e. bias\nchoice/uncertainty of tweets as the dependent variable, only\ntwo image emotion conditions of happy or angry are appli-\ncable since each tweet contains at most one type of emotion.\nModel Specification\nFor each model, we included users\u2019 unique IDs and the\nsource id as random effects. The reference conditions for\ncredibility choice and uncertainty models are image emo-\ntion = mixed, source orientation = left, source type = main-\nstream, and the image shown = False. The reference condi-\ntions for bias choice and uncertainty models are image emo-\ntion = happy, source orientation = left, source type = main-\nstream, and the image shown = False.\nFigure 2: Participants\u2019 responses to questions \u201cHow would\nyou describe your political outlook with regard to eco-\nnomic/social issues?\u201d for Study 1.\n473\n0.73 ***\n1.25 ***\n1.24 **\n1.61 ***\n1.13image_shown [with image]source_type [misinformation]source_orientation [right]image_emotion [angry](Intercept)\n0.5 1.0 1.5 2.0\nOdds RatiosBias Choice\n0.37 ***\n1.04\n1.14 **\n1.17 ***\n1.03\n0.4 0.8 1.2\nOdds RatiosBias UncertaintyFigure 3: Study 1 fixed effects odds ratios for bias choice\n(left) and bias uncertainty (right). Error bars indicate 95%\nconfidence intervals. Asterisks indicate statistical signifi-\ncance than zero using p-values: *** 99.9%, ** 99%, * 95%.\nParticipants\nIn this study\u2019s pre-registration, we originally planned to re-\ncruit up to 300 participants from crowdsourcing platforms.\nHowever, due to concerns about the quality of data collected\nfrom MTurk and time constraints, we shifted our participant\npool to university students. We recruited a total of 81 uni-\nversity students with an average age of 21 years old (std\n5.6). Per our pre-registration, we excluded responses from\n9 participants with missing responses (due to unexpected\ntechnical difficulties), resulting in 72 accepted responses (48\nwomen, 23 men, and 1 other; 40 white, 13 African Ameri-\ncan, 7 other Asian, 4 East Asian, 5 Hispanic, 2 middle east-\nern, and 1 native American). Participants were mostly liberal\nin regards to social, political issues and more balanced in\nregards to economic issues (see Fig 2). Per random assign-\nment, 30 participants were assigned to the angry condition,\n24 participants to the happy condition, and the remaining\n18 to the mixed condition. Participants took an average of\n26 minutes to complete the study. All participants either re-\nceived course extra credits or research credits.\nResults\nContent Bias. We used two mixed-effects beta regres-\nsions to study the effects on users\u2019 judgments about the\nbias of individual tweets (see Fig 3-left). Users found con-\ntent from right-leaning sources to be more biased (\u03b2 =\n0.216 [0 .086,0.34],z= 3.25, p < . 01). Similarly, users\njudged content from misinformation sources to be more bi-\nased (\u03b2 = 0.474 [0. 34,0.60],z= 7.11, p < . 001). We\nalso found that users rated tweets containing angry images as\nmore biased (\u03b2 = 0.219 [0. 11,0.32],z= 3.94, p < . 001).\nThis is in line with our hypothesis that angry imagery will\nlead to an increase in users\u2019 perceived bias. Surprisingly,\neven though the content shown to users was solely sorted\nbased on facial emotions in images, we did not find a notice-\nable effect of emotional images on users\u2019 bias ratings. It is\npossible that sources choose images to amplify tweets\u2019 con-\ntent, and users\u2019 judgments were not based on the emotion in\nfacial expressions alone.\nIn our mixed-effects model on users\u2019 uncertainty around\ntheir judgments (see Fig 3-right), we found that compared\nto left-leaning mainstream accounts, users were more likely\nto have larger uncertainty ranges for right-leaning accounts\n1.30 *\n0.87\n0.94\n0.77 **\n0.58 ***\n0.87image_shown [with image]source_type [misinformation]source_orientation [right]image_emotion [angry]image_emotion [happy](Intercept)\n0.5 1.0 1.5 2.0\nOdds RatiosCredibility Choice\n0.31 ***\n1.21\n1.07\n0.84\n0.69 ***\n0.94\n0.5 1.0 1.5 2.0\nOdds RatiosCredibility UncertaintyFigure 4: Study 1 fixed effects coefficients for credibility\nchoice (left) and credibility uncertainty (right). Error bars\nindicate 95% confidence intervals. Asterisks indicate statis-\ntical significance than zero using p-values: *** 99.9%, **\n99%, * 95%.\n(\u03b2= 0.127 [0 .041,0.21],z= 2.90p < . 01). We also\nfound a similar positive effect for misinformation accounts\n(\u03b2= 0.154 [0. 06,0.24], z= 3.52, p < . 001).\nSource Credibility. We used mixed-effects beta regres-\nsion to investigate the effects of study conditions on users\u2019\njudgments about source credibility (see Fig 4-left). We\nfound that in reference to left-leaning accounts, users were\nmore likely to rate right-leaning accounts as less credible\n(\u03b2=\u22120.264 [\u22120.43,\u22120.08], z=\u22122.961, p < . 01). Users\nwere also more likely to rate misinformation sources as less\ncredible (\u03b2 =\u22120.539 [\u22120.71,\u22120.36], z=\u22126.024, p <\n.001). We did not observe significant effects for the happy or\nangry conditions. In other words, we did not find evidence\ntowards our hypotheses that being exposed to a source that\nrepeatedly publishes tweets with angry or happy facial ex-\npressions impacts perceptions of source credibility.\nWe also used a mixed-effects beta regression to study\nwhether users\u2019 confidence around their decisions (repre-\nsented through uncertainty ranges in the Line + Range tech-\nnique). Interestingly, source type was the only factor that\nsignificantly affected users\u2019 uncertainty about source credi-\nbility. Users were likely to be more confident in their deci-\nsions when source type was categorized as misinformation\n(\u03b2=\u22120.373 [\u22120.55,\u22120.18], z=\u22123.921, p < . 001).\nQualitative Analysis of Users\u2019 Comments\nEach user had the option to answer one open-ended question\nfor each account, asking \u201cplease describe how the tweets\n(text and images) influenced your decisions about this ac-\ncount?\u201d Even though leaving comments was an optional\npart of the study, we received comments from all 72 par-\nticipants, and the majority of participants left comments for\nall 8 trials. In total, we collected 572 comments about users\u2019\ndecision-making influences. Since thematic analysis with a\nlarge number of comments was challenging, we used topic\nmodeling to facilitate the qualitative analysis of the com-\nments and arrive at different themes of influences on users\u2019\njudgments. We categorized the extracted topics into four\ngeneral themes: 1) sources\u2019 political orientation (left-leaning\nor right-leaning), 2) opinionated versus factual reporting, 3)\nspecific language usage or tone (e.g. source is angry or uses\ntabloid-like language), and 4) the effect of images. Although\n474\nangryHappyMisinfo Mainstream\nLeft Right Left RightMixedFigure 5: Mean and 95% bootstrapped confidence interval\nfor bias and credibility choice for different news accounts\n(sources) in study 1.\nthe majority of users\u2019 comments were related to different\ncues tied to the text of the tweets, in this section, we provide\na summary of comments related to images.\nTopics 9 (25 comments from 14 users) and 5 (19 com-\nments from 10 users) included comments that mentioned\nimages as a factor in their decision. These comments made\na wide range of image-related observations, from unflatter-\ning imagery to facial expressions and lack of seriousness.\nFor example, a user described how they perceived the image\nand the text to be not aligned: \u201cSome of the tweets seemed\nto be about data rather than opinions. Some of the pictures\nseemed to take away merit. \u201d Another comment explicitly\nmentioned facial expressions of individuals helping them\ndecide that the tweets have a left bias: \u201cMany of the tweets\nseemed to be credible as most were quotes by others. Some\nof the pictures had facial expressions that made the tweets\nseem left swinging. \u201d\nA number of comments cited comic or not serious im-\nagery as the basis for their decisions. For example, \u201cDiffi-\ncult to take the meme-like images seriously. \u201d And, \u201cThe im-\nages were cartoonish and difficult to take it seriously. It was\nobviously making fun of Trump. \u201d. A few comments men-\ntioned unprofessional or unflattering images as the basis for\ntheir judgments: \u201c[...] and somewhat unflattering pictures\nof those who either commented against them or who may\ndo (or not do) something against them. \u201d And, \u201cSome of the\npictures were not professional and showed the president and\nalliances in a negative light. \u201d\nThese comments show that, at least for a group of users,\nimages can serve as a cue for a source\u2019s lack of credibil-\nity. However, the majority of comments about pictures and\nimages did not specifically mention emotions in images as\nthe basis for their decisions. This might be due to the fact\nthat tweets in our dataset contained a diverse set of topics.\nIt is possible that users look for a more systematic negative\ntreatment of specific topics or individuals rather than a com-\nbination of negative imagery from a wide range of topics.\nStudy 1 Discussion\nWe hypothesized that sources\u2019 systematic posting of tweets\nwith angry images would lead to higher perceived content\nbias and lower perceived source credibility. We also hy-\npothesized that happy images would lead to a reverse ef-\nfect. We found partial evidence for our hypotheses: we ob-\nserved an increase in perceived bias for tweets containing\npost\nquestionnaireHappy Angry No Image\nDebrie/f_ingEach user randomly assigned toConsentDemographic\nQuestionnaireInstructionsFigure 6: Study 2 conditions and process\nAngry Condition Happy Condition\nFigure 7: Sample of study 2 Angry (left) and Happy (right)\nstimuli for one tweet related to Kim Jong Un.\nangry images (see Fig 5 for raw point estimates for each\naccount). We did not find evidence that this perceived con-\ntent bias led to lower judgments of source credibility. We\nbelieve this is in line with recent findings by Wallace and\ncolleagues asserting that higher perceived bias does not nec-\nessarily lead to lower perceived credibility of sources (Wal-\nlace, Wegener, and Petty 2020b). However, perceptions of\ncontent bias were likely affected by the general tone and top-\nical focus of individual accounts (note that source type =\nmisinformation had the largest effect on perceived bias.\nOur qualitative analysis of comments also hints that users\nrely heavily on text content to judge both bias in content\nand the credibility of sources. In retrospect, the topical di-\nversity in the first study, both between different sources and\nwithin tweets published by a single source, does not nec-\nessarily portray systematic biases which would reduce the\nperceived credibility of a source. In the next study, we ex-\nplore this issue by fixing the textual content shown to users\nand manipulating only the emotional valence of facial ex-\npressions in images.\nStudy 2\nIn study 1, the content shown to users was not limited to spe-\ncific topics and we found that users\u2019 judgments were likely\ninfluenced by different topical focuses of sources. To bet-\nter evaluate the impact of emotional facial expressions on\nusers\u2019 judgments, we shifted the focus of study 2 to explore\nthe effects of mainstream sources\u2019 systematic emotional por-\ntrayal of specific politicians. To address such a scenario, in\nall experimental conditions of this study 2, users received\nthe same 8 sets of tweet texts about 8 different politicians.\nPer random assignment, users received either angry or happy\nimages of the politicians. We included no images as a con-\ntrol condition in which users received the same sets of tweets\nwithout any accompanying images of those politicians.\n475\nPolitician Notes\nDonald Trump Former President of the US\nHillary Clinton Former US Secretary of State\nBarack Obama Former president of the US\nTheresa May Former prime minister of the UK\nEmanuel Macron President of France\nAngela Merkel Former Chancellor of Germany\nKim Jong Un Supreme Leader of North Korea\nVladimir Putin President of Russia\nTable 2: Politicians selected for study 2\nTo curate the stimuli for the present study, the tweets\ndataset from the previous study was filtered to tweets that\nincluded mentions of 8 different prominent politicians (see\nTable 2). Users were instructed that each set of tweets men-\ntioning each politician were from a single source. To limit\nthe impact of emotional text content on users\u2019 judgments,\nwe downselected tweets with the following steps: we first\nconducted sentiment analysis on the tweet texts using Vader\nSentiment (Hutto and Gilbert 2014). Next, for each set of\ntweets, we selected tweets with the highest neutral senti-\nment scores. Finally, we qualitatively evaluated and removed\ntweets with inaccurate scores from the sentiment analysis\nlibrary. This resulted in 30 to 40 tweets mentioning each\npolitician from mainstream news sources that were mostly of\nneutral tone. The images for this study were manipulated in\na between-subjects manner such that users saw either happy,\nangry, or no images of each politician. For example, a user\nin the Happy condition, viewed tweets mentioning Kim Jung\nUn accompanied by happy images of him, while a user in the\nAngry condition evaluated the same tweets but with angry\nimages of him, and the control (no-image) condition viewed\nno images (see Figure 7 for one of the example tweets).\nThe procedures of the study were equivalent to Study\n1, with one exception: To measure the interaction between\nusers\u2019 prior attitude towards each politician and the experi-\nmental conditions, for each set of tweets users first answered\ntwo questions in a pop-up form about their familiarity and\nfavorability towards that politician on 5-point Likert scales.\nHypotheses\nSince study 2 is a continuation of study 1, we expected to see\na similar effect of angry emotions on content bias. We also\nhypothesized that users in the angry condition would rate\nsources as less credible. Guided by research on motivated\nreasoning (Kahan 2012), we expected to observe an interac-\ntion between users\u2019 prior favorability of different politicians\nand sources\u2019 negative and positive visual bias towards those\npoliticians. More specifically, we hypothesized that favora-\nbility negatively interacts with angry emotion and positively\ninteracts with happy emotion to predict users perceived bias\nand credibility. Furthermore, we also investigate the impact\nof users\u2019 familiarity with each politician on their judgments.\nDependent & Independent Variables\nThe dependent variables in Study 2 are identical to Study\n1. We considered four total dependent variables(DV): (1)\nFigure 8: Participants\u2019 responses to questions \u201dHow would\nyou describe your political outlook with regard to eco-\nnomic/social issues?\u201d for Study 2.\nthe tweet bias choice (bounded value between [0,1]), (2)\nuncertainty range around tweet bias (bounded value be-\ntween [0,1]), (3) source credibility choice (bounded value\nbetween [0,1]), (4) uncertainty around source credibility\nchoice (bounded value between [0,1]). For our independent\nvariables (IV), we included the image emotion condition\n(angry, happy, or no image), as well as users\u2019 prior favor-\nability and familiarity towards each politician.\nModel Specification\nFor each model, we included users\u2019 unique IDs and the\npolitician\u2019s name as random effects. After comparing multi-\nple model specifications using AIC, we also included inter-\naction terms between users\u2019 favorability and familiarity of\neach politician with the image emotion. The omitted refer-\nence conditions are image emotion = no image.\nParticipants\nIn study 2, we recruited a total of 126 participants. The av-\nerage age of participants was 35 years old (std of 11.3). 81\nparticipants were recruited from Amazon Mechanical Turk\nand received a 2-dollar incentive. To get closer to our pre-\nregistration target, the rest of the participants were university\nstudents who received either research or course credit for\ntheir participation. Per our pre-registration, we excluded re-\nsponses from 12 participants with missing responses (due to\nunexpected technical difficulties) resulting in 114 accepted\nresponses (54 women, 59 men, and 1 prefer not to say; 87\nwhite, 10 African American, 7 East Asian, 5 Hispanic, 3\nOther Asian and 2 middle eastern). Participants were mostly\nliberal in regards to social, and political issues and more bal-\nanced in regards to economic issues (See Fig 8). 34 partici-\npants were randomly assigned to the angry condition, 42 par-\nticipants were assigned to the happy condition, and the re-\nmaining 38 were assigned to the no image condition. Partic-\nipants took an average of 21 minutes to complete the study.\nResults\nContent Bias. We used two mixed-effects beta regressions\nto study the effects of experimental conditions on users\u2019\njudgment and uncertainty of bias of individual tweets (see\nfig 9-left). We found that users viewing tweets with angry\nimages rated tweets as more biased in comparison to when\nno images are shown to users (\u03b2 = 0.511 [0 .18,0.84], z=\n3.031, p < . 01). This is in line with our hypothesis that when\nthe content of messages is the same, angry imagery leads to\nan increase in perceived bias. Similar to study 1, we did not\n476\n0.58 ***\n1.13\n1.68 **\n1.11 ***\n1.07 *\n1.06\n1.08\n1.01\n1.01image_emotion [angry] * favorabilityimage_emotion [happy] * favorabilityimage_emotion [angry] * familiarityimage_emotion [happy] * familiarityfavorabilityfamiliarityimage_emotion [angry]image_emotion [happy](Intercept)\n0.5 1.0 1.5 2.0 2.5\nOdds RatiosBias Choice\n0.84\n0.87\n1.01\n1.00\n1.03\n1.02\n1.04\n1.03\n0.5 1.0\nOdds RatiosBias Uncertainty\n0.28 ***Figure 9: Study 2 fixed effects Odds Ratios for bias choice\n(left) and bias uncertainty (right). Error bars indicate 95%\nconfidence intervals. Asterisks indicate statistical signifi-\ncance than zero using p-values: *** 99.9%, ** 99%, * 95%.\nobserve any effect in the happy condition. Moreover, we did\nnot find any evidence of an interaction between favorabili-\nty/familiarity and image emotion. We did, however, observe\nan overall positive effect of familiarity on users\u2019 perceived\nbias (\u03b2 = 0.115 [0 .05,0.17], z= 3.82, p < . 001). We also\nobserved a small positive effect of favorability on perceived\nbias (\u03b2 = 0.074 [0 .01,0.13], z= 2.471, p < . 05). We did\nnot observe any significant effects of any of the independent\nvariables on users\u2019 uncertainty (See Fig 9).\nSource Credibility: We used mixed-effects beta regres-\nsion to investigate the effects of study conditions on users\u2019\njudgments about source credibility (see Fig 10-left). We\nfound that users rated sources as less credible when tweets\nwere accompanied by angry facial expressions (\u03b2 =\n\u22120.397 [\u2212 0.73,\u22120.05], z=\u22122.308, p < . 05). This find-\ning aligns with our hypothesis that angry facial expressions\nmight lead to a decrease in the perceived source credibility.\nWe also find that familiarity also decreases users\u2019 perceived\ncredibility of sources (\u03b2 =\u22120.153 [\u22120.245,\u22120.06], z=\n\u22123.256, p < . 01). We did not observe an interaction effect\nbetween favorability and the image emotion.\nFor the mixed effect model of users\u2019 uncertainty, we\nobserved a significant effect of happy emotion on users\u2019\njudgment uncertainty (\u03b2 =\u22120.44 [\u22120.8\u22120.02], z=\n\u22122.09, p < . 01). We also observed that higher familiar-\nity with subjects leads to more certain judgments (\u03b2 =\n\u22120.168 [\u2212 0.28\u22120.05], z=\u22122.9, p < . 01). We also ob-\nserved a positive interaction between happy emotion and\nusers\u2019 familiarity (\u03b2 = 0.16 [0 .0240. 31], z= 2.224, p <\n.05), such that there was greater uncertainty in the happy\ncondition for more familiar subjects.\nQualitative Analysis of Users\u2019 Comments\nIn study 2, we collected a total of 881 comments from 116\nusers. Similar to study 1, we analyzed users\u2019 descriptions\nof the rationale behind their judgments using NMF topic\nmodeling (Cichocki and Phan 2009) and thematic analysis\nof the documents most representative of each topic. This\n1.76 ***\n0.76\n0.66 *\n0.86 **\n0.96\n0.98\n0.93\n0.94\n0.97image_emotion [angry] * favorabilityimage_emotion [happy] * favorabilityimage_emotion [angry] * familiarityimage_emotion [happy] * familiarityfavorabilityfamiliarityimage_emotion [angry]image_emotion [happy](Intercept)\n0.5 1.0 1.5 2.0 2.5\nOdds RatiosCredibility Choice\n0.32 ***\n0.64 *\n0.69\n0.84 **\n0.92\n1.19 *\n0.95\n1.07\n1.13\n0.4 0.8 1.2\nOdds RatiosCredibility UncertaintyFigure 10: Study 2 fixed effects Odds Ratios for credibility\nchoice (left) and credibility uncertainty (right). Error bars\nindicate 95% confidence intervals. Asterisks indicate statis-\ntical significance than zero using p-values: *** 99.9%, **\n99%, * 95%.\nhelped us categorize the 20 extracted topics into 5 higher-\norder themes. Four of the themes were similar to the ones\nwe extracted from study 1 and discussed cues found in the\ntext of the tweets. Here again, we will offer a qualitative\noverview of users\u2019 comments about images.\nWe identified three topics that contain descriptions from\nusers related to visual information. Topic 19 with 71 com-\nments from 34 users and Topic 14 with 47 comments from\n24 users include mentions of facial expressions, angry emo-\ntions, and unflattering portrayals. For example, a user found\nthe text of tweets mostly unbiased and explained how por-\ntrayed facial expressions of Hillary Clinton was the basis for\ntheir judgment: \u201cWhile the text didn\u2019t involve much biased\nwords, the usage of certain images of Hillary Clinton depict-\ning her facial expressions in an array of negative emotions\nshowed a biased view. \u201d\nSome users also found a combination of text with\n\u201cweirdly close up\u201d and \u201cunflattering\u201d images leading them\nto believe a source is less credible: \u201cThe texts were mostly\nbland, except for \u201c...what do you think\u201d, which is a tabloid-\nlike phrase for me. Photos were weirdly close-up facial\nviews that were generally unflattering, which makes me won-\nder a bit about credibility... \u201d. Another interesting set of com-\nments was about users perceiving the images as not correlat-\ning with the tweets: \u201cMost of the tweets were very normal,\nbut there were a couple that had angry Macron pictures that\ndid not correlate with the headlines presented. \u201d And \u201cThe\nmajority of tweets were unbiased with their headlines. Some\nof the pictures might have been a bit questionable. \u201d\nA group of comments described how images did not in-\nfluence users\u2019 judgments. Topic 16 with 66 comments from\n32 users, includes examples of such commentary. For exam-\nple, for tweets about Kim Jung Un, a user mentioned: \u201cThe\nimages were not the best images nor were they the worst im-\nages of him, but there was still a negative bias. \u201d Another\nuser mentioned how images and tweets related to Donald\nTrump were both neutral and honest looking: \u201cThe tweets\nseemed to be honest and state the honest news about what is\n477\nUS Non-USFemale*\n*\n**Figure 11: Mean and 95% bootstrapped confidence interval\nfor bias and credibility choice for each set of tweets about\neach politician.\nhappening while the images associated with them. \u201d\nStudy 2 Discussion\nWe asked participants to judge sources that each focused\non a specific politician. The text was constant for all users,\nwhile we manipulated the tweets to include either negative\n(angry) or positive (happy) facial expressions of those politi-\ncians or to include no images as a control condition. We\nfound that in comparison to users in the no image condi-\ntion, users in the angry condition found the content to be\nmore biased and the sources to be less credible. We did not\nfind a reverse effect of happy images on users\u2019 judgments.\nThe difference between the happy and angry condition in our\nstudy could be better explained by a study on perceptions of\nnegative or positive portrayals of politicians, in which Lub-\ninger and Brantner found that participants mostly agreed on\nwhat constituted as negative, but perceptions of positive por-\ntrayals varied widely (Lobinger and Brantner 2015). This\nsuggests that negative portrayals might be more salient and\nmore likely to be commonly agreed upon and thus might\nhave a stronger effect on users\u2019 judgments. Additionally,\nusers\u2019 comments also included many mentions of angry,\nnegative, or unflattering portrayals of these politicians, while\nwe did not observe mentions of positive bias in users\u2019 com-\nments.\nAlthough the effect is rather small, we observed that users\nrate tweets about politicians they are more familiar with as\nmore biased and to come from less credible sources (See\nFig 12 for counts of favorability and familiarity responses\nfor each politician). Recent work on judgments on misin-\nformation suggests that prior exposure and familiarity with\nmisinformation increases the perceived accuracy of content\nin which users would have to rely on their memory to as-\nsess the accuracy of news headlines or articles (Pennycook,\nCannon, and Rand 2018). Since to assess the credibility and\nbias of anonymous content, users would not rely on their\nmemories, we suspect that the impact of familiarity on these\njudgments might be different. Our comment analysis high-\nlighted that users often rely on more analytical approaches\nand different heuristics such as negativity, word usage, or\nemotions in facial expressions to judge bias and credibil-\nity of sources. An explanation for the effect of familiarity\non users\u2019 judgment about bias and credibility could be that\nwhen users are more familiar with politicians, they might be\nmore sensitive to the details of texts and images of content\nthey view and therefore possibly more likely to identify cues\nFavorability Familiarity\nFigure 12: Favorability and familiarity counts (Study 2).\nimpacting their credibility and bias judgments.\nFinally, we observed a small overall effect of favorability\non users\u2019 perception of tweet bias, but we did not observe an\ninteraction between favorability and emotions in images. Al-\nthough we elicited users\u2019 self-described political orientation,\nwe elected to instead use favorability and familiarity of spe-\ncific politicians to more directly study the impact of users\u2019\nexisting attitudes on their credibility and bias judgments. We\nexpected that if a user is engaged in motivated reasoning,\nthey would find attitude-consistent portrayals (e.g., an angry\nportrayal of a politician they dislike) as a cue for credibil-\nity, while portrayals that conflict with preexisting attitudes\nwould be associated with lower credibility judgments. How-\never, our results do not provide evidence that, in such exper-\nimental settings, editemotions in facial expressions impacts\nusers who might be engaged in motivated reasoning based\non their favorability towards politicians. This result is in line\nwith work showing that motivated reasoning is not a primary\nfactor in users\u2019 judgments when they are asked to evaluate\nnews content (Pennycook and Rand 2019).\nGeneral Discussion and Conclusion\nStudy 1 showed that angry facial emotions lead to higher\nratings of content bias, but not lower credibility ratings for\nsources. However, these judgments were possibly impacted\nby differences in the textual content and topics across the\nsources we used. In study 2 we controlled for the textual con-\ntent while investigating the systematic negative or positive\nvisual portrayals of politicians on users\u2019 perceptions of cred-\nibility and bias. Our results provide partial evidence for our\nhypothesis: systematic negative visual treatments of politi-\ncians led to both higher perceived content bias and lower\nperceived source credibility. However, we did not find strong\nevidence for our hypotheses around the interactions between\nusers\u2019 prior favorability of politicians and sources\u2019 system-\natic negative or positive visual portrayal of those politicians.\nWe also explored the impact of different study conditions\non users\u2019 certainty around their judgments. In study 1, mis-\ninformation sources were associated with more certain cred-\nibility judgments. One possible interpretation for this effect\nis that many misinformation sources are more likely to use\ncontent with more suspicious language and images (V olkova\net al. 2019, 2017). In study 2, we observed that users were\nmore certain in their credibility judgments when they were\n478\nmore familiar with a politician. Users with greater direct\nknowledge of a politician may have been more sensitive to\nthe accuracy of the content and were therefore able to make\nmore confident credibility judgments.\nAlthough these results provide evidence of the impact of\nemotional facial expressions in images on users\u2019 judgments,\nwe found that users also rely heavily on other cues in the\ntweet content such as political orientation and opinionated\nlanguage. It is also important to note that there are many\nmore aspects related to emotions in images that remain to\nbe studied. A comparison between angry, happy, and neutral\nemotions is an appropriate comparison which we omitted\ndue to the difficulty of curating a balanced dataset with neu-\ntral images of all eight politicians. Moreover, within each\nof the angry or happy emotional categories, there are finer\ndistinctions between different emotional facial expressions,\nranging from extremely angry/happy to subtle frown/grin\nthat might impact users\u2019 judgments. Comments about satiri-\ncal images point to the importance of how subtle changes in\nfacial expressions could impact users\u2019 perceptions of emo-\ntional facial expressions. There are also other emotional\ndimensions such as sadness, surprise, fear, or disgust that\nmight potentially impact users\u2019 judgments. Many of these\nsubtle differences might be present in our stimuli and may\nhave impacted participants\u2019 judgments. Finally, facial ex-\npressions rarely contain one unique emotion, and subtle\nchanges might communicate different meanings to individ-\nuals. In addition to facial expressions, prior research has es-\ntablished the influence of other visual features (including\nbody expression and scene information) (Kret et al. 2013;\nKret and Gelder 2012, 2013) on the perception of emotions.\nAlthough these factors were outside of the scope of the two\nstudies reported in this paper, they are promising next steps\nto investigate. In future studies, Generative Adversarial Neu-\nral (GAN) networks (Goodfellow et al. 2020) can be used to\nproduce image datasets with finer control over the facial ex-\npressions or other emotional content (Todorov et al. 2021).\nWe acknowledge some general limitations in the design\nand execution of our studies. First, participants in our study\n1 were leaning liberal in regards to social issues and were\nof younger age, it is important to replicate these findings\nfor a more balanced age and political orientation sample as\nwell. Second, to remove potential confounds introduced by\nbody posture and image backgrounds, we cropped all im-\nages to include only faces. However, this decision might\nhave a generally negative effect on users\u2019 bias judgments\nas seen in some of the participants\u2019 comments and prior\nresearch identifying face size as a specific type of visual\nbias (Peng 2018). Additionally, for users\u2019 judgments, we did\nnot measure differences between positive and negative con-\ntent bias. Eliciting valenced judgments about content bias\nmay provide more insight into how these judgments depend\non users\u2019 pre-existing attitudes or familiarity with a sub-\nject. In the future, it is important to control for and study\nthe impact of factors such as face size and positive/negative\nbias on users\u2019 judgments. Moreover, To limit the impact of\nusers\u2019 preconceived notions of sources, we masked all ac-\ncount names from our stimuli. Even though encountering\nnew and unknown sources is a realistic scenario and espe-cially important in the context of misinformation, in many\ncases, users might have a self-selected set of sources they\ntrust and refer to. It is possible that users\u2019 prior knowledge\nof sources significantly impacts how they evaluate content.\nFor example, a user who trusts Fox News or Washington\nPost might be less sensitive to any visual bias portrayed by\nthese sources toward a politician. An important future step\nfor our research is to investigate how users update their trust\nin familiar sources when they observe a systematic usage of\nbiased visual content. Other factors such as gender and polit-\nical orientation of politicians are also likely to impact users\u2019\njudgments of bias and credibility and require closer atten-\ntion in future studies. Although we did not explicitly pre-\nregister and control for such factors in our experiments, we\ndid not observe noticeable differences between politicians in\nour stimuli (See Fig 11).\nBroader Perspective and Ethics\nThis study was conducted in experimental settings. The pro-\ncedures of the study were approved by our institution\u2019s IRB,\nevery participant signed an informed consent, and through-\nout the study, the participant\u2019s identities remained anony-\nmous. We believe our results could potentially be used by\nmalicious actors to further influence consumers\u2019 judgments.\nHowever, as most social media interactions are driven by\npolarized outrage (Rathje, Van Bavel, and van der Linden\n2021), these results can engage the community in an ethi-\ncal discourse on the curation of sensationalized visual me-\ndia by mainstream and misinformation organizations. More-\nover, to learn about the true impacts of content on users\u2019\nbeliefs and attitudes, we believe that results of experimental\nsettings should be repeated \u201cin the wild\u201d (Mosleh, Penny-\ncook, and Rand 2022). Such in-the-wild experiments, how-\never, require attention to both platform rules (in our case,\nTwitter), and ethical guidelines for interacting with users on\nsocial media. These ethical questions for social media exper-\niments are critical, as it is difficult to maintain anonymity on\nthese platforms. In the future, as we move towards combat-\ning harmful information on social media, such ethical con-\nsiderations become truly essential.\nConclusion\nAcross two consecutive preregistered studies with a total of\n207 participants, we find evidence that negative (angry) fa-\ncial expressions in social media news images lead to an in-\ncrease in users\u2019 perception of bias in content. We also found\nthat users judge sources to be less credible when they ob-\nserve a potentially systematic negative portrayal of different\npoliticians. While this paper was motivated by the preva-\nlence and impact of misinformation on our democracies and\nsocieties, our current globally politicized and polarized po-\nlitical ecosystem calls for a more critical view of the whole\nmedia landscape. This paper highlights the importance of\nmoving beyond the accuracy of textual content in the con-\nstant struggle to build trust in credible news sources. Sensa-\ntionalized news content, even in subtle ways such as emo-\ntional facial expressions, might impair users\u2019 trust in oth-\nerwise trustworthy news. These effects can lead to harm-\n479\nful outcomes evident in many current issues such as vaccine\nhesitancy and polarization.\nReferences\nAndalibi, N.; and Buss, J. 2020. The human in emotion\nrecognition on social media: Attitudes, outcomes, risks. In\nProceedings of the 2020 CHI Conference on Human Factors\nin Computing Systems, 1\u201316.\nArsenault, A.; and Castells, M. 2006. Conquering the minds,\nconquering Iraq: The social production of misinformation in\nthe United States\u2013a case study. Information, Communica-\ntion & Society, 9(3): 284\u2013307.\nBerger, J.; and Milkman, K. L. 2012. What makes online\ncontent viral? Journal of marketing research, 49(2): 192\u2013\n205.\nBowman, N. D.; and Cohen, E. 2020. 17 Mental Shortcuts,\nEmotion, and Social Rewards: The Challenges of Detecting\nand Resisting Fake News. Fake News: Understanding Media\nand Misinformation in the Digital Age, 223.\nCampellone, T. R.; and Kring, A. M. 2013. Who do you\ntrust? The impact of facial emotion and behaviour on deci-\nsion making. Cognition & emotion, 27(4): 603\u2013620.\nCichocki, A.; and Phan, A.-H. 2009. Fast local algorithms\nfor large scale nonnegative matrix and tensor factorizations.\nIEICE transactions on fundamentals of electronics, commu-\nnications and computer sciences, 92(3): 708\u2013721.\nde Melo, C.; Gratch, J.; Carnevale, P.; and Read, S. 2012.\nReverse appraisal: The importance of appraisals for the ef-\nfect of emotion displays on people\u2019s decision making in a\nsocial dilemma. In Proceedings of the Annual Meeting of\nthe Cognitive Science Society, volume 34.\nGoodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\nWarde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y .\n2020. Generative adversarial networks. Communications of\nthe ACM, 63(11): 139\u2013144.\nGrabe, M. E.; and Bucy, E. P. 2009. Image bite politics:\nNews and the visual framing of elections. Oxford University\nPress.\nHovland, C. I.; Janis, I. L.; and Kelley, H. H. 1953. Commu-\nnication and persuasion. Yale University Press.\nHutto, C.; and Gilbert, E. 2014. Vader: A parsimonious rule-\nbased model for sentiment analysis of social media text. In\nProceedings of the International AAAI Conference on Web\nand Social Media, volume 8.\nKahan, D. M. 2012. Ideology, motivated reasoning, and cog-\nnitive reflection: An experimental study. Judgment and De-\ncision making, 8: 407\u201324.\nKarduni, A.; Cho, I.; Wesslen, R.; Santhanam, S.; V olkova,\nS.; Arendt, D. L.; Shaikh, S.; and Dou, W. 2019. Vulnerable\nto misinformation? Verifi! In Proceedings of the 24th In-\nternational Conference on Intelligent User Interfaces, 312\u2013\n323.\nKarduni, A.; Markant, D.; Wesslen, R.; and Dou, W. 2020.\nA Bayesian cognition approach for belief updating of corre-\nlation judgement through uncertainty visualizations. IEEE\nTransactions on Visualization and Computer Graphics, 1\u20131.Karduni, A.; Wesslen, R.; Santhanam, S.; Cho, I.; V olkova,\nS.; Arendt, D.; Shaikh, S.; and Dou, W. 2018. Can You Ver-\nifi This? Studying Uncertainty and Decision-Making About\nMisinformation using Visual Analytics. In International\nConference on Web and Social Media (ICWSM).\nKnobloch, S.; Hastall, M.; Zillmann, D.; and Callison, C.\n2003. Imagery effects on the selective reading of Internet\nnewsmagazines. Communication Research, 30(1): 3\u201329.\nKret, M.; and Gelder, B. 2012. Islamic Headdress Influences\nHow Emotion is Recognized from the Eyes. Frontiers in\npsychology, 3: 110.\nKret, M.; and Gelder, B. 2013. When a smile becomes a fist:\nThe perception of facial and bodily expressions of emotion\nin violent offenders. Experimental brain research. Experi-\nmentelle Hirnforschung. Experimentation cerebrale, 228.\nKret, M.; Roelofs, K.; Stekelenburg, J.; and de Gelder, B.\n2013. Emotional signals from faces, bodies and scenes in-\nfluence observers\u2019 face expressions, fixations and pupil-size.\nFrontiers in Human Neuroscience, 7.\nLazer, D. M.; Baum, M. A.; Benkler, Y .; Berinsky, A. J.;\nGreenhill, K. M.; Menczer, F.; Metzger, M. J.; Nyhan, B.;\nPennycook, G.; Rothschild, D.; et al. 2018. The science of\nfake news. Science, 359(6380): 1094\u20131096.\nLo, W. H.; and Cheng, B. K. L. 2017. The use of melodra-\nmatic animation in news, presence and news credibility: a\npath model. Journalism Studies, 18(6): 787\u2013805.\nLobinger, K.; and Brantner, C. 2015. Likable, funny or\nridiculous? A Q-sort study on audience perceptions of vi-\nsual portrayals of politicians. Visual Communication, 14(1):\n15\u201340.\nMartel, C.; Pennycook, G.; and Rand, D. G. 2020. Reliance\non emotion promotes belief in fake news. Cognitive re-\nsearch: principles and implications, 5(1): 1\u201320.\nMasch, L.; and Gabriel, O. W. 2020. How Emotional Dis-\nplays of Political Leaders Shape Citizen Attitudes: The Case\nof German Chancellor Angela Merkel. German Politics,\n29(2): 158\u2013179.\nMcCabe, D. P.; and Castel, A. D. 2008. Seeing is believ-\ning: The effect of brain images on judgments of scientific\nreasoning. Cognition, 107(1): 343\u2013352.\nMcInnes, L.; Healy, J.; and Astels, S. 2017. hdbscan: Hier-\narchical density based clustering. Journal of Open Source\nSoftware, 2(11): 205.\nMessaris, P.; and Abraham, L. 2001. The role of images in\nframing news stories. Framing public life: Perspectives on\nmedia and our understanding of the social world, 215\u2013226.\nMorris, M. R.; Counts, S.; Roseway, A.; Hoff, A.; and\nSchwarz, J. 2012. Tweeting is believing? Understanding mi-\ncroblog credibility perceptions. In Proceedings of the ACM\n2012 conference on computer supported cooperative work ,\n441\u2013450.\nMosleh, M.; Pennycook, G.; and Rand, D. G. 2022. Field\nexperiments on social media. Current Directions in Psycho-\nlogical Science, 31(1): 69\u201375.\nMullen, B.; Futrell, D.; Stairs, D.; Tice, D. M.; Baumeis-\nter, R. F.; Dawson, K. E.; Riordan, C. A.; Radloff, C. E.;\n480\nGoethals, G. R.; Kennedy, J. G.; et al. 1986. Newcasters\u2019 fa-\ncial expressions and voting behavior of viewers: Can a smile\nelect a president? Journal of Personality and Social Psychol-\nogy, 51(2): 291.\nPehlivanoglu, D.; Lin, T.; Deceus, F.; Heemskerk, A.; Ebner,\nN. C.; and Cahill, B. S. 2021. The role of analytical reason-\ning and source credibility on the evaluation of real and fake\nfull-length news articles. Cognitive research: principles and\nimplications, 6(1): 1\u201312.\nPeng, Y . 2018. Same candidates, different faces: Uncover-\ning media bias in visual portrayals of presidential candidates\nwith computer vision. Journal of Communication, 68(5):\n920\u2013941.\nPennycook, G.; Cannon, T. D.; and Rand, D. G. 2018. Prior\nexposure increases perceived accuracy of fake news. Journal\nof experimental psychology: general, 147(12): 1865.\nPennycook, G.; and Rand, D. G. 2019. Lazy, not biased:\nSusceptibility to partisan fake news is better explained by\nlack of reasoning than by motivated reasoning. Cognition,\n188: 39\u201350.\nPennycook, G.; and Rand, D. G. 2021. The psychology of\nfake news. Trends in cognitive sciences.\nPetty, R. E.; Wegener, D. T.; and Fabrigar, L. R. 1997. At-\ntitudes and attitude change. Annual review of psychology,\n48(1): 609\u2013647.\nPornpitakpan, C. 2004. The persuasiveness of source cred-\nibility: A critical review of five decades\u2019 evidence. Journal\nof applied social psychology, 34(2): 243\u2013281.\nPorter, S.; Bellhouse, S.; McDougall, A.; Ten Brinke, L.; and\nWilson, K. 2010. A prospective investigation of the vulnera-\nbility of memory for positive and negative emotional scenes\nto the misinformation effect. Canadian Journal of Be-\nhavioural Science/Revue canadienne des sciences du com-\nportement, 42(1): 55.\nRathje, S.; Van Bavel, J. J.; and van der Linden, S. 2021.\nOut-group animosity drives engagement on social media.\nProceedings of the National Academy of Sciences, 118(26).\nReis, J.; Benevenuto, F.; de Melo, P. O.; Prates, R.; Kwak,\nH.; and An, J. 2015. Breaking the news: First impressions\nmatter on online news. arXiv preprint arXiv:1503.07921.\nRichards, B. 2007. Emotional governance: Politics, media\nand terror. Springer.\nSchroff, F.; Kalenichenko, D.; and Philbin, J. 2015. Facenet:\nA unified embedding for face recognition and clustering. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, 815\u2013823.\nSchweitzer, N.; Baker, D. A.; and Risko, E. F. 2013. Fooled\nby the brain: Re-examining the influence of neuroimages.\nCognition, 129(3): 501\u2013511.\nShi, Z.; Wang, A.-L.; Emery, L. F.; Sheerin, K. M.; and\nRomer, D. 2017. The importance of relevant emotional\narousal in the efficacy of pictorial health warnings for\ncigarettes. Nicotine & Tobacco Research, 19(6): 750\u2013755.\nSmall, D. A.; and Verrochi, N. M. 2009. The face of need:\nFacial emotion expression on charity advertisements. Jour-\nnal of marketing research, 46(6): 777\u2013787.Todorov, A. T.; Uddenberg, S. D.; Peterson, J. C.; Griffiths,\nT. L.; and Suchow, J. W. 2021. Data-driven, photorealistic\nsocial face-trait encoding, prediction, and manipulation us-\ning deep neural networks. US Patent App. 17/026,797.\nTormala, Z. L.; and Petty, R. E. 2004. Source credibility and\nattitude certainty: A metacognitive analysis of resistance to\npersuasion. Journal of Consumer Psychology, 14(4): 427\u2013\n442.\nTormala, Z. L.; and Rucker, D. D. 2007. Attitude certainty:\nA review of past findings and emerging perspectives. Social\nand Personality Psychology Compass, 1(1): 469\u2013492.\nVan Kleef, G. A.; De Dreu, C. K.; and Manstead, A. S.\n2010. An interpersonal approach to emotion in social de-\ncision making: The emotions as social information model.\nInAdvances in experimental social psychology, volume 42,\n45\u201396. Elsevier.\nVlasceanu, M.; Goebel, J.; and Coman, A. 2020. The\nEmotion-Induced Belief Amplification Effect. In Proceed-\nings of the Annual Meeting of the Cognitive Science Society .\nV olkova, S.; Ayton, E.; Arendt, D. L.; Huang, Z.; and\nHutchinson, B. 2019. Explaining multimodal deceptive\nnews prediction models. In Proceedings of the International\nAAAI Conference on Web and Social Media, volume 13,\n659\u2013662.\nV olkova, S.; Shaffer, K.; Jang, J. Y .; and Hodas, N. 2017.\nSeparating facts from fiction: Linguistic models to classify\nsuspicious and trusted news posts on twitter. In Proceed-\nings of the 55th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 2: Short Papers), volume 2,\n647\u2013653.\nWallace, L. E.; Wegener, D. T.; and Petty, R. E. 2020a. In-\nfluences of source bias that differ from source untrustworthi-\nness: When flip-flopping is more and less surprising. Jour-\nnal of personality and social psychology, 118(4): 603.\nWallace, L. E.; Wegener, D. T.; and Petty, R. E. 2020b. When\nsources honestly provide their biased opinion: Bias as a dis-\ntinct source perception with independent effects on credi-\nbility and persuasion. Personality and Social Psychology\nBulletin, 46(3): 439\u2013453.\nWardle, C.; and Derakhshan, H. 2017. Information Disor-\nder: Toward an interdisciplinary framework for research and\npolicymaking. Council of Europe report, DGI (2017), 9.\nWobbrock, J. O.; Hattatoglu, L.; Hsu, A. K.; Burger, M. A.;\nand Magee, M. J. 2021. The Goldilocks zone: young adults\u2019\ncredibility perceptions of online news articles based on vi-\nsual appearance. New Review of Hypermedia and Multime-\ndia, 1\u201346.\nZillmann, D.; Knobloch, S.; and Yu, H.-s. 2001. Effects of\nphotographs on the selective reading of news reports. Media\nPsychology, 3(4): 301\u2013324.\n481", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Images, emotions, and credibility: Effect of emotional facial expressions on perceptions of news content bias and source credibility in social media", "author": ["A Karduni", "R Wesslen", "D Markant", "W Dou"], "pub_year": "2023", "venue": "Proceedings of the \u2026", "abstract": "Images are an indispensable part of the news we consume. Highly emotional images from  mainstream and misinformation sources can greatly influence our trust in the news. We"}, "filled": false, "gsrank": 335, "pub_url": "https://ojs.aaai.org/index.php/ICWSM/article/view/22161", "author_id": ["tFg77gIAAAAJ", "F40SbCkAAAAJ", "lXiXiHAAAAAJ", "vi_9wEkAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:vjKZNQrvkOUJ:scholar.google.com/&output=cite&scirp=334&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D330%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=vjKZNQrvkOUJ&ei=QLWsaMqsGZXUieoPmrax2A8&json=", "num_citations": 14, "citedby_url": "/scholar?cites=16541984258459775678&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:vjKZNQrvkOUJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://ojs.aaai.org/index.php/ICWSM/article/download/22161/21940"}}, {"title": "Nudging users to slow down the spread of fake news in social media", "year": "2020", "pdf_data": "NUDGING USERS TO SLOW DOWN THE SPREAD OF FAKE NEWS IN SOCIAL MEDIA\nChristian von der Weth Jithin Vachery Mohan Kankanhalli\nSchool of Computing, National University of Singapore\nfchrisjjithinjmohang@comp.nus.edu.sg\nABSTRACT\nThe success of fake news spreading on social media is to a\nlarge extent because of normal users unknowingly parroting\nor sharing such content. Educational interventions such as in-\nformation campaigns or revised curricula aim to raise users\u2019\nawareness and critical thinking skills. However, these are\nlong-term efforts with an uncertain outcome. In this paper,\nwe present ShareAware, our prototype for nudging users into\na more conscious posting and sharing behavior. ShareAware\nuses linguistic analyses to infer the factuality of content and\ncredibility of sources before being posted or shared. The re-\nsults provide users with immediate feedback to discourage the\ndissemination of questionable content. We demonstrate the\nbene\ufb01ts of our approach using a series of simulations.\nIndex Terms \u2014fake news, social media, user nudging\n1. INTRODUCTION\n\u201cFalsehood \ufb02ies, and the truth comes limping\nafter it\u201d (Jonathan Swift, 1710)\nIn recent years, the adverse effects of \u201cfake news\u201d came\nunder public scrutiny, particularly within politics, health, and\n\ufb01nances. While disinformation has long been used to shape\npeople\u2019s thoughts and behavior, social media has ampli\ufb01ed\nits adverse effects signi\ufb01cantly. Firstly, never has it been so\neasy to access, publish, and share information, including the\ndeployment of so-called social bots, i.e., software-controlled\nuser accounts. Secondly, journalistic norms such as objectiv-\nity and balance are often forgotten, ignored, or purposefully\ndismissed. And lastly, besides bots, users often share content\nwithout fact-checking, especially when it contains controver-\nsial or emotionally charged content. Current countermeasures\nto fake news can be classi\ufb01ed into three categories:\nFrom a legal perspective, anti-fake news laws impose\n\ufb01nes for fake news and enforce platform providers to curate\nor remove content. However, fake news and related notions\nare not well de\ufb01ned, making it very dif\ufb01cult to put such no-\ntions into a legal framework. It would also make governments\nthe arbiters of truth. This form of hard paternalism raises le-\ngitimate concerns of censorship and misuse [1]. Moreover,manual fact-checking and re-actively taking down false infor-\nmation do not scale and typically take effect only after the\ndamage is done.\nFrom a technological perspective, a plethora of methods\nto tackle the publication and diffusion of disinformation have\nbeen proposed. The most common research directions con-\ncern the identi\ufb01cation of social bots, automated credibility\nassessment, and fact-checking. Most efforts utilize state-of-\nthe-art data mining and machine learning techniques to distin-\nguish between human users and social bots or between false\nor truthful content; see Section 2. These tasks are inherently\nvery challenging and can be viewed as a cat-and-mouse game\n\u2013 any improvements in detection will result in the develop-\nment of improved bots or better fake content.\nFrom a societal perspective, the success of disinformation\ncan be attributed to two human \u201c\ufb02aws\u201d. Firstly, disinforma-\ntion often mobilizes the user\u2019s cognitive biases and heuristics,\nmaking it more likely for users to fall for it [2]. Secondly,\ndisinformation is typically novel, controversial, emotionally\ncharged, or partisan, making it more \u201cinteresting\u201d and hence\nmore likely to be shared [1]. Educational interventions such\nas public information campaigns or reformed school curricula\naim to improve users\u2019 critical thinking skills as well as their\ndigital literacy in general. However, these are either one-time\nor long-term efforts with uncertain outcomes [3] .\nSumming up, legal and technical solutions focus on the\n\u201cbad guys\u201d \u2013 that is, social bots and human users ranging\nfrom individuals to state actors that intentionally generate and\nspread disinformation. However, the success of fake news go-\ning viral also strongly depends on normal users without any\nmalicious intentions of sharing disinformation [1]. While in-\nformation campaigns might (temporarily) raise users\u2019 aware-\nness, such educational interventions typically happen outside\nof the context of users\u2019 everyday social media use. Posting\nand sharing content on social media is typically very situa-\ntional actions that often do not re\ufb02ect users\u2019 attitudes. This is\nparticularly true due to the \u201cinfective\u201d nature of fake news.\nIn this work, we motivate a hybrid approach using techno-\nlogical solutions to provide in-situ educational interventions\nto nudge the \u201cgood guys\u201d into a more conscious social media\nuse. Nudging is a form of soft paternalism to guide users by\nsuggesting, instead of enforcing a certain behavior. To this\nend, we present ShareAware, which analyzes content before\n978-1-7281-1485-9/20/$31.00 c\r2020 IEEE\nAuthorized licensed use limited to: National University of Singapore. Downloaded on February 28,2021 at 02:51:09 UTC from IEEE Xplore.  Restrictions apply. \nbeing posted or shared. If the analyses raise any concerns\nabout posts\u2019 factuality or credibility of the sources the users\nget noti\ufb01ed. We implement ShareAware as a browser exten-\nsion to enable seamless integration into user\u2019s social media\nexperience. We present and discuss the current features of our\nShareAware platform, provide preliminary results towards its\neffectiveness, as well as outline our long-term research goals.\nWe argue that our approach both combines and complements\ncurrent legal, technological, and societal solutions.\n2. RELATED WORK\nSocial bot detection. Most methods to identify social\nbots use supervised machine learning leveraging on the user,\ncontent, social network, temporal features, etc. (e.g. [4, 5]).\nThe underlying assumption is that genuine human users and\nsocial bots exhibit suf\ufb01ciently distinct behavior patterns. In\ncontrast, unsupervised methods aim to detect social bots by\n\ufb01nding accounts that share strong similarities with respect to\nsocial network and (potentially coordinated) posting/sharing\nbehavior (e.g. [6, 7]). The challenge is that bots with mali-\ncious intent get better and better at blending into the popula-\ntion of genuine user accounts.\nManual and automated fact-checking. Fact-checking\nwebsites such as Snopes or Politifact validate or debunk pop-\nular claims including personal statements, rumors, urban leg-\nends, etc. Sites such as Media Bias/Fact Check (MBFC) eval-\nuate news sites regarding their credibility and biases. Also,\nmost social platforms hire dedicated staff to fact-check sub-\nmitted content. However, manual fact-checking scales very\npoorly with the amount of information published online. Var-\nious solutions for automated fact-checking have been pro-\nposed (e.g., [8, 9]). However, fully automated fact-checking\nsystem are far from mature [10]. More common are hybrid\nsolutions where humans are supported by automated systems.\nUser nudging in social media. What to post or share is\noften very situational and rushed, resulting in users often re-\nporting regrets [11, 12]. A common cause for regret is the\n(unintentional) self-disclosure of sensitive information. Ac-\nquisti et al. evaluated series of privacy nudges [13]. For ex-\nample, users see a subset of contacts who will be able to read\na post to remind users of their audience. Another cause for\nregret is a blunder, which is concerned with mistakes and fac-\ntuality issues [12]. Nekmat [14] conducted a series of user\nsurveys to evaluate the effectiveness of fact-check alerts (e.g.,\nthe reputation of a news source). The results show that such\nalerts trigger users\u2019 skepticism, thus lowering the likelihood\nof sharing information from questionable sources.\n3. ShareAware PLATFORM\nOur ShareAware platform aims to nudge users into a more\nconscious posting and sharing behavior. Before a user posts\nor shares content, ShareAware analyzes it and makes subtleeducational interventions if the content raises concerns about\nits level of certainty and/or credibly of its sources. In this sec-\ntion, we use the \ufb01ctitious tweet \u201cTrump said that supermar-\nkets might fail to restock food. \u201d in the midst of the COVID-19\npandemic as a running example.\n3.1. Existing User Account & Link Analysis\nFor Twitter, we can leverage on existing efforts towards social\nbot detection as a proof of concept to utilize such informa-\ntion as user nudges. More speci\ufb01cally, we use the Botometer\nAPI [4] returning a score between 0 (low) and 5 (high), rep-\nresenting the likelihood that a Twitter account is a bot. We\nadopt this score but color-code it for visualization: green (0-\n2), yellow (2-3), orange (3-4), red (4-5); see Figure 3.\nShareAware displays credibility information for linked\ncontent in a social media post. To this end, we collected\ndata for 2.7k+ online news sites provided by Media Bias Fact\nCheck (MBFC).1MBFC assigns each news site one of six\nfactuality labels and one of nine bias or category labels. We\nprovide an API that for a given URL returns the correspond-\ning credibility information, for example (simpli\ufb01ed):\n{\n\"unshortened\": \"https://www.breitbart.com/entertainment\n/2020/01/24/f-donald-trump-rapper-yg-arrested-...\",\n\"domain\": \"www.breitbart.com\",\n\"rating\": {\n\"ratings\": {\n\"fact\": \"mixed factual\",\n\"bias\": \"fake news\"\n},\n\"labels\": {\n\"fact\": [\"not always credible\", ...],\n\"bias\": [\"questionable source\", \"not credible\", ...]\n}\n}\n}\n3.2. Triple Extraction\nTextual content (e.g., social media posts) represents unstruc-\ntured information. To support downstream tasks such as fac-\ntuality degree annotation (see below), we convert text into a\nstructured, triple-based representation. This refers to the task\nof Open Information Extraction, which is most commonly ap-\nplied to statements from factual content (e.g., to enrich knowl-\nedge bases); see [15]. In contrast, we focus on posts that can\nexpress doubts, uncertainty, counterfactuals, etc.\nOur triple extraction algorithm takes as input a depen-\ndency graph [16]; see Figure 1 for an example. From this\ngraph, we derive all triples based on universal dependencies2\n(edge labels). To preserve the connection between statements\nwithin on sentence, we extract three types of triples:\nSubject-Predicate-Object: spo -triples represent basic,\nstandalone statements such as \u201csupermarkets restock food\u201c ,\n1https://mediabiasfactcheck.com/\n2https://universaldependencies.org/\nAuthorized licensed use limited to: National University of Singapore. Downloaded on February 28,2021 at 02:51:09 UTC from IEEE Xplore.  Restrictions apply. \nTrump/NNP-1said/VBD-2\nnsubj\nfail/VB-6ccomp\n./.-10punct\nthat/IN-3\nsupermarkets/NNS-4might/MD-5mark\nnsubjaux\nrestock/VB-8xcomp\nto/TO-7nsubj:xsubj mark\nfood/NN-9dobjFig. 1 : Dependency graph for example sentence \u201cTrump said that\nsupermarkets might fail to restock food. \u201d\nTrump\nsaid\nfailsupermarkets\nrestock\nfood\nFig. 2 : Graph representation of\nextracted triples from example\nsentence \u201cTrump said that su-\npermarkets might fail to restock\nfood. \u201d The nested boxes re\ufb02ect\nthe tree-like reference structure\nbetween all statements.\ntypically indicated by the presence of an object (e.g., dobj ).\nNote that the object can be null (e.g., \u201csupermarkets closed. \u201d )\nSubject-Predicate-Reference: spr -triples represent state-\nments about statements, i.e., the target of an spr-triple is a\nreference to another triple. Common cases are causal com-\nplements ( ccomp ) such as \u201csaid that [...] failed\u201d ), and open\nclausal complements ( xcomp ) such as \u201cfailed to restock\u201d ).\nspr-triples are of particularly importance when it comes to\nevaluate the factuality of their referenced statements.\nReference-Predicate-Reference: rpr -triples connect two\nstatements. For example, adverbial clause modi\ufb01ers ( adcvl )\nsuch as \u201cTrump was blunt when Obama visited\u201d connect the\nstatements \u201cTrump was blunt\u201c and\u201cObama visited\u201d .\nWhen applied to the example post, our triple extraction\nalgorithm returns the following set of triples (simpli\ufb01ed):\nID:1 Trump ! said! ID:2\nID:2 supermarkets ! fail! ID:3\nID:3 supermarkets ! restock! food\nIn this example, ID:1 and ID:2 are spr-triples and ID:3 is an\nspo-triple. Figure 2 visualizes the triple set as a hierarchi-\ncal statement graph. Note that for each non-reference node\nin a triple, we retain important information such as negation,\nadverbs, adjectives, and auxiliaries that can affect on the fac-\ntuality of a statement. For example, we keep the information\nthat\u201cmight\u201d is an adverbial modi\ufb01er of \u201cfail\u201c .positive negative underspec.\ncertain CT+ CT- CTu\nprobable PR+ PR- n/a\npossible PS+ PS- n/a\nunderspec. n/a n/a Uu\nTable 1 : Possible factuality degrees (n/a = impossible degree) [17]\n3.3. Factuality Annotation\nOur example post makes the following assertions regarding\nfactuality: (a) Trump is not certain that supermarkets fail,\n(b) Trump is not certain that supermarkets did notrestock\nfood, and (c) the author herself does not assert the correctness\nof the statements about supermarkets but only (d) that Trump\nmade those statements. To formalize these assertions, we\nadopt the framework proposed by Saur \u00b4\u0131 and Pustejovsky [17].\nFactuality degree. Linguistic literature commonly agrees\non four discrete levels of certainty ( certain ,probable ,possi-\nble,underspeci\ufb01ed ) and three polarity values ( positive ,nega-\ntive,underspeci\ufb01ed ); Table 1 shows the resulting set of pos-\nsible factuality degrees. For example, PR- andPS- is as-\nsociated with statements that are not probable and not cer-\ntain, respectively. The factuality degree of a statement can be\nmodi\ufb01ed by individual words and whole phrases. For exam-\nple, given the statement \u201csupermarkets might fail to restock\nfood\u201d , the core statement \u201csupermarkets restock food\u201d (CT+)\ngets modi\ufb01ed twice: Firstly, it inherits the level of uncertainty\nfrom \u201csupermarkets might fail\u201d. (PS+). And secondly, failed\n(to)is a simple implicative verb that \ufb02ips the polarity of its\nreferenced statement. As a results, the factuality degree of\n\u201csupermarkets restock food\u201d evaluates to PS- (unlikely).\nSource. By default, the source of a statement in a sen-\ntence is the writer of the sentence, denoted by s0. However, a\nsentence might contain source-introducing predicates (SIPs)\nthat introduce new sources in the discourse. Common SIPs\nare predicates of report (e.g., say,tell), predicates of knowl-\nedge (e.g., know ,forget ), predicates of belief and opinion\n(e.g., think ,consider ), and others. For example, \u201cTrump said\n[...]\u201d introduces Trump as a new source which is asserting\nthe factuality of the following statement(s). Note that this\nassertion made by source sTrump is itself an assertion made\nby the author, denoted with strump 0. In principle, sources\ncan be arbitrarily nested. For example, the sentence \u201cTrump\nsaid that Obama claimed that Hillary did X\u201d yields the source\nshillary obama trump 0(amongs0,strump 0, etc.). This nest-\ning of sources also means that a statement has a factuality\ndegree for each \u201couter\u201d source. In the previous example, for\ninstance, the writer s0is uncommitted regarding the factuality\nof statement X(this is true for most SIPs since they push any\nfollowing assertions to the new source) .\nFactuality calculation (by example). With these two no-\ntions of source and factuality degree, we systematically assign\neach statement (here: triple) one or more factuality labels de-\npending on the mentioned sources and linguistic information\nAuthorized licensed use limited to: National University of Singapore. Downloaded on February 28,2021 at 02:51:09 UTC from IEEE Xplore.  Restrictions apply. \nFig. 3 : Example of interventions made by ShareAware browser ex-\ntension when submitting a new tweet.\n(adverbs, implicative verbs, SIPs, etc.). An individual factu-\nality label is a tuple hs;fdiof a sourcesand a factuality de-\ngreefd. A detailed discussion of the algorithm is beyond the\nscope of the paper, and we point the interested reader to [17].\nWhen applied to our example post, the algorithm returns the\nfollowing factuality labels for the three triples:\nID:1 Trump ! said! ID:2\nfhs0;CT +ig\nID:2 supermarkets ! fail! ID:3\nfhs0;Uui;hstrump 0;PS +ig\nID:3 supermarkets ! restock! food\nfhs0;Uui;hstrump 0;PS\u0000ig\nThe annotations re\ufb02ect our intuitive assertions we made at\nthe beginning of this section. For example, in the label\nfhs0;Uui;hstrump 0;PS\u0000ig for ID:3,hstrump 0;PS\u0000ire-\n\ufb02ects assertions (b/c) and hs0;Uuire\ufb02ects assertion (d).\n3.4. Educational Interventions\nWe implemented the frontend of ShareAware as a browser\nextension for the seamless, integration of factuality and credi-\nbility information into users\u2019 normal social media experience.\nThe browser extension intercepts post and share requests and\ninjects factuality and credibility information in terms of warn-\ning messages. Using the results of the user account and link\nanalysis is straightforward (cf. Section 3.1). Factuality labels,\non the other hand, can be interpreted in various ways; and how\nto display them is an on-going research question. For the time\nbeing, we use the following heuristics \u2013 we warn users if: (a)\na statement made by the author is considered uncertain (fac-\ntuality is neither CT+ norCT-), (b) a statement is made by\nanother source instead of the author; in this case, we ask the\nauthor to reconsider the source\u2019s credibility. Figure 3 showsthe resulting warning messages for our example post when a\nuser would submit it as a new tweet. Given these warnings,\nthe user can change the post accordingly (e.g., link to a more\ncredible news source), immediately submit it \u201cas is\u201d, or wait\nfor the post the be automatically submitted after a delay.\n4. EVALUATION\nTo get some \ufb01rst insights into the effectiveness of\nShareAware, we leverage on existing surveys showing that\nusers respond to factuality nudges [14] and conduct a series\nof simulations to evaluate the effect of such nudges on the\ninformation diffusion with a social network.\nDissemination Model & Methodology. We model a so-\ncial network as a directed graph with Nnodes represent-\ning the users, and Independent Cascade Model [18] cap-\ntures the propagation of fake news. We use four graphs:\none graph derived from a real-world Twitter follower net-\nwork (Higgs dataset) [19] and three random graphs (scale-free\ngraph, Erd \u02ddos-R \u00b4enyi graph, and a clustered graph simulating\npartisan networks). Each graph contains bN(randomly cho-\nsen) bots and (1\u0000b)Nnon-bots, i.e., regular users. Each bot\nforwards a fake news message with probability pbot= 1:0.\nWithout nudging, the probability puser that a user forwards a\nfake news message is proportional to the outdegree of mes-\nsage sender. This re\ufb02ects that users are more likely to share\nmessages from in\ufb02uential users. We simulate information dis-\nsemination in discrete time steps. At each time step ti, a user\nor bot may receive a message and forwards it with probability\npuser orpbot. An attack, i.e., the initial submission of a fake\nnews message, is started by sseed bots at time t0.\nWe model the usage of ShareAware using two parameters:\n\u000b, the fraction of users using our platform, and \f, the fraction\nto which the forwarding probability puser of a user (which\nuses ShareAware) is decreased when presented with a nudge.\nFor example, with \f= 0 users will not forward the mes-\nsage; with\f= 0:5the forwarding probability is 0:5puser.\nWe use two metrics to measure the effectiveness of nudges:\n(1) the number of infected nodes in the graph, i.e., the num-\nber of users that have received and forwarded the fake news\nmessage; (2) the number of effected nodes per time step as an\nestimator for the dissemination time.\nExperiments & Results. For each network, we ran 2,000\nsimulations attacks (i.e., seeding a new fake news message)\nand measured the number of infected nodes and the number\nof infections per round. In the case of the clustered network,\nwe assume that all seed nodes and bots are in the same cluster.\nIf not stated otherwise, we assume that 10% of all nodes in the\nnetwork are bots ( b= 0:1), and attack is started by 200 seed\nnodes (s= 200 ). In all the following plots, we show the mean\nover all attacks. The standard deviation is consistently min-\nimal, and we, therefore, omit their visualization using error\nbars. Due to space constraints, we only show the results for\nthe Higgs network (Fig. 4) and the clustered network (Fig. 5);\nAuthorized licensed use limited to: National University of Singapore. Downloaded on February 28,2021 at 02:51:09 UTC from IEEE Xplore.  Restrictions apply. \n 0 0.2 0.4 0.6 0.8 1\u03b1 0 0.2 0.4 0.6 0.8 1\u03b2 0 10 20 30 40 50 60 70Infected Users (%) 10 15 20 25 30 35 40 45 50 55(a) overall dissemination\n 0 0.2 0.4 0.6 0.8 1\u03b1 0 0.2 0.4 0.6 0.8 1\u03b2 0 1 2 3 4 5 6Infections per Round (%) 1.5 2 2.5 3 3.5 4 4.5 (b) rate of dissemination\n 0 10 20 30 40 50 60 70\n 0 50 100  150  200  250  300  350  400  450  500Infected Users (%)\nNumber of seed bots0% bots\n1% bots\n2% bots\n3% bots\n5% bots\n10% bots (c) Effects of #seeds and #bots\nFig. 4 :Higgs network , #Nodes: 456k, #Edges: 14M\n 0 0.2 0.4 0.6 0.8 1\u03b1 0 0.2 0.4 0.6 0.8 1\u03b2 0 5 10 15 20 25 30 35 40Infected Users (%) 5 10 15 20 25\n(a) overall dissemination\n 0 0.2 0.4 0.6 0.8 1\u03b1 0 0.2 0.4 0.6 0.8 1\u03b2 0 0.2 0.4 0.6 0.8 1 1.2 1.4 Infections per Round (%) 0.2 0.4 0.6 0.8 1 (b) rate of dissemination\n 0 10 20 30 40 50 60 70\n 0 50 100  150  200  250  300  350  400  450  500Infected Users (%)\nNumber of seed bots0% bots\n1% bots\n2% bots\n3% bots\n5% bots\n10% bots (c) Effects of #seeds and #bots\nFig. 5 :Clustered Network , #Nodes: 100k, #Edges: 1M, #Clusters: 4\nthe results for the other random graphs show similar trends.\nFor the clustered network, we show our two metrics with re-\nspect to the nodes that are not part of the attacking cluster.\nUnsurprisingly, for all networks, the more users use\nShareAware (larger values for \u000b), and the more they are in-\n\ufb02uenced by it (larger values for \f), the more contained and\nslower is a fake news attack. Note that even in the optimal\nsetting where no fake news messages are forwarded by reg-\nular users (\u000b= 1:0and\f= 1:0), both the overall number\nof infected nodes and the infections per round is still notice-\nable. This is due to two reasons. Firstly, apart from the seed\nbots, the message is always forwarded by any other bot in\nthe network. Moreover, secondly, we make the worst-case as-\nsumption by picking the bots from all nodes with the highest\noutdegrees (i.e., users with the most followers). In short, suc-\ncessfully nudging users can signi\ufb01cantly limit both the spread\nand the speed that fake news travels across social media, thus\nlimiting its potential consequences. In the context of our run-\nning example, this is more likely to reduce the number of peo-\nple engaging in panic buying. It will also give supermarkets\nmore time to \ufb01ll their stocks, as well as give governments\nmore time for interventions (e.g., publishing corrective mes-\nsages or fact checks on both social media and trusted web-\nsites). In practice, the main challenges will be to promote the\nuse of platforms such as ShareAware (maximizing \u000b) as well\nas to provide effective and trusted nudges (maximizing \f).\nFor the sake of completeness, Fig. 4(c) and Fig. 5(c) show\nthe effects of the number of seed bots sand the number of\nbotsbin networks; we report the overall number of infected\nnodes. In both cases, we set \u000b= 0:05and\f= 0:5, which\nare arguably more realistic values compared to a very highnumber of nudged users. As expected, the number of infected\nnodes increases with increasing values for sandb; however,\nthe increases are sublinear for both parameters.\n5. LIMITATIONS & ROADMAP\nShareAware is an early prototype towards user nudging in so-\ncial media. In this section, we brie\ufb02y discuss current limita-\ntions and provide a roadmap for our ongoing efforts.\nMultimodal content analysis. Apart from considering\navailable credibility data about information sources (links or\nusers), we have proposed a linguistic approach to evaluate\nfactuality information of written statements. A natural next\nstep is to extend these efforts to multimodal content, particu-\nlarly images and videos. This includes digital forensic tech-\nniques to spot tampered photos or content retrieval techniques\nto \ufb01nd related images and videos from credible sources.\nDownstream task support. We convert unstructured text\ninto structured triple-based representation to evaluate the fac-\ntuality labels of each statement. Such a structured representa-\ntion improves and enables a wide range of downstream tasks\nsuch as indexing, searching, and (entity) linking. Effectively\nand ef\ufb01ciently performing such tasks can further advance fake\nnews detection, e.g., by \ufb01nding and tracing related statements\nabout the same topic but from different sources.\nDeeper semantic understanding. Given two posts\n\u201cTrump said A\u201d and\u201cTrump said B\u201c , we treat statements A\nandBequally even if the former is a trivial or funny statement\nwhile the latter is of great political importance. Arguably, not\nall fake news is equal with respect to their potential gravitas.\nTo improve factuality nudges \u2013 for example, through ranking\nAuthorized licensed use limited to: National University of Singapore. Downloaded on February 28,2021 at 02:51:09 UTC from IEEE Xplore.  Restrictions apply. \nor \ufb01ltering by scoring statements \u2013 we need to understand the\nsemantic meaning of a statement better and de\ufb01ne meaningful\nmetrics to compare different statements.\nUX/UI design. In this paper, we focused on the backend\narchitecture for the analysis of social media content in terms\nof factuality and credibility information. However, when and\nhow to present this information to users signi\ufb01cantly impacts\nthe effectiveness of factuality nudges. An inherent trade-off\nis the level of detail and ease of understanding. For our cur-\nrent prototype, we mainly use simple templates to generate a\nwarning message based on our analysis results.\nFactuality nudges beyond social media. Our in-situ ap-\nproach using a browser extension makes ShareAware directly\napplicable to all online platforms beyond social media. For\nexample, we can inject the credibility information of a link\nsource into any website, including search result pages, on-\nline newspapers, online forums, etc. Such a more platform-\nagnostic solution poses additional challenges towards UX/UI\ndesign to enable helpful but also smooth user experience.\nTowards privacy nudges. Similar to existing works\n(e.g. [13]), we aim to support privacy nudges to also warn\nusers in case of potentially harmful self-disclosure. In this\ncontext, ShareAware analyzes a post with respect to sensitive\ninformation that can be extracted or inferred from the content.\n6. CONCLUSIONS\nNudging users to slow down the spread of fake news in\nsocial media complements existing automated approaches\nthat focus on the \u201cbad guys\u201d. In this work, we focused on\nconcerns regarding factuality in posts and the credibility\nof the sources. Our simulations show that guiding users\ntowards a more conscious posting and sharing behavior can\nsigni\ufb01cantly reduce the overall reach and dissemination\nspeed of fake news. Based on these promising \ufb01rst results, in\nthe future, we will investigate how to improve on ShareAware\nto consider different modalities, e.g., to identify and commu-\nnicate factuality concerns found in images and videos.\nAcknowledgements. This research is supported by the\nNational Research Foundation, Singapore under its Strategic\nCapability Research Centres Funding Initiative. Any opin-\nions, \ufb01ndings and conclusions or recommendations expressed\nin this material are those of the author(s) and do not re\ufb02ect\nthe views of National Research Foundation, Singapore.\n7. REFERENCES\n[1] S. V osoughi, D. Roy, and S. Aral, \u201cThe Spread of True\nand False News Online,\u201d Science , vol. 359, 2018.\n[2] G. Pennycook and D.G. Rand, \u201cWho Falls for Fake\nNews? The Roles of Bullshit Receptivity, Overclaiming,\nFamiliarity, and Analytic Thinking,\u201d Journal of Person-\nality, 2019.[3] D.M.J. Lazer, M.A. Baum, Y . Benkler, A.J. Berinsky,\nK.M. Greenhill, F. Menczer, M.J. Metzger, B. Nyhan,\nG. Pennycook, D. Rothschild, M. Schudson, C.R. Sun-\nstein, E.A. Thorson, D.J. Watts, and J.L. Zittrain, \u201cThe\nScience of Fake News,\u201d Science , vol. 359, 2018.\n[4] K.C. Yang, O. Varol, P.M. Hui, and F. Menczer, \u201cScal-\nable and Generalizable Social Bot Detection through\nData Selection,\u201d in AAAI) , 2020.\n[5] Sneha K. and Emilio F., \u201cDeep Neural Networks for Bot\nDetection,\u201d Information Sciences , vol. 467, 2018.\n[6] N. Chavoshi, H. Hamooni, and A. Mueen, \u201cDeBot:\nTwitter Bot Detection via Warped Correlation,\u201d in\nICDM . 2016, IEEE.\n[7] M. Mazza, S. Cresci, M. Avvenuti, and W. Quattrocioc-\nchi, \u201cRTbust: Exploiting Temporal Patterns for Botnet\nDetection on Twitter,\u201d in WebSci . 2019, ACM.\n[8] G. Karadzhov, P. Nakov, L. M `arquez, A. Barr \u00b4on-\nCede \u02dcno, and I. Koychev, \u201cFully Automated Fact Check-\ning Using External Sources,\u201d in RANLP , 2017.\n[9] N. Hassan, F. Arslan, C. Li, and M. Tremayne, \u201cToward\nAutomated Fact-Checking: Detecting Check-Worthy\nFactual Claims by ClaimBuster,\u201d in KDD . 2017, ACL.\n[10] L. Graves, \u201cUnderstanding the Promise and Limits of\nAutomated Fact-Checking,\u201d Tech. Rep., 2018.\n[11] L. Zhou, W. Wang, and K. Chen, \u201cTweet Properly: An-\nalyzing Deleted Tweets to Understand and Identify Re-\ngrettable Ones,\u201d in WWW , 2016.\n[12] \u201cI Read My Twitter the Next Morning and Was As-\ntonished: A Conversational Perspective on Twitter Re-\ngrets,\u201d in CHI. 2013, ACM.\n[13] A. Acquisti, I. Adjerid, R. Balebako, L. Brandimarte,\nL.F. Cranor, S. Komanduri, P.G. Leon, N. Sadeh,\nF. Schaub, M. Sleeper, Y . Wang, and S. Wilson,\n\u201cNudges for Privacy and Security: Understanding and\nAssisting Users\u2019 Choices Online,\u201d ACM Comput. Surv. ,\nvol. 50, no. 3, 2017.\n[14] E. Nekmat, \u201cNudge Effect of Fact-Check Alerts: Source\nIn\ufb02uence and Media Skepticism on Sharing of News\nMisinformation in Social Media,\u201d IEEE Transactions\non Dependable and Secure Computing , 2020.\n[15] C. Niklaus, M. Cetto, A. Freitas, and S. Handschuh, \u201cA\nSurvey on Open Information Extraction,\u201d in COLING .\n2018, ACL.\n[16] D. Chen and C. Manning, \u201cA Fast & Accurate Depen-\ndency Parser using Neural Networks,\u201d in EMNLP , 2014.\n[17] R. Saur \u00b4\u0131 and J. Pustejovsky, \u201cAre You Sure That This\nHappened? Assessing the Factuality Degree of Events\nin Text,\u201d Comput. Linguist. , vol. 38, 2012.\n[18] David Kempe, Jon Kleinberg, and \u00b4Eva Tardos, \u201cMax-\nimizing the Spread of In\ufb02uence through a Social Net-\nwork,\u201d in SIGKDD . 2003, ACM.\n[19] M. De Domenico, A. Lima, P. Mougel, and M. Mu-\nsolesi, \u201cThe Anatomy of a Scienti\ufb01c Rumor,\u201d Scienti\ufb01c\nReports , vol. 3, 2013.\nAuthorized licensed use limited to: National University of Singapore. Downloaded on February 28,2021 at 02:51:09 UTC from IEEE Xplore.  Restrictions apply. ", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Nudging users to slow down the spread of fake news in social media", "author": ["C von der Weth", "J Vachery"], "pub_year": "2020", "venue": "2020 IEEE International \u2026", "abstract": "The success of fake news spreading on social media is to a large extent because of normal  users unknowingly parroting or sharing such content. Educational interventions such as"}, "filled": false, "gsrank": 337, "pub_url": "https://www.comp.nus.edu.sg/~chris/docs/publications/icmew20-ShareAwareFakeNews.pdf", "author_id": ["vWqDaykAAAAJ", "AdjTbBIAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:X6f4vgLVstYJ:scholar.google.com/&output=cite&scirp=336&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D330%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=X6f4vgLVstYJ&ei=QLWsaMqsGZXUieoPmrax2A8&json=", "num_citations": 14, "citedby_url": "/scholar?cites=15470661877741692767&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:X6f4vgLVstYJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://www.comp.nus.edu.sg/~chris/docs/publications/icmew20-ShareAwareFakeNews.pdf"}}]