[{"title": "The demand for news: Accuracy concerns versus belief confirmation motives", "year": "2022", "pdf_data": "Anand Chopra, Felix; Haaland, Ingar; Roth, Christopher\nWorking Paper\nThe demand for news: Accuracy concerns versus belief\nconfirmation motives\nECONtribute Discussion Paper, No. 157\nProvided in Cooperation with:\nReinhard Selten Institute (RSI), University of Bonn and University of Cologne\nSuggested Citation: Anand Chopra, Felix; Haaland, Ingar; Roth, Christopher (2022) : The demand\nfor news: Accuracy concerns versus belief confirmation motives, ECONtribute Discussion Paper,\nNo. 157, University of Bonn and University of Cologne, Reinhard Selten Institute (RSI), Bonn and\nCologne\nThis Version is available at:\nhttps://hdl.handle.net/10419/262101\nStandard-Nutzungsbedingungen:\nDie Dokumente auf EconStor d\u00fcrfen zu eigenen wissenschaftlichen\nZwecken und zum Privatgebrauch gespeichert und kopiert werden.\nSie d\u00fcrfen die Dokumente nicht f\u00fcr \u00f6ffentliche oder kommerzielle\nZwecke vervielf\u00e4ltigen, \u00f6ffentlich ausstellen, \u00f6ffentlich zug\u00e4nglich\nmachen, vertreiben oder anderweitig nutzen.\nSofern die Verfasser die Dokumente unter Open-Content-Lizenzen\n(insbesondere CC-Lizenzen) zur Verf\u00fcgung gestellt haben sollten,\ngelten abweichend von diesen Nutzungsbedingungen die in der dort\ngenannten Lizenz gew\u00e4hrten Nutzungsrechte.Terms of use:\nDocuments in EconStor may be saved and copied for your personal\nand scholarly purposes.\nYou are not to copy documents for public or commercial purposes, to\nexhibit the documents publicly, to make them publicly available on the\ninternet, or to distribute or otherwise use the documents in public.\nIf the documents have been made available under an Open Content\nLicence (especially Creative Commons Licences), you may exercise\nfurther usage rights as specified in the indicated licence.\nFunding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under\nGermany \u00b4sExcellence Strategy \u2013EXC 2126/1 \u2013390838866 is gratefully acknowledged .www.econtribute.deECON tribute\nDiscussion Paper No. 157\nJune 2022 (updated version)F\nelix Chopra Ingar Haaland Christopher RothThe Demand for News: Accuracy Concerns \nversus Belief Confirmation Motives\nThe Demand for News: Accuracy Concerns\nversus Belief Con\ufb01rmation Motives\nFelix Chopra Ingar Haaland Christopher Roth *\nJune 22, 2022\nAbstract\nWe examine the relative importance of accuracy concerns and belief con\ufb01rmation\nmotives in driving the demand for news. In experiments with US respondents, we\n\ufb01rst vary beliefs about whether an outlet reports the news in a right-wing biased,\nleft-wing biased, or unbiased way. We then measure demand for a newsletter\ncovering articles from this outlet. Respondents only reduce their demand for\nbiased news if the bias is inconsistent with their own political beliefs, suggesting a\ntrade-off between accuracy concerns and belief con\ufb01rmation motives. We quantify\nthis trade-off using a structural model and \ufb01nd a similar quantitative importance\nof both motives. ( JELD83, D91, L82)\nKeywords : News Demand, Media Bias, Accuracy Concerns, Belief Con\ufb01rmation\n*We are very grateful for very helpful discussions with Jesse Shapiro which shaped our thinking about\nthe experimental design. We also thank Peter Andre, Roland B\u00e9nabou, Stefano DellaVigna, Armin Falk,\nMatthew Gentzkow, Salvatore Nunnari, Simon Quinn, Gautam Rao, Erik S\u00f8rensen, Bertil Tungodden,\nBasit Zafar, Florian Zimmermann, and participants at the 2022 Belief Based Utility Conference in\nAmsterdam for very helpful comments. Maximilian Fell, Sophia Hornberger, Apoorv Kanongo and\nEmir Kavukcu provided excellent research assistance. IRB approval was obtained from the German\nAssociation for Experimental Economic Research (GfeW) and the ethics committee of the University of\nCologne. The experiments were pre-registered in the AsPredicted registry (#78800, #80266, #87947, and\n#89081). Financial support from the Russell Sage Foundation (Small Awards in Behavioral Economics),\nthe Research Council of Norway through its Centre of Excellence Scheme (FAIR project No 262675),\nthe Institute on Behavior and Inequality (briq), and the German Research Foundation (DFG) through\nCRC TR 224 (Project B03) is gratefully acknowledged. Roth acknowledges funding by the Deutsche\nForschungsgemeinschaft (DFG, German Research Foundation) under Germany\u2019s Excellence Strategy\n\u2013 EXC 2126/1-390838866. Chopra: University of Bonn, Felix.Chopra@uni-bonn.de; Haaland: NHH\nNorwegian School of Economics, Ingar.Haaland@nhh.no; Roth: University of Cologne and ECONtribute,\nroth@wiso.uni-koeln.de.\n1 Introduction\nMounting empirical evidence documents that news outlets often report the news in a\npolitically biased way (Gentzkow and Shapiro, 2010). Economic models differ in their\nexplanation for why media bias occurs in equilibrium. One class of models assumes\nthat readers value accuracy but also have a preference for news that distort signals\ntowards readers\u2019 prior beliefs (Mullainathan and Shleifer, 2005). A second class of\nmodels assumes that readers only value accuracy but instead face uncertainty about\nthe accuracy of news outlets (Gentzkow and Shapiro, 2006). This uncertainty leads\nreaders to attribute a higher accuracy to news outlets that provide signals that align with\nreaders\u2019 prior beliefs.\nThese two major theories thus make fundamentally different assumptions about\nthe relative importance of accuracy concerns versus belief con\ufb01rmation motives in\ndriving the demand for news. The relative importance of these two motives, in turn, has\nimportant implications for the optimal regulation of media markets, such as the welfare\neffects of regulations to increase competition. A major identi\ufb01cation challenge when\ntrying to quantify the relative importance of the two motives is that theories based on\nbelief con\ufb01rmation motives often make predictions that are observationally equivalent\nwith Bayesian updating about source quality (Gentzkow and Shapiro, 2006). This makes\nit challenging to quantify the relative importance of the two motives with naturally\noccurring data where beliefs about the media\u2019s reporting strategies are unobserved.\nTo solve the identi\ufb01cation challenge, we design experiments to directly vary beliefs\nabout the reporting strategy of a news outlet. We vary beliefs about whether a news\noutlet selectively reports the facts most favorable to either the Democratic Party (left-\nwing bias) or to the Republican Party (right-wing bias) or whether it reports all facts\nfrom an underlying report containing facts favorable to both parties (no bias). Since\nour respondents observe the full report available to the news outlet and the underlying\nsource of the report is \ufb01xed, the design allows our respondents to make direct inferences\nabout the outlet\u2019s reporting strategy. While theories based on accuracy concerns predict\nthat readers should decrease their demand for biased news irrespective of the direction\nof the bias, theories of belief con\ufb01rmation predict political heterogeneity based on the\ndirection of the bias.\nTo quantify the relative importance of accuracy concerns and belief con\ufb01rmation\nmotives in driving the demand for news, we conduct experiments with over 7,000\n1\nUS respondents using the online survey platform Proli\ufb01c. In our \ufb01rst experiment, we\nexperimentally vary beliefs about whether a news outlet is either right-wing biased\nor unbiased. To do so, we \ufb01rst tell our respondents that the Congressional Budget\nOf\ufb01ce (CBO), Congress\u2019s of\ufb01cial nonpartisan provider of cost and bene\ufb01t estimates for\nlegislation, published a report about the \u201cDemocrats\u2019 $15 Minimum Wage Bill\u201d (Raise\nthe Wage Act of 2021) in which it estimated that the plan would lift 900,000 people out\nof poverty (contradicting claims made by Republicans) and reduce employment by 1.4\nmillion jobs (contradicting claims made by Democrats). We next tell our respondents\nthat The Boston Herald wrote an article about the CBO \ufb01ndings.\nTo generate exogenous variation in perceptions of the reporting strategy, we use\nthe fact that The Boston Herald published two different articles about the bill: one\narticle published on February 26, 2021, that only cited the unemployment statistic, and\na second article published on March 2, 2021, that cited both statistics.1Our treatment\nvaries whether our respondents are informed about the reporting in the February 26\narticle that only cited the employment statistic ( right-wing bias treatment) while the\nremaining half of our respondents are informed about the reporting in the March 2\narticle that cited both statistics ( no bias treatment). We administer the treatments\nwithout referring explicitly to bias, selective reporting, or accuracy. To measure how\nthis treatment affects the demand for news, we offer all respondents the chance to sign\nup for a weekly newsletter that we created for the purpose of the experiment. The\nnewsletter features the top three articles about economic policy published in The Boston\nHerald and respondents who sign up for the newsletter receive weekly emails through\ntheir Proli\ufb01c account for one month. Our main outcome of interest is whether our\nrespondents sign up for the newsletter.\nOur second experiment uses an analogous design to shift beliefs about left-wing\nbias. We \ufb01rst inform our respondents that the CBO had published a report about\nthe \u201cRepublican Healthcare Plan\u201d (the American Health Care Act of 2017) in which\nit estimated that the plan would decrease the federal de\ufb01cit by over $100 billion\n(contradicting claims made by Democrats) and leave over 20 million more people\nuninsured (contradicting claims made by Republicans). We again exploit that The\nBoston Herald published two different articles that differed in their reporting: one\narticle about the Senate version of the bill that only cited the statistic on the number of\n1The Boston Herald is one of the oldest newspapers in the US and is based near Boston, MA. In 2020,\nits print edition had a circulation of about 25,000 and its reporting is considered slightly right-of-center.\n2\nuninsured, and one article about the House version of the bill that cited both statistics.\nThe key difference compared to the previous experiment relates to the direction of\nthe bias: half of our respondents are informed that The Boston Herald only cited the\nstatistic about the number of uninsured in its coverage of the Senate version of the plan\n(left-wing bias treatment) while the remaining half are informed that The Boston Herald\ncited both statistics in its coverage of the House version of the plan ( no bias treatment).\nIn our analysis of the results, we \ufb01rst con\ufb01rm that our treatments generate a signi\ufb01-\ncant \ufb01rst stage on perceptions of accuracy and political bias of the newsletter among\nboth Biden and Trump voters. In Experiment 1, both Biden and Trump voters in the\nright-wing bias treatment think that the newsletter has signi\ufb01cantly lower accuracy\nand is more right-wing biased compared to respondents in the no bias treatment. In\nExperiment 2, both Biden and Trump voters in the left-wing bias treatment think that the\nnewsletter has signi\ufb01cantly lower accuracy and is more left-wing biased compared to\nrespondents in the no bias treatment. The magnitudes of the \ufb01rst stage on accuracy and\nbias are economically signi\ufb01cant in both experiments. For instance, Biden and Trump\nvoters in the left-wing bias treatment think that the newsletter has between 54.2% to\n72% of a standard deviation lower accuracy than respondents in the no bias treatment.\nTurning to our main \ufb01ndings on newsletter demand, we \ufb01nd a striking political\nheterogeneity in treatment effects depending on the direction of the bias, consistent\nwith theories based on belief con\ufb01rmation motives. Speci\ufb01cally, the right-wing bias\ntreatment has a close to zero impact on newsletter demand among Trump voters. If\nanything, the right-wing bias treatment increases newsletter demand among Trump\nvoters by a non-signi\ufb01cant 0.5 percentage points (95% C.I. [-3.55,4.48]; p= 0.821). By\ncontrast, the left-wing bias treatment signi\ufb01cantly reduces newsletter demand among\nTrump voters by 5.2 percentage points (95% C.I. [-10.01,-0.41]; p= 0.033), corre-\nsponding to a 27.3% reduction in demand compared to the no bias group mean of\n19.1%. These patterns reverse for Biden voters who signi\ufb01cantly reduce their demand\nin response to the right-wing bias treatment by 8.6 percentage points (95% C.I. [-11.94,-\n5.33]; p< 0.001)\u2014corresponding to a 47.7% reduction in demand compared to the\nno bias group mean of 18.1%\u2014yet only reduce their demand by a non-signi\ufb01cant 2.6\npercentage points (95% C.I. [-6.37,1.17]; p= 0.176) in response to the left-wing biased\ntreatment. These asymmetric responses are consistent with readers having a preference\nfor belief con\ufb01rmation and inconsistent with models in which readers only care about\nthe accuracy of news. At the same time, we do not observe a signi\ufb01cant increase in\n3\nnews demand in any of the treatments, suggesting that readers also place some value on\nthe accuracy of news. Taken together, our results are thus in line with readers making a\ntrade-off between accuracy concerns and belief con\ufb01rmation motives.\nTo quantify the relative importance of accuracy concerns and belief con\ufb01rmation\nmotives in driving news demand, we use the experimental variation in conjunction with\na simple discrete-choice model. Intuitively, the model combines information about the\nrelative magnitude of the treatment effects on perceived accuracy and political bias with\ninformation about the magnitude of treatment effects on newsletter subscriptions to\nidentify the relative importance of the two motives. Our structural estimates suggest that\npreferences for belief con\ufb01rmation and accuracy concerns are of similar quantitative\nimportance for the demand for news in this context.\nTo shed more light on how our respondents interpreted our main treatment variation,\nwe conducted a separate mechanism experiment. In this experiment, we use open-ended\nquestions to elicit beliefs about the potential motives behind The Boston Herald\u2019s\nreporting of one statistic ( bias treatments) or the reporting of both statistics ( no bias\ntreatments) from the CBO reports. The unprompted responses reveal that respondents\nin the bias treatments have thoughts about political bias on top of their minds: 53.9%\nof respondents in the bias treatments mention political bias as the explanation for The\nBoston Herald selectively reporting only one statistic and no one mentions balanced\nreporting. By comparison, in the no bias treatments, 20.7% of respondents mention\nbalanced reporting and only 12.4% mention political bias. Our data also reveals that\nonly a very small fraction of respondents mention other potential motives underlying the\nselective reporting, such as entertainment, cognitive constraints, or rational delegation.\nThese results thus provide direct evidence that people intuitively interpret the action of\nselectively reporting only one statistic from the CBO reports as a clear sign of political\nbias and associate the action of reporting both statistics with balanced reporting. As\nsuch, this data supports the assumption from our structural model that our treatments\nmainly shifted beliefs about accuracy and bias.\nTo examine how people justify their demand for biased news, we collect direct data\non people\u2019s motives for subscribing to the newsletter at the end of the main experiments.\nTo get an unprompted response, we asked respondents to answer an open-ended question\non their motives for subscribing or not subscribing to the newsletter. Respondents in\ntheno bias treatments frequently mention getting accurate and unbiased news as a\nkey motive for signing up for the newsletter, while respondents in both of the bias\n4\ntreatments are signi\ufb01cantly less likely to mention such accuracy concerns and more\nlikely to provide a generic justi\ufb01cation, such as wanting to follow the news cycle or\ntheir interest in economic policy. These responses underscore that people do not invoke\njusti\ufb01cations that are consistent with alternative theories for why people consumed\nbiased news, such as diversi\ufb01cation or delegation motives. Rather, our \ufb01nding that\nrespondents in both of the bias treatments are signi\ufb01cantly less likely to mention\naccuracy concerns and more likely to provide generic justi\ufb01cations is consistent with\npeople providing rationales that allow them to maintain a positive self-image (Benabou\nand Tirole, 2006).\nOur results contribute to the literature on media bias (DellaVigna and Kaplan, 2007;\nDurante and Knight, 2012; Gentzkow et al., 2018; Mullainathan and Shleifer, 2005;\nPerego and Yuksel, 2022). To measure media bias, previous studies have developed\ntext-based measures that rank newspapers according to the similarity of their language\nto that of politicians (Gentzkow and Shapiro, 2006). For example, more frequent use of\nthe term \u201cdeath tax\u201d rather than \u201cestate tax\u201d might indicate a tendency to slant towards\nthe right. However, it is not obvious that one term conveys more information than the\nother. Thus, while previous studies suggest that readers have a demand for slanted\nlanguage (Garz et al., 2020; Gentzkow and Shapiro, 2010; Gentzkow et al., 2014), this\n\ufb01nding does not allow for strong conclusions about whether readers make a trade-off\nbetween accuracy concerns and belief con\ufb01rmation motives. Our main contribution is\nto provide direct evidence on the relative importance of accuracy concerns and belief\ncon\ufb01rmation motives in a clean and natural setting. The relative importance of these\nmotives plays a major role in theoretical analyses of media markets (Baron, 2006;\nChan and Suen, 2008; Gentzkow and Shapiro, 2006; Mullainathan and Shleifer, 2005)\nand is of critical importance for the debate on whether policymakers should introduce\nregulations to increase competition in media markets (Foros et al., 2015).\nWe also contribute to a literature on people\u2019s demand for information (Bursztyn et\nal., 2021; Capozza et al., 2021; Faia et al., 2021; Falk and Zimmermann, 2017; Fuster\net al., 2022; Ganguly and Tasoff, 2016; Montanari and Nunnari, 2019; Nielsen, 2020;\nZimmermann, 2015).2Chopra et al. (2022) examine how the demand for news changes\nin response to an added fact-checking service, demonstrating that fact-checking is not\nnecessarily an effective tool to reduce ideological segregation in news consumption.\n2More broadly our evidence relates to a literature on motivated belief updating (Exley, 2015;\nSchwardmann and van der Weele, 2019; Schwardmann et al., 2022; Di Tella et al., 2015; Thaler, 2019).\n5\nOur key contribution to the information demand literature is to identify the relative\nimportance of accuracy concerns and belief con\ufb01rmation motives in the news domain.\nTo differentiate between accuracy concerns and belief con\ufb01rmation motives, we employ\na new identi\ufb01cation strategy in which we vary beliefs about whether a news outlet\nreports the news in a right-wing biased, left-wing biased, or politically unbiased way.\nIn contrast to much of the previous experimental literature on information demand,\nwe vary perceptions of bias about a real-world news outlet rather than features of an\nabstract signal structure. Moreover, our main outcome provides high external validity\nby measuring subscriptions to a newsletter covering actual newspaper articles from a\nreal-world outlet.\nFinally, we contribute to a growing literature on structural behavioral economics\n(see DellaVigna, 2018, for a comprehensive review). Prior work has provided estimates\nof key behavioral parameters by combining parsimonious behavioral models with\nexperimentally-induced variation (Allcott and Taubinsky, 2015; Allcott et al., 2021;\nAugenblick et al., 2015; DellaVigna and Pope, 2018; DellaVigna et al., 2022). We use\nexogenous variation in perceptions of accuracy and bias in reporting to estimate the\nrelative importance of different motives in shaping people\u2019s demand for news using a\nparsimonious discrete choice model. Our estimates underline an important quantitative\nrole of both accuracy concerns and preferences for belief con\ufb01rmation in driving news\ndemand. An important bene\ufb01t of the structural estimation is that it provides greater\ncomparability with future studies that might try to quantify the relative importance of\naccuracy concerns compared to belief con\ufb01rmation motives in other settings.\nThe remainder of the paper proceeds as follows. Section 2 describes the exper-\nimental design. Section 3 presents both the reduced form results and the structural\nestimates. Section 4 presents evidence on psychological mechanisms and discusses al-\nternative mechanisms. Section 5 concludes. The Online Appendix provides a theoretical\nframework, additional empirical results, and the full set of experimental instructions.\n2 Experimental design\nOur study features two main experiments that examine how varying beliefs about the\naccuracy and political bias of a news outlet affect demand for a newsletter featuring\narticles from that outlet. Experiment 1 varies beliefs about whether a news outlet\n6\nselectively reports the facts most favorable to the Republican Party ( right-wing bias )\nwhile Experiment 2 varies beliefs about whether it selectively reports the facts most\nfavorable to the Democratic Party ( left-wing bias ). Figure 1 presents an overview of the\nmain design features and Section F of the Online Appendix presents the full instructions\nfor both experiments.\n[Insert Figure 1 here]\n2.1 Sample\nWe collected the data for our main experiments in collaboration with Proli\ufb01c, a leading\nmarket research company commonly used in social science research (Haaland et al.,\n2021). We collect data with Proli\ufb01c not only because of the high quality of responses\ncompared to other survey platforms (Eyal et al., 2021) but also because of the ability to\nemail respondents the newsletter via their Proli\ufb01c account without the need for collecting\nemail addresses. The data for our main experiments was collected in November and\nDecember 2021. We collected a sample of 1,464 Biden voters and 1,235 Trump voters\nfor Experiment 1 and 1,466 Biden voters and 849 Trump voters for Experiment 2.3\nOur samples are heterogeneous and resemble the US population in terms of several\nobservables (income, region, and gender; see Table B.1). In both experiments, the two\ntreatment groups are balanced in terms of observable characteristics in the full sample\n(Table B.2 and Table B.3).\n2.2 Experiment 1: Right-wing bias vs. no bias\nWe \ufb01rst describe the design of Experiment 1 in which we vary beliefs about whether a\nnews outlet selectively reports the facts most favorable to the Republican Party ( right-\nwing bias ) or reports facts favorable to both the Republican Party and the Democratic\nParty ( no bias ).\n3We aimed for gender-balanced samples of 1,500 Biden voters and 1,500 Trump voters in both\nexperiments. Respondents could only participate in one of the two experiments, making it especially\ndif\ufb01cult to recruit enough Trump voters in Experiment 2 (there are about six times as many Biden voters\nas Trump voters active on the Proli\ufb01c platform). In both experiments, the median time to complete the\nsurvey was about six minutes. We employed a simple attention check at the beginning of the survey,\nwhich over 95% of respondents pass, to screen out inattentive respondents.\n7\nBackground characteristics We \ufb01rst measure basic demographics, such as age,\ngender, education, income, and the region of residence. We then elicit whether our\nrespondents voted for Joe Biden or Donald Trump in the 2020 Presidential Election.4\nWe then measure their news consumption during the last 12 months, their interest in\neconomic news, and whether they currently subscribe to any newsletters.\nPre-treatment beliefs Subsequently, we elicit beliefs about how The Boston Herald\nreported about a CBO report containing facts favorable to both Democrats and Republi-\ncans. Speci\ufb01cally, we tell our respondents that the Congressional Budget Of\ufb01ce (CBO),\nCongress\u2019s of\ufb01cial nonpartisan provider of cost and bene\ufb01t estimates for legislation,\npublished a report about the \u201cDemocrats\u2019 $15 Minimum Wage Bill\u201d (Raise the Wage\nAct of 2021) in which it estimated that the plan would lift 900,000 people out of poverty\n(contradicting claims made by Republicans) and reduce employment by 1.4 million\njobs (contradicting claims made by Democrats).\nWe next tell our respondents that The Boston Herald wrote an article about the\neconomic impact of the $15 Minimum Wage Bill after the CBO published its report.\nWe then measure beliefs about how The Boston Herald covered the CBO \ufb01ndings by\nasking them to guess whether it only reported the statistic on the number of people lifted\nout of poverty (left-wing bias), only the statistic on the effects on reducing employment\n(right-wing bias), or both statistics (no political bias).\nBy making our respondents observe the full report available to the news outlet, our\ndesign allows our respondents to make direct inferences about its reporting strategy.\nWe chose to make the CBO the source of the underlying report for two reasons. First,\nthe CBO is known to be nonpartisan (to stay politically neutral, it only assesses the\nconsequences of proposed policies and does not make its own policy recommenda-\ntions). Second, all major newspapers in the US generally feature articles covering the\nCBO\u2019s evaluation of legislative proposals, making it a familiar and natural source for a\nnewspaper article.\n4When recruiting respondents on Proli\ufb01c, we pre-screen on having voted for either Donald Trump\nor Joe Biden. We ask about voting status in the survey to identify respondents who provide responses\ninconsistent with the screening criteria. Only a few respondents provided responses inconsistent with the\nscreening criteria, and we excluded these respondents from further analysis.\n8\nTreatments To generate exogenous variation in beliefs about selective reporting, we\nexploit the fact that The Boston Herald published two different articles about the $15\nMinimum Wage Bill: one article published on February 26, 2021, that only cited the\nunemployment statistic, and a second article published on March 2, 2021, that cited both\nstatistics.550% of our respondents are randomly assigned to learn about the selective\nreporting in the February 26 article that only mentioned the unemployment statistic\n(right-wing bias treatment). We frame the treatment information in a neutral way to\nminimize experimenter demand effects:\nThe article, published in The Boston Herald on February 26, 2021, reported\nthat the bill would reduce employment by 1.4 million jobs but not that it\nwould lift 900,000 people out of poverty.\nThe remaining 50% of respondents are assigned to learn about the balanced reporting\nin the March 2 article that reported both statistics ( no bias treatment):\nThe article, published in The Boston Herald on March 2, 2021, reported\nthat the bill would reduce employment by 1.4 million jobs andthat it would\nlift 900,000 people out of poverty.\nWe had two main reasons to select The Boston Herald as the news outlet for the\nexperiment. First, we wanted to feature a news outlet for which people had relatively\nweak priors compared to more popular news outlets, such as The New York Times or\nThe Wall Street Journal. Weaker priors about accuracy and political bias make beliefs\nabout the outlet\u2019s reporting strategy potentially more malleable to information about\npast reporting.\nSecond, we wanted an active control group design in which respondents would\nreceive different pieces of truthful information about how a news outlet covered the\nCBO \ufb01ndings. The Boston Herald was the only news outlet we identi\ufb01ed that had\nwritten multiple articles about the same CBO reports that also differed in whether or\nnot it selectively reported about the CBO \ufb01ndings. Active control group designs have\nseveral advantages compared to passive control group designs (Haaland et al., 2021).\n5See \u201cWho wins, who loses with higher minimum wage\u201d by Farren, Michael and Forzani, Agustin.\nThe Boston Herald , March 2, 2021, and \u201c$15 minimum wage hurts vulnerable workers the most\u201d by\nBuhajla, Stefani. The Boston Herald , February 26, 2021.\n9\nFirst, an active control group allows for a cleaner identi\ufb01cation of treatment effects\nbecause it holds more features of the environment constant compared to passive control\ngroup designs, such as respondents\u2019 attention and exposure to new information. In a\ndesign with a passive control group, respondents who do not learn about how the outlet\nreported about the CBO \ufb01ndings might be more curious to learn about the answer. That\nis, with a passive control group, curiosity motives could plausibly differ between the\ntreatment and control group, while these motives are less likely to differ in an active\ncontrol group design. Second, with an active control group, identi\ufb01cation does not\ndepend on people\u2019s prior beliefs, allowing us to identify causal effects of beliefs about\nselective reporting for a broader population. Furthermore, since prior beliefs are not\nexogenously assigned, interpretation of heterogeneous treatment effects is more dif\ufb01cult\nin designs with a passive control group.\nMain outcome measure: Newsletter demand After giving respondents differential\ninformation about whether The Boston Herald reported in a balanced or selective way\nabout the CBO \ufb01ndings, we measure demand for a weekly newsletter featuring stories\nfrom The Boston Herald:\nWe would like to offer you the opportunity to sign up for our weekly\nnewsletter.\nOur Weekly Economic Policy Newsletter will cover the top three arti-\nclesabout economic policy published in The Boston Herald .\nIf you say \u201cYes\u201d below, we will message you the newsletter on your Proli\ufb01c\naccount on a weekly basis over the next month.\nOur main outcome of interest is the binary decision to sign up for this newsletter.\nOur focus on newsletter subscriptions is motivated by the fact that newsletters are a\npopular way of staying informed about politics, with 21% of Americans receiving news\nfrom a newsletter over the course of a week (Newman et al., 2020). Moreover, by\nincluding only the three top articles in our newsletter, we reduce the expected cost of\nour respondents to stay up to date about economic policies\u2014both in terms of time costs\nand search efforts.\nOn the decision screen, we also clarify that the articles included in the newsletter can\nbe accessed for free by visiting The Boston Herald\u2019s website. To \ufb01x beliefs about the\n10\nresearchers\u2019 political leanings, we clarify that we are non-partisan academic researchers\nwho provide the newsletter as a free service for people to stay informed about the most\nimportant news related to economic policy. Finally, we explain that the newsletter is a\nnon-commercial product.\nIn practice, we sent the newsletter to our respondents on the Mondays of each\nof the four weeks after they decided to subscribe to the newsletter. A key advantage\nof conducting our experiment on Proli\ufb01c is that we can administer the newsletter to\nrespondents via direct messages on Proli\ufb01c without eliciting any personally identi\ufb01able\ninformation. Instead, respondents receive an email noti\ufb01cation when we message\nthem the newsletter. This, in turn, ensures that we can measure newsletter demand\nirrespective of privacy concerns. Appendix Section E provides information about the\nlogistical details and the newsletter\u2019s design.6\nPost-treatment beliefs about accuracy and political bias of the newsletter After\nchoosing whether to subscribe to the newsletter, we measure post-treatment beliefs\nabout the accuracy and political bias of the newsletter. We also elicit perceptions about\nthe trustworthiness, entertainment value, quality, and complexity of the newsletter. We\nmeasure these beliefs using \ufb01ve-point Likert scales.\n2.3 Experiment 2: Left-wing bias vs. no bias\nIn Experiment 2, we vary beliefs about whether a news outlet selectively reports the fact\nmost favorable to the Democratic Party ( left-wing bias ) or reports facts favorable to both\nthe Republican Party and the Democratic Party ( no bias ). The design of this experiment\nclosely resembles the design of Experiment 1, and most questions and outcomes are\nidentical across the two experiments. We highlight the key design differences below\n(see also Figure 1).\nPre-treatment beliefs We measure beliefs about how The Boston Herald reports\nabout the \u201cRepublican Health Care Plan\u201d (the American Health Care Act of 2017).\nRespondents are told that the CBO estimated that the plan would decrease the fed-\n6Each week we received a large number of thank you messages from respondents. A much smaller\nnumber of subscribers wrote to us that they would like to unsubscribe from the newsletter. Overall, this\nfeedback from subscribers illustrates both the bene\ufb01ts and costs of receiving the newsletter.\n11\neral de\ufb01cit by $119 billion (contradicting claims made by Democrats) and leave 23\nmillion more people uninsured (contradicting claims made by Republicans). 50% of\nrespondents are asked about their beliefs about the Senate version of the Republican\nHealthcare Plan, while the remaining 50% are asked about the House version of the\nRepublican Healthcare Plan.7This design choice is motivated by the fact that The\nBoston Herald reported different CBO statistics for these two versions of the Republican\nHealth Care Plan, as explained below.\nTreatments The Boston Herald published two articles about the Republican Health-\ncare Plan. In the article about the Senate version of the Republican Healthcare Plan,\nThe Boston Herald reported only that the plan would leave over 20 million more people\nuninsured ( left-wing bias treatment). In the other article about the House version of\nthe Republican Healthcare Plan, The Boston Herald reported both CBO statistics ( no\nbias treatment).8In our design, 50% of respondents are randomly assigned to learn\nabout the coverage of the article that only mentioned the consequences on the number\nof uninsured people ( left-wing bias treatment), which we again frame in a neutral way\nto minimize experimenter demand effects:\nThe Boston Herald article about the Senate Republican Healthcare Plan\nreported that the plan would leave over 20 million more people uninsured\nbut not that it would decrease the de\ufb01cit by over $100 billion.\nThe remaining 50% of respondents learn about the article that mentioned both statistics\n(no bias treatment):\nThe Boston Herald article about the House Republican Healthcare Plan\nreported that the plan would leave over 20 million more people uninsured\nandthat it would decrease the de\ufb01cit by over $100 billion.\nNewsletter and post-treatment beliefs We then employ the same main outcome\nvariable as in Experiment 1, namely the binary decision to subscribe to a newsletter\n7Prior beliefs about reporting are virtually identical for the Senate version and the House version of\nthe Republican Healthcare Plan (as shown in Figure B.1).\n8See \u201cCBO: 22 million more uninsured by 2026 under Senate health bill\u201d (Associated Press),\npublished in The Boston Herald , June 26, 2017, and \u201cCBO House GOP health bill projection: 23 million\nmore uninsured\u201d (Associated Press), published in The Boston Herald , May 24, 2017.\n12\nfeaturing the three top stories about economic policy from The Boston Herald. We also\nmeasure post-treatment beliefs about accuracy and political bias as well as other beliefs\nabout newsletter characteristics as in Experiment 1.\n2.4 Hypotheses\nOur design allows us to study whether and how people trade off the accuracy of news\nagainst the political bias in reporting by testing the predictions of three classes of models:\n(i) models where people only care about accuracy, (ii) models where people only care\nabout belief con\ufb01rmation, and (iii) models where both accuracy and belief con\ufb01rmation\nmotives shape the demand for news. To \ufb01x ideas, let Yg\nidenote the demand for news in\ntreatment arm i2fL;N;Rgand political group g2fB;Tg, where Brepresents Biden\nvoters, Trepresents Trump voters, and L,NandRdenote the left-wing bias ,no bias\nandright-wing bias treatment arm, respectively.\nFirst, we consider models where the demand for news only depends on the perceived\ninformativeness (e.g. Gentzkow and Shapiro, 2006). These models would predict that\nthe demand for news is strictly larger in the no bias treatment arm compared to the\nother treatment arms, i.e., Yg\nL<Yg\nNandYg\nR<Yg\nNfor both political groups g2fB;Tg(as\nshown in Section A of the Online Appendix). The intuition underlying this observation\nis that the right-wing and the left-wing bias treatment increase the perceived likelihood\nof selective reporting in a setting where full information disclosure would have been\npossible.9\nSecond, we turn to models where the demand for news is driven only by belief\ncon\ufb01rmation motives (as discussed in Loewenstein and Molnar, 2018). The predictions\nof such models depend on the political beliefs of our respondents. Speci\ufb01cally, models\nof belief con\ufb01rmation assume that Biden voters have a preference for reading left-wing\nbiased news, while Trump voters have a preference for reading right-wing biased news.\nIn short, demand should increase whenever the perceived political bias moves towards\nthe political belief of our respondents. We would thus expect YB\nL>YB\nN>YB\nRamong\nBiden voters, and the opposite pattern YT\nL<YT\nN<YT\nRamong Trump voters.\n9While it seems reasonable that reporting both statistics is normatively better than selectively\nreporting only one statistic in our context, it is important to emphasize that there is in general no\nnormative benchmark for how to select which facts to report when full disclosure is notpossible (e.g.,\nwhen a report includes many different statistics and a news outlet by necessity have to engage in selective\nreporting).\n13\nWe \ufb01nally turn to models in which people make a trade-off between accuracy and\nbelief con\ufb01rmation motives (e.g. Mullainathan and Shleifer, 2005). When the bias in\nreporting is not aligned with respondents\u2019 political views, we obtain the unambiguous\nprediction that YB\nR<YB\nNandYT\nL<YT\nNbecause there is no con\ufb02ict between accuracy and\nbelief con\ufb01rmation motives. However, such a con\ufb02ict arises whenever the alignment\nbetween respondents\u2019 political views and the perceived political bias in reporting\nincreases at the cost of lower accuracy in reporting. The sign of the overall effect on\nthe demand for news depends on (i) the relative importance of accuracy compared to\nbelief con\ufb01rmation motives, and (ii) the underlying magnitude of \ufb01rst-stage changes\nin perceptions of accuracy and bias in reporting. Without knowing these quantities,\nthe comparison between YB\nLandYB\nNand the comparison between YT\nRandYT\nNare thus\nambiguous. Note that if both motives are equally important drivers of the demand for\nnews, one would expect similar levels of demand in these cases: YB\nL\u0019YB\nNandYT\nR\u0019YT\nN.\nAppendix Table B.4 provides a summary of the predictions.\n3 Main results\nThis section presents our main results. We \ufb01rst present evidence on the \ufb01rst stage\nof the treatment on perceptions of accuracy and the political bias of the newsletter.\nWe then present the main treatment effects on demand for the newsletter. We \ufb01nally\nuse a discrete choice model to estimate the relative importance of accuracy concerns\ncompared to belief con\ufb01rmation motives.\n3.1 Beliefs about the accuracy and political bias of the newsletter\nTable 1 shows treatment effects on beliefs about the accuracy and political bias of the\nnewsletter separately for Trump voters (Panel A) and Biden voters (Panel B). Columns\n1 and 4 show that Trump voters in the right-wing bias treatment think that the newsletter\nhas 16.5% of a standard deviation lower accuracy ( p= 0.003) while Trump voters in\ntheleft-wing bias think that the newsletter has 54.2% of a standard deviation lower\naccuracy ( p< 0.001). We also observe treatment heterogeneity in accuracy perceptions\namong Biden voters: Biden voters in the right-wing bias treatment think that the\nnewsletter has 90.3% of a standard deviation lower accuracy ( p< 0.001) while Biden\n14\nvoters in the left-wing bias treatment think that the newsletter has 72% of a standard\ndeviation lower accuracy ( p< 0.001).10The political heterogeneity in treatment effects\non accuracy perceptions is consistent with the mechanism in Gentzkow and Shapiro\n(2006) and motivates our structural approach (outlined in Section 3.3) that accounts for\nheterogeneous treatment effects on perceptions.\nWe next examine treatment effects on perceptions of political bias. Column 2\nof Table 1 shows that Trump voters in the right-wing bias treatment think that the\nnewsletter has 49% of a standard deviation lower left-wing bias ( p< 0.001) while\nTrump voters in the left-wing bias treatment think that the newsletter has 26.6% of a\nstandard deviation higher left-wing bias ( p< 0.001). Biden voters in the right-wing\nbias treatment think that the newsletter has 84.9% of a standard deviation lower left-\nwing bias ( p< 0.001) while Biden voters in the left-wing bias treatment think that the\nnewsletter has 30.5% of a standard deviation higher left-wing bias ( p< 0.001).\nOur experiments thus generate situations in which perceptions of accuracy always\ndecrease but in which perceptions of political bias move in opposite directions. Experi-\nment 1 creates a potential con\ufb02ict between accuracy concerns and belief con\ufb01rmation\nmotives for Trump voters but not for Biden voters. Conversely, Experiment 2, creates a\npotential con\ufb02ict between accuracy concerns and belief con\ufb01rmation motives for Biden\nvoters but not for Trump voters. This exogenous variation in accuracy and political bias\nallows us to test for the presence of belief con\ufb01rmation motives in the demand for news.\n3.2 Reduced form results on newsletter demand\nColumns 3 and 6 of Table 1 present treatment effects on the demand for the newsletter in\nExperiment 1 and 2, respectively (Figure 2 displays these treatment effects graphically\nwithout control variables). As shown in Panel A of Table 1, we \ufb01nd no statistically\nsigni\ufb01cant effect of the right-wing bias treatment on newsletter demand among Trump\nvoters in Experiment 1. If anything, the treatment increases newsletter demand among\nTrump voters by 0.5 percentage points (95% C.I. [-3.55,4.48]; p= 0.821). However,\nwhile the point estimate is close to zero and not statistically signi\ufb01cant, the con\ufb01dence\n10Table B.5 shows that treatment effects on accuracy perceptions are robust to using conceptually\nrelated outcomes: both Biden and Trump voters assigned to the bias treatments display lower trust in the\nnewsletter and associate it with lower quality. On top of this, the \ufb01rst stage on accuracy perceptions looks\nvery similar if we construct an \u201caccuracy index\u201d combining the accuracy, quality, and trust outcomes.\n15\ninterval is consistent with economically signi\ufb01cant changes in demand in both directions.\nIn Experiment 2, the left-wing bias treatment signi\ufb01cantly reduces newsletter demand\namong Trump voters by 5.2 percentage points (95% C.I. [-10.01,-0.41]; p= 0.033),\ncorresponding to a 27.3% reduction in demand compared to the no bias group mean of\n19.1%.11\nThese patterns reverse for Biden voters. As shown in Panel B of Table 1, in contrast\nto the muted effects of the right-wing bias treatment among Trump voters, Biden voters\nsigni\ufb01cantly reduce their demand for the newsletter by 8.6 percentage points in response\nto the right-wing bias treatment (95% C.I. [-11.94,-5.33]; p< 0.001), corresponding to\na 47.7% reduction in demand compared to the no bias group mean of 18.1%. However,\nin response to the left-wing bias treatment, Biden voters only reduce their demand by a\nnon-signi\ufb01cant 2.6 percentage points (95% C.I. [-6.37,1.17]; p= 0.176).12\nThe political heterogeneity in treatment effects, in which our respondents only\nsigni\ufb01cantly reduce their demand for biased news if the bias is inconsistent with their\nown political beliefs, is inconsistent with models in which readers only care about the\naccuracy of news (as discussed in Appendix Section A). At the same time, that we do\nnot observe a signi\ufb01cant increase in demand for the newsletter in any of the treatments\nsuggests that our respondents also care about the accuracy of news. Taken together, our\nresults are thus in line with behavioral models where readers face a trade-off between\naccuracy concerns and belief con\ufb01rmation motives. Our \ufb01rst main result follows.\nResult 1. People strongly reduce their demand for biased news, but only if the political\nbias in reporting is inconsistent with their own political beliefs.\n[Insert Figure 2 here]\n[Insert Table 1 here]\n3.3 Structural estimates of preference parameters\nOur reduced form results suggest that people\u2019s demand for news are driven by both\naccuracy concerns and belief con\ufb01rmation motives, but they do not allow us to quantify\ntherelative importance of these motives. In this section, we \ufb01ll this gap by using the\nexogenous variation in perceptions of accuracy and bias induced by our treatments\n11Thep-value for a test of equality of treatment effects across experiments for Trump voters is 0.072.\n12Thep-value for a test of equality of treatment effects across experiments for Biden voters is 0.017.\n16\nto estimate a parsimonious discrete choice model. This structural model allows us to\ncombine the quantitative information on the effects of the treatments on both accuracy\nand bias perceptions alongside with our quantitative estimates on the effects on news\ndemand.\nDiscrete choice model Agent ihas to decide whether to subscribe to our newsletter\n(yi=1)or not ( yi=0). The agent will subscribe to the newsletter if his expected utility\nuifrom subscribing is positive, such that yi=1(ui\u00150). Following Mullainathan and\nShleifer (2005), we focus on the trade-off between accuracy and belief con\ufb01rmation,\nand thus assume that the agent\u2019s expected utility from subscribing to the newsletter\nconsists of a component capturing a preference for accuracy in reporting, a component\ncapturing a preference for belief con\ufb01rmation, and the price of the newsletter. As we\noffer the newsletter free of charge, the expected utility is\nui=\u00afu+asi+bbi+ei (1)\nwhere si=Ei(\u02dcsi)is the agent\u2019s subjective belief about the newsletter\u2019s accuracy; bi=\nEi(\u02dcbi)is the agent\u2019s subjective belief about how much the newsletter will con\ufb01rm his\nprior beliefs; and eiis a random taste shock. The parameters aandbcapture the agent\u2019s\nwillingness to trade off accuracy against belief con\ufb01rmation. As we elicit subjective\nbeliefs about accuracy and belief con\ufb01rmation in our experiment, we will directly\nsubstitute them for siandbiin our structural estimation of the above utility function.13\nEstimation and identi\ufb01cation We estimate the model parameters, q= (a;b;\u00afu),\nboth for the full sample as well as separately for Biden and Trump voters to explore\nheterogeneity in preferences. As proxies for siandbi, we use the z-scored post-treatment\nbelief measures of perceived accuracy and political bias in reporting.14In particular, we\nrecode the perceived political bias such that larger values correspond to a stronger left-\n13We are deliberately agnostic about the underlying information-theoretic decision problem giving\nrise to a potential preference for accuracy because revealed preferences in our experiment should be a\nfunction of respondents\u2019 subjective beliefs about the accuracy and expected belief con\ufb01rmation of the\nnewsletter. However, one possibility is that the agent has to learn about the state of the economy w, and\ntake a subsequent action aiwith a payoff v(ai;w) =\u0000a(ai\u0000w)2after reading the newsletter n(or not),\nwhich would give rise to a demand for accuracy in reporting.\n14We normalize these measures to have a mean of zero and a standard deviation of one among\nrespondents in the no bias treatment arms. The results are robust to using the non-z-scored beliefs.\n17\nwing (right-wing) bias for Biden (Trump) voters. Next, if perceptions of accuracy and\nbias were uncorrelated with the error term, one could simply use newsletter subscription\nchoices and the belief data from Experiments 1 and 2 to estimate the parameters q\nusing a Probit model. However, this exclusion restriction is unlikely to hold in practice\nwithout relying on an exogenous shifter. We therefore estimate an IV probit model as\noutlined by the set of equations below in which the binary dependent variable yiis the\ndecision to sign up to our newsletter:\nyi=1(\u00afu+asi+bbi+ei\u00150) (2)\nsi=Z0\nigs+e0\ni (3)\nbi=Z0\nigb+e00\ni (4)\nHere, we instrument respondents\u2019 perceptions with a saturated set of treatment arm\nindicators, Zi(see equations 3 and 4). We use Stata\u2019s ivprobit routine to estimate\nthe parameters of interest using the ef\ufb01cient estimator proposed by Newey (1987). In\nspeci\ufb01cations where we pool both Democrats and Republicans, the set of instruments,\nZi, also includes interactions between the treatment arm indicators and a binary indicator\nfor whether the respondent voted for Trump to account for heterogeneous \ufb01rst-stage\neffects on beliefs across political groups. We also include a binary indicator for whether\nthe respondent voted for Trump as a control variable to allow for differences in the\noutside option ( \u00af u) across political groups in the pooled speci\ufb01cation.\nThe main advantage of this estimation strategy is that we exploit only exogenous\nvariation in perceptions to disentangle people\u2019s accuracy and belief con\ufb01rmation mo-\ntives: While the bias treatments in both experiments decrease the perceived accuracy\nrelative to the no bias treatment, the right-wing bias treatment in Experiment 1 shifts\nthe perceived bias to the right, while the left-wing bias treatment in Experiment 2 shifts\nthe perceived bias to the left.\nDiscussion of assumptions First, we focus only on the accuracy-belief con\ufb01rmation\ntrade-off. While demand for our newsletter could also re\ufb02ect other motives\u2014such as\na demand for entertainment\u2014our estimation strategy remains valid if these motives\ndo not differentially affect demand across treatment arms.15Second, we assume\n15The open-ended data from the mechanism experiment (Experiment 3), which we present in Section\n4.2, suggests that the treatment indeed mostly sparked thoughts about bias and accuracy and furthermore\n18\nthat there is no internal saturation point in terms of the newsletter\u2019s political bias.\nAn alternative approach would be to assume that people receive disutility from the\ndifference between their preferred level of media bias, b\u0003, and the perceived bias of\nthe newsletter,\u0000bjb\u0000b\u0003j. This is equivalent to equation (1) if b\u0003\nD\u0014b\u0014b\u0003\nR, i.e.,\nwhenever the newsletter is perceived to be more centric than the preferred level of\nbias among Biden voters ( b\u0003\nD) and Republicans ( b\u0003\nR). In practice, we expect this to\nhold as our respondents perceive The Boston Herald as a relatively unbiased news\noutlet to begin with. Third, we assume that our survey measures of accuracy and bias\ncapture underlying perceptions well and are comparable to each other. We designed\nour survey measures to be as comparable as possible by eliciting them both on the\nsame type of scale with the same number of response options. In addition, we only use\nz-scored perceptions in our estimation to further ensure the comparability of our survey\nmeasures.\nResults Table 2 presents the parameter estimates of the discrete choice model. Con-\nsistent with the predictions of standard models, the estimates using the full sample\nsuggest a preference for accurate news ( p< 0.01, column 1). At the same time, the\nmodel estimates suggest that the demand for news is also driven by a preference for\nbelief con\ufb01rmation ( p< 0.01, column 1), which corroborates our reduced form re-\nsults. Indeed, our estimates imply a relative weight on accuracy of a=(a+b) =\n0:241=(0:241+0:345) =0:412, and we cannot reject the null hypothesis that respon-\ndents assign equal weights to accuracy and belief con\ufb01rmation ( p> 0.10). Columns 2\nand 3 of Table 2 examine heterogeneity in preferences between Biden and Trump voters.\nAmong Biden voters, we again \ufb01nd both a preference for accurate news ( p< 0.01) as\nwell as a preference for belief con\ufb01rmation ( p< 0.01, column 2). The estimates for\nTrump voters are qualitatively similar but more noisily estimated. If anything, we \ufb01nd\nthat Biden voters assign a smaller weight to accuracy compared to belief con\ufb01rmation\nmotives than Trump voters. However, we cannot reject the null hypothesis that both\ngroups care equally about accuracy and belief con\ufb01rmation ( p> 0.10). Our second\nmain result can thus be summarized as follows:\nResult 2. Both accuracy and belief con\ufb01rmation motives are important drivers of the\ndemand for news, and our model estimates suggest that people assign about equal\ndid not trigger many thoughts related to entertainment, cognitive constraints, or other features of news\narticles. This motivates an approach that focuses only on perceptions of accuracy and political bias.\n19\nweight to both motives in the context of our experiment.\n[Insert Table 2 here]\nRobustness We obtain similar structural estimates across a series of robustness checks.\nFirst, we obtain similar results when we re-estimate the model without z-scoring beliefs\nabout accuracy and belief con\ufb01rmation (as shown in Table B.6), which suggests that\nthis normalization procedure does not affect our estimates of the relative importance of\naccuracy concerns and belief con\ufb01rmation motives. Second, our results are robust to\nreplacing the z-scored accuracy belief measure with an index based on post-treatment\nbeliefs about accuracy, quality, and trustworthiness (as shown in Table B.7). Third, we\nalso estimate an analogous linear probability model using a two-stage least-squares\nestimator where we again use our treatment assignments as instruments (Angrist and\nKrueger, 1995; Inoue and Solon, 2010). Panel A of Table B.8 shows that we obtain\nquantitatively very similar estimates of the relative importance of accuracy compared to\nbelief con\ufb01rmation motives using a linear probability model. Thus, while the choice of\na linear versus a non-linear second stage model affects the scale of the coef\ufb01cients, the\nimplied relative magnitudes are quantitatively robust across speci\ufb01cations. Fourth, we\nmitigate concerns about consistency bias in survey responses affecting our structural\nestimates (Falk and Zimmermann, 2015). The results from a robustness exercise\naddressing this concern are presented in Panel B of Table B.8. Appendix Section C\nprovides more details about this concern and how we address it. Fifth, we examine\nwhether a preference for accuracy in combination with, (i), a preference for simplicity\nin reporting or, (ii), a preference for entertainment could also explain the treatment\neffects with plausible parameter values. To do so, we repeat our main estimation\nbut replace our measure of belief con\ufb01rmation with a measure of, (i), the perceived\nsimplicity (reverse-coded complexity belief) or, (ii), perceived entertainment value\nof the newsletter. Table B.9 provides the results from this exercise. The coef\ufb01cients\nare mostly statistically insigni\ufb01cant and unstable in sign across political subgroups.\nThis indicates that it is dif\ufb01cult to rationalise the patterns of treatment effects based on\npreferences for simplicity or entertainment on top of a preference for accuracy.\nTaken together, these \ufb01ve additional checks underscore the robustness of the main\n\ufb01ndings from the structural model.\n20\n4 Psychological mechanisms and robustness\nIn this section, we provide evidence on psychological mechanisms underlying our\ntreatment effects and discuss some potential alternative mechanisms.\n4.1 Motives for news demand\nOur experimental \ufb01ndings and our model-based preference estimates suggest that both\nBiden and Trump voters have a preference for reading like-minded news that sometimes\ncon\ufb02icts with their desire for accuracy in reporting. To examine how people justify\ntheir demand for news, we collect direct data on people\u2019s motives for subscribing to the\nnewsletter at the end of the main experiments. To get an unprompted response, we asked\nour respondents to answer an open-ended question on their motives for subscribing or\nnot subscribing to the newsletter (the full instructions are provided in Section F.1.4 of\nthe Online Appendix). This data provides us with a direct lens into people\u2019s reasoning\nabout the motives underlying their subscription decision.\nText analysis In a \ufb01rst step, we use the methodology proposed by Gentzkow and\nShapiro (2010) to identify phrases that are characteristic of responses to the open-ended\nquestions of subscribers and non-subscribers across treatment arms. Speci\ufb01cally, given\ntwo groups AandBof respondents, we calculate Pearson\u2019s c2statistic for each word w,\nc2\nwAB=(fwAf\u0018wB\u0000fwBf\u0018wA)2\n(fwA+fwB)(fwA+f\u0018wA)(fwB+f\u0018wB)(f\u0018wA+f\u0018wA)(5)\nwhere fwAandfwBdenote the total number of times that the word wwas mentioned by\nrespondents in group AandB, respectively. Similarly, f\u0018wAandf\u0018wBrefer to the total\nnumber of times words other than wwere mentioned. We then focus on the words with\nthe largest c2.\nFigure 3 presents the 50 words with the largest c2statistic for subscribers (Panel\nA) and non-subscribers (Panel B). Words that are more characteristic of justi\ufb01cations\nprovided by respondents in the left-wing orright-wing biased treatments are shown in\ngreen, while words more characteristic of respondents in the unbiased treatments are\nshown in red. Panel A reveals that words related to \u201cunbiased\u201d are more diagnostic for\nsubscribers in the no bias treatments, while subscribers in the bias treatments avoid\n21\nusing terms related to bias or accuracy. Panel B shows that these patterns are reversed for\nnon-subscribers: Non-subscribers in the bias treatments justify their non-subscription\nwith the political bias of the newsletter, while respondents in the no bias treatment arms\nagain bring up more generic reasons, such as wanting to follow the news cycle or their\ninterest in economic policy.\n[Insert Figure 3 here]\nFrequency of mentioning bias Motivated by the previous \ufb01ndings, we more closely\nexamine respondents\u2019 tendency to justify their decision by referring to the political\nbias of the newsletter. Table 3 presents OLS regression estimates pooling respondents\nfrom Experiments 1 and 2. The dependent variable in columns 1\u20133 is a binary indicator\ntaking value one if respondents mention the word \u201cunbiased\u201d or any of its synonyms\nin their responses to the open-ended question.16Subscribers who were assigned to\ntheleft-wing bias or the right-wing bias treatment arms are 4.1 percentage points less\nlikely to utilize synonyms of \u201cunbiased\u201d (column 1, p= 0.013), a substantial effect\ncompared to a no bias group mean of 7.8%. On the other hand, respondents in the\nbias treatments who did not subscribe to our newsletter are marginally more likely to\nmention synonyms of \u201cunbiased\u201d in their responses (column 2, p= 0.051). The opposite\npattern emerges once we consider synonyms of \u201cbiased\u201d and construct an analogous\ndependent variable taking value one if respondents utilized any of the following words:\n\u201cbiased\u201d, \u201cpartisan\u201d, \u201ctendentious\u201d, or \u201cslanted.\u201d Column 4 shows that subscribers are\nnot signi\ufb01cantly more likely to mention synonyms of \u201cbiased.\u201d Yet, non-subscribers\nare 4.4 percentage points more likely to mention terms related to \u201cbiased\u201d (column\n5,p< 0.001), which is a substantial effect compared to the no bias group mean of\n1.9%. Our data thus suggests that our treatments either affect the composition of\nrespondents selecting into the newsletter subscription, or that respondents \ufb02exibly\nadjust their rationales for subscription in response to our treatments (see Bursztyn et\nal., 2022a,b, for evidence on the role of rationales in justifying socially stigmatized\nbehavior). People\u2019s rationales for choosing to consume biased news do not actively\nfeature the political bias of the newsletter, consistent with people providing rationales\n16The synonyms were obtained from the website thesaurus.com and include: \u201cdisinterested\", \u201cdispas-\nsionate\", \u201cequitable\", \u201chonest\", \u201cimpartial\u201d, \u201cneutral\u201d, \u201cnonpartisan\u201d, \u201copen-minded\u201d, \u201caloof\u201d, \u201ccold\u201d,\n\u201cequal\u201d, \u201ceven-handed\u201d, \u201cfair\u201d, \u201cnondiscriminatory\u201d, \u201cobjective\u201d, \u201con-the-fence\", \u201cstraight\u201d, \u201cunbigoted\u201d,\n\u201cuncolored\u201d, \u201cuninterested\u201d, \u201cunprejudiced.\u201d\n22\nthat allow them to maintain a positive self-image (Benabou and Tirole, 2006).\n[Insert Table 3 here]\n4.2 Mechanism experiment: Interpretation of treatment\nTo shed light on the psychological mechanisms, we measure respondents\u2019 thoughts\nabout the motives behind different reporting decisions by the news outlet. For this pur-\npose, we conducted an additional pre-registered experiment on Proli\ufb01c (Experiment 3;\nsee Table B.10). The experiment was conducted in February 2022 with 388 respondents\n(240 Biden voters and 148 Trump voters).17\nDesign Half of the respondents are informed that the CBO evaluated the consequences\nof the \u201c$15 Minimum Wage Bill\u201d while the remaining half of the respondents are\ninformed that the CBO evaluated the consequences of the \u201cRepublican Healthcare\nPlan.\u201d We also tell our respondents about the competing claims made by Democrats\nand Republicans about the respective plans. We then randomly assign respondents to\nthe same bias andno bias treatments on the respective plans as described in sections\n2.2 and 2.3. We then elicit people\u2019s thoughts on why The Boston Herald reported only\none statistic (in the bias treatment) or both statistics (in the no bias treatment) using\nopen-ended text responses. To ensure high levels of effort, we ask our respondents to\nwrite two to three sentences. For example, respondents assigned to the $15 Minimum\nWage Bill and the right-wing bias treatment were asked:\nWhy do you think that The Boston Herald reported that the bill would\nreduce employment by 1.4 million jobs but not that it would lift 900,000\npeople out of poverty?\nRespondents assigned to the Republican Healthcare Plan received analogous instruc-\ntions tailored to that plan (see Section F of the Online Appendix for the instructions).\n17The median response time was four minutes and we excluded respondents from previous experi-\nments. We aimed for a politically balanced sample of Trump and Biden voters but we found it challenging\nto recruit enough Trump voters after excluding previous survey respondents from participation. As noted\npreviously, there are about six times as many Biden voters as Trump voters active on the Proli\ufb01c platform.\n23\nHand-coded data We hand-code the open-ended responses about the reporting strat-\negy using a pre-speci\ufb01ed procedure. We assign each response to one of the following\nthree categories: First, if respondents mention that the outlet was politically biased, we\nassign them to the \u201cbias\u201d category (for instance, the following example responses were\nclassi\ufb01ed as \u201cbiased\u201d: \u201cI think it\u2019s biased reporting,\u201d \u201cPerhaps they are a Republican\nnewspaper,\u201d \u201cI believe it is a left-leaning newspaper,\u201d or \u201cThey clearly support the\nDemocrats\u201d). Second, if respondents mention that the newspaper was trying to provide a\nbalanced view of the facts, we assign them to the \u201cbalanced\u201d category (for instance, the\nfollowing example responses were classi\ufb01ed as \u201cbalanced\u201d: \u201cThey were probably trying\nto report fairly without bias,\u201d \u201cThey were trying to give the full picture,\u201d and \u201cThey\ntried to report fairly and accurately\u201d would all be classi\ufb01ed). Third, all other responses\nare assigned the \u201cother\u201d category. In addition to the pre-speci\ufb01ed categories, we also\ncategorized responses that mentioned motives related to entertainment, complexity, or\nrational delegation.\nResults based on hand-coded data Figure 4 shows that respondents assigned to\nthebias treatments are 41.1 percentage points more likely to refer to political bias\n(p< 0.001) compared to a mean of 12.4% in the no bias treatments. Respondents\nassigned to the no bias treatment, on the other hand, are 20.1 percentage points more\nlikely to talk about balanced reporting ( p< 0.001) compared to a mean of 0% in\nthebias treatments. These effects are both statistically and economically signi\ufb01cant\nand highlight that respondents interpret the reporting decision to be either driven by\nmotives to deliver accurate or biased reporting.18Respondents\u2019 unprompted responses\nalso reveal that other perceived motives, such as rational delegation, entertainment, or\ncognitive constraints, only play a very minor role. Only four out of 388 respondents\nprovide responses consistent with rational delegation in which the newspaper selectively\nreports statistics considered more important by their readers. Another three respondents\nmention entertainment motives. Finally, two respondents thought the selective reporting\nwas motivated to reduce the complexity of the reporting. These \ufb01ndings thus corroborate\nthe idea that our experiment is well-suited to quantify the relative importance of accuracy\nconcerns and belief con\ufb01rmation motives in driving the demand for news.\n[Insert Figure 4 here]\n18We \ufb01nd consistent patterns for whether people mentioned balanced reporting in their open-ended\nresponses (as shown in Figure B.2).\n24\nText analysis As a complement to the hand-coded data, we also use a more unstruc-\ntured approach to analyze the text data. We use the methodology proposed by Gentzkow\nand Shapiro (2010), which we described in more detail in Section 4.1, to determine\nthe words that are most characteristic of being in the no bias or the bias treatment\narms. Figure 5 presents the 50 words that are most characteristic of responses by Biden\nvoters (Panel A) and by Trump voters (Panel B). We \ufb01nd that words related to \u201cbias\u201d\nare more characteristic of responses in the bias treatment arms, while words, such as\n\u201cnon-partisan\u201d, \u201cunbiased\u201d, \u201cfair\u201d, and \u201cfactual\u201d are more typical of responses in the no\nbias treatment arms. This corroborates the \ufb01ndings from the hand-coding exercise that\nour treatments seem to put thoughts about bias and accuracy on top of people\u2019s minds.\n[Insert Figure 5 here]\n4.3 Discussion of alternative mechanisms\nA key assumption for our structural model is that the treatments only affect people\u2019s\nnews demand through their impact on beliefs about accuracy and political bias. In\nthis section, we discuss potential alternative mechanisms driving our treatment effects,\nincluding cognitive constraints, cross-learning about entertainment, diversi\ufb01cation\nmotives, and experimenter demand effects.\nCognitive constraints Respondents in the no bias treatments might expect the ar-\nticles from The Boston Herald to be more cognitively demanding as these articles\nmay be more likely to cover more facts compared to respondents in any of the bias\ntreatments. Alternatively, respondents might associate the unbiased newsletter with\nhigher complexity if they think it is psychologically more costly to process and integrate\ncon\ufb02icting pieces of evidence.\nThe open-ended responses from Experiment 3 demonstrate that complexity was not\non top of people\u2019s minds when interpreting the treatment variation: Only two out of\n388 respondents thought The Boston Herald only reported one statistic to reduce the\ncomplexity of the article or to make it easier to understand. If we consider the structured\npost-treatment beliefs measures from the main experiments, there is some evidence\nthat Biden voters in the bias treatments associate the newsletter with lower complexity\n(as shown in Table B.11). However, since people did not talk about complexity in\n25\nthe open-ended responses, a likely explanation is that these respondents changed their\nbeliefs about the complexity of the newsletter only when prompted to think about it\nafter deciding whether to subscribe to the newsletter. Furthermore, several patterns\nin our data are inconsistent with cognitive constraints driving the treatment effects.\nFirst, explanations based on cognitive constraints predict a similarly sized decrease\nin demand irrespective of the direction of the political bias. As shown in columns 2\nand 6 of Table B.11, the magnitudes of treatment effects on perceptions of complexity\namong Biden voters are almost identical across the two experiments. Yet, inconsistent\nwith a story based on cognitive constraints, Biden voters only signi\ufb01cantly reduce their\ndemand for the newsletter in response to the right-wing bias treatment. Second, Trump\nvoters do not signi\ufb01cantly update their beliefs about the complexity of the newsletter\u2014\neven when prompted to think about it\u2014making it unlikely that cognitive constraints\ndifferentially affected newsletter demand across treatment groups.\nEntertainment motives It is conceivable that the treatments may affect perceptions\nof the newsletter\u2019s entertainment value. For instance, people might think that balanced\nreporting is less likely to lead to feelings of surprise and suspense (Ely et al., 2015).\nThe open-ended responses from Experiment 3 demonstrate that entertainment was not\non top of people\u2019s minds when interpreting the treatment variation: Only three out of\n388 respondents mentioned entertainment in their responses. Turning to the structured\npost-treatment beliefs measures, we \ufb01nd some evidence that respondents update about\nthe entertainment value of the newsletter (as shown in Table B.11). However, the\nlack of references to entertainment motives in Experiment 3 suggests that people\nonly adjusted their beliefs about entertainment ex-post when they were prompted to\nspeci\ufb01cally think about this dimension. Furthermore, the structured post-treatment\nbelief measures in Experiment 4 (see Appendix Section C) show that only Biden voters\nsigni\ufb01cantly updated their beliefs about the entertainment value of the newsletter when\nthere was less scope for ex-post rationalization of the newsletter subscription decision\n(Table B.13). Finally, conceptually disentangling belief utility and entertainment utility\nis not straightforward since part of the utility from belief con\ufb01rmation might relate to\nthe entertainment value of having your beliefs con\ufb01rmed. If biased news were perceived\nto be more entertaining in general, unrelated to any form of belief con\ufb01rmation utility,\nwe would not expect to see any political heterogeneity in treatment effects.\n26\nDiversi\ufb01cation motive People\u2019s news demand might be driven by the objective to\nread news articles from a diversi\ufb01ed portfolio of outlets with an average ideological bias\nthat is close to zero. Even if any individual outlet covers the news with a political bias,\ncombining the signals across sources might allow people to obtain a more objective\nassessment of the state of the world. Importantly, this motive hinges on people\u2019s news\nconsumption outside the experiment, but not on people\u2019s political views. To assess the\nplausibility of this mechanism, we asked respondents pre-treatment to indicate all news\noutlets from which they have received news over the past 12 months using a list of 21\npopular outlets across the political spectrum. We then classify each outlet as either\nleft-wing or right-wing biased, and then split the sample into respondents who, (i), do\nnot read news from any of these outlets, (ii), who read more left-wing than right-wing\nsources, and, (iii), those who read more right-wing than left-wing sources. We then\nseparately estimate treatment effects on people\u2019s newsletter demand in Experiment 1\nand 2 for each of these three subgroups (as shown in Table B.14).\nFirst, the diversi\ufb01cation motive would predict a positive treatment effect whenever\nthe perceived bias of The Boston Herald shifts away from the bias of the majority of\noutlets that a respondent currently reads. In contrast, column 2 shows a statistically\nsigni\ufb01cant decrease in demand among respondents who mainly read left-wing biased\noutlets in Experiment 1 where we increase the perceived right-wing bias of The Boston\nHerald. In the symmetric case in Experiment 2, we \ufb01nd a negative point estimate,\nalthough the small sample size limits the statistical power in this case (column 6).\nSecond, diversi\ufb01cation would predict a negative treatment effect among people who do\nnot read news from any other source, for which we only \ufb01nd mixed empirical support\n(columns 1 and 4). Taken together, this suggests that a diversi\ufb01cation motive alone is\ninsuf\ufb01cient to rationalize our patterns of treatment effects.\nExperimenter demand effects It is possible that respondents in the different treat-\nment groups hold different beliefs about the experimenter\u2019s expectations. However, we\ndo not believe that experimenter demand is a major concern in our setting. First, our\nexperimental manipulation is implicit in nature as we only factually state the newspaper\nreporting without framing it in terms of bias or accuracy. Second, the patterns of\nheterogeneity by political ideology and experiments suggest that our patterns could\nonly be explained by heterogeneously occurring demand effects. Third, trying to please\nthe experimenter by signing up for an unwanted newsletter is a costly action as it\n27\nentails receiving weekly emails with unwanted content for a month. Fourth, recent\nevidence suggests that experimental subjects respond only moderately to explicit signals\nabout the experimenter\u2019s expectations, indicating a limited quantitative importance of\nexperimenter demand effects (de Quidt et al., 2018; Mummolo and Peterson, 2018).\nTo further alleviate potential concerns about experimenter demand effects, we use\nalmost 5000 hand-coded responses based on participants\u2019 guesses about the study\u2019s\npurpose from an open-ended question, which we elicited in our main experiments.\nAs shown Appendix Figure B.3, only 4.1% of our respondents correctly guess the\nstudy\u2019s purpose (i.e., how perceptions of bias shape people\u2019s news consumption).\nMost participants guess that the purpose is related to understanding perceptions of\nbias (36.2%), measuring opinions and attitudes (16.9%), and studying political views\n(10.4%). A sizable fraction of respondents also express that they simply do not know\n(11.5%). As an additional robustness check, we re-run our main speci\ufb01cations for\nthe subsample of respondents who did not correctly guess the study purpose. This\nrobustness check shows that results are virtually unchanged for this subsample for\nwhich demand effects are particularly unlikely to confound treatment effects (Appendix\nTable B.15). Taken together, our hand-coded data on guesses of the study\u2019s purpose\nindicates that experimenter demand effects are unlikely to explain the treatment effects.\n5 Concluding remarks\nIn this paper, we conducted several large-scale experiments with American voters to\nquantify the relative importance of accuracy concerns and belief con\ufb01rmation motives\nin driving the demand for news. For this purpose, we designed experiments which\nvary whether a news outlet reports the news in a right-wing biased, left-wing biased,\nor politically unbiased way. We then study people\u2019s demand for a newsletter featuring\narticles from this outlet. Our main \ufb01nding is that both Biden and Trump voters strongly\nreduce their demand for politically biased news, but only if the bias is inconsistent with\ntheir own political views: Trump voters strongly reduce their demand for left-wing\nbiased news, but not for right-wing biased news. The reverse patterns hold for Biden\nvoters. The political heterogeneity is consistent with the predictions of behavioral\nmodels of news demand in which readers trade off accuracy concerns against belief\ncon\ufb01rmation motives. In a second step, we quantify the relative importance of accuracy\n28\nand belief con\ufb01rmation motives by using the experimental variation in perceptions\nof accuracy and political bias to estimate a parsimonious discrete-choice model. The\nestimates of the key preference parameters reveal that people attach about equal weights\nto accuracy and belief con\ufb01rmation motives, suggesting that both motives play a key\nrole in shaping news demand.\nThere are growing concerns that biased news contributes to increasing political\npolarization and the rise of populism (Guriev and Papaioannou, 2022; Sunstein, 2018).\nIt is thus important to understand why media bias occurs in equilibrium. Our \ufb01ndings\nimprove our understanding of the origins of media bias by providing direct experimental\nevidence on potential demand-side drivers: People value accuracy, but also demand\nnews that con\ufb01rms their existing beliefs. This result lends empirical support to demand-\nside explanations of media bias, such as behavioral models where \ufb01rms cater to people\u2019s\npreference for like-minded news by slanting their reporting towards the beliefs of their\nreaders. While other factors may also contribute to the origin of media bias, our \ufb01ndings\nsuggest that accounts that assume that consumers only value the accuracy of news are\nlikely to be incomplete.\nA preference for like-minded news has important implications for regulation and\nother efforts aimed at \ufb01ghting media bias and fake news. First, competition among\nmedia outlets should increase media bias in equilibrium if consumers have a demand for\nbiased news (Mullainathan and Shleifer, 2005). Regulatory efforts to increase the com-\npetitive pressure in media markets\u2014such as limiting ownership concentration\u2014may\nthus back\ufb01re. Second, a preference for biased news complicates the welfare analysis\nof efforts aimed at reducing media bias. Speci\ufb01cally, it creates a trade-off between\nsatisfying consumers\u2019 preference for like-minded news and mitigating the negative\npolitical externalities of media bias. Our \ufb01ndings thus demonstrate the complexity of\noptimal regulation.\nThis paper studies the demand for political news, where the relative importance of\naccuracy concerns and belief con\ufb01rmation motives is of particular interest as informed\ncitizens are a necessary input to the functioning of democratic institutions. However, it\nis plausible to expect that the relative preference for accuracy in reporting is stronger\nin news domains where the costs of being misinformed are primarily borne by the\nreader\u2014rather than arising in the form of a political externality. Future research should\nthus explore how people resolve the trade-off between accuracy concerns and belief\ncon\ufb01rmation in other domains, such as \ufb01nancial news.\n29\nReferences\nAllcott, Hunt and Dmitry Taubinsky , \u201cEvaluating behaviorally-motivated policy:\nExperimental evidence from the lightbulb market,\u201d The American Economics Review ,\n2015.\n, Matthew Gentzkow, and Lena Song , \u201cDigital addiction,\u201d Technical Report, Na-\ntional Bureau of Economic Research 2021.\nAngrist, Joshua D. and Alan B. Krueger , \u201cSplit-Sample Instrumental Variables Esti-\nmates of the Return to Schooling,\u201d Journal of Business & Economic Statistics , 1995,\n13(2), 225\u2013235.\nAugenblick, Ned, Muriel Niederle, and Charles Sprenger , \u201c Working over Time:\nDynamic Inconsistency in Real Effort Tasks,\u201d Quarterly Journal of Economics , 05\n2015, 130(3), 1067\u20131115.\nBaron, David P. , \u201cPersistent media bias,\u201d Journal of Public Economics , 2006, 90(1),\n1\u201336.\nBenabou, Roland and Jean Tirole , \u201cBelief in a Just World and Redistributive Politics,\u201d\nQuarterly Journal of Economics , 2006.\nBlackwell, David , \u201cComparison of Experiments,\u201d in Jerzy Neyman, ed., Proceedings\nof the Second Berkeley Symposium on Mathematical Statistics and Probability ,\nUniversity of California Press Berkeley, CA 1951, pp. 93\u2013102.\n, \u201cEquivalent Comparisons of Experiments,\u201d Annals of Mathematical Statistics , 1953,\npp. 265\u2013272.\nBursztyn, Leonardo, Aakaash Rao, Christopher P. Roth, and David H.\nYanagizawa-Drott , \u201cOpinions as Facts,\u201d Working Paper , 2021.\n, Georgy Egorov, Ingar Haaland, Aakaash Rao, and Christopher Roth , \u201cJusti-\nfying Dissent,\u201d University of Chicago, Becker Friedman Institute for Economics\nWorking Paper 2020-73 , 2022.\n,,,, and , \u201cScapegoating During Crises,\u201d American Economic Association\nPapers & Proceedings , 2022.\nCapozza, Francesco, Ingar Haaland, Christopher Roth, and Johannes Wohlfart ,\n\u201cStudying Information Acquisition in the Field: A Practical Guide and Review,\u201d\nDiscussion paper 124, ECONtribute 2021.\nChan, Jimmy and Wing Suen , \u201cA Spatial Theory of News Consumption and Electoral\nCompetition,\u201d Review of Economic Studies , 07 2008, 75(3), 699\u2013728.\n30\nChopra, Felix, Ingar Haaland, and Christopher Roth , \u201cDo People Demand Fact-\nchecked News? Evidence from U.S. Democrats,\u201d Journal of Public Economics , 2022,\n205, 104549.\nde Quidt, Jonathan, Johannes Haushofer, and Christopher Roth , \u201cMeasuring and\nBounding Experimenter Demand,\u201d American Economic Review , 2018, 108(11),\n3266\u20133302.\nDellaVigna, Stefano , \u201cStructural behavioral economics,\u201d in B. Douglas Bernheim,\nStefano DellaVigna, and David Laibson, eds., Handbook of Behavioral Economics ,\nV ol. 1 of Applications and Foundations 1 , North-Holland, 2018, pp. 613\u2013723.\nand Devin Pope , \u201cWhat Motivates Effort? Evidence and Expert Forecasts,\u201d Review\nof Economic Studies , 2018, 85(2), 1029\u20131069.\nand Ethan Kaplan , \u201cThe Fox News Effect: Media Bias and V oting,\u201d Quarterly\nJournal of Economics , 2007, 122(3), 1187\u20131234.\n, John A List, Ulrike Malmendier, and Gautam Rao , \u201cEstimating social prefer-\nences and gift exchange at work,\u201d American Economic Review , 2022.\nDurante, Ruben and Brian Knight , \u201cPartisan Control, Media Bias, and Viewer Re-\nsponses: Evidence from Berlusconi\u2019s Italy,\u201d Journal of the European Economic\nAssociation , 2012, 10(3), 451\u2013481.\nEly, Jeffrey, Alexander Frankel, and Emir Kamenica , \u201cSuspense and Surprise,\u201d\nJournal of Political Economy , 2015, 123(1), 215\u2013260.\nExley, Christine L. , \u201cExcusing Sel\ufb01shness in Charitable Giving: The Role of Risk,\u201d\nReview of Economic Studies , 2015.\nEyal, Peer, Rothschild David, Gordon Andrew, Evernden Zak, and Damer Ekate-\nrina, \u201cData quality of platforms and panels for online behavioral research,\u201d Behavior\nResearch Methods , 2021, pp. 1\u201320.\nFaia, Ester, Andreas Fuster, Vincenzo Pezone, and Basit Zafar , \u201cBiases in infor-\nmation selection and processing: Survey evidence from the pandemic,\u201d Technical\nReport, National Bureau of Economic Research 2021.\nFalk, Armin and Florian Zimmermann , \u201cConsistency as a Signal of Skills,\u201d Man-\nagement Science , 2015.\nand , \u201cBeliefs and Utility: Experimental Evidence on Preferences for Information,\u201d\nWorking Paper , 2017.\n31\nForos, \u00d8ystein, Hans Jarle Kind, and Lars S\u00f8rgard , \u201cMerger Policy and Regulation\nin Media Industries,\u201d in Simon P. Anderson, Joel Waldfogel, and David Str\u00f6mberg,\neds., Handbook of Media Economics , V ol. 1A of Handbook of Media Economics ,\nNorth-Holland, 2015, pp. 225\u2013264.\nFuster, Andreas, Ricardo Perez-Truglia, and Basit Zafar , \u201cExpectations with En-\ndogenous Information Acquisition: An Experimental Investigation,\u201d Review of Eco-\nnomics and Statistics , 2022.\nGanguly, Ananda and Joshua Tasoff , \u201cFantasy and Dread: The Demand for Informa-\ntion and the Consumption Utility of the Future,\u201d Management Science , 2016, 63(12),\n4037\u20134060.\nGarz, Marcel, Gaurav Sood, Daniel F. Stone, and Justin Wallace , \u201cThe supply of\nmedia slant across outlets and demand for slant within outlets: Evidence from US\npresidential campaign news,\u201d European Journal of Political Economy , 2020, 63,\n101877.\nGentzkow, Matthew A. and Jesse M. Shapiro , \u201cMedia Bias and Reputation,\u201d Journal\nof Political Economy , 2006, 114(2), 280\u2013316.\nand , \u201cWhat Drives Media Slant? Evidence From U.S. Daily Newspapers,\u201d\nEconometrica , 2010, 78(1), 35\u201371.\n, Michael B. Wong, and Allen T. Zhang , \u201cIdeological Bias and Trust in Information\nSources,\u201d Working Paper , 2018.\nGentzkow, Matthew, Jesse M. Shapiro, and Michael Sinkinson , \u201cCompetition and\nIdeological Diversity: Historical Evidence from US Newspapers,\u201d American Eco-\nnomic Review , 2014, 104(10), 3073\u20133114.\nGuriev, Sergei and Elias Papaioannou , \u201cThe political economy of populism,\u201d Journal\nof Economic Literature , 2022.\nHaaland, Ingar, Christopher Roth, and Johannes Wohlfart , \u201cDesigning Informa-\ntion Provision Experiments,\u201d Journal of Economic Literature , 2021.\nInoue, Atsushi and Gary Solon , \u201cTwo-Sample Instrumental Variables Estimators,\u201d\nReview of Economics and Statistics , 2010, 92(3), 557\u2013561.\nLoewenstein, George and Andras Molnar , \u201cThe renaissance of belief-based utility\nin economics,\u201d Nature Human Behaviour , 2018, 2(3), 166\u2013167.\nMontanari, Giovanni and Salvatore Nunnari , \u201cAudi alteram partem: An experiment\non selective exposure to information,\u201d Technical Report, Technical report, Working\nPaper 2019.\n32\nMullainathan, Sendhil and Andrei Shleifer , \u201cThe Market for News,\u201d American Eco-\nnomic Review , 2005, 95(4), 1031\u20131053.\nMummolo, Jonathan and Erik Peterson , \u201cDemand Effects in Survey Experiments:\nAn Empirical Assessment,\u201d American Political Science Review , 2018, 113(2), 517\u2013\n529.\nNewey, Whitney K. , \u201cEf\ufb01cient estimation of limited dependent variable models with\nendogenous explanatory variables,\u201d Journal of Econometrics , 1987, 36(3), 231\u2013250.\nNewman, Nic, Richard Fletcher, Anne Schulz, Simge Andi, and Rasmus Kleis\nNielsen , \u201cReuters Institute Digital News Report 2020,\u201d Technical Report, Reuters\nInstitute for the Study of Journalism 2020.\nNielsen, Kirby , \u201cPreferences for the resolution of uncertainty and the timing of infor-\nmation,\u201d Journal of Economic Theory , 2020, 189, 105090.\nPerego, Jacopo and Sevgi Yuksel , \u201cMedia Competition and Social Disagreement,\u201d\nEconometrica , 2022, 90(1), 223\u2013265.\nSchwardmann, Peter and Jo\u00ebl van der Weele , \u201cDeception and self-deception,\u201d Na-\nture Human Behavior , 2019, 3(10), 1055\u20131061.\n, Egon Tripodi, and Jo\u00ebl J Van der Weele , \u201cSelf-persuasion: Evidence from \ufb01eld\nexperiments at two international debating competitions,\u201d American Economic Review ,\n2022.\nSuen, Wing , \u201cThe Self-Perpetuation of Biased Beliefs,\u201d The Economic Journal , 2004,\n114(495), 377\u2013396.\nSunstein, Cass R ,# Republic: Divided Democracy in the Age of Social Media , Prince-\nton University Press, 2018.\nTella, Rafael Di, Ricardo Perez-Truglia, Andres Babino, and Mariano Sigman ,\n\u201cConveniently Upset: Avoiding Altruism by Distorting Beliefs About Others\u2019 Altru-\nism,\u201d American Economic Review , 2015, 105(11), 3416\u20133442.\nThaler, Michael , \u201cThe \u201cFake News\u201d Effect: An Experiment on Motivated Reasoning\nand Trust in News,\u201d Working Paper , 2019.\nZimmermann, Florian , \u201cClumped or Piecewise? Evidence on Preferences for Infor-\nmation,\u201d Management Science , 2015, 61(4), 740\u2013753.\n33\nMain \ufb01gures and tables\nFigure 1: Overview of the experimental design\n1/25/22, 2:58 PM Untitled Diagram\n1/1Pre-treatment beliefs about CBO reporting in the Boston\nHerald  \nExperiment 1 (right-wing bias) : $15 Minimum Wage Bill  \nExperiment 2 (left-wing bias) : Healthcare Plan  Consent form, attention check, background questions\nTreatment: Biased reporting \nExperiment 1 (right-wing bias) : The article, published in The \nBoston Herald on February 26 , 2021, reported that the bill would  \nreduce employment by 1.4 million jobs but not  that it would lift \n900,000 out of poverty  \nExperiment 2 (left-wing bias) : The Boston Herald article about  \nthe Senate Republican Healthcare Plan reported that the plan  \nwould leave over 20 million more people uninsured but not  that it \nwould decrease the deficit by over $100 billion.  Treatment: Unbiased reporting \nExperiment 1 (right-wing bias) : The article, published in The \nBoston Herald on March 2, 2021, reported that the bill would  \nreduce employment by 1.4 million jobs and that it would lift \n900,000 out of poverty   \nExperiment 2 (left-wing bias) : The Boston Herald article about  \nthe House Republican Healthcare Plan reported that the plan  \nwould leave over 20 million more people uninsured and that it \nwould decrease the deficit by over $100 billion.  \nNewsletter demand   \nOur Weekly Economic Policy Newsletter  will cover the top three \narticles about economic policy  published in The Boston Herald.\nIf you say \"Yes\" below, we will message you the newsletter on your  \nProlific account on a weekly basis over the next month.\nWould you like to subscribe to the newsletter?  \nPost-treatment beliefs about accuracy, trust, quality, and  \nother newsletter characteristics. \nOpen-ended motives for signing/not signing up for the  \nnewsletter. \nNote: This \ufb01gure provides an overview of the main design features of Experiment 1\n(right-wing bias) and Experiment 2 (left-wing bias). Appendix Section F contains the full\nexperimental instructions.\n34\nFigure 2: Newsletter demand by treatment and voting status\n(a) Biden voters: Right-wing bias\n p < 0.001\nn = 742 n = 727\n0.05.1.15.2Mean \u00b1 s.e.m.\nNo bias Right-wing bias (b) Trump voters: Right-wing bias\n p = 0.629\nn = 613 n = 623\n0.05.1.15.2Mean \u00b1 s.e.m.\nNo bias Right-wing bias\n(c) Biden voters: Left-wing bias\n p = 0.285\nn = 741 n = 728\n0.05.1.15.2Mean \u00b1 s.e.m.\nNo bias Left-wing bias (d) Trump voters: Left-wing bias\n p = 0.093\nn = 418 n = 432\n0.05.1.15.2Mean \u00b1 s.e.m.\nNo bias Left-wing bias\nNote: This \ufb01gure presents the share of respondents who chose to subscribe to the weekly\npolitics newsletter by treatment and voting status. Panel (a) and Panel (b) present results\nfrom Experiment 1. Panel (c) and Panel (d) present results from Experiment 2. Panel (a) and\nPanel (c) focus on the subsample of respondents who voted for Joe Biden, while Panel (b)\nand Panel (d) focus on respondents who voted for Donald Trump. The p-values are obtained\nfrom a two-sample t-test of equality of means. Standard errors of the mean are shown.\n35\nFigure 3: Motives for news demand: Most distinctive phrases\n(a) Distinctive phrases of subscribers by treatment status\ncompar\nview\nread\naffect\nattentcuriou\nnice\nbalancperspectreli\nunbiasfinanci\nbenefici\ncaoppos\ntrustinfo\npoliciherald nonbiassiteworld\nreceivboston qualitiopportundecid\nappear inbox\nworthaltern appeal\nborecatch\nencountentertain\nexamin\nexistfarm\nfutur\nhealthcarheard\ninclud judgmentleftistletter\nminimummonthorient\npay\n0.0000.0020.004Chi\u2212squared\n(b) Distinctive phrases of non-subscribers by treatment status\nbias\nreport\nemailstorihear\nprolifslant\nherald\nsitetruthleav\nbill\nqualitiinfo\nboston\nhalfplan\npostprovidbad\ndetailpick read\naccur\nmainstreamtold\ntypebase financi\nbegindistract\nfrankli know\nmagazin privacireadili\nwashingtonsourc left\ncommunfoundcboentir\nglass misinform agenda bia\njobworri familiar0.0000.0020.004Chi\u2212squared\nNote: This \ufb01gure uses respondents\u2019 answers to the open-ended question of why they\nsubscribed (or did not subscribe) to the newsletter from Experiment 1 and 2 (see Table B.10).\nPanel (a) uses responses to the open-ended questions from respondents who subscribed to\nthe newsletter on why they subscribed to the newsletter, while Panel (b) uses responses\nfrom respondents who did not subscribe on why they did not subscribe to the newsletter.\nEach panel displays the 50 phrases with the largest c2statistic using the method proposed\nby Gentzkow and Shapiro (2010). We exclude stop words and reduce all words to their\nstem using the Porter stemmer. Phrases with a positive c2statistic are more distinctive of\nopen-responses in the left-wing orright-wing biased treatment arms (in green). Phrases with\na negative c2statistic are more distinctive of responses in the unbiased treatment arm (in\nred).36\nFigure 4: Treatment effects on mentioning political bias in the open-ended responses\n(a) Biden voters: Right-wing bias\n p < 0.001\nn = 57 n = 57\n0.1.2.3.4.5.6.7Mean \u00b1 s.e.m.\nNo bias Right-wing bias (b) Trump voters: Right-wing bias\n p = 0.008\nn = 38 n = 42\n0.1.2.3.4.5.6.7Mean \u00b1 s.e.m.\nNo bias Right-wing bias\n(c) Biden voters: Left-wing bias\n p < 0.001\nn = 64 n = 62\n0.1.2.3.4.5.6.7Mean \u00b1 s.e.m.\nNo bias Left-wing bias (d) Trump voters: Left-wing bias\n p < 0.001\nn = 34 n = 34\n0.1.2.3.4.5.6.7Mean \u00b1 s.e.m.\nNo bias Left-wing bias\nNote: The \ufb01gure presents treatment effects on whether respondents mention political bias\nin their responses to the open-ended motives question in Experiment 3 (see Table B.10).\nSpeci\ufb01cally, respondents were asked why they think The Boston Herald reported in the way\nit did. Each panel displays the share of respondents whose responses were hand-coded to\nthe \u201cbias\u201d category (example responses: \u201cI think it\u2019s biased reporting,\u201d \u201cPerhaps they are a\nRepublican newspaper,\u201d \u201cI believe it is a left-leaning newspaper,\u201d or \u201cThey clearly support\nthe Democrats\u201d would all be classi\ufb01ed as \u201cbiased\u201d). Panel (a) and Panel (b) compare the\nright-wing bias treatment to the no bias treatment (analogous to Experiment 1). Panel (c)\nand Panel (d) compare the left-wing bias treatment to the no bias treatment (analogous to\nExperiment 2). Panel (a) and Panel (c) focus on the subsample of respondents who voted for\nJoe Biden, while Panel (b) and Panel (d) focus on respondents who voted for Donald Trump.\nThep-values are obtained from a two-sample t-test of equality of means. Standard errors of\nthe mean are shown.\n37\nFigure 5: Perceived motives for reporting: Most distinctive phrases\n(a) Biden voters\nestimfocu\nnonpartisanview\npublishbillleanbiasomit\nlive\nbusineg\nunbiasliberpriceleavreader\nagendadecreas\nfactualfairhappenexplainfeelhighlight\npotenti deficit paper\ndatafindaccur\ntrue minimumfoundafford attent directli list\nofficpoliciwordappeal approach caterchose\ndetaileditoripaint\nrun suspect\n\u22120.0050\u22120.00250.00000.0025Chi\u2212squared\n(b) Trump voters\ncost\nbias\nstatesupport\nlowerpositpoverti\npublishamountcreat\nhour raisreal\nliber\nappeal corpor\nhappenhealthsavesourcchosedifficult\ndueeffect\nfree\nintentlight\nservic neg\nbusijob\naccount\nassumattempt\ncongressioncurrentdata\nextrafairfewerhelp\nimprovincom\njournalistleft\nmiddl misrepresmultiploffic\noutlet\n\u22120.0020.0000.0020.004Chi\u2212squared\nNote: This \ufb01gure uses data from the mechanism experiment in which we measured perceived\nmotives for the reporting strategy of The Boston Herald using open-ended questions (Ex-\nperiment 3, see Table B.10). The \ufb01gure displays the 50 phrases with the largest c2statistic\nusing the method proposed by Gentzkow and Shapiro (2010). We exclude stop words and\nreduce all words to their stem using the Porter stemmer. Panel (a) uses responses to the\nopen-ended motives question from Biden voters, while Panel (b) uses responses from Trump\nvoters to calculate the c2statistics. Phrases with a positive c2statistic are more distinctive\nof responses in the biased treatment arms (in green). Phrases with a negative c2statistic are\nmore distinctive of responses in the unbiased treatment arm (in red). The terms \u201ccbo\u201d and\n\u201creport\u201d, which have c2values of\u00000:0126 and\u00000:0098 , were omitted to better scale the\nother phrases.\n38\nTable 1: Main results: The demand for biased news\nExperiment 1: Right-wing bias Experiment 2: Left-wing bias\n(1) (2) (3) (4) (5) (6)\nAccuracy Left-wing bias Demand Accuracy Left-wing bias Demand\nPanel A: Biden voters\nBias treatment (a) -0.903*** -0.849*** -0.086*** -0.720*** 0.305*** -0.026\n(0.057) (0.061) (0.017) (0.055) (0.059) (0.019)\nN 1,464 1,464 1,469 1,466 1,466 1,469\nZ-scored Yes Yes No Yes Yes No\nControls Yes Yes Yes Yes Yes Yes\nNo bias treatment mean 0 0 0.181 0 0 0.189\np-value: Ex. 1 = Ex. 2 0.026 0.000 0.017 0.026 0.000 0.017\nPanel B: Trump voters\nBias treatment (b) -0.165*** -0.490*** 0.005 -0.542*** 0.266*** -0.052**\n(0.056) (0.063) (0.020) (0.072) (0.072) (0.024)\nN 1,235 1,235 1,236 849 849 850\nZ-scored Yes Yes No Yes Yes No\nControls Yes Yes Yes Yes Yes Yes\nNo bias treatment mean 0 0 0.162 0 0 0.191\np-value: Ex. 1 = Ex. 2 0.000 0.000 0.072 0.000 0.000 0.072\np-value: a = b 0.000 0.005 0.001 0.073 0.947 0.395\nNote: This table presents OLS regression estimates using data from Experiment 1 (columns 1\u20133) and Experiment\n2 (columns 4\u20136) where the dependent variables are post-treatment beliefs about accuracy (columns 1 and 4), the\nperceived left-wing bias of the newsletter (columns 2 and 5), and newsletter demand (columns 3 and 6). Panel\nA and Panel B present results for Biden and Trump voters, respectively. \u201cBias treatment\u201d is a binary variable\ntaking value one for respondents assigned the right-wing bias (columns 1\u20133) or the left-wing bias (columns\n4\u20136) treatment arm, and zero for respondents in the no bias treatment arm. \u201cDemand\u201d is a binary variable\ntaking value one for respondents who said \u201cYes\u201d to receiving the weekly newsletter, and zero for those who\nsaid \u201cNo.\u201d \u201cAccuracy\u201d of the newsletter is measured on a 5-point Likert scale from \u201cVery inaccurate\u201d to \u201cVery\naccurate.\u201d \u201cLeft-wing bias\u201d is measured on a 5-point Likert scale from \u201cVery right-wing biased\u201d to \u201cVery left-\nwing biased.\u201d \u201cAccuracy\u201d and \u201cLeft-wing bias\u201d have been z-scored using the relevant no bias group mean and\nstandard deviation. \u201c p-value: Ex. 1 = Ex. 2\u201d provides p-values for tests of the equality of coef\ufb01cients between\nExperiment 1 and Experiment 2. \u201c p-value: a = b\u201d provides p-values for tests of the equality of coef\ufb01cients\nbetween Trump and Biden voters. All regressions include a set of basic control variables: gender, age, education,\nrace and ethnicity, log income, employment status, Census region, voting, political af\ufb01liation, ideology, interest\nin economic news, whether they have read any of a list of 21 newspapers during the last 12 months, whether\nthey have read The Boston Herald, whether they currently subscribe to any newsletters, and their pre-treatment\nbeliefs about how The Boston Herald reported about the CBO \ufb01ndings.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n39\nTable 2: Structural model: Preferences for accuracy and biased news\nParameter estimates:\n(1) (2) (3)\nFull sample Biden voters Trump voters\nPreference for accuracy ( a) 0.241*** 0.204** 0.266\n(0.076) (0.085) (0.190)\nPreference for belief con\ufb01rmation ( b) 0.345*** 0.374*** 0.190\n(0.081) (0.091) (0.160)\nImplicit weight on accuracy\u0010\na\na+b\u0011\n0.412*** 0.353*** 0.583**\n(0.111) (0.131) (0.270)\nN 5,014 2,930 2,084\nNote: This table presents the parameter estimates of the discrete choice model outlined in equations 2,\n3 and 4 in Section 3.3. Column 1 presents parameter estimates for the full sample, while columns 2\nand 3 present estimates for Biden and Trump voters, respectively. Speci\ufb01cally, we estimate an IV\nprobit model using Newey\u2019s (1987) two-step estimator as implemented by Stata\u2019s ivprobit routine.\nWe use data from Experiments 1 and 2 where we elicit newsletter subscription choices and percep-\ntions within-subject. The dependent variable is a binary indicator taking value one for respondents\nwho choose to sign up to the newsletter. The endogenous regressors are z-scored perceptions of\nquality and belief con\ufb01rmation. We instrument these perceptions with a saturated set of treatment\nstatus indicators. In column 1, we also include interactions of the treatment assignment with a binary\nindicator for whether a respondent voted for Trump as instruments to capture differential \ufb01rst-stage\neffects of the treatments. We include a binary indicator for whether a respondent voted for Trump as\na control variable in column 1.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n40\nTable 3: Motives for subscription vs. non-subscription to the newsletter\nMentions at least one synonym of:\nUnbiased Biased\n(1) (2) (3) (4) (5) (6)\nBiased -0.041** 0.009* 0.009* 0.002 0.044*** 0.044***\n(0.016) (0.005) (0.005) (0.015) (0.006) (0.006)\nNews demand 0.061*** 0.026**\n(0.013) (0.010)\nBiased x News demand -0.049*** -0.042**\n(0.017) (0.016)\nN 789 4,052 4,841 789 4,052 4,841\nSample Subscriber Non-subscriber All Subscriber Non-subscriber All\nNo bias treatment mean 0.078 0.017 0.028 0.046 0.019 0.024\nNote: This table presents OLS regression estimates pooling respondents from Experiment 1 and 2 where the depen-\ndent variables are binary indicators for whether respondents mentioned synonyms of \u201cunbiased\u201d (columns 1\u20133) or\n\u201cbiased\u201d (columns 4\u20136). Speci\ufb01cally, the dependent variable in columns 1\u20133 is a binary indicator taking value one\nif respondents mention the word \u201cunbiased\u201d or any of its synonyms in their open response to the question why\nthey subscribed (did not subscribe) to the newsletter. The synonyms are \u201cdisinterested\", \u201cdispassionate\", \u201cequi-\ntable\", \u201chonest\", \u201cimpartial\u201d, \u201cneutral\u201d, \u201cnonpartisan\u201d, \u201copen-minded\u201d, \u201caloof\u201d, \u201ccold\u201d, \u201cequal\u201d, \u201ceven-handed\u201d,\n\u201cfair\u201d, \u201cnondiscriminatory\u201d, \u201cobjective\u201d, \u201con-the-fence\", \u201cstraight\u201d, \u201cunbigoted\u201d, \u201cuncolored\u201d, \u201cuninterested\u201d,\n\u201cunprejudiced.\u201d Synonyms are taken from the website thesaurus.com. The dependent variable in columns 4\u20136 is\nconstructed analogously using \u201cbiased\u201d and any of the following synonyms: \u201cpartisan\u201d, \u201ctendentious\u201d, \u201cslanted.\u201d\n\u201cBiased\u201d is a binary indicator taking value one for respondents assigned to the \u201cleft-wing bias\u201d or the \u201cright-wing\nbias\u201d treatment arms, and zero otherwise. \u201cNews demand\u201d is a binary variable taking value one for respondents\nwho said \u201cYes\u201d to receiving the weekly newsletter, and zero for those who said \u201cNo.\u201d Columns 1 and 4 focus on\nthe subsample respondents who subscribed to the newsletter, while columns 2 and 5 focus on those who did not\nsubscribe. Columns 3 and 6 include all respondents. All regressions include experiment \ufb01xed effects.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n41\nFor online publication only:\nThe Demand for News: Accuracy Concerns versus\nBelief Con\ufb01rmation Motives\nFelix Chopra, Ingar Haaland, and Christopher Roth\nSection A presents theoretical results.\nSection B contains additional tables and \ufb01gures.\nSection D contains information about our preregistration and the ethics approval.\nSection E contains additional details about the publication and distribution of our\nweekly newsletter.\nSection F provides screenshots of the experimental instructions.\n1\nA Theoretical appendix\nThis section formalizes the intuition that our active control designs in Experiment 1 and\n2 (see Section 2) should decrease the perceived Blackwell informativeness of Boston\nHerald articles in the narrow context of reporting about CBO \ufb01ndings. Proposition\n1 below outlines suf\ufb01cient conditions for the left-wing biased andright-wing biased\ntreatment to strictly decrease the perceived Blackwell informativeness compared to the\nrespective no bias treatment. As a result, this provides us with the empirical prediction\nthat for neoclassical agents that care only about the accuracy of news reporting, our\ntreatments should decrease newsletter demand.\nWhile it seems intuitively reasonable that reporting both statistics is more infor-\nmative than selectively reporting only one statistic in our context, it is important to\nemphasize that there is in general no normative benchmark on the reporting of facts\nwhen full disclosure is notpossible. For example, suppose that a newsletter receives\nthree signals, (s1;s2;s3) = ( L;R;R), about an unobserved state q, but can only report\none signal. From the reader\u2019s perspective, the optimal reporting rule will depend on the\nprior beliefs and the cost of making a Type I and Type II error when conditioning actions\non one\u2019s belief about q(a point made by Suen 2004). Thus, readers with different priors\nprefer different reporting rules, making it not possible to de\ufb01ne a complete ordering of\nreporting rules in terms of their informativeness.\nWe therefore chose to focus on a setting where it seems ex-ante very likely that news\noutlets are not constrained in whether they report only one or both of the main \ufb01ndings\nfrom the CBO reports.1Thus, when evaluating the reporting of The Boston Herald\nonly in the narrow sense of how it covers CBO reports, an increase in the probability of\nreporting both statistics necessarily increases its informativeness in the Blackwell sense.\nBelow, we outline the formal argument.\nSetup There is a binary state space Q=fL;Rgwith a typical element denoted by\nqand an agent with prior belief q2D(Q)about the hidden state. The agent has\nthe option to acquire information from a news outlet (The Boston Herald), which\n1For example, we veri\ufb01ed that all top 15 US newspapers by circulation (as of June 2019) reported\nboth \ufb01ndings from the CBO report about the Healthcare Plan, suggesting that news outlets do not face\nbinding constraints that would require them to choose between reporting either the effects on the de\ufb01cit\nor the effects on the number of uninsured.\n2\npublishes a newsletter nthat is informative about the state q. To introduce scope for\ninformation suppression, we assume that the news outlet receives a set of private signals\ns=fs1;:::; sKg2Sabout q. The set consists of Kbinary signals si2Q, where K2N\nis drawn randomly and independently of q. The signals, si, take value Lwith probability\npqwhere pR<pL, and value Rotherwise. The news outlet can disclose any subset of\nsin its newsletter n, i.e. n\u0012s. Note that this implies that it cannot distort individual\nsignals but only choose to suppress a subset of signals. In our experiments, The Boston\nHerald received two con\ufb02icting signals from the Congressional Budget Of\ufb01ce about the\nconsequences of the $15 Minimum Wage Bill (Experiment 1) or the consequences of\nthe healthcare plan (Experiment 2), i.e. s=fL;Rgin both experiments.\nInformativeness The source signal can thus be represented as an information struc-\nture (S;p)with state-dependent likelihood p:Q!D(S). We are agnostic about the\nnews outlet\u2019s incentives to suppress information, subsuming them in the reader\u2019s belief\nr:S!D(N)about how the news outlet reports conditional on s. From the agent\u2019s\nperspective, the informativeness of nis an invariant of the state-dependent distribution\nover news articles, s:Q!D(N), induced by the agent\u2019s belief about the quality of the\nnews outlet\u2019s source, p, and the belief about how the news outlet reports, r. Consider\ntwo articles nandn0with distributions s;s0:Q!D(N). We use Blackwell\u2019s (1951)\nnotion of informativeness and say that nis(Blackwell) more informative than n0if(n;s)\nissuf\ufb01cient for(n0;s0), that is: there is a stochastic transformation tsuch that n0and\nt(n)are identically distributed. Intuitively, we obtain n0by adding noise to n. This\nis the benchmark for evaluating the informativeness of an information structure: any\nagent with access to an article nthat is more informative than n0can attain an expected\npayoff at least as large as the maximal expected payoff attainable with n0, regardless\nof the prior qand the decision problem a2Awith payoffs u(a;q)(Blackwell, 1953).\nThis provides the prediction that the demand for news should be strictly increasing in\nthe perceived informativeness of the news.\nHow does strategic suppression of signals affect the informativeness of news?\nSuppose the news outlet received the signals s=fs1;:::; sKgand let s(s0js)denote the\nagents\u2019 belief that the news outlet would report s0\u0012safter receiving s. Intuitively, the\ninformativeness of the article nshould be strictly increasing in the probability of fully\nconveying the set of signals. Indeed, the Blackwell informativeness strictly increases\nif we decrease the probability s(s0js)of reporting a \ufb01ltered signal s0(sand instead\n3\nincrease the probability of full information transmission, s(sjs).\nProposition 1 (Informativeness) .Fixs=fs1;:::; sKg2Sand two reporting strategies\nr;r0:S!D(N). Let s;s0:Q!D(N)be the information structures induced by\ncombining the source signal p:Q!D(S)with the reporting strategies, respectively.\nSuppose that\n(i)r(sjs)\u0015r0(sjs),\n(ii)r(tjs)\u0014r0(tjs)for all t(s,\n(iii)r(\u0001js0) =r0(\u0001js0)for all s06=s.\nThen, the information structure sis Blackwell more informative than s0.\nProof. It suf\ufb01ces to show that the conclusion obtains if we strengthen the assumption by\nadditionally assuming that r(tjs)<r0(tjs)for some t(sand that for all other t0(s\nwith t06=t, we have r(t0js) =r0(t0js). The general case then follows by applying the\nresult to the sequence r=r1;:::;rL=r0where rkandrk+1differ at most on the set\nfs;s0gfor some s0\u0012sandL=jP(s)j. Suppose that n2Nis a random variable with\nstate-dependent distribution s. To show that sis Blackwell more informative than s0,\nit suf\ufb01ces to construct an n-measurable random variable n02Nwith state-dependent\ndistribution s0, thereby establishing statistical suf\ufb01ciency. We construct n0as follows:\nletn0=nwhenever n6=sand set b=r0(sjs)=r(sjs). Ifn=s, then n0takes value\nswith probability band value twith probability 1\u0000b. One can then verify that\nconditional on the state q2Q, the distribution of n0iss0(\u0001jq). This concludes the\nproof.\nIn our active no bias group designs, the right-wing bias and the left-wing bias\ntreatment exogenously decrease the probability r(sjs)of reporting both statistics from\nthe CBO report compared to the no bias treatment, while increasing the probability of\nselective reporting. By Proposition 1, this means that respondents in the right-wing\nbias and the left-wing bias should perceive the newsletter as strictly less informative\ncompared to respondents in the no bias treatment.\n4\nB Additional tables and \ufb01gures\nTable B.1: Summary statistics\n(1)\nUS pop.(2)\nExp 1(3)\nExp 2(4)\nExp 3(5)\nExp 4\nMale 0.492 0.468 0.436 0.479 0.481\nAge (years) 47.78 35.487 36.304 35.737 38.829\nWhite 0.763 0.834 0.840 0.827 0.821\nEmployed 0.620 0.681 0.724 0.724 0.715\nCollege 0.329 0.649 0.678 0.683 0.695\nHigh income 0.482 0.443 0.429 0.461 0.446\nNortheast 0.17 0.174 0.194 0.157 0.189\nMidwest 0.21 0.231 0.235 0.206 0.204\nSouth 0.38 0.389 0.398 0.412 0.396\nWest 0.24 0.206 0.173 0.224 0.211\nV ote Trump 0.469 0.457 0.367 0.381 0.493\nObservations 2,705 2,319 388 1,910\nNote: This table displays the mean value of basic covariates for the US population (column 1) as well\nas for each experiment (see Table B.10 for an overview of the experiments). We obtained population\ndata from the 2019 American Community Survey and the U.S. Census Bureau \u201cQuickFacts\u201d tool.\n\u201cMale\u201d is a binary variable taking value one for male respondents, and zero otherwise. \u201cAge\u201d is the\nnumerical age of the respondent in years. \u201cWhite\u201d is a binary variable taking value one if the re-\nspondent selected \u201cCaucasian/White,\u201d and zero otherwise. \u201cEmployed\u201d is a dummy variable taking\nvalue one if the respondent is employed full-time, part-time, or self-employed. \u201cHigh income\u201d is\na binary variable taking value one if the respondent has pre-tax household annual income above\n$75,000. \u201cCollege degree\u201d is a binary variable taking value one if the respondent has at least a bach-\nelor\u2019s degree. \u201cNortheast,\u201d \u201cMidwest,\u201d \u201cWest\u201d and \u201cSouth\u201d are binary variables with value one if the\nrespondent lives in the respective region, and zero otherwise. \u201cV oted for Trump\u201d is a binary variable\ntaking value one if the respondent voted for Donald Trump in the 2020 US presidential election, and\nzero if the respondent voted for Joe Biden.\n5\nTable B.2: Test of balance of treatment vs. control: Experiment 1\nBias (B) No bias (NB) P-value (B - NB) Observations\nMale 0.47 0.47 0.949 2705\nAge 35.80 35.18 0.239 2705\nWhite 0.84 0.83 0.789 2705\nIncome (midpoint) 72051.85 69667.90 0.169 2705\nCollege degree 0.65 0.65 0.817 2705\nFull-time employee 0.49 0.49 0.953 2705\nNortheast 0.17 0.18 0.795 2705\nMidwest 0.23 0.24 0.589 2705\nWest 0.21 0.20 0.774 2705\nSouth 0.39 0.39 0.666 2705\nNote: This table provides a balance test between the treatment and no bias group\nusing all respondents from Experiment 1 (see Table B.10 for an overview of the ex-\nperiments). \u201cMale\u201d is a binary variable taking value one for male respondents, and\nzero otherwise. \u201cAge\u201d is the numerical age of the respondent in years. \u201cWhite\u201d is a\nbinary variable taking value one if the respondent selected \u201cCaucasian/White,\u201d and\nzero otherwise. \u201cIncome (midpoint)\u201d is coded continuously as the income bracket\u2019s\nmidpoint (Less than $15,000, $15,000 to $24,999, $25,000 to $49,999, $50,000\nto $74,999, $75,000 to $99,999, $100,000 to $149,999, $150,000 to $200,000,\n$200,000 or more). \u201cCollege degree\u201d is a binary variable taking value one if the\nrespondent has a college degree, and zero otherwise. \u201cFull-time employee\u201d is a\nbinary variable taking value one if the respondent is a full-time employee, and zero\notherwise. \u201cNortheast,\u201d \u201cMidwest,\u201d \u201cWest\u201d and \u201cSouth\u201d are binary variables with\nvalue one if the respondent lives in the respective region, and zero otherwise.\n6\nTable B.3: Test of balance of treatment vs. control: Experiment 2\nBias (B) No bias (NB) P-value (B - NB) Observations\nMale 0.41 0.46 0.018 2319\nAge 36.24 36.36 0.837 2319\nWhite 0.85 0.83 0.456 2319\nIncome (midpoint) 68377.16 69221.31 0.643 2319\nCollege degree 0.67 0.68 0.514 2319\nFull-time employee 0.55 0.52 0.253 2319\nNortheast 0.20 0.19 0.682 2319\nMidwest 0.24 0.23 0.816 2319\nWest 0.17 0.18 0.776 2319\nSouth 0.39 0.40 0.754 2319\nNote: This table provides a balance test between the treatment and no bias group\nusing all respondents from Experiment 2 (see Table B.10 for an overview of the ex-\nperiments). \u201cMale\u201d is a binary variable taking value one for male respondents, and\nzero otherwise. \u201cAge\u201d is the numerical age of the respondent in years. \u201cWhite\u201d is a\nbinary variable taking value one if the respondent selected \u201cCaucasian/White,\u201d and\nzero otherwise. \u201cIncome (midpoint)\u201d is coded continuously as the income bracket\u2019s\nmidpoint (Less than $15,000, $15,000 to $24,999, $25,000 to $49,999, $50,000\nto $74,999, $75,000 to $99,999, $100,000 to $149,999, $150,000 to $200,000,\n$200,000 or more). \u201cCollege degree\u201d is a binary variable taking value one if the\nrespondent has a college degree, and zero otherwise. \u201cFull-time employee\u201d is a\nbinary variable taking value one if the respondent is a full-time employee, and zero\notherwise. \u201cNortheast,\u201d \u201cMidwest,\u201d \u201cWest\u201d and \u201cSouth\u201d are binary variables with\nvalue one if the respondent lives in the respective region, and zero otherwise.\n7\nTable B.4: Predictions of different models of the demand for news\nExperiment 1: Right-wing bias Experiment 2: Left-wing bias\n(1) (2)\nPanel A : Accuracy\nBiden voters YB\nR<YB\nN YB\nL<YB\nN\nTrump voters YT\nR<YT\nN YT\nL<YT\nN\nPanel B : Belief con\ufb01rmation\nBiden voters YB\nR<YB\nN YB\nL>YB\nN\nTrump voters YT\nR>YT\nN YT\nL<YT\nN\nPanel C : Both motives\nBiden voters YB\nR<YB\nN ambiguous\nTrump voters ambiguous YT\nL<YT\nN\nNote: This table summarizes the predictions outlined in Section 2.4. Panel A summarizes the\npredictions of models where people only care about accuracy. Panel B summarize the predic-\ntions of models where people only care about belief con\ufb01rmation. Panel C summarizes the\npredictions of models where both accuracy and belief con\ufb01rmation motives shape the demand\nfor news. Yg\nidenote the demand for news in treatment arm i2fL;N;Rgand political group\ng2fB;Tg, where Brepresents Biden voters, Trepresents Trump voters, and L,NandRdenote\ntheleft-wing bias ,no bias andright-wing bias treatment arm, respectively.\n8\nTable B.5: Treatment effects on perceptions of accuracy: Robustness\nExperiment 1: Right-wing bias Experiment 2: Left-wing bias\n(1) (2) (3) (4) (5) (6) (7) (8)\nAccuracy Trust Quality Index Accuracy Trust Quality Index\nPanel A: Biden voters\nBias treatment (a) -0.903*** -0.824*** -0.545*** -0.842*** -0.720*** -0.662*** -0.504*** -0.703***\n(0.057) (0.056) (0.054) (0.056) (0.055) (0.053) (0.053) (0.054)\nN 1,464 1,464 1,464 1,464 1,466 1,466 1,466 1,466\nZ-scored Yes Yes Yes Yes Yes Yes Yes Yes\nControls Yes Yes Yes Yes Yes Yes Yes Yes\nPanel B: Trump voters\nBias treatment (b) -0.165*** -0.143** -0.135** -0.162*** -0.542*** -0.522*** -0.376*** -0.546***\n(0.056) (0.056) (0.057) (0.056) (0.072) (0.072) (0.069) (0.072)\nN 1,235 1,235 1,235 1,235 849 849 849 849\nZ-scored Yes Yes Yes Yes Yes Yes Yes Yes\nControls Yes Yes Yes Yes Yes Yes Yes Yes\np-value: a = b 0.000 0.000 0.000 0.000 0.073 0.166 0.304 0.118\nNote: This table presents OLS regression estimates using data from Experiment 1 (columns 1\u20134) and Experiment 2 (columns 4\u20138)\nwhere the dependent variables are post-treatment beliefs about the newsletter. Panel A and Panel B show results for Biden and\nTrump voters, respectively. \u201cBias treatment\u201d is a binary variable taking value one for respondents assigned to the right-wing biased\n(columns 1\u20134) or left-wing biased (columns 5\u20138) treatment arm. \u201cAccuracy\u201d of the newsletter is measured on a 5-point Likert scale\nfrom \u201cVery inaccurate\u201d to \u201cVery accurate.\u201d \u201cTrust\u201d is the trustworthiness of the newsletter and measured on a 5-point Likert scale\nfrom \u201cNot trustworthy at all\u201d to \u201cVery trustworthy.\u201d \u201cQuality\u201d of the newsletter is measured on a 5-point Likert scale from \u201cVery\nlow quality\u201d to \u201cVery high quality.\u201d \u201cIndex\u201d is a simple average of the accuracy, trust, and quality outcomes. All outcomes are z-\nscored using the relevant no bias group mean and standard deviation. All regressions include the standard set of control variables.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n9\nTable B.6: Structural model: Preferences for accuracy and biased news \u2014 Robustness\nto using non-z-scored perceptions of accuracy and bias\nParameter estimates:\n(1) (2) (3)\nFull sample Biden voters Trump voters\nPreference for accuracy ( a) 0.337*** 0.297** 0.353\n(0.103) (0.117) (0.250)\nPreference for belief con\ufb01rmation ( b) 0.423*** 0.475*** 0.224\n(0.112) (0.136) (0.191)\nImplicit weight on accuracy\u0010\na\na+b\u0011\n0.443*** 0.385*** 0.612**\n(0.113) (0.135) (0.264)\nN 5,014 2,930 2,084\nNote: This table presents parameter estimates that are analogous to the IV probit estimates presented\nin Table 2 except for one difference: Instead of using the z-scored post-treatment measure of per-\nceived accuracy and belief con\ufb01rmation (which are measured on 5-point Likert scales), we use\nnon-z-scored perceptions of accuracy and belief con\ufb01rmation.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n10\nTable B.7: Structural model: Preferences for accuracy and biased news \u2014 Robustness\nto using an index of accuracy-related beliefs\nParameter estimates:\n(1) (2) (3)\nFull sample Biden voters Trump voters\nPreference for accuracy ( a) 0.270*** 0.244*** 0.274\n(0.082) (0.093) (0.191)\nPreference for belief con\ufb01rmation ( b) 0.367*** 0.412*** 0.192\n(0.097) (0.117) (0.166)\nImplicit weight on accuracy\u0010\na\na+b\u0011\n0.424*** 0.372*** 0.588**\n(0.111) (0.130) (0.274)\nN 5,014 2,930 2,084\nNote: This table presents parameter estimates that are analogous to the IV probit estimates presented\nin Table 2 except for one difference: Instead of using the z-scored post-treatment measure of per-\nceived accuracy (which is measured on a 5-point Likert scale), we use a z-scored index based on the\nperceived accuracy, quality and trustworthiness of the newsletter. Trustworthiness of the newsletter\nis measured on a 5-point scale from \u201cNot trustworthy at all\u201d to \u201cVery trustworthy.\u201d Quality of the\nnewsletter is measured on a 5-point scale from \u201cVery low quality\u201d to \u201cVery high quality.\u201d\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n11\nTable B.8: Structural model: Robustness to using a linear probability model\nParameter estimates:\n(1) (2) (3)\nFull sample Biden voters Trump voters\nPanel A: 2SLS\nPreference for accuracy ( a) 0.053*** 0.047** 0.057\n(0.018) (0.021) (0.045)\nPreference for belief con\ufb01rmation ( b) 0.071*** 0.082*** 0.040\n(0.020) (0.024) (0.038)\nImplicit weight on accuracy\u0010\na\na+b\u0011\n0.424*** 0.365** 0.588*\n(0.132) (0.153) (0.305)\nN 5,014 2,930 2,084\nPanel B: Two-sample 2SLS\nPreference for accuracy ( a) 0.054*** 0.055** 0.035\n(0.019) (0.024) (0.058)\nPreference for belief con\ufb01rmation ( b) 0.048** 0.059** 0.025\n(0.022) (0.030) (0.045)\nImplicit weight on accuracy\u0010\na\na+b\u0011\n0.527*** 0.481** 0.583**\n(0.194) (0.218) (0.229)\nN: Choice data 5,014 2,930 2,084\nN: Belief data 1,896 963 933\nNote: This table presents the parameter estimates of a linear probability model where the dependent\nvariable is a binary indicator taking value one for respondents who choose to sign up to the newsletter.\nColumn 1 presents parameter estimates for the full sample, while columns 2 and 3 present estimates\nfor Biden and Trump voters, respectively. Panel A (\u201c2SLS\u201d) presents two-stage least-squares esti-\nmates where we instrument the endogeneous regressors (z-scored perceptions of accuracy and belief\ncon\ufb01rmation) with a saturated set of treatment arm indicators. We use data from Experiments 1 and 2\nwhere we elicit newsletter subscription choices and perceptions within-subject. In column 1, we also\ninclude interactions of the treatment assignment with a binary indicator for whether a respondent\nvoted for Trump as instruments to capture differential \ufb01rst-stage effects of the treatments. We include\na binary indicator for whether a respondent voted for Trump as a control variable in column 1. Robust\nstandard errors are shown in parentheses. Panel B (\u201cTwo-sample 2SLS\u201d) presents analogous two-\nsample two-stage least squares estimates. The endogenous regressors are again z-scored perceptions\nof accuracy and belief con\ufb01rmation. However, to estimate the \ufb01rst stage, we only use the \u201cbelief data\u201d\nfrom Experiment 4 (where we only elicit perceptions) and regress z-scored perceptions of accuracy\nand belief con\ufb01rmation on a saturated set of treatment indicators. To estimate the second stage model,\nwe use data from Experiments 1 and 2 and estimate a linear probability model using the predicted\nperceptions based on the \ufb01rst-stage estimates. We use the same set of instruments and controls as in\nPanel A. Standard errors in Panel B are obtained from a bootstrap procedure that resamples both the\nchoice data (from Experiment 1 and 2) and the belief data (from Experiment 4) with replacement.\n*p< 0.10, ** p< 0.05, *** p< 0.01.\n12\nTable B.9: Structural model: Replacing the belief con\ufb01rmation measure with perceived\ncomplexity or perceived entertainment value\nParameter estimates:\n(1) (2) (3)\nFull sample Biden voters Trump voters\nPanel A: Complexity instead of bias\nPreference for accuracy ( a) 0.278 -0.080 0.364\n(0.240) (0.690) (0.260)\nPreference for simplicity ( b) -0.171 -1.257 0.701\n(0.696) (1.936) (1.102)\nWeight on accuracy\u0010\na\na+b\u0011\n2.598 0.060 0.342\n(20.312) (0.400) (0.283)\nN 5,014 2,930 2,084\nPanel B: Entertainment instead of bias\nPreference for accuracy ( a) 0.364*** 0.526*** 0.261\n(0.130) (0.177) (0.202)\nPreference for entertainment ( b) 0.009 -0.500 0.529\n(0.429) (0.528) (0.522)\nWeight on accuracy\u0010\na\na+b\u0011\n0.975 20.335 0.331\n(1.129) (306.462) (0.311)\nN 5,014 2,930 2,084\nNote: This table presents parameter estimates that are analogous to the IV probit estimates presented\nin Table 2 except for two differences: Panel A replaces the z-scored post-treatment measure of\nbelief con\ufb01rmation with a z-scored post-treatment measure of perceived simplicity (i.e., the reverse-\ncoded perception of complexity). Panel B replaces the z-scored post-treatment measure of belief\ncon\ufb01rmation with a z-scored post-treatment measure of entertainment value.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n13\nTable B.10: Overview of experiments\nExperiment Sample Treatment Arms Main Outcomes\nExperiment 1:\nRight-wing bias vs.\nno bias\n(November 2021)Proli\ufb01c:\nn= 2,705\nAsPredicted ID:\n#78800Right-wing bias treatment : Information\nabout how The Boston Herald covered only\none statistic from the CBO report on the\nMinimum Wage Bill\nNo bias treatment : Information about\nhow The Boston Herald covered both\nstatistics from the CBO report on the\nMinimum Wage BillDemand for a newsletter\ncovering the top 3\narticles from The Boston\nHerald;\nPost-treatment beliefs\nabout newsletter\ncharacteristics\nExperiment 2:\nLeft-wing bias vs.\nno bias\n(December 2021)Proli\ufb01c:\nn= 2,319\nAsPredicted ID:\n#80266Left-wing bias treatment : Information\nabout how The Boston Herald covered only\none statistic from the CBO report on the\nHealthcare Bill\nNo bias treatment : Information about\nhow The Boston Herald covered both\nstatistics from the CBO report on the\nHealthcare BillDemand for a newsletter\ncovering the top 3\narticles from The Boston\nHerald;\nPost-treatment beliefs\nabout newsletter\ncharacteristics\nExperiment 3:\nMechanisms on\ninterpretation of\ntreatment\n(February 2022)Proli\ufb01c:\nn= 388\nAsPredicted ID:\n#87947Bias treatments : Information about how\nThe Boston Herald covered one statistic\nfrom the CBO report on the Healthcare\nBill/Minimum Wage Bill\nNo bias treatments : Information about\nhow The Boston Herald covered both\nstatistics from the CBO report on the\nHealthcare Bill/Minimum wage billOpen-ended question on\nwhy The Boston Herald\nreported the statistics in\nthis particular way\nExperiment 4:\nFirst-stage\nExperiment\n(February 2022)Proli\ufb01c:\nn= 1,910\nAsPredicted ID:\n#89081Bias treatments : Information about how\nThe Boston Herald covered one statistic\nfrom the CBO report on the Healthcare\nBill/Minimum Wage Bill\nNo bias treatments : Information about\nhow The Boston Herald covered both\nstatistics from the CBO report on the\nHealthcare Bill/Minimum wage billPost-treatment beliefs\nabout accuracy and bias\nNote: This table provides an overview of all experiments.\n14\nTable B.11: Secondary results: Beliefs about other newsletter characteristics\nExperiment 1: Right-wing bias Experiment 2: Left-wing bias\n(1) (2) (3) (4) (5) (6) (7) (8)\nEntertainment Complexity Easy No outlet bias Entertainment Complexity Easy No outlet bias\nPanel A: Biden voters\nBias treatment (a) -0.306*** -0.281*** 0.118** -0.551*** -0.141*** -0.272*** 0.139*** -0.548***\n(0.051) (0.053) (0.052) (0.022) (0.050) (0.052) (0.053) (0.021)\nN 1,464 1,464 1,464 1,469 1,466 1,466 1,466 1,469\nZ-scored Yes Yes Yes No Yes Yes Yes No\nControls Yes Yes Yes Yes Yes Yes Yes Yes\nNo bias treatment mean 0 0 0 0.806 0 0 0 0.870\np-value: Ex. 1 = Ex. 2 0.017 0.902 0.715 0.912 0.017 0.902 0.715 0.912\nPanel B: Trump voters\nBias treatment (b) 0.150*** -0.076 -0.012 -0.359*** -0.155** -0.049 -0.105 -0.407***\n(0.058) (0.055) (0.057) (0.026) (0.067) (0.069) (0.069) (0.031)\nN 1,235 1,235 1,235 1,236 849 849 849 850\nZ-scored Yes Yes Yes No Yes Yes Yes No\nControls Yes Yes Yes Yes Yes Yes Yes Yes\nNo bias treatment mean 0 0 0 0.664 0 0 0 0.780\np-value: Ex. 1 = Ex. 2 0.001 0.736 0.279 0.232 0.001 0.736 0.279 0.232\np-value: a = b 0.000 0.008 0.094 0.000 0.706 0.008 0.005 0.000\nNote: This table presents OLS regression estimates using data from Experiment 1 (columns 1\u20134) and Experiment 2 (columns 5\u20138) where the\ndependent variables are post-treatment beliefs about the newsletter and The Boston Herald\u2019s reporting. Panel A and Panel B show results for\nBiden and Trump voters, respectively. \u201cBias treatment\u201d is a binary variable taking value one for respondents assigned to the right-wing biased\n(columns 1\u20133) or left-wing biased (columns 4\u20136) treatment arm. \u201cEntertainment\u201d of the newsletter is measured on a 5-point Likert scale from \u201cNot\nentertaining at all\u201d to \u201cVery entertaining.\u201d \u201cComplex\u201d is the belief about the complexity of the newsletter and measured on a 5-point Likert scale\nfrom \u201cVery simple\u201d to \u201cVery complex.\u201d \u201cEasy\u201d is the belief about the dif\ufb01culty of understanding the newsletter and measured on a 5-point Likert\nscale from \u201cVery easy\u201d to \u201cVery dif\ufb01cult.\u201d \u201cNo outlet bias\u201d is a binary variable taking value one for respondents who think that The Boston Herald\nwould disclose both key \ufb01ndings from a CBO report, and zero otherwise (see Section F.1.1 for the instructions we used). The outcome variables in\ncolumns 1\u20133 and 5\u20137 are z-scored using the relevant no bias group mean and standard deviation. All regressions include the standard set of control\nvariables.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n15\nTable B.12: Experiment 4: Treatment effects on perceptions of accuracy and bias\nLeft-wing bias Right-wing bias\n(1) (2) (3) (4)\nAccuracy Left-wing bias Accuracy Left-wing bias\nPanel A: Biden voters\nBias treatment -0.576*** 0.384*** -0.980*** -0.822***\n(0.093) (0.111) (0.095) (0.107)\nN 486 486 477 477\nControls Yes Yes Yes Yes\nPanel B: Trump voters\nBias treatment -0.477*** 0.408*** -0.231** -0.308***\n(0.094) (0.100) (0.096) (0.104)\nN 473 473 460 460\nControls Yes Yes Yes Yes\nNote: This table presents OLS regression estimates using data from Experiment 4 (see Table B.10\nfor an overview of experiments) where the dependent variables are perceptions of the newsletter\u2019s\naccuracy (columns 1 and 3) and the perceived left-wing bias of the newsletter (columns 2 and 4).\nPanel A shows results for Biden voters and Panel B shows results for Trump voters. \u201cBias treatment\u201d\nis a binary indicator for whether respondents were informed that The Boston Herald reported the\nnews in a left-wing biased way (columns 1 and 2) or in a right-wing biased way (columns 3 and 4),\nand zero otherwise. \u201cAccuracy\u201d of the newsletter is measured on a 5-point Likert scale from \u201cVery\ninaccurate\u201d to \u201cVery accurate.\u201d \u201cLeft-wing bias\u201d is measured on a 5-point Likert scale from \u201cVery\nright-wing biased\u201d to \u201cVery left-wing biased.\u201d All outcomes have been z-scored using the relevant\nno bias group mean and standard deviation. All regressions include standard control variables.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n16\nTable B.13: Experiment 4: Treatment effects on secondary outcomes\nLeft-wing bias Right-wing bias\n(1) (2) (3) (4) (5) (6) (7) (8)\nTrust Quality Entertainment Complexity Trust Quality Entertainment Complexity\nPanel A: Biden voters\nBias treatment -0.495*** -0.313*** -0.157* -0.300*** -0.980*** -0.825*** -0.419*** -0.271***\n(0.088) (0.092) (0.084) (0.091) (0.100) (0.095) (0.093) (0.089)\nN 486 486 486 486 477 477 477 477\nControls Yes Yes Yes Yes Yes Yes Yes Yes\nPanel B: Trump voters\nBias treatment -0.393*** -0.446*** -0.123 -0.064 -0.145 -0.081 0.039 -0.037\n(0.093) (0.102) (0.099) (0.093) (0.094) (0.099) (0.100) (0.095)\nN 472 472 472 472 460 460 460 460\nControls Yes Yes Yes Yes Yes Yes Yes Yes\nNote: This table shows OLS regression estimates using data from Experiment 4 where the dependent variables are post-treatment beliefs about\nthe newsletter (see Table B.10 for an overview of experiments). Panel A shows results for Biden voters and Panel B shows results for Trump\nvoters. \u201cBias treatment\u201d is a binary indicator for whether respondents were informed that The Boston Herald reported the news in a left-wing\nbiased way (columns 1\u20134) or in a right-wing biased way (columns 5\u20138). \u201cTrust\u201d is the trustworthiness of the newsletter and measured on a\n5-point Likert scale from \u201cNot trustworthy at all\u201d to \u201cVery trustworthy.\u201d \u201cQuality\u201d of the newsletter is measured on a 5-point Likert scale from\n\u201cVery low quality\u201d to \u201cVery high quality.\u201d \u201cEntertainment\u201d of the newsletter is measured on a 5-point Likert scale from \u201cNot entertaining at all\u201d\nto \u201cVery entertaining.\u201d \u201cComplex\u201d is the belief about the complexity of the newsletter and measured on a 5-point Likert scale from \u201cVery\nsimple\u201d to \u201cVery complex.\u201d All outcomes are z-scored using the relevant no bias group mean and standard deviation. All regressions include\nthe standard set of control variables.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n17\nTable B.14: Heterogeneity in effects by news demand outside the experiment\nDependent variable: Newsletter demand\nExperiment 1: Right-wing bias Experiment 2: Left-wing bias\n(1) (2) (3) (4) (5) (6)\nRespondents who read:No other\noutletMainly\nleft-wingMainly\nright-wingNo other\noutletMainly\nleft-wingMainly\nright-wing\nBias treatment 0.004 -0.047*** -0.129*** -0.035 -0.049** -0.031\n(0.023) (0.018) (0.042) (0.029) (0.020) (0.052)\nN 599 1,515 340 447 1,408 241\nControls Yes Yes Yes Yes Yes Yes\nMean of dep. var. 0.093 0.158 0.179 0.107 0.196 0.187\nNote: This table presents OLS regression estimates using data from Experiment 1 (columns 1\u20133) and\nExperiment 2 (columns 4\u20136) where the dependent variable is a binary indicator taking value one for\nrespondents who said \u201cYes\u201d to receiving the weekly newsletter, and zero for those who said \u201cNo.\u201d\nColumns 1 and 4 restrict to respondents who indicated pre-treatment that they do not read news from\nany of the 21 news outlets that we listed. Columns 2 and 5 restrict to respondents who read more left-\nwing than right-wing biased outlets, while columns 3 and 6 restrict to respondents who read more\nright-wing than left-wing biased outlets. We used a classi\ufb01cation of outlet ideology from the website\nmediabiasfactcheck.com as of January 26, 2022. \u201cTreatment\u201d is a binary variable taking value one\nfor respondents assigned the right-wing biased (Experiment 1) or the left-wing biased treatment arm\n(Experiment 2), and zero otherwise. All regressions include the standard set of control variables.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n18\nTable B.15: Robustness: Results among respondents who did not correctly guess the study\npurpose\nExperiment 1: Right-wing bias Experiment 2: Left-wing bias\n(1) (2) (3) (4) (5) (6)\nAccuracy Left-wing bias Demand Accuracy Left-wing bias Demand\nPanel A: Biden voters\nBias treatment (a) -0.876*** -0.815*** -0.077*** -0.695*** 0.266*** -0.018\n(0.058) (0.063) (0.017) (0.056) (0.061) (0.020)\nN 1,376 1,376 1,376 1,393 1,393 1,393\nZ-scored Yes Yes No Yes Yes No\nControls Yes Yes Yes Yes Yes Yes\nNo bias treatment mean 0 0 0.178 0 0 0.186\np-value: Ex. 1 = Ex. 2 0.022 0.000 0.023 0.022 0.000 0.023\nPanel B: Trump voters\nBias treatment (b) -0.164*** -0.456*** 0.006 -0.537*** 0.254*** -0.057**\n(0.057) (0.064) (0.021) (0.074) (0.072) (0.025)\nN 1,191 1,191 1,191 814 814 814\nZ-scored Yes Yes No Yes Yes No\nControls Yes Yes Yes Yes Yes Yes\nNo bias treatment mean 0 0 0.159 0 0 0.195\np-value: Ex. 1 = Ex. 2 0.000 0.000 0.051 0.000 0.000 0.051\np-value: a = b 0.000 0.004 0.002 0.126 0.721 0.220\nNote: This table presents OLS regression estimates using data from Experiment 1 (columns 1\u20133) and Experiment\n2 (columns 4\u20136) where the dependent variables are post-treatment beliefs about accuracy (columns 1 and 4),\nthe perceived left-wing bias of the newsletter (columns 2 and 5), and newsletter demand (columns 3 and 6).\nPanel A and Panel B present results for Biden and Trump voters, respectively. The regressions only include the\nsubset of respondents who did not correctly guess the hypothesis of the study at the open-ended question about\nstudy purpose at the end of the survey. \u201cBias treatment\u201d is a binary variable taking value one for respondents\nassigned the right-wing bias (columns 1\u20133) or the left-wing bias (columns 4\u20136) treatment arm, and zero for\nrespondents in the no bias treatment arm. \u201cDemand\u201d is a binary variable taking value one for respondents who\nsaid \u201cYes\u201d to receiving the weekly newsletter, and zero for those who said \u201cNo.\u201d \u201cAccuracy\u201d of the newsletter is\nmeasured on a 5-point Likert scale from \u201cVery inaccurate\u201d to \u201cVery accurate.\u201d \u201cLeft-wing bias\u201d is measured\non a 5-point Likert scale from \u201cVery right-wing biased\u201d to \u201cVery left-wing biased.\u201d \u201cAccuracy\u201d and \u201cLeft-\nwing bias\u201d have been z-scored using the relevant no bias group mean and standard deviation. \u201c p-value: Ex. 1\n= Ex. 2\u201d provides p-values for tests of the equality of coef\ufb01cients between Experiment 1 and Experiment 2.\n\u201cp-value: a = b\u201d provides p-values for tests of the equality of coef\ufb01cients between Trump and Biden voters.\nAll regressions include a set of basic control variables: gender, age, education, race and ethnicity, log income,\nemployment status, Census region, voting, political af\ufb01liation, ideology, interest in economic news, whether\nthey have read any of a list of 21 newspapers during the last 12 months, whether they have read The Boston\nHerald, whether they currently subscribe to any newsletters, and their pre-treatment beliefs about how The\nBoston Herald reported about the CBO \ufb01ndings.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n19\nTable B.16: Test of balance of treatment vs. control: Experiment 3\nBias (B) No bias (NB) P-value (B - NB) Observations\nMale 0.49 0.47 0.758 388\nAge 35.12 36.36 0.364 388\nWhite 0.82 0.84 0.533 388\nIncome (midpoint) 71576.92 67551.81 0.383 388\nCollege degree 0.70 0.67 0.540 388\nFull-time employee 0.54 0.53 0.845 388\nNortheast 0.16 0.16 0.924 388\nMidwest 0.23 0.18 0.230 388\nWest 0.19 0.26 0.102 388\nSouth 0.42 0.40 0.744 388\nNote: This table provides a balance test between the treatment and no bias group\nusing all respondents from Experiment 3 (see Table B.10 for an overview of the ex-\nperiments). \u201cMale\u201d is a binary variable taking value one for male respondents, and\nzero otherwise. \u201cAge\u201d is the numerical age of the respondent in years. \u201cWhite\u201d is a\nbinary variable taking value one if the respondent selected \u201cCaucasian/White,\u201d and\nzero otherwise. \u201cIncome (midpoint)\u201d is coded continuously as the income bracket\u2019s\nmidpoint (Less than $15,000, $15,000 to $24,999, $25,000 to $49,999, $50,000\nto $74,999, $75,000 to $99,999, $100,000 to $149,999, $150,000 to $200,000,\n$200,000 or more). \u201cCollege degree\u201d is a binary variable taking value one if the\nrespondent has a college degree, and zero otherwise. \u201cFull-time employee\u201d is a\nbinary variable taking value one if the respondent is a full-time employee, and zero\notherwise. \u201cNortheast,\u201d \u201cMidwest,\u201d \u201cWest\u201d and \u201cSouth\u201d are binary variables with\nvalue one if the respondent lives in the respective region, and zero otherwise.\n20\nTable B.17: Test of balance of treatment vs. control: Experiment 4\nBias (B) No bias (NB) P-value (B - NB) Observations\nMale 0.48 0.48 0.968 1910\nAge 38.63 39.03 0.554 1910\nWhite 0.82 0.82 0.680 1910\nIncome (midpoint) 71156.05 68794.64 0.248 1910\nCollege degree 0.69 0.70 0.954 1910\nFull-time employee 0.51 0.48 0.272 1910\nNortheast 0.18 0.20 0.240 1910\nMidwest 0.20 0.21 0.855 1910\nWest 0.22 0.21 0.665 1910\nSouth 0.40 0.39 0.465 1910\nNote: This table provides a balance test between the treatment and no bias group\nusing all respondents from Experiment 4 (see Table B.10 for an overview of the ex-\nperiments). \u201cMale\u201d is a binary variable taking value one for male respondents, and\nzero otherwise. \u201cAge\u201d is the numerical age of the respondent in years. \u201cWhite\u201d is a\nbinary variable taking value one if the respondent selected \u201cCaucasian/White,\u201d and\nzero otherwise. \u201cIncome (midpoint)\u201d is coded continuously as the income bracket\u2019s\nmidpoint (Less than $15,000, $15,000 to $24,999, $25,000 to $49,999, $50,000\nto $74,999, $75,000 to $99,999, $100,000 to $149,999, $150,000 to $200,000,\n$200,000 or more). \u201cCollege degree\u201d is a binary variable taking value one if the\nrespondent has a college degree, and zero otherwise. \u201cFull-time employee\u201d is a\nbinary variable taking value one if the respondent is a full-time employee, and zero\notherwise. \u201cNortheast,\u201d \u201cMidwest,\u201d \u201cWest\u201d and \u201cSouth\u201d are binary variables with\nvalue one if the respondent lives in the respective region, and zero otherwise.\n21\nTable B.18: Heterogeneity by ideology\nDependent variable: Newsletter demand\n(1) (2)\nExperiment 1 Experiment 2\nBias treatment 0.018 -0.023\n(0.035) (0.043)\nBias treatment\u0002Liberal -0.020** -0.003\n(0.010) (0.012)\nLiberal 0.030** 0.030**\n(0.013) (0.014)\nN 2705 2319\nNote: This table presents OLS regression estimates using data from Experiment 1 (column 1) and\nExperiment 2 (column 2) where the dependent variables are newsletter demand. \u201cNewsletter de-\nmand\u201d is a binary variable taking value one for respondents who said \u201cYes\u201d to receiving the weekly\nnewsletter, and zero for those who said \u201cNo.\u201d \u201cBias treatment\u201d is a binary variable taking value one\nfor respondents assigned the right-wing bias (column 1) or the left-wing bias (column 2) treatment\narm, and zero for respondents in the no bias treatment arm. \u201cLiberal\u201d is measured on a 5-point Likert\nscale from 1: Very conservative to 5: Very liberal . All regressions include the standard set of control\nvariables.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n22\nFigure B.1: Pre-treatment beliefs about bias by treatment status\n(a) Experiment 1: Beliefs about the coverage of the Healthcare Plan\n0.2.4.6\nPoverty onlyEmployment onlyBoth statistics Poverty onlyEmployment onlyBoth statisticsUnbiased BiasedFraction\nP-value obtained from a Kolmogorov-Smirnov test for equality of distribution functions: 0.927\n(b) Experiment 2: Beliefs about the coverage of the $15 Minimum Wage Bill\n0.2.4.6\nUninsured onlyDeficit only Both statistics Uninsured onlyDeficit only Both statisticsUnbiased BiasedFraction\nP-value obtained from a Kolmogorov-Smirnov test for equality of distribution functions: 0.899\nNote: Panel (a) and Panel (b) display the distribution of pre-treatment beliefs about reporting\nby The Boston Herald for Experiment 1 and 2, respectively. Each panel displays the\ndistribution of pre-treatment beliefs separately for respondents in the no bias treatment arm\n(\u201cunbiased\u201d) and the biased treatment arm (\u201cbiased\u201d), i.e., the right-wing bias treatment\nin Experiment 1 and the left-wing bias treatment in Experiment 2 (see Table B.10 for an\noverview of experiments).\n23\nFigure B.2: Treatment effects on mentioning balanced reporting in the open-ended\nresponses\n(a) Biden voters: Right-wing bias\n p = 0.012\nn = 57 n = 57\n0.1.2.3.4Mean \u00b1 s.e.m.\nNo bias Right-wing bias (b) Trump voters: Right-wing bias\n p = 0.001\nn = 38 n = 42\n0.1.2.3.4Mean \u00b1 s.e.m.\nNo bias Right-wing bias\n(c) Biden voters: Left-wing bias\n p < 0.001\nn = 64 n = 62\n0.1.2.3.4Mean \u00b1 s.e.m.\nNo bias Left-wing bias (d) Trump voters: Left-wing bias\n p = 0.005\nn = 34 n = 34\n0.1.2.3.4Mean \u00b1 s.e.m.\nNo bias Left-wing bias\nNote: The \ufb01gure presents treatment effects on whether respondents mentioned balanced\nreporting in their responses to the open-ended motives question in Experiment 3 (see\nTable B.10). Speci\ufb01cally, respondents were asked why they think The Boston Herald\nreported in the way that it did. Each panel displays the share of respondents whose responses\nwere hand-coded to the \u201cno bias\u201d category (e.g., \u201cThey were probably trying to report fairly\nwithout bias,\u201d \u201cThey were trying to give the full picture,\u201d and \u201cThey tried to report fairly\nand accurately\u201d would all be classi\ufb01ed as \u201cbalanced\u201d). Panel (a) and Panel (b) compare the\nright-wing bias treatment to the no bias treatment (analogous to Experiment 1). Panel (c)\nand Panel (d) compare the left-wing bias treatment to the no bias treatment (analogous to\nExperiment 2). Panel (a) and Panel (c) focus on the subsample of respondents who voted for\nJoe Biden, while Panel (b) and Panel (d) focus on respondents who voted for Donald Trump.\nThep-values are obtained from a two-sample t-test of equality of means. Standard errors of\nthe mean are shown.\n24\nFigure B.3: Perceived study purpose\n36.2\n4.111.5\n0.39.016.9\n11.7\n10.4\n010203040Percent\nbias correct dk junk media opinion other politics\nNote: This \ufb01gure shows the distribution of the perceived study purpose among our respondents\nin Experiment 1 and 2. Speci\ufb01cally, at the end of the main experiments, respondents were\nasked the following open-ended question: \u201cIf you had to guess, what would you say was\nthe purpose of this study?\u201d A team of research assistants hand-coded the responses based\non the following coding scheme: bias: Explicit mentions of bias in the media. Example\nresponse: \u201cAssessing social perceived bias towards left wing vs ring wing media sources.\u201d\ncorrect : People correctly guessing the study\u2019s hypothesis (how perceptions of bias shape\npeople\u2019s news consumption). Example response: \u201cI believe that the purpose of the study\nis to see how participants respond to bias in news stories in the media.\u201d junk: Nonsensical\nresponses. media : Generic mentions that the study is about perceptions of media (without\nexplicitly mentioning bias). Example response: \u201cHow perceptions of news organizations\nmesh with reality.\u201d opinion : Generic mentions that the study tries to assess opinions and\nattitudes. Example response: \u201cTo see how people judge a news source such as the Boston\nHerald.\u201d dk(don\u2019t know): People expressing uncertainty. Example response: \u201cI don\u2019t know.\u201d\nother : Responses that do not \ufb01t into any of the other categories. politics : People generically\ntalking about politics. Example response: \u201csomething about political parties.\u201d\n25\nC Structural estimates: Robustness\nOne potential concern is that perceptions of accuracy and bias are endogenous to\nchoices. Speci\ufb01cally, respondents might have a taste for providing survey responses\nthat are internally consistent (Falk and Zimmermann, 2015). In our main experiments,\nwe elicit demand for our newsletter before asking respondents to state their beliefs\nabout the newsletter\u2019s accuracy and political bias. A taste for consistency would thus\nimply that the act of subscribing to our newsletter has an effect on respondents\u2019 stated\nbelief that is independent from our treatments, which would imply that we do not\nobtain unbiased estimates of respondents\u2019 beliefs in our main experiments. This could\npotentially bias our structural estimates of the relative importance of accuracy compared\nto belief con\ufb01rmation motives. However, note that this cannot affect our results if\nthe magnitude of the consistency bias in survey responses is identical for the survey\nmeasures eliciting beliefs about accuracy and beliefs about the political bias.\nTo address this concern, we conducted an additional pre-registered experiment on\nProli\ufb01c in February 2022 (Experiment 4; see Table B.10).2In this experiment, we\nadminister the same treatments as in Experiment 1 and 2 but respondents are not offered\nthe chance to subscribe to the newsletter. Instead, we inform them about the existence of\nthe newsletter and then elicit respondents\u2019 post-treatment beliefs about the newsletter\u2019s\naccuracy and bias using the same survey measures as in our previous experiments.\nWhile this addresses concerns about consistency bias in survey responses, the absence\nof an active choice might lower engagement with the survey. We therefore view this as\na complementary robustness check.\nWe then use a two-sample instrumental variables strategy to estimate a linear\nprobability model where the binary dependent variable is the decision to sign up to\nour newsletter (Angrist and Krueger, 1995; Inoue and Solon, 2010). Speci\ufb01cally, we\nuse OLS to estimate equations the \ufb01rst-stage effect of our treatments on perceptions\nof accuracy and belief con\ufb01rmation (see equations 3 and 4) using the belief data\nfrom Experiment 4 (where we only elicit perceptions). We then use the choice data\nfrom Experiments 1 and 2 and estimate a linear probability model using the predicted\nperceptions of accuracy and belief con\ufb01rmation obtained from the \ufb01rst-stage regression\n2Our sample includes 968 Biden voters and 942 Trump voters. The median response time was 3.5\nminutes. To recruit enough Trump voters, we allowed 624 Trump voters who had participated in the\nmain experiments three to four months prior to participate in Experiment 4. Reassuringly, we see no\ntreatment heterogeneity based on the original treatment assignment.\n26\nas regressors. For inference, we obtain standard errors using a bootstrap procedure\nthat resamples the choice data (from Experiments 1 and 2) and the belief data (from\nExperiment 4) with replacement.\nPanel B of Table B.8 presents the parameter estimates from this robustness exercise.\nThe estimates using the full sample support the quantitative importance of people\u2019s\npreference for belief con\ufb01rmation ( p< 0.01, column 1). Again, the implied weight on\naccuracy is close to and not statistically signi\ufb01cantly different from 0.5, corroborating\nthe robustness of our model estimates. If anything, the point estimates are closer to 0.5\nand exhibit less heterogeneity across political groups (columns 2 and 3).\nThis suggests that consistency bias in survey responses is unlikely to account for\nour structural \ufb01nding that accuracy and belief con\ufb01rmation motives are approximately\nequally important drivers of people\u2019s demand for news.\n27\nD Research transparency\nPreregistration Our experiments were all preregistered in the AsPredicted registry\n(#78800, #80266, #87947, #89081). The preregistration includes details on the experi-\nmental design, the sampling process, planned sample size, exclusion criteria, and the\nmain analyses. Below, we document deviations from the preregistration for our main\nexperiments (Experiments 1 and 2):\n\u2022The set of control variables speci\ufb01ed in our pre-analysis plan erroneously omitted\nrespondents\u2019 pre-treatment belief about how The Boston Herald reported the\nnews (two indicators). In our main speci\ufb01cation, we control for pre-treatment\nbeliefs.\n\u2022In both experiments, Proli\ufb01c\u2019s subject pool was not large enough to achieve the\ntargeted sample size of 1,500 Trump voters within the pre-speci\ufb01ed sampling\nperiod of \ufb01ve days. In Experiment 1, we managed to recruit 1,236 Trump voters,\nwhile we managed to recruit 850 Trump voters in Experiment 2.\n\u2022In our main analysis, the treatment indicator takes value one for respondents in\nthe \u201cright-wing biased\u201d or \u201cleft-wing biased\u201d treatment arm, and value zero for\nrespondents in the \u201cunbiased\u201d treatment arm. This is numerically equivalent to\nthe speci\ufb01cation we speci\ufb01ed on AsPredicted.\nEthical approval The experimental study received ethics approval from the German\nAssociation for Experimental Economic Research, and the ethics committee of the\nUniversity of Cologne.\nData and code availability The experimental data and the analysis code will be\nmade available upon publication.\n28\nE Newsletter\nThis section provides additional details about how we published our newsletter.\nSelection of news articles We employed the following procedure to select three\narticles for each edition of the weekly newsletter. On Mondays, when the next edi-\ntion of the newsletter is to be published, we used a Firefox browser and went on\nhttps://duckduckgo.com . The advantage of this search engine over other engines,\nsuch as Google, is that search results are not biased by the researcher\u2019s own search his-\ntory or interests. After setting the search engine\u2019s settings to \u201cRegion: US (English)\u201d and\n\u201cTime: Past week\u201d, we used the following search query: site:bostonherald.com\neconomic policy . We then selected the top three articles matching the newsletter\u2019s\nfocus on economic policy from the results page.\nNewsletter editions Each edition of our newsletter had the same basic structure.\nAcross editions, we exchanged the article headlines and links to the articles. The\ntemplate we used for our newsletter editions is presented below:\nThank you very much for participating in our survey [last week, two weeks\nago, three weeks ago, four weeks ago] . According to our records, you\nalso wanted to subscribe to our weekly newsletter featuring articles related\nto economic policy over the next month. This is the [\ufb01rst, second, third,\nfourth and \ufb01nal] of four editions of our newsletter. The newsletter includes\nthe top three articles published in The Boston Herald based on readership.\nIndividual links to the articles included this week are included below.\nArticle 1: Biden\u2019s climate plan aims to reduce methane emissions\nLink: https://www.bostonherald.com/2021/11/02/bidens-climate-plan-aims-\nto-reduce-methane-emissions/\nArticle 2: Fed pulls back economic aid in face of rising uncertainties\nLink: https://www.bostonherald.com/2021/11/03/fed-pulls-back-economic-\naid-in-face-of-rising-uncertainties/\nArticle 3: Biden hails infrastructure win as \u2019monumental step forward\u2019\nLink: https://www.bostonherald.com/2021/11/06/biden-hails-infrastructure-\nwin-as-monumental-step-forward/\n29\nLogistics We released the newsletter on Mondays on the following dates in 2022 at\nabout 6 am Eastern Time: Nov 8, Nov 15, Nov 22, Nov 29, Dec 7, Dec 13, Dec 20.\nTo provide respondents with our newsletter, we used the capability of Proli\ufb01c to send\ndirect messages to respondents on Proli\ufb01c\u2019s platform. This allows us to distribute the\nnewsletter without having to elicit any personally identi\ufb01able information. This, in turn,\nensures that we can measure newsletter demand irrespective of privacy concerns. If\nrespondents indicated that they wish to unsubscribe from our newsletter, we did not\nsend them any additional editions of our newsletter in the following weeks.\nArticles Below is a complete list of all articles we included across newsletters.\n\u2022 Biden\u2019s climate plan aims to reduce methane emissions\n\u2022 Fed pulls back economic aid in face of rising uncertainties\n\u2022 Biden hails infrastructure win as \u2019monumental step forward\u2019\n\u2022 Yellen says quashing COVID is key to lowering in\ufb02ation\n\u2022 Biden bill would give local news outlets \u2018shot in the arm\u2019\n\u2022 Biden bill includes boost for union-made electric vehicles\n\u2022 House OKs $2T social, climate bill in Biden win; Senate next\n\u2022 Biden signs $1T infrastructure deal with bipartisan crowd\n\u2022 No settlement for separated migrant families amid criticism\n\u2022 Biden Administration approves 2nd large US offshore wind farm\n\u2022 Some fear China could win from US spat with Marshall Islands\n\u2022 Will Maine\u2019s anti-mining laws keep needed minerals underground?\n\u2022 Massive $4 billion ARPA, surplus tax revenue bill set for passage\n\u2022 Biden, Putin square off as tension grows on Ukraine border\n\u2022Auditor: Feds gave nearly $4 billion in pandemic relief to businesses that were probably ineligible\n\u2022 New in\ufb02ation number feeds angst about Democrats\u2019 $2T bill\n\u2022 Job listings and new quitting remain near record highs\n\u2022 In\ufb02ation hits a 39-year high and isn\u2019t going away\n\u2022 SALT in the wound: Expanded state and local tax deduction stranded as bill dies\n\u2022 People pressure governments worldwide to act on in\ufb02ation\n\u2022 Here come the rate hikes: Fed sees 3 in 2022\n30\nF Screenshots\nF.1 Experiment 1: Right-wing biased news\nF.1.1 Pre-treatment questions\n31\n32\n33\n34\n35\n36\n37\nF.1.2 Treatment: Right-wing biased news\n38\n39\nF.1.3 Treatment: Unbiased news\n40\n41\nF.1.4 Post-treatment outcomes\n42\n43\n44\n45\n46\n47\n48\n49\nF.2 Experiment 2: Left-wing biased news\nF.2.1 Treatment: Left-wing biased news\n50\n51\nF.2.2 Treatment: Unbiased news\n52\n53\nF.3 Post-treatment outcomes\n54\n55\nF.4 Experiment 3: Open-ended motives\nF.4.1 Treatment 1: No bias (minimum wage bill)\n56\nF.4.2 Treatment 2: Right-wing bias (minimum wage bill)\n57\nF.4.3 Treatment 3: No bias (healthcare plan)\n58\nF.4.4 Treatment 4: Left-wing bias (healthcare plan)\n59\nF.5 Experiment 4: Beliefs about newsletter characteristics\nF.5.1 Left-wing bias: Prior (control)\n60\nF.5.2 Left-wing bias: Prior (treatment)\n61\nF.5.3 Right-wing bias: Prior (control)\n62\nF.5.4 Right-wing bias: Prior (treatment)\n63\nF.5.5 Left-wing bias: Information provision (control)\nF.5.6 Left-wing bias: Information provision (treatment)\nF.5.7 Right-wing bias: Information provision (control)\n64\nF.5.8 Right-wing bias: Information provision (treatment)\nF.5.9 Post-treatment outcomes\n65\n66\n67", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "The demand for news: Accuracy concerns versus belief confirmation motives", "author": ["F Anand Chopra", "I Haaland", "C Roth"], "pub_year": "2022", "venue": "NA", "abstract": "We examine the relative importance of accuracy concerns and belief confirmation motives  in driving the demand for news. In experiments with US respondents, we first vary beliefs"}, "filled": false, "gsrank": 484, "pub_url": "https://www.econstor.eu/handle/10419/262101", "author_id": ["", "Pss00QoAAAAJ", "J4gmk2MAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:h4HSyLiQv2QJ:scholar.google.com/&output=cite&scirp=483&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D480%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=h4HSyLiQv2QJ&ei=XbWsaM-eLcDZieoPqdqh8QU&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:h4HSyLiQv2QJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://www.econstor.eu/bitstream/10419/262101/1/ECONtribute-157-2022.pdf"}}, {"title": "Evidence", "year": "2024", "pdf_data": "      \n \u00a92024 AIBRT. 9 -4-24 rev.  \nAIBRT  \nAmerican Institute for Behavioral Research and Technology  \n \n \n \n \n \nTHE EVIDENCE  \nAlarming Findings from the World\u2019s First \nNationwide \u201cDigital Shield,\u201d 2016 -2024, and Why \nSuch Systems Are Essential for Democracy  \n \nRobert Epstein, Ph.D.  \n \nADVANCE  COPY. SUBMITTED FOR PUBLICATION. Comments \nwelcome. Do not quote or cite without permission  of the author . \n \nTHE EVIDENCE  \n1   \n \nTABLE OF CONTENTS  \n \n \nOverview\u2026 3 \n \nIntroduction\u2026 5 \n \nPart One  \n \nStudies that Make Dubious Claims About the  \nNature of Political Bias in Big Tech Content\u2026 6 \n \n1. The 2019 Economist  study \u20268 \n2. The 2019 Stanford University Study \u202614 \n3. A 2023 S tudy by  S. C. Lewis a nd Others \u202616 \nConclusions \u202618 \n \nPart Two  \nNew and Remarkably Powerful Forms of  \nInfluence Made Possible by the Internet \u202620 \n \n1. The Search Engine Manipulation Effect (SEME) \u202626  \n2. The Search Suggestion Effect (SSE) \u202636 \n3. The Answer Bot Effect (ABE )\u202638 \n4. Targete d Messaging Effect (TME) \u202639 \n5. Differential  Demographics Effect  (DDE )\u202641 \n6. Video Manipulation Effect  (VME) \u202643 \n7. Opinion Matching Effect (OME) \u202644 \n8. Digital Personalization Effect (DPE) \u202647 \n9. Multiple Exposure Effect (MEE) \u202649 \n10. Multiple Platforms Effec t (MPE) \u202650 \n \n  \nEPSTEIN  \n \n2 \n   \nPart Three  \nPart Three: Development and Deployment  \nof a Nationwide System for Preserving and  \nAnalyzing Personalized Ephemeral Content \u202652 \n \n1. 2016 US Presidential Election \u202652 \n2. 2018 US Midterm Elections \u202657 \n3. 2020 US Presidential Election &  \n2021 Georgia Senate Runoff Elections \u202664 \n4. 2022 US Midterm Elections \u202674 \n5. 2023 -2024 Nationwide Monitoring System \u202683 \n \nSummary  and Conclusions \u202698 \nBiography\u2026107  \nReferences\u2026108  \n  \nTHE EVIDENCE  \n3 Overview  \n \n This essay describe s the development and deployment of a \nnationwide  system for preserving and analyzing the online ephemeral \ncontent being sent to Americans  by technology companies 24 hours a \nday. Online ephemeral content has been shown in controlled studies to \nhave unprecedented power to alter people\u2019s thinking and behavi or \nwithout their awareness. Normally, because such content is ephemeral, \nit gives tech companies the ability to influence people without leaving a \npaper trail for authorities to trace; hence, the importance of building \nsystems for preserving such content.  \n This essay also addresses an important public policy issue: To what \nextent , if any,  have tech companies been using ephemeral con tent for \npolitical purposes? I address this issue  by first summarizing and \ncritiquing three recent studies that have defended the tech companies. I \nshow that two of these studies have ties to the tech companies and that \nall of them have fatally flawed methodology. I argue that because \nephemeral content is highly personalized, the only way we can get an \naccurate picture of how suc h content is being employed is to  \u201clook over \nthe shoulders\u201d of a large, representative sample of real users as they are \nreceiving such content, and then aggregating and analyzing the content, \nmuch as the Neilsen company does worl dwide to rate the viewershi p of \nmany types of entertainment media.  The monitoring system my team and \nI have built aggregates and analyzes such content in real time; in so \ndoing, we have repeatedly discovered  highly biased content sufficient to \nhave shifted millions of votes in natio nal elections in the US.  \n I conclude that large -scale monitoring systems must become a \npermanent feature of the internet to protect our democracy, our \nautonomy, and the minds of our children from potentially profound \nmanipulations by the algorithms of Big Tech companies, both now and \nin the foreseeable future.  \n \n Robert Epstein  \nEPSTEIN  \n \n4 \n    \nTHE EVIDENCE  \n5  \n \nOnce men turned their thinking over to machines in the \nhope that this would set them free. But that only \npermitted other men with machines to enslave them.  \u2013\nFrank Herbert, Dune  \n \nIntroduction  \n \n This essay  contains three section s following this Introduction . In Part \nOne, I will explain why several widely cited studies that claim to show \nthat Big Tech platforms are politically unbiased, or at least are not biased \nto favor liberal co ntent in the US, actually contain no such evidence. In \nPart Two, I will summarize more than a decade of randomized, \ncontrolled studies my research team and I have been conducting \u2013 all \npublished or soon -to-be published in peer -reviewed journals \u2013 which \ndemonstrate the enormous and unprecedented power that Big Tech \ncompanies have to shift opinions and votes without people\u2019s awareness \nand in ways that cannot be counteracted.  \n Finally, i n Part Three, I will summarize our progress since 2016 in \nbuilding increa singly larger and more informative  systems that monitor \npersonalized ephemeral content being sent by Big Tech companies to \nregistered voters throughout the US . At present (September 4 , 2024), the \nsystem we have in place has capture d and analyzed more than 99 million \ninstances  of ephemeral content \u2013 fleeting content that impacts people and \nthen disappears, leaving no paper trail \u2013 being sent to the computers of a \npolitically -balanced sample of more than 15 ,000 r egistered voters in all \n50 US states. These dat a show that Google, YouTube and other platforms \nare consistently sending significantly liberally biased content to liberal, \nmoderate, and conservative voters alike. We know from our \nexperimental research (Part Two) that the type and level of biased \ncontent  currently  being sent to US voters is sufficient to shift betwee n \n6.4 and 25.5 million votes to one candidate in the upcoming 2024 \nPresidential election.  \n \n  \nEPSTEIN  \n \n6 \n  Part One: Studies that Make Dubious Claims About the \nNature of Political Bias in Big Tech Content  \n \n Politically c onservative organizations  in the US have been claiming \nfor years \u2013 or at least since Donald Trump became President in early \n2017 \u2013 that Big Tech platforms  have been systematically suppressing \nconservative content shown to online users , perhap s for many years  \n(Amiri, 2023; Bolyard, 2018; Lanum, 2022 ; Moon & Pariseau 2023; \nNolan, 2020 ; Passifiume, 2022 ; Sandler, 2021 ; cf. Feeney 2020 ; Gogarty  \net al.,  2020 ). According to several  recent , credible reports, however \u2013 \none published by The Economist  (The Economist, 2019 ), another \npublished by researchers at Stanford University and the University of \nIllinois at Urbana  (Metaxa  et al. , 2019 ), and a third published by S. C. \nLewis and his colleagues at the University of Oregon, George \nWashington University,  and the University of Masschusetts at Amherst \n\u2013 Big Tech platforms do not suppress conservative content  (Lewis et al., \n2023) . Indeed, they might even elevate or promote such content (Barrett \n& Sims, 2021; Gatewood & O\u2019Connor, 2020; Gogarty et al. 2020 ; \nGonz\u00e1lez -Bail\u00f3n et al., 2022 ; Husz\u00e1r et al., 2022).  \n Still other studies draw conclusions that I consider to be relatively \ntrivial; for example, a study by Kulshrestha  et al. (2019) finds, among \nother things, that Google tends to return biased search results  for which \nthe bias matches that of the searc h terms entered by the user. We a re \ninterested in bias that is not driven by such search terms. In most \nelections \u2013 especially close ones \u2013 the winner is determined by how \nundecided voters tend to lean toward on e candid ate or the other as the \nelection draws near  (Lipsitz, 2009 ; Mayer, 2008 ). Undecided voters \u2013 \npeople trying to make up their minds \u2013 tend to use neutral  search terms, \nnot biased ones. In other words, they might enter the term \u201ctrump\u201d or the \nterm \u201ctrump health care\u201d rather than  \u201ctrump is a criminal .\u201d We believe \nthat to look objectively at the question of bias on search engines or other \nonline platforms, one must see how they respond to neutral content \u2013 in \nother words, how search engines will  respond to the kind of content \nundecided voters might be most likely to enter.  \n In the present paper, I  will first critique the three  studies I  mentioned \nabove , focusing primarily on what I  see as their methodological flaws  \nand ultimately asking a key question: Wh at kind of methodology would \nTHE EVIDENCE  \n7 one need to employ to determine with confidence whether tech \ncompanies current ly present politically biased content to users?  \n Full disclosure: I do not share what is generally viewed as the \nconservative political viewpoint in  the US. I have leaned left most of my  \nlife, and I currently consider myself to be a  moderate  politically. Like \nour Founding Fathers , I am also opposed to the party system. As George \nWashington said in his farewel l address of 1796, disagreement  among \nmembe rs of political parties \u201c agitates the community with ill -founded \njealousies and false alarms, [and] kindles the animos ity of one part \nagainst another\u201d (Washington,  1793) . According to John Adams, \u201c a \ndivision of the republic into two great parties \u2026 is to be  dreaded as the \ngreatest politi cal evil under our Constitution\u201d ( Adams, 1780).  Thomas \nJefferson put the matter more drolly, insisting that \u201c if I could not go to \nheaven but with a party, I would not go there at all\u201d (Jefferson, 1789) .  \n In today\u2019s highly pa rtisan political climate, my disclosure is, I \nbelieve, both necessary and important. I am putting my politics on the \ntable to make it clear that the projects and ideas I will describe in this \npaper were not born of political ideology. I conduct research  in the spirit \nof seeking the truth. I believe, moreover, that all reasonable people \nshould value the preservation of human autonomy, free speech, and the \nfree-and-fair election more than they value any particular political party \nor candidate.  Although it wou ld be disingenuous of  me to assert that I \ncan completely set aside my personal values and beliefs in discussing the \nissues I will address in this paper, it is my goal to do so. I will take great \npains to try to assure the reader that my political leanings do not interfere \nwith the arguments and analyses I offer. I ask my readers to do the same \nin evaluating my work.  \n Below  are the three  investigations of political bias in search engine \nresults and social media platforms that I mentioned above, along with \nmy critiques of these investigations.  Following these critiques, I will, far \nmore briefly, describe three  other  recent studies that appear \u2013 at least at \nfirst glance \u2013 to shed light on the political bias issue. In fact, none of \nthem does so. In Part Three of this essay , I will describe  the rigorous \nsystems my team and I have been developing and deploying since 2016 \nto measure political bias on Big Tech platforms  in ways that present a \ntruer picture of the personalized content that Big Tech companies are \nsendi ng to real users.   \n \nEPSTEIN  \n \n8 \n  A Critique of Three Studies of Online Political Bias  \n \n1. The 2019 Economist  study  \n \n It is a long tradition at The Economist , one of the oldest and most \nrespected news organizations in the English -speaking world  (founded in \n1843 ), to pub lish its articles without identifying the authors of those \narticles. On June 8, 2019, it published the results of an \u201cexperiment\u201d in \nwhich it sought to evaluate possible political bias on Google (The \nEconomist, 2019 ), the search engine used by roughly 92% of the world\u2019s \npopulation outside the People\u2019s  Republic of China (Lahey, & Skopec, \n2024).  The article, entitle d \u201cGoogle Rewards Reputable Reporting, Not \nLeft-Wing Politics,\u201d  listed no authors, of course, and, as far as I know, it \nhad never been subjected t o peer review.  \n The experiment, the article said, \u201ccompar[ed]  a news site\u2019s share of \nsearch results with a statistical prediction based on its output, reach and \naccuracy\u201d  (Economist, 2019, p. 6).  The word \u201cexperiment\u201d is used \nsomewhat differently in differ ent fields, but in the social and behavioral \nsciences, for a procedure to be called an experiment, it must  have , at a \nminimum, at least one control group and one experimental (or \n\u201ctreatment\u201d) group, and participants must be randomly assigned to the \ndiffere nt groups. These criteria allow experiments to be used to draw \nconclusions about causal relationships between variables. Given these \ncriteria, the procedure described by The Economist  was not, in fact, an \nexperiment. It had no control group, and it did not  employ random \nassignment. This is nitpicking, however. Whatever terminology we \nmight use to describe The Economist \u2019s study, the central question \nremains: Did it produce evidence that would reasonably allow them to \nconclude that Google search result s are n ot politically biased?  \n To avoid any possibility that I  might misrepresent their procedure, I  \nwill quote their own description of it at length:  \n \n We first wrote a program to obtain Google results for any \nkeyword. Using a browser with no history, in a polit ically centrist part \nof Kansas, we searched for 31 terms for each day in 2018, yielding \n175,000 links.  \n Next, we built a model to predict each site\u2019s share of the links \nGoogle produces for each keyword, based on the premise that search \nresults should refle ct accuracy and audience size, as Google claims. \nTHE EVIDENCE  \n9 We started with each outlet\u2019s popularity on social media and, using \ndata from Meltwater, a media -tracking firm, how often they covered \neach topic. We also used accuracy ratings from fact -checking websites, \ntallies of Pulitzer prizes and results from a poll by YouGov about \nAmericans\u2019 trust in 37 sources.  \n If Google favoured liberals, left -wing sites would appear more \noften than our model predicted, and right -wing ones less. We saw no \nsuch trend. Overall, centr e-left sites like the New York Times  got the \nmost links \u2014but only about as many as one would expect by chance.  \n \n The Economist  article included a graphic summarizing the numbers \nthat led them to draw that conclusion, after which they completely \nundermined t hat conclusion by admitting to the fatal shortcoming of their \nstudy: \u201c Our study does not prove Google is impartial. In theory, Google \ncould serve un -biased links only to users without a  browsing history. \u201d \n The problem here is that Google has long taken gre at pride in its \nability to personalize  search results (Fredrick, 2022; Statt,  2018 ). All, or \nvirtually all, Google search results are generated based on the massive \nprofile of information the company has collected about each and every \nindividual. In other words, the conclusion drawn in The Economist  study \napplies to virtually no one. The alert reader should have been concerned \nby that phrase, \u201cu sing a browser with no history ,\u201d since all real users \nhave such histories, and Google\u2019s search algorithm can easil y distinguish \na bot \u2013 a computer program that has no history and no profile \u2013 from a \nreal person. The researchers made the task even easier for G oogle\u2019s \nsearch algorithm by making  sure that all its inquiries came from a single \nlocation \u2013 \u201ca politically cen trist part of Kansas .\u201d In other words, all the \ncontent the magazine\u2019s computer was entering into the search engine \ncame from a single Internet Protocol (IP)  address  \u2013 the unique identifier \nevery device has that identifies it for the purpose of communicatin g with \nother devices on the internet. Lest there be any doubt, The Economist  \nmade it obvious to Google\u2019s search algorithm that the entity accessing \nGoogle\u2019s search engine was indeed a bot, not a real person.  \n The alert reader might also have been concerned  about the fact that \nthe researchers were using the same 31 search terms each day.  Even if \nthe study had used real people to generate its data, it is possible that \nGoogle could have detected that the same search terms were being used \nrepeatedly by the same  small group of users  at the same IP address . In \nthat case, the algorithm might have generated unbiased results for those \nEPSTEIN  \n \n10 \n  search terms  simply because of the suspicious nature of the input . \nGoogle\u2019s engineers have been countering efforts to game their \nalgor ithms for many years, and they are alert to any form of questionable  \ndata (Nyguen, 2021); detecting and countering such data is at the heart \nof Google\u2019s long battle against the growing Search Engine Optimization  \n(SEO ) industry (Flaherty, 2017).   \n Later in this essay,  I will present evidence showing unequivocally \nthat under certain conditions, Google\u2019s search algorithm will \nautomatically generate politically unbiased content. For now, I ask the \nreader to consider the possibility that The Economist  procedure was \nfundamentally flawed because it presented content that Google could \neasily identify as non -human.  \n We have other concerns with The Economist study. First, many \nmajor news sources get 40% of their online traffic  from Google  \n(Schwartz, 2024 ). Could The E conomist  risk loosing Google  traffic by \npublishing results suggesting that Google was fixing elections? More to \nthe point, do we know whether Google has ever demoted or removed \ncompanies from its search results because it disapproved of the activities \nof such companies?  \n Google indeed demotes or removes companies from its search index \nevery day , and when this occurs, those compani es have no recourse \n(Constine, 2014 ; Halliday, 2011; Langley, 2023 ; Lomas, 2016 ). \nMoreover, courts in the US have defended Googl e\u2019s behavior in such \ncases. In the case of a  Florida -based company called eV entures, Google \ncut off access to hundreds of websites owned by the company; as is \ntypical, they did so without explanation. eVentures tried complaining, \nbut that was difficult to do given that Google \u2013 one of the largest and \nmost powerful companies in the world \u2013 provided almost  no customer \nservice . As a Vox journalist wrote in 2016, \u201cIt's nearly impossible to \ncontact Google for help. No direct email. No phone support. Not even \nchat. You're basically on your own \u201d (Lowenstein, 2016 ). In 2014, Forbes  \ncalled Google customer service an \u201coxymoron\u201d ( Soloman, 2014 ). When \nGoogle failed to reply to  demand letters sent by e Venture\u2019s lawyers, \neVenture sued. Here is a portion of the ruling agai nst eV enture issued by \nJudge John E. Steele in 201 7: \n \n First, as Google argues, the removal of e -ventures\u2019 websites from \nGoogle\u2019s search engines is not a false statement and is thus protected \nTHE EVIDENCE  \n11 First Amendment speech. There is no dispute that Google made no \npublic announcement regarding the removal of e -ventures\u2019 websites or \nthe reasons behind the removal. But even if Google had published a \npress release that e -ventures\u2019 websites were violating Google\u2019s \nguidelines, that publication would be protected because the statement \nis true. e -ventures\u2019 websites were in violation of Google\u2019s Guidelines, \nand thus the removal of those websites was true speech, if it was spe ech \nat all.  \n But there is a more fundamental reason why the First Amendment \nbars e -ventures\u2019 claims. Google\u2019s actions in formulating rankings for \nits search engine and in determining whether certain websites are \ncontrary to Google\u2019s guidelines and thereby subject to removal are the \nsame as decisions by a newspaper editor regarding which content to \npublish , which article belongs on the front page, and which article is \nunworthy of publication. The First Amendment protects these \ndecisions, whether they are fair or unfair, or m otivated by profit or \naltruism ( E-Ventures Worldwide, LLC v. Google, Inc., 2016 , p. 8)  \n \nIf The Economist  study tells us nothing about the political bias that \nreal users might be seeing, what could be the motivation for publishing \nsuch a study? Unless a whistleblower comes forward from The \nEconomist , or unless internal documents or emails  are leaked, we will \nnever know for sure, but we do know the anxiety that Google engenders \nin the corporate world . Executives at thousands of businesses worldwide \nlive in constant fear that they might somehow trigger people or \nalgorithms at Google to demot e them in Google search results or to \nremove them entirely from Google\u2019s index. Even worse, a company \nmight be demoted or removed based on the vague impression of a Google \nemployee about the poor \u201cquality\u201d of the content (Farley, 2022; \nSchwartz, 2022), or even because Google simply revised its search \nalgorithm. The company used to announce major revisions to its \nalgorithm regularly but largely  abandoned this practice in 2012 , with \nonly one major revision annouc ement in 2015  (Dame, 2015 ). In 2022, \nwe learned  that Google was at that time manually altering  its search \nalgorithm about 4,500 times a year (Schwartz, 2022). The fear issue was \ndiscussed in a penetrating essay entitled \u201cWhy We Fear Google,\u201d \nauthored by Mathias D\u00f6pfner, the longtime CEO of Axel Springe r, \nEurope\u2019s largest publishing conglomerate  (D\u00f6pfner , 2014). According \nto D\u00f6pfner, \u201c Our business relationship is that of the Goliath of Google \nto the David of Axel Springer. When Google changed an algorithm, one \nEPSTEIN  \n \n12 \n  of our subsidiaries lost 70 percent of its t raffic within a few days. The \nfact that this subsidiary is a competitor of Google\u2019s is certainly a \ncoincidence.\u201d  \n Second, The Economist  could be considered one of Google\u2019s many \nbusiness partners. In 2013, Eric Schmidt, then Executive Chairman of \nGoogle, jo ined the magazine\u2019s board of directors. The Economist  also \nshares all its emails \u2013 incoming and outgoing, including the attachments \n\u2013 with Google. Figure 1 shows the expanded header of an email sent to \nme by an editor at The Economist . Note that the email routes through a \nGoogle server. This means that The Economist , like many companies \naround the world, uses G -Suite \u2013 Google\u2019s collection of business \napplications (which includes Gmail) \u2013 to run its business. Among other \nthings, it means that when staff at The Economist was planning and \nconducting its bias study, Google employees could e asily have become \naware of the  plan and just as easily could have fed the magazine\u2019s \nanonymized computer in Kansas poli tically unbiased search results.  \n \n \nFigure 1 . Expanded h eader of an email sent to the author of the present \nessay by an editor at The Economist , showing that the email was routed \nthrough a Google server . \n \n Alas, The Economist  is not alone in surrendering its privacy to \nGoogle LLC. Many  major news organizations \u2013 organizations that \noccasionally are investigating Google itself \u2013 share their emails and other \ncontent with Google, among them, The New York Times  and The \nGuardian  (Epstein, 2018 a). So do thousands of schools, colleges, and \nmajor universities around the world, including prestigious research -\ndriven institutions such as Columbia University and the entire University \n\nTHE EVIDENCE  \n13 of California system, including UCLA and UC Berkeley, a bastion of \nhigh tech (Epstein, 2018 a).  \n \n2. The 2019 Stanford University Study \n \n A secon d study \u2013 also published in 2019 \u2013 looked at possible political \nbias in Google search results using more sophisticated methods (Metaxa \net al., 2019), in this case focusing on the 2018 midterm elections in the \nUS. The authors, who were mostly based at Stanf ord University at the \ntime, analyzed \u201c a total of over 4 million URLs, scraped daily from \nGoogle search queries for all candidates running for federal office in the \nUnited States in 2018 \u201d (Metaxa et al., 2019 , p. 1). They scraped this \ninformation only from the first  page of Google search results, which \ngene rally shows 10 results per page .  To be more specific, they scraped \nGoogle \u201c daily for every candidate running for federal congressional \noffice during the 2018 election cycle and covering May 29, 2018 throu gh \nthe election on November 6, 2018 (163 days) \u201d (Metaxa et al., 2019, p. \n6). \n There is a very old saying in computer science \u2013 so old that the saying \ncame before the phrase \u201ccomputer science\u201d did \u2013 known by the acrynym \nGIGO (pronounced \u201cGUY -go\u201d), which sta nds for Garbage -In-Garbage -\nOut. Before we look at the authors\u2019 analyses of the data , we need to ask \nwhere it came from, and here we have a problem similar to the one we \nidentified in The Economist  study. The best way for me to explain this \nproblem is to us e language from the paper:  \n \nWe used five scrapers to collect the blue links on the first page of \nsearch results daily. Each scraper had its own IP address, instantiated \nusing the Amazon Elastic Compute Cloud, and rotated through a list \nof user -agent string s such that each request appeared to come from a \nnormal operating system and modern web browser\u2026. This data \ncollection process takes approximately 10 hours per day\u2026. Recent \nwork investigating personalization in political web search has found \nthat personali zation has \u201clittle impact\u201d on such queries [ Lazer et al., \n2018] , but in order for our data to most closely reflect a generic user, \nwe also add a depersonalization parameter (\u201cpws=0\u201d) to the end of \neach query URL to avoid any history -based personalization.  \n \nEPSTEIN  \n \n14 \n   To put this in  plain English, the authors used Amazon Cloud to \nsimulate five computer s, and, again through simulation, made each of \nthese simulated computers \u201c[ appear] to come from a normal operating \nsystem  and modern web browser.\u201d The problem here is th at, like the \neditors at The Economist , the authors of this study made it  trivially easy \nfor Google\u2019s search alg orithm to identify the searcher as a bot \u2013 a \n\u201cgeneric user .\u201d Lest Google\u2019s algorithm have any doubt about this, the \nauthors added \u201ca depersonaliz ation parameter \u201d to the end of each search \nquery.  These various steps, taken together, are like shouting \u201cI am a bot \nbeing used to measure search bias!\u201d through a digital megaphone.  \n Note that the authors justify this overly optimistic, if not na\u00efve, \nappro ach to d ata collection by claiming that, \u201cRecent work focusing on \nsearch personalization and politics has found little evidence of results \nbeing personalized in response to queries of a political nature ,\u201d and they \nrely for this claim on a single paper pres ented at a computer science \nmeeting  in 2018 (Lazer  et al., 2018 ). In fact, that study found clear \nevidence of personalization on Google, including \u201c\u2026substantial \ndifferences in SERP [search engine results page] composition by root \nquery, with Twitter compon ents appearing the most frequently in SERPs \nfor queries that stemmed from the root \u2018Donald Trump\u2026.\u2019\u201d The authors \nof the 2018 study also reported that \u201cpersonalization on Google Search \nincreased with the amount of Alphabet services that participants reporte d \nregularly using ( \u03c1 = .07, p < .001) and was 19.3% higher for participants \nwho were logged -in to their Google accounts than for those who were \nnot ( U = 1.52 x 107, p < .001). \u201d Elsewhere in the study, the authors \nreported finding significantly greater levels of personalizati on for people \nwho regularly used multiple  Google platf orms (Google +, Android, \nYoutube, Gmail, Google Calender ) whether users were logged in or not.   \n Other, more recent studies have confirmed th e existence of \npersonalization in Google s earch even when use rs are not logged in \n(Akbar et al., 2023 ). This finding is consistent with Google\u2019s own clear \nstatements about its personalization practices, exempli fied by this \nrevelatory statement made in a Google Blogspot in 2009:  \u201cToday we're \nhelping people get better  search results by extending  personalized \nsearch  to signed -out users worldwide, and i n more than forty languages\u201d  \n(Horling & Kulick , 2009 ). The very idea that  Google, which surveills its \nusers more aggressively than any company or government entity in \nhistory, would collect vast tomes of personal data about each of its users \nTHE EVIDENCE  \n15 and then not use those data to customize content for its users is patently \nabsurd (cf. Leith, 2023 ; Rushe, 2014;  Schmidt , 2018 ). Bear in mind that \nGoogle invented  the surveillance busin ess model, which is now used by \nthousands of businesses worldwide (Zuboff , 2019 , p. 15 ).  \n Possibly relevant here is the fact that Google is also one of Stanford \nUniversity\u2019s largest donors , with nearly every program at the university \nthat is technology re lated receiving Google funds  (Angwin, J. & \nFaturechi, R., 2014; Standford Law Center, 2006; Orenstein, 2011 ). \nGoogle  has also paid generous consulti ng fees \u2013 as high as $400,000 \n(Mullins & Nicas 2017 ) \u2013 to hundreds of academic researchers to \nconduct resea rch that benefits the company (Tech Transparency Project, \n2017 ). One investigation of Google\u2019s funding of acad emic projects \nfound that researchers \u201cdid not disclose the Google funding in nearly \ntwo-thirds of cases (65%) \u201d (Tech Transparency Project, 2017 ).  \n \n3. A 2023 S tudy by  S. C. Lewis and Others  \n \n A 2023 study by S. C. Lewis and his colleagues  at the University of \nOregon, George Washington University, and the University of \nMasschusetts at Amherst  (Lewis et al., 2023) seemed to cast doubt upon \nthe widely h eld view that online platforms trap people in \u201cfilter bubbles\u201d \n(Carlson , 2018 ; Hindman, 2009; Smyrnaios & Rebillard, 2019 ). The \nresearchers recruited \u201ca demographically diverse U.S. participants (N = \n1,598)\u201d from the Amazon Mechanical Turk (MTurk) subject pool and \npaid them to conduct simple searches on Google Search, Google News, \nYouTube (owned by Google), Facebook, and Twitter. The same four \nsearch terms \u2013 immigration, state Republican s, Trump , and crime  \u2013 were \nused for each search.  They also rated the po litical leanings of each of \ntheir participants, grouping them into three categories \u2013 liberal, \nmoderate, and conservative \u2013 based on self-ratings. The researchers \nconcluded that (1) each platform presented relatively similar results to \npeople in all three political groups, and (2) that each platform \u201cprioritized \ncertain types of content o ver others.\u201d   \n The study is troubling in several  respects. First, study participants \nwere not screened to determine whether they were registered voters, \neligible to vote in  the US, or even  old enough to vote; nor did the authors \nreveal any such information to the reader. It is reasonable to assume that \nparticipants were a mix of voting -eligible and non -voting -eligible \nEPSTEIN  \n \n16 \n  people, with the proportions unknown. That mix would like ly reduce any \nsigns of political bias in platform -generated content; why, after all, \nwould Google bother sending politically -biased content to someone who \ncan\u2019t vote?  \n The researchers also used only four search terms, which could easily \nbe terms that are likely to generate similar  results  from similar sources . \nA much larger number of search terms might have yield ed very different \nfindings. The  researchers also trusted their participants to sign into the \nplatforms they were using but had no way of confirmin g whether people  \ndid so. Finally, and most remarkably, they trusted their participants to \ncopy and paste the content they received into an online forum  accurately ; \nin other words, they had not set up an automated me ans for gathering \ndata. As you will see l ater in this paper, all of these methodological flaws \nare avoided in the monitoring systems my team and I have been \ndeveloping and improving since 2016.  \n Setting aside the flaws in the Lewis study, their results  generally  \nsupport  people\u2019s claims \u2013 especial ly the claims of conservatives \u2013 that \nmuch of Google\u2019 s content leans left; the content was homogeneous, yes, \nbut it was also mostly  left leaning  whether it was sent to liberals, \nmoderates, or conservatives . Unfortunately, the authors  did not explain \nwhat m ethods they may have used to rate the political leaning s of the \nnews sources they mentioned.  \n \nConclusions  \n \n Mainly because of methodological flaws, these three studies do not \nprovide convincing evidence that Google search results are not \npolitically biased . The study by Lewis et al. (2023) suggests, in fact,  that \nGoogle sends left -leaning content to liberals, moderates, a nd \nconservatives alike, which could be interpreted as political bias, no \nmatter what its algorithmic underpinnings.  \n Three other recent em pirical studies could also be said to show \npolitical bias on Big Tech platforms, and at first glance these seem to \nshow favoritism  toward conservative content . They have even been \ninterpreted by journalists as proving scientifically that Big Tech is not \nanti-right (e.g., Economist, 2019;  Feeney, 2020 ). The problem with these \nstudies is that they are all \u201camplification\u201d studies , and they also all look \nat content only on X (f.k.a. Twitter) (Chen et al., 2021; Gonz\u00e1lez -Bail\u00f3n  \nTHE EVIDENCE  \n17 et al., 2022; Husz\u00e1r  et al., 2022) . Specifically , they measure the extent to \nwhich certain political content  on X gets circulated  among users. Each \nstudy finds that right -leaning content \u2013 sometimes  disturbing conspiracy \ntheories \u2013 gets circulated  more than left -wing content does.  \n This is  supposed to lead us to conclude that Big Tech companies are \nnot suppressing right -wing content , but the amplification st udies provide \nno such evidence, mainly because the content at issue is being shared \namong  people with relatively extreme left - or right -wing views. They are \nnot the undecided people my colleagues and I have been studying in our \nexperiments for the last decade. Content gets amplified when people with \nstrong viewpoints get excited by it \u2013 content that often contains words or \nphrases that li nguists dub \u201chigh -arousal\u201d and/or \u201c low-valence\u201d (Shuman \net al., 2013 ; Warriner, 2013 ). The fact that such content circulates at a \nhigher volume among conservatives than among liberals (at least on X) \nsuggests merely that conservative content is mor e arousi ng to \nconservatives than  liberal content is to liberals  \u2013 perhaps simply that \nconservatives are more easily excitable than liberals ( Kanai et al., 2011 ). \nAt both extremes, we are talking about people with strong views, not \nabout the undecided voters  who are especially vulnerable to \nmanipulation both by Big Tech companies and political campaign s. \n The question my team and I have posed pertains to the latter kinds of \npeople: the undecided or uncommitted \u2013 the people who are vulnerable \nto influence. As you wil l see, our research (which began in 2013) and \nour monitoring systems (which we first implemented in 2016) show us \nthat (a) Big Tech companies have exclusive access to new forms  of \ninfluence made possible by the internet that can shift the thinking and \nbeha vior of vulnerable users by substantial  margins, and (b) Big Tech \ncompanies are in fact deploying these techniques nationwide in the US  \nand, quite possibly, in many other countries around the world . \n Before proceeding to the next section of this essay, I w ould be remiss \nif I did not point out that overwhelming evidence exists that confirms the \npolitical bias of the major tech companies, particularly Alphabet, \nAmazon, Apple, Facebook, Microsoft, and Oracle . Credible websites \nsuch as OpenSecrets.org show that  95% or more of donations from \nemployees at these  companies go to Democrat s (Oberhaus, 2020 ; c.f. \nKanter, 2018 ). A le aked video from Google recorded  days after Trump\u2019s \nwin in 2016 showed the company\u2019s  top executives pledging to make sure \nthat Trump would n ever win again  (Wakabayashi, 2018 ). In a leaked \nEPSTEIN  \n \n18 \n  2015 email, Eric Schmidt, then head of Google , offered to head the \ntechnical side of Hillary Clin ton\u2019s 2016 political campaign (Fernholz, \n2016 ), and Clinton\u2019s Chief Technology Officer, Stephanie Hannon, was \na former Google executive  (Fernholz & Pasick, 2015 ). Alphabet, \nGoogle\u2019s parent company, was Clinton\u2019s largest corporate donor  in 2016 \n(Dunn, 2016 ).  \n It is also worth noting that Google\u2019s relationship with the Democratic \nparty pre -dated Hilary Clinton. During Obama\u2019s second term : Obama\u2019s \nChief Technology Officer, Megan Smith, was a former vice president at \nGoogle, and former Google executives occupied high positions in the \ndepartments of State, Defense, Commerce, Education, Justice, Veterans \nAffairs , as well  as in the Patent Office and  the Federal Reserv e (Dayen, \n2016 ). While Obama was in office, Google representatives sat in on \nWhite House m eetings, on average, more than once a month \u2013 about 10 \ntimes as often as representatives from other companies (Chimiele wski, \n2016; Dayen, 2016 ). In January 2013, just days after Obama\u2019s  second \nterm began, his administration  shut down a DOJ investigation of Google \nwhich , in its December 2012 report, had recommended that the \ninvestigation be expanded . The repor t ended : \n \nStaff concludes that Google's conduct has resulted \u2013 and will \nresult \u2013 in real harm to consumers and to innovation in the \nonline search and advertising markets. Google has \nstrengthened its monopolies over search and search advertising \nthrough anticompetitive m eans, and has forestalled \ncompetitors\u2019  and would -be competitors\u2019  ability to challenge \nthose monopolies, and this will have lasting negative effects \non consumer welfare . (WSJ.com News Graphics, n.d. ).  \n \nPart Two: New and Remarkably Powerful Forms of  \nInflue nce Made Possible by the Internet  \n \n As I have noted elsewhere in some detail (Epstein, 2012 c), on \nJanuary 1, 2012, I received nine messages from Google saying that my \nmain website at the time \u2013 DrRobertEpstein.com \u2013 contained malware \nand that Google would block access to it until I eliminated the malware. \nI checked, and Google was indeed blocking access  to my website  on both \nits search engine and its browser ; somehow or other, it was also blocking \nTHE EVIDENCE  \n19 access on Apple\u2019s Safari browser and Mozilla\u2019s Firefox. I qu ickly \nlearned, as many people had before me, that Google had no customer \nservice department  (they do now , although it\u2019s minimal ), so when they \ncut you off, you we re on your own.  \n As a longtime coder, this event got me curious about Google in two \nways , and my curiosity continued to grow long after my website was \nfully reinstated (roughly 9  days after Google had first blocked access to \nit). First, I wondered why I had been  contacted by a private company and \nnot by government agency or nonprofit organization ; to put this another \nway, who made Google sheriff of the internet? S econd, I wondered  how \nGoogle could block access to a website through browsers it did not own, \nor, presumably, control.  I later learned that 90 percent of the donations \nto Mozilla, the nonpr ofit organization that created the Firefox browser, \ncame from Google. I also learned that Google was paying enormous \nannual sums to Apple to be the default brow ser on Apple\u2019s Safari \nbrowser. In 2022, that payment  was an  astounding $20 billion ( Nylen, \n2024 ). \n The first question, it turns out, was easy to answer. No authority on \nearth had made Google sheriff. The sheriffs of our towns and cities are \nempowered through elections or appointments. In Google\u2019s case, it \nsimply appointed itself . No authority ever  questioned this usurpation  of \npower because (a) Google, as the world\u2019s dominant search engine, \ncrawled  the internet fa r more aggressively than any  other entity, public \nor pri vate, so it made sense that it c ould identify dangerous websites \nmore quickly and more accurately than any other entity, and (b) Google, \nas usual, was providing this notification service free of charge. But, as \nmy uncle (and your uncle too, no doubt) used to say, \u201cThere\u2019s no such \nthing as a free lunch,\u201d and an increasing number of thought  leaders \nworldwide have noted over the years that Google \u2019s \u201cfree\u201d services are \nby no means free (Epstein, 2016 b; Lewis, 2017; Lohr 2021 ; Morrison, \n2022; Newton, 2018; Thompson , 2018 ). \n That Google notifies website owners when its crawlers find malware \nis fine, I suppose, but blocking access  to websites is another story. Note \nthat Google notifies website owners after  it has blocked access; why not \nsimply notify the website owner and give him or her an opportunity fix \nthe problem  without  blocking access ? Goog le isn\u2019t the kind of  sheriff \nwho knocks on your door to tell you you left the lights on on your car; \nthis is a sheriff who knocks on your door to let you know he or she  just \nEPSTEIN  \n \n20 \n  had your car towed . It\u2019s a sheriff who possesses and exercises \ntremendous power, a cting entirely on his or her  own authority. Blocking \naccess to websites \u2013 millions each day \u2013 is one of many examples of \nGoogle\u2019s extreme arrogance; they do what they please, because no laws \nor regulations a re in place to stop them ( Blodget 2012; Epstein, 2016 a; \nForbes, 2024; O\u2019Connor 2012; R omm, 2020 ). \n Regarding the second question, i t took me several years , but I \neventually answered it in some detail in an investigative article I wrote \nfor U.S. News & World Report  entitled \u201cThe New Censorship\u201d ( Epstein, \n2016 a, https://TheNewCensorship.com ). Google is able to block access \nto website s through non -Google browsers because those browsers \u2013 \nFirefox, Safari, and others \u2013 make use of Google\u2019s \u201cquarantine list\u201d to \ncheck the safety of a website before sending a user there. That is a list of \nwebsites Google wants to block, sometimes because they contain \nmalware, but sometimes simply because Google executives, employees, \nor algorithms find the content of those websites to be objectionable.  \nThe exact criteria Google uses for blocking access are se cret and \never-changing. The general catchprase the company uses to explain a \nblockage is \u201ca violation of our policies\u201d ( Google, n.d -a). Those policies \nare seldom defined in detail, but leaks from the company over the years \nhave revealed that human beings  often have considerable discretionary \nauthority to block sites. Especially illuminating is a \u201cproprietary and \nconfidential\u201d 161 -page manual called \u201cSearch Quality Rating Program: \nGeneral Guidelines\u201d that leaked from the company in 2012 (McGee , \n2012). It del ineates the criteria hu man raters should use in deciding  \nwhether websites should be demoted, deleted, or blocked. Instructions  \nlike the following examples appear 22 times in the manual:  \u201cUse your \nbest judgment. Do not struggle with your selection\u2026\u201d (p. 13) . \u201cDo not \nstruggle with each rating. Give your best rating and move on. If you are \nhaving trouble deciding between two ratin gs, please use the lower rating. \nSometimes, you may even have difficulty choosing among three ratings. \nWhen this happens, please use  your best judgment. \u201d (p. 32). \u201cSo as \nalways, please use your judgment\u201d (p. 74). \u201c Please do not struggle with \neach Page Quality rating. Just as you are advised to do in URL rating, \nplease give your  best rating and move on \u201d (p. 82).  Over time, both to cut \ncosts and boost speed, Google has used machine learning techniques to \ntrain algorithms to make website quality judgments similar to the ones \nTHE EVIDENCE  \n21 their human raters make; this sometimes  leads to outrageous mistakes \nthat Google has to step back manually ( Singel, 2011 ). \nThe scale at which  algorithms, employees, and outside consult ants \nare demoting , blocking,  or deleting content on Google Search, YouTube, \nand Google\u2019s other surveillance platforms is likely in the millions; the \ncompany has done an excellent job of hi ding the exact numbers. \nWhatever the details may be, the power  that Google has to rapidly \nsuppress content is enormous. Consider: In 2011, Matt Cutts, then head \nof the company\u2019s web spam team, authorized the blockage of an entire \ndomain of more than 11 mil lion websites, saying that the content o f those \nwebsites seemed \u201cspammy \u201d (Cutts, 2011; Lee 2011; Murphy, 2011 ). If \nthat incident just made you imagine \u2013 even for a second \u2013 that Google \nhas the power to shut down the entire internet \u2013 well, you are correct.  As \nreported by The Guardian , on Saturday, January 31, 2009, at 2:40 pm \nGMT , Google blocked access to the entire internet for 40 minutes  \n(Davies, 2009 ). Google attributed this event to \u201chuman error .\u201d In \u201cThe \nNew Censorship,\u201d I suggested  that this particula r time interval was \nchosen  for this \u201cerror\u201d to occur because it was one of the rare time \nperiods when every stock market on earth was closed; thus the prank \u2013 \nor test, or whatever it really was \u2013 could be conducted without seriously \ndisrupting the world ec onomy  and drawing t oo much attention  from \ngovernment officials ; indeed, this cyber  event was barely noticed . In \nAugust, 2017, Google \u201caccidentally\u201d blocked access to half the websites \nin Japan, impacting internet performance for hours , and this time, \naccording to journalist Mallory  Locklear, \u201ct he impact was so large that \nJapan \u2019s Internal Affairs and Communications Ministry initiated an \ninvestigation into the issue\u201d (Locklear, 2017) .  \n So Google can and does suppress content, and it can also boost \ncontent as well \u2013 in this case using  \u201cwhite lists\u201d instead of blacklists. For \npresent purposes, the question we need to answer is whether Google is \nsystematically manipulating content in the U S in ways that favor one \npolitical party over another. Earlier in this essay, I explained why design  \nflaws in recent published studies prevent them from shedding light on \nthis issue. In Part III of this paper, I will explain how my team and I have \nprogrammed around these flaws to develop a  large,  reliable system for \ncapturing and preserving the real ephe meral content that real users are \nreceiving from Google and other tech companies.  Although that issue is \nat the heart of the present essay, I can\u2019t present it meaningfully until we \nEPSTEIN  \n \n22 \n  first look at the basic research my associates and I have been conducting \nsince 2013 on new methods of influence that the internet has made \npossible. When we begin in Part Three to look at the real content users \nare viewing, we need to have in mind why such content is being sent. \nThat\u2019s where an understanding of these new methods  of influence \nbecomes essential.  \n In all, I will briefly describe 10 effects we have discovered, studied, \nquantified, and either published in peer -reviewed journals or are in the \nprocess of so doing. I will describe them roughly in the order in which \nwe be gan to study them. As far as I know, all these effects are \nfundamentally new; by that I mean that they were never possible (except \nin relatively trivial ways, perhaps) before the internet was created.  \n Other forms of influence are also present on the inte rnet, but they are \nmerely souped -up versions of old forms of influence that h ave existed \nfor decades or, in s ome cases, centuries. These other forms of influence \nare also inherently competitive, which means they are not a threat to \ndemocracy. When messages  and videos go viral on Facebook or TikTok, \nfor example, that is akin to what used to happen when shocking news \nstories spread rapidly on radio, television, or telephone networks. When \nconflicting messages spread rapidly in echo chambers among people in \ndifferent political groups, again, that is the internet variant on how \nconflicting messages used to spread rapidly among people belonging to \ndifferent political parties through newspapers, newsletters, phone calls, \nand so on.  \n Even though the high speed at which conflicting, competing \nmessages can spread these days on the internet poses new and serious \nchallenges to humanity \u2013 an issue explored disturbingly in the 2020 \nNetflix film, \u201cThe Social D ilemma,\u201d and in books such as  Ten \nArguments for Deleting Your S ocial Media Accounts Right Now  (Lanier, \n2018 ), Weapons of Math Destruction  (O\u2019Neil, 2016) , Zucked: Waking \nUp To the Facebook Catastrophe  (McNamee , 2019), and The Age of \nSurveillance Capitalism  (Zuboff , 2019) \u2013 the methods my associates and \nI have discovere d since 2013 are fundamentally different from  traditional \nforms of influence in five respects: First, they all are almost entirely \ninvisible to the people they affect . Other forms of invisible influence \nexist  and have been studied for decades  (Berger,  2016 ), but the internet \nhas created a new universe in which many especially powerful new types \nof invisible influence have been made possible. T hey are very different \nTHE EVIDENCE  \n23 from the online messages and advertisements users can see, just as they \ncan see billboards an d television commercials .  \n Second, these new methods of influence are controlled almost \nexclusively by a handful of Big Tech companies; I exaggerate only \nslightly when I say that no one else can use them  (I will note one \nexception later in this essay).   \n Third, and because of the two characteristics I just mentioned, \npeople, organizations, or governments that might disagree with the \ninfluence that is being exerted have no way to counteract that influence . \nIf your political party buys a television commercia l, my political party \ncan see what you are doing and can counteract your influence with its \nown commercials. But if a major online platform wants to shift opinions \nor voting preferences in some way, there is nothing you can do about it \n\u2013 assuming, that is,  that you somehow become  aware of what they are \ndoing.  \n Fourth, people are constantly being subjected to these new forms of \ninfluence because for all practical purposes they have nowhere else to \ngo. People who can\u2019t afford iPhones buy Android phones, whic h means \nthey are sharing all their phone activity \u2013 whether they are online or \noffline \u2013 with Google. As I mentioned earlier, those who can afford \niPhones are still being tracked by Google, thanks to the enormous fee \nGoogle pays to Apple each year (Nylen, 2024; cf Schmidt, 2018 ). \nPractically every possible domain of influence on the inte rnet is \ndominated by a monopoly;  this has been increasi ngly the case as tech \ncompanies like Google  have stepped out of their niche into other \ndomains (e.g. search engine, so cial media, smartphone) to corral users \ninto what some experts called a \u201cwalled garden\u201d  (Bure ll, 2019; Zaiceva, \n2022; Jaita in, 2022 ). People have tried to set up competitive platforms \nthat preserve people\u2019s privacy and that don\u2019t deliberately manipulate \nusers, but most have failed, and the few survivors have remained small \nand inconsequential ( Pegoraro, 2022; Waikar , 2021 ). \n Fifth, and finally, the Big Tech monopolies generally lean the same \nway politically, with 95% of political don ations from the six bigg est tech \nfirms (Oberhaus, 2020; cf. Kanter 2018) \u2013 Alphabet, Amazon, Apple, \nFacebook, Microsoft, and Oracle \u2013 and 99% of political donations from \nTwitter going to Democrats in 2020  (Smith, 2020). A ccording to the \nFEC, for political donations above $200, le ss than 5% of employees from \nFacebook and Google donated to Trump (Oberhaus, 2020).  According \nEPSTEIN  \n \n24 \n  to OpenSecrets.org, in 2020, Alphabet, Microsoft, Amazon, Facebook, \nand Apple  were five of the largest donors of any type  to Joe Biden\u2019s \ncampaign. If the effects we have discovered are additive across platforms \n(see the section on the Multiple P latforms Effect  below), then it i s not \njust a single company that poses a potential threat to democracy and \nhuman autonomy; it i s an entire industry.  \n Here are the 10 new fo rms of influence my associates and I have been \ndiscovering, studying, and quantifying, beginning with SEME in 2013:  \n \n  \nTHE EVIDENCE  \n25 1. The Search Engine Manipulation Effect (SEME)  \nhttps://SearchEngineManipulationEffect.com   \n \n Early SEME research . As I noted above, o n January 1st, 2012, I \nreceived nine emails fro m Google info rming me that my website had \nbeen hacked, that it now contained malware, and that Google was \nblocking access to it. That event got me looking at Google with a critical \neye. I wondered why I had been notified about this infection by a private \ncompany, for one thing. Why hadn\u2019t I been contacted by a government \nagency or a nonprofit organization ? And as a longtime programmer, I \nwas curious about how Google was blocking access to my website \n(actually, to mul tiple websit es of mine) not only through it s own \nplatforms, such as Google s earch and the Chrome browser, but also \nthrough Firefox, a browser created by the nonprofit Mozilla Foundation , \nand even through Safari, Apple\u2019s browser.  \n Toward the end of 2012, my  increasing concerns about the company \nprompted m e to publish a four -part essay i n The Huffington Post  calling \nfor Google\u2019s regulation ( Epstein , 2012b , 2012c , 2012d , 2012e ; cf. \nEpstein, 201 2a). I also found myself getting interested in new research, \nmainly  in the marketing field, showing the power that high -ranking \nsearch results on G oogle had to draw clicks ( see Dean, 2023 ). That \nresearch led me to ask a simple question: Could bias in high -ranking \nsearch results alter the thinking of people who were undecided on some \ntopic \u2013 perhaps even alter the voting preferences of undecided voters?  \n In January 2013, with the assistance of Ronald Robertson, a member \nof AIBRT\u2019s staff, I conducted a randomized, controlled experiment to \nsee whether biased search results could shift the voting preferences of \nundecided voters. The sample size was small (n = 102), but the \ndemographic characteristics of the sample were similar in most respects \nto those of the American voting population. Participants were randomly \nassigned to one of  three groups \u2013 one seeing search results favoring one \npolitical candidate, one seeing search results favoring the opponent, and \nthe third seeing a mix of search results (the control group). The \nexperiment was conducted in a storefront setting, with partic ipants \nrecruited using newspaper and online advertisements. It was conducted \non a Google Search simulator we called Kadoodle, which work ed like \nGoogle does (Figure 2). Moreover, the search results were real \u2013 all \nsourced from Google \u2013 and so were the web p ages to which the search \nEPSTEIN  \n \n26 \n  results linked. The candidates were Tony Abbott and Julia Gillard , who \nran for Prime Minister of Australia in 2010. We had chosen  this election \nto guarantee that our participants would be undecided.  Indeed, when we \nasked our partic ipants to indicate how familiar they were with each \ncandidate ; the average familiarity level was 1.3 on a scale from 1 to 10, \nwhere 1 was labeled \u201cUnfamiliar\u201d and 10 was labeled \u201cFamiliar.\u201d   \n \n \nFigure 2. Kadoodle: the mock search engine we created to \nrepli cate the search engine user experience.  \n \nBefore allowing our participants to conduct research on the \ncandidates using Kadoodle, we showed them brief, balanced decriptions \nof each candidate ( Figure S1 in Supplementary Materials ), and we then \nasked them eight questions \u2013 six in which they could express opinions \nabout each candidate, and two in which they could express their voting \npreferences, either by indicating their preference on a scale or by \nanswering a forced -choice question (\u201cIf you had to vote right now, which \ncandidate would you vote for ?\u201d) (Figure S2). As we predicted, at this \n\nTHE EVIDENCE  \n27 point, both opinions and voting preferences for participants in all three \ngroups were roughly evenly split between the two candidates; in other \nwords, none of groups show ed any preference for one candid ate over the \nother.  People in all three groups were then given up to 15 minutes to \nconduct research on the candidates using Kadoodle, which gave them \naccess to a total of 30 search results listed on five pages of search results \n(Figure 3 ). After they compl eted their search, we again asked those eight \nquestions to see whether opinions or voting preferences were impacted \nby the bias in the search results. Then, finally, we asked whether \nanything about the content participants saw had \u201cbothered\u201d them. This \nwas our way of determining whether people noticed the bias in the search \nresults; we couldn\u2019t ask directly about \u201cbias\u201d be cause leading questions \nof that sort have long been known to inflate measures artificially (Loftus, \n1975 ; Gous & Wheatcroft, 2020 ). \n I predicted that we could shift people\u2019s voting preferences in the two \nbias groups by 2 or 3 percent. That might not sound like much, but it can \nmake  all the difference in close elections, and that 2010 election in \nAustralia was in fact won by a margin of only  0.24%. \n But I was mistaken. Post search, the overall shift in the two bias \ngroups combined was an astounding (48.4%), which I immediately \nassumed was incorrect \u2013 a result of some procedural  or statistical error \non our part, perhaps. That shift we ultimatel y labeled \u201cvote manipulation \npower\u201d (VMP), which we define as the post -manipulation percentage \nincrease in the number of people voting for the candidate favored in the \nmanipulation. This measure, we felt, would be the main metric of interest \nto campaign pr ofessionals, and I continued to use this measure (or some \nclose variant of it) in experiments I conducted over the next 12 years.  \n In this first experiment, w e also noticed that 75% of the participants \nin our two bias groups apparently saw no bias in th e search results we \nhad shown them. We subsequently  repeat ed the experiment with another \ngroup  (n = 102 ), this time slightly masking the bias in the search results \nto see whether we could reduce the proportion of people (25%) who \nnoticed the bias. Specifically, in high -ranking search results, all of which \nfavored one candidate, in the fourth position  we introduced one result \nfavoring the opposing candidate (Figure 3d). \n \nEPSTEIN  \n \n28 \n  \n \nFigure 3. Search rankin gs for the three experiments . (A), For subjects in \nGroup 1 of Experiment 1, 30 search results that linked to 30 corresponding \nweb pages were ranked in an order that fa vored candidate Julia Gillard. (B) , \nFor subjects in Group 2 of Experiment 1, the search results were displayed \nin precisely the opposite order so that they favored the opposing cand idate, \nTony Abbott. (C) , For subjects in Group 3 of Experiment 1 (the control \ngroup), the rank ing favored neither candidate. (D) , For subjects in Groups 1 \nand 2 of Experiment 2, the rankings bias was mas ked slightly by swapping \nresults that had originally appeared in positions 4 and 27. Thus, on the first \npage of search results, five of the six results \u2013 all but the one in the 4th \nposition \u2013 favored one candidate. (E) , For subjects in Groups 1 and 2 of \nExperiment 3, a more aggressive mask was employed by swapping results \nthat had originally appeared in positions 3 and 28.  \n \n In this second experiment, the proportion of people who apparently \nfailed to notice the bias in the search results increased to 85%, and the  \nVMP  increased to 63.3% . In a third experiment with another group  (n = \n102), we used a more aggressive mask \u2013 inserting a search result favoring \nthe opposing candidate in the third position of search results (Figure 3). \nThe VMP  was 36.7% , and this ti me not a single participant appeared to \nnotice  any bias in the search results.   \n In all three of these small -n experiments, we had shown that biased \nsearch results could produce remarkably large and predictable shifts in \nboth voting preferences and opinion s. Perhaps even more notable, w e \nalso showed  that a simple masking procedure could hide the nature of \nTHE EVIDENCE  \n29 our manipulation; presumably, more so phisticated masking procedures \nwould do an even better job.  \n In a fourth experiment, we replicated the effect again, this time with \nan online nationwide experiment with registered voters in all 50 US \nstates ( n = 2,100 ). We employed the same aggressive masking procedure \nwe used in the previous experiment (a position -3 swap), and we again \nfound a large VMP ( 37.1%) and pred ictable shifts in opini ons.3 Because \nthe sample was so large, we could now look for demographic effects, \nand, indeed, different demographic groups showed very different levels \nof vulnerability to the manipulation; the most vulnerable group we found \nin this  sample was \u201cmoderate Republicans,\u201d who achieved an \nastonishing VMP of 80.0% \u2013 the highest VMP we have ever detected in \nmore than a decade of SEME experiments.  \n Even more disturbing in this experiment, however, was what we \nlearned about a small number of i ndividuals ( n = 120) who appeared to \nnotice bias in the biased search results we showed them. One would think \nthat noticing the bias would protect so meone from its effect, but that i s \nnot what we found. The VMP for people  in the bias groups  who noticed \nthe bias (45.0%) was higher than the VMP for people in the bias groups \nwho did not notice bias (36.3% ). We have found this pattern repeated ly \nin subsequent research.4 It most likely occurs because people mistakenly \nbelieve that computer output is inherently o bjective and impartial  \n(Agudo & Matute 2021, Bogert, et al., 2021, Logg et al., 2018; c.f . \nHoward et al., 2020) . As we will explain below, it is also possible that \noperant conditioning plays a role in the impact that high -ranking search \nresults have on use rs. \n The four experiments described above relied on search results and \nweb pages from a past election, which raised an obvious question: Would \nwe find significant VMPs if we looked that the effect of biased search \nresults on real voters in the middle of a real election campaign? For our \nfifth experiment, we used multiple methods (including newspaper \nadvertisements) to recruit more than 2,000 voters from throughout India \nwho had not yet voted and who were still undecided \u2013 this during the \nweeks leading up to  their 2014 national Lok Sabha election, as well as \nduring the 36-day period during which people were allowed to cast votes. \nOver the internet, we used the same methodology we had employed in \nthe first four experiments to see if we could shift opinions and  voting \npreferences toward any of three main candidates running for Prime \nEPSTEIN  \n \n30 \n  Minister. Overall, we found that we could shift undecided voters by 20% \nor more, with VMPs exceeding 60% in some demographic groups.  \n We published these five experiments in 2015 in the Proceedings of \nthe National Academy of Sciences  (PNAS) (Epstein & Robertson, 2015).  \nThe paper included a model showing how bias in search results and the \nresulting possible increase in support for one political candidate cou ld \ninteract synergistically to produce a rapid increase in support for that \ncandidate; that model is replicated in the Supplementary Materials  of the \npresent paper  (Figure S3). That paper also included a table that allows \none to determine \u2013 given both the proportion of undecided inte rnet voters \nin the population and the projected win margin \u2013 whether a particular \nupcoming election can be flipped by biased se arch results; again, that \ncontent is included in the Supplementary Materials  of the present paper  \n(Figure S4) \n The possible role that operant conditioning plays in SEME . Why  is \nSEME such a large effect? It occurred to my associates and I that operant \nconditioning might play a role. 86% of the searches people conduct on a \nsearch engine are for simple facts, such as what is the capita l of \nKentucky?  (Quan, 2024 ). Invariably \u2013 at least on Google \u2013 the correct \nanswer turns up in the first position, with those immediately below it also \nusually containing similar information. As a result, people rapidly learn \nto trust what is at the top of the list. Just like the rat in the Skinner box, \npeople are learning to attend to and click on the highest -ranking search \nresults, because that attention and those clicks are frequently reinforced \nby access to the answer one is seeking. Over time, people le arn to pay \nlittle attention to low -ranking search results \u2013 and especially to search \nresults on pages past the first page of search results \u2013 with 50% of all \nclicks made to just the top two search results ( Dean, 2023 ).  \n In 2024, Vanessa Zankich, Michael L othringer, and I published a \nreport in which we showed evidence that operant conditioning might \nindeed be taking place \u2013 and that it might also explain why SEME is so \nlarge  (Epstein et al., 2024 b). Before experiencing a typical SEME \nexperiment (such as tho se described above), two groups of participants \nwere subjected to different training regimens. In the High -Trust group, \npeople were asked to find factual answers to simple questions using \nKadoodle, and they also found those answers in the highest -ranking \nsearch result. In the Low -Trust group, people asked those same \nquestions, but the correct answer could turn up anywhere in the search \nTHE EVIDENCE  \n31 results except  in the first two positions. In the Low -Trust group, we had \npresumably broken the trust people normally place d in high -ranking \nsearch results \u2013 at least for our own search engine. Each group then \nexperienced a typical SEME procedure \u2013 the same procedure for each \ngroup \u2013 and the VMP for the High -Trust group turned out to be slightly \nmore than twice as large (34.6% ) as the VMP for the Low -Trust group \n(17.1%). When that trust was broken, high -ranking search results \nfavoring one political candidate had less effect. The difference in those \nVMPs suggested that operant conditioning \u2013 in this case, the recent \nreinforcemen t histories of the participants \u2013 indeed plays a role in the \nmagnitude of SEME. SEME is, in effect, a list effect with a difference: \nit is supported by a daily regimen of condition \u2013 a regimen that never \nstops. Because those high -ranking positions are so v alued, when \n(roughly 14% of the time) someone asks an open -ended question of some \nsort \u2013 What\u2019s the best way to lose weight ? Is Donald Trump honest? \nWhat are the best movies of 2024?  \u2013 they tend to trust and click on the \nhigh-ranking search results, and th at is why biased search results can be \nused to shift the voting preferences of undecided voters so easily.  \n Could biased search results shift opinions about anything at all?  For \nmost of a decade, the SEME experiments my team and I conducted \nfocused on elec tions. In 2024, however, Ji Li and I published a SEME \nreplication in which we showed that biased search results could be used \nto shift opinions about three very different topics \u2013 fracking, sexual \norientation, and artificial intelligence. Since we were not  shifting voting \npreferences, we shorte ned VMP to \u201cMP\u201d \u2013 manipulation power \u2013 and \nthe MPs varied between 17.8% and 30.9%.  The three experiments we \ndescribed in this study led us to a startling conclusion \u2013 one that had \nnever crossed my mind in more than a deca de of SEME research: \u201cIf our \nfindings prove to be robust, we are exposing what might be considered \nan unforeseen consequence of the creation of search engines, namely that \neven without human interference, search algorithms will inevitably alter \nthe thinkin g and behavior of billions of people worldwide on perhaps \nany topic for which they have  not yet formed strong opinions\u201d (Epstein \n& Li, 2024).  \n If you only skimmed over the previous sentence, please read it again. \nThe experiments we conducted on \u201cmultiple t opics\u201d have profound  \nimplications. They suggest that right now, search results are shifting the \nthinking and behavior of billions of people worldwide without their \nEPSTEIN  \n \n32 \n  knowledge \u2013 sometimes in small ways and sometimes dramatically . This \nis because search engin es, by definition, have no equal -time rules built \ninto them. They are designed to filter  and order  \u2013 always selecting a \nsmall number of links from among millions and always displaying one \nof those links at the top of the list. In other words, search engine s are \ndesigned  to be biased; we wouldn\u2019t want them any other way. We want \nto know what is correct and what is best; a random ordering of search \nresults (like we sometimes display in our experiments) would be useless.  \n A second  SEME model . Earlier I mention ed a SEME model that was \nincluded in the Supplementary Materials of the 2015 PNAS  paper  \n(Figure S2). The model showed a synergistic relationship that might exist \nbetween two search -engine -related effects: the increase in voter \npreferences that might be pro duced by biased search results, and the \nimpact that voter preferences might have on search rankings; stronger \nvoter preferences will presumably boost the rankings of the preferred \ncandidate.  In theory, this possible synergy would allow Google to show \npeopl e only subtly biased search results \u2013 in other words, search results \nin which the bias has been heavily masked \u2013 and yet still produce a \ndramatic shift in the voting preferences of undecided voters. To my \nknowledge, no whistleblower or leaks have turned up  so far to confirm \nthat the company deliberately engineers this kind of synergy, but it \nalmost certainly exists. Bias i n search results can boost support for one \ncandidate, and that increase can in turn elevate that candidate in search \nrankings ; that feedb ack loop can produce a powerful synergistic effect.  \n I have since developed  another  iterative model that uses only modest \nassumptions  to estimate how biased search results might gradually shift \nundecided voters toward one party or another. It assumes that people \nonly conduct one search per week for the 26 weeks preceeding an \nelection , and  I assume that the search results are only mostly  liberally \nbiased. On any given week, they  shift one -third of users to the political \nright and two -thirds to the political left. In all, they shift only a small \nproportion  of undecided users each week  \u2013 a far small er proportion than \nSEME experiments suggest might be possible . The biased search results \nin the model  do not have any impact o n people who have already shifted  \nleft or right; when you read further and learn about the \u201cmultiple \nexposure effect,\u201d you will learn that that is an overly optimistic \nassumption. W e begin with 900,000 people, an  equal mix of \nconservatives, liberals, and people who are undecided. Over the 6 -month \nTHE EVIDENCE  \n33 period, the liberals end up with a margin of 120,000 votes over the \nconservatives (Fig ure 4). \n  \nFigure. 4. Hypothetical impact of biased search results on voter \npreferences over a 6 -month period.  The curves above are generated using a \nsimple iterative model and making only modest assumptions. We begin  with \n900,000 individuals  equally split among people who are undecided, leaning left \n(liberals), and leaning right (conservatives). The model assumes that each of \nthese people conducts only one search per w eek on a search engine  that shows \nsearch results that are somewhat liberally biased. That bias shifts only a small \npercentage of users to the political left and  a smaller percentage  of users to the \npolitical right each week. At the end of 26 weeks, 180,000  people have shifted \nto the left, giving liberals a total of 480,000 supporters, and 60,000 people have \nshifted to the right, giving conservatives a total of 360,000 supporters. In \ncontrolled experiments, a single search on a Google simulator can actually \nproduce a shift among undecided voters of between 20% and 80% (see text on \nSEME  above ), and repeated exposure to similarly biased content on a search \nengine can produce increasingl y larger shifts (see text on the multiple exposure \neffect below ). \n \n\nEPSTEIN  \n \n34 \n   \n \n \n2. The Search Suggestion Effect (SSE)  \nhttps://SearchSuggestionEffect.com   \n \n In June 2016, a small news organization called SourceFed released a \n7-min on YouTube reporting that Google was, for some unknown \nreason, giving only positive search suggestions (those phrases they flash \nat you while you\u2019re  typing a search term) for Presidential candidate \nHillary Clinton, while allowing positive, neutral, and negative search \nsuggestions to appear for Donald Trump and others  (Flores, 2016 ). They \nalso reported that the Yahoo and Bing search engines displayed m ainly \nnegative search suggestions for Clinton, such as \u201cClinton is the devil\u201d \nand \u201cClinton is evil.\u201d All Google would show was \u201cHillary Clinton is \nwinning\u201d and \u201cHillary Clinton is awesome\u201d ( Figure 5). SourceFed also \nreported that Google Trends, which still  shows data about the searches \npeople are conducting on Goog le worldwide, showed that people were \nactually searching for the negative search suggestions that Bi ng and \nYahoo displayed about Clinton; whereas virtually no one was searching \nfor \u201cHillary Clinton  is winning\u201d and \u201cHillary Clinton is awesome.\u201d  So \nmuch for Google\u2019s claim that their search results were derived from \npeople\u2019s s earch behavio r (Google Help Center, n.d. ).  \n \n \nFigure 5 . SourceFeed\u2019s screenshot of Hilary Clinton\u2019s search result on \nGoogle, Ya hoo, and Bing .  \n \n\nTHE EVIDENCE  \n35  In any case, when the video reached  a million views on YouTube, \nYouTube blocked access to it, and that block remains in place at this \nwriting (August 26 , 2024). SourceFed quickly posted a 3 -min version of \nthe video on Facebook; that versi on is still available and, at this writing, \nit has been viewed more than 25 million time s.5   \n Those videos got me curious enough to start conducting a series of \nexperiments that summer \u2013 four relatively simple experiments that taught \nme a bout the radically d ifferent rates at which different search terms \nattract clicks, followed by another SEME replication. The first four \nexperiments revealed that negative search suggestions \u2013 that is, \nsuggestions containing what linguists call \u201clow -valence\u201d works (such as \n\u201csuicide\u201d and \u201cdeath\u201d) often attract 10 to 20 times as many clicks as \nneutral or positive suggestions. This phenomenon exemplies what social \nscientists in multiple fields call \u201cnegativity bias.\u201d For sound evolutionary \nreasons, a stimulus that might be a threa t attracts more attention and \nimpacts memory more than a neutral or positive stimulus does. So one \nof the simplest ways for a search engine to support a candidate (or a cause \nor company) is to suppress negative search suggestions for that \ncandidate. When n egative suggestions are shown for the opposing \ncandidate, those suggestions will attract an enormous number of clicks, \nwhich will then generate negative search results, which will then lead \npeople to web pages making the opposing candidate look bad. Those first \nexperiments also revealed the optimal number of search suggestions one \nshould show to maximize control over someone\u2019s search; that number is \n4, which is exactly how many search suggestions Google typically \nshowed people from 2010 until October 2017  (Schwartz, 2010, 2017 ).  \nThat\u2019s when I went public with some of the preliminary findings from \nmy SSE experiments. From then on, Google began showing 10 search \nsuggestions again  on la ptops and desktops , as it had before 2010.  \n In the fifth experiment, most participants were first asked to click on \nsearch suggestions before search results appeared. People were randomly \nassigned to one of four groups: one in which  there were no search \nsuggestions, one in which  all four search suggestions were positive, one \nin which all four were negative,  and one in which one suggestion was \nnegative and the other three were positive. As you might expect, the \nVMPs in the first and second groups were high, positive, and roughly \nequal. In the third group, the magnitude of the VMP was about the same \nas it was in the first two groups, but the direction  changed: voting \nEPSTEIN  \n \n36 \n  preferences turned away  from the \u201cfavored\u201d candidate (since all the \nsearch suggestions for that candidate were negative). And in the fourth \ngroup, the VMP was near zero, because that one negative s earch \nsuggestion drew about as many clicks as the three positive suggestions \ndid combined.  \n All told, we showed in Experiment 5 that the differential  suppression \nof negative search suggestions could turn a 50/50 split among undecided \nvoters into an astonis hing 90/10 split  (Epstein et al., 2024a ). That little \nanecdotal news  item that SourceFed pointed the way to a much bigger \nnews story \u2013 important from both a scientific perspective and a political \nperspective.  \n \n3. The Answer Bot Effect (ABE)  \nhttps://AnswerBotEffect.com   \n \n In 2014, Goog le starting posting \u201cfeatured snippets\u201d \u2013 boxes that \nsometimes answered your question directly \u2013 above search results  \n(Sullivan, 2018 ). For convenience, I\u2019ll call them \u201canswer boxes,\u201d which \nI consider to be  a relatively benign  instance  of a much larger cat egory of \ndevices I call \u201canswer bots.\u201d In 2022, my associates and I published a \nstudy that described three experiments that quantified the power of \nanswer bots to shift opinions and voting preferences, yet again using real \ndata from an election in Australi a (Epstein et al., 2022).  \n Experiments 1 and 2 showed that biased answer boxes on Ka doodle, \nour Google simulator, c ould shift voting preferences by as much as \n38.6%, even when the search results below the biased answer box were \nthemselves unbiased . The pr esence of an answer box also suppressed  \nsearch; that is, it often reduced both search time and clicks to search \nresults.  These findings serve as glimpses of what\u2019s to come: a world in \nwhich various devices simply give us answers, and search engines \u2013 if \nthey continue to exist at all \u2013 are used only by diehard research fanatics.  \n The findings from the first two experiments prompted us very rapidly \nto develop a simulator of the intelligent personal assistant known as \n\u201cAlexa,\u201d developed and controlled by the A mazon company. Our \nsimulator uses Alexa\u2019s voice, and it can answer a question that \nparticipants in our experiments ask by clicking that question \u2013 one on a \nlist of several or many questions. All three of these experiments \nemployed the same general design t hat we used in early SEME \nTHE EVIDENCE  \n37 experiments: We g ave instructions, ask ed demographic questions, g ave \nbasic information about two political candidates, and then ask ed eight \npre-manipulation questions \u2013 six opinion questions and two voting -\npreference questions.  Then p eople were  assigned at random to various \n\u201ctreatment\u201d gro ups, and the manipulation beg an. When the manipulation \nwas over, people were asked those eight questions again, and we look ed \nfor shifts. Finally \u2013 in an attempt to detect people\u2019s awareness of bias in \nour content \u2013 we asked people whether anything bothered them about the \ncontent and gave them an opportunity to elucidate.  \n In Experiment  3, whi ch was randomized, controlled, \ncounterbalanced, and double -blind, we found that a single question -and-\nanswer seque nce that favored one candidate could produce a VMP over \n40.0%, and that multiple question -and-answer sequences that favored \none candidate could produce a VMP over 65.0%.  \n Given the direction that the digital world is racing toward right n ow \n\u2013 toward answer bots , most of which will soon be controlled by \nconversational AIs like ChatGPT -4o (Cheng, 2023; Heaven, 2021;  \nStoker -Walker & Nature Magazine 2023 ) \u2013 the results of our answer bot \nexperiments are alarming. With the new generative convers ational AIs \nbeing integrated as I write this into Apple\u2019s Siri, the Windows operating \nsystem , and hundreds of other applications that we can barely live \nwithout, our experiments suggest that these new entities have, or shortly \nwill have, the power to shif t our thinking on any issues we are unsure \nabout. These new entitles also will increase the surveillance capacities of \nthe companies that control them, and, as physicist Steven Hawking \nwarned  (Hawking, 2018 ), they will also soon have the ability to control  \nreal-time systems of virtually any sort, among them weapon systems, \nfinancial systems, and communication systems.  \n \n4. Targeted Messaging Effect (TME)  \nhttps://TargetedMessagingEffect.com   \n \n In 2023, my associates and I published a quantification of what we \ncalled the targed messaging effect ( TME), which we defined as \u201c the \ndifferential impact of sending a consequential message, such as a link to \na damning news story about a political candidate, to members of just one \ndemographic group, such as a group of undecided voters \u201d (Epstein et al., \n2023 ). In plain language, we wanted to see what would happen if large \nEPSTEIN  \n \n38 \n  online platforms, such as Google, Facebook, Instagram, or TikTok, sent \nbiased messages to one or more segments of their user populations. The \nbest documented case of such an event has been pr ovided to us by \nemployees of Meta (when it was called Facebook) working with faculty \nmembers at the University of California San Diego. In a paper published \nin Nature in 2012, these researchers reported the results of an experiment \nthey carried out  with Facebook users \u2013 entirely without their knowledge \nor consent \u2013 that showed that \u201cgo -vote\u201d reminders they sent to 61 million \nof those users on Election Day in 2010 (the US midterm elections)  \nresulted in an additional 340,000 people voting who otherwis e would \nhave stayed home ( Bond et al., 2012 ).  \n We consider such a message to be targeted because it was sent to \nonly a subset of Facebook users. Two years later, Harvard legal scholar \nJonathan Zittrain published an article in The New Republic  worrying \nabout the possibility that Big Tech platforms like Facebook might send \nout such messages to members of just one political party \u2013 a \nmanipulation that no one would be able to detect and that could easily \ndetermine outcome of a close election (Zittrain, 2014). Th is kind of \nmanipulation is of especially great concern because the messages are \nephemeral, which means they leave no paper trail for authorities to trace.  \n In 2023, my associates and I published a study that summarized the \nresults of a series of four exper iments that quantified the possible impact \nof targeted messages under controlled conditions.  All four experiments \nwere conducted on our Twitter simulator, called Twiddler (Figure S5). \nA total of more than 2,100 eligible US voters participated in the \nexperi ments, and this time we used content from the 2019 election for \nthe Prime Minister of Australia. In all four of these experiments \nparticipants were instructed to scroll through a sequence of between 30 \nand 35 tweets looking for evidence that one candidate or the other would \ndo a better job of protecting the security of Australia. In fact, the 30 \norganic tweets in this sequence (which appeared to come from regular \nTwitter users) did not favor either candidate in this respect. This task \ncould be considered a distractor task ( W\u00f6stmann et al., 2022 ). In some \nexperiments, some participants were randomly assigned  to groups  in \nwhich the 30 organic tweets in the Twitter feed were occasionally \ninterrupted by tweets that appeared to come directly from the company. \nThese were  designed to resemble real tweets that Twitter (now \u201cX\u201d) \ncommonly sen t to users with labels such as \u201c Breaking News  \u2013 Twitter \nTHE EVIDENCE  \n39 Alert .\u201d These special tweets always contained information that made one \ncandidate or the other look especially good (e.g., \u201c  Scott  Morrison \nawarded an honorary doctorate from the University of Melbourne, in \nrecognition for his humanitarian efforts during the Australian \nwildfires.\u201d ) or bad (e.g., \u201c Bill Shorten charged with driving under the \ninfluenc e while vacationing in Adelaide .\u201d). Up to five of these tweets \ncould be inserted among the 30.  \n We again employed the pre/post manipulation design we have \ndescribed above. We found,  among other things, that compli mentary \ntweets produced only small shifts in voting preferences, if any, whereas \nderogatory tweets produced VMPs as high as 87.0%, with only 2.1% of \nthe participants in bias groups showing awareness that they were viewing \nbiased content.  \n Do Big Tech companies actually send targeted messages to the \nmembers of just one political party? As you  will see in Part III of this \nessay, when you monitor the personalized Big Tech content being sent \nto a large, politically -balanced group of registered voters, the answer \nturns out to be  yes.  \n \n5. Differential  Demographics Effect  (DDE)  \nhttps://DifferentialDemographicsEffect.com   \n \n On Election Day in  2018 (midterms elections in the US), Google \nattracted high praise from jour nalists for posting a colorful Go-Vote  \nreminder on its home pag e (Figure S6 .), which is likely seen more than \n500 million times a day in the US ( Text S1). Because of the research  I \nwas conducting on new forms of manipulation that the internet had made \npossible, it occurred to me immediately that these go -vote reminders \nwere actually vote manipul ations. But how did this work, and how could \nI quantify the manipulation?  \n I realized i mmediately that there was a serious analytical problem \nhere. Commentators at the time automatically assumed that the go -vote \nreminders were being sent by Google to all of its users. Although my \nteam and I had launched our second small monitoring system bef ore the \n2018 election (see Section Three below), we were not recording images \nor text on Google\u2019s home page. Unless one is collecting such data \u2013 or \nunless secrets are leaked from the company \u2013 one has no way of knowing \nwhether Google was indeed reminding all of its users or just a subset. In \nEPSTEIN  \n \n40 \n  the latter case, sending such reminders in a partisan fashion \u2013 say, just to \nDemocrats \u2013 might give them hundreds of th ousands of extra votes that \nday. Facebook\u2019s 2012 study ( Bond et al., 2012 ) and my own research on \nTME (see above)  had shown that.  But what, I wondered, would happen \nif Google really did send the prompt to all of its users?  \n Because I had no w ay of detecting targeted messag ing that day, I wa s \nnow asking what was truly an academic question,  but one I found to be \nintrigu ing. Could a search engine or social media platform manipulate \nthe outcome of an election by sending exactly the same content to all of \nits users? In effect, I was giving Google the benefit of the doubt \u2013 \nassuming that it had indeed messaged everyone while  still wondering \nwhether it was somehow manipulating votes.  \n Without partisan targeting, I realized that a universal message would \nhave no net effect only if the population in question was homogeneous \nin most respects. Was the adult population of Google us ers in the US \nhomogeneous? Google itse lf kept such information secret  (why, I \nwondered?), but various researchers had conducted surveys suggesting \nthat Google\u2019s user base actually leaned left politically ( Lee, 2013 ). In \nJanuary , 2019, I published an opini on piece containing the best estimates \nI could find of the demographic characteristics of Google\u2019 s users, and I \nthen showed in detail how a data analyst at Google might have calculated \nthe consequences of sending a universal go -vote reminder. The bottom \nline: Democrats would have benefitted, with 49,500 more Democrats \ngoing to the polls than Republicans (Epstein, 2019 a). I have reprinted \nmost of this article in the Supplementary Materials so the re ader can view \nthe calculations (Text S2). I called this outc ome an example of the \n\u201cdifferential demographics effect\u201d (DDE).  \n Since that time, my staff and I have investigated DDE by \nsystematically reanalyzing datasets from a number of our controlled \nexperiments on online manipulation , and w e recently presented our  \nmethodology and analyses at a scientific meeting ( Epstein et al., 2024 e), \nwhere we characterized DDE as follows:  \n \nIn its simplest form, DDE occurs when the same potentially \nconsequential content is sent to a large body of users which \ncontains two subgroup s of different demographic \ncharacteristics. If it is known in advance that (a) each subgroup \nwill respond differently to the content, and (b) the subgroups \nexist in known but different proportions within the population, \nTHE EVIDENCE  \n41 then sending that content to all mem bers of the population will \nproduce a predictable margin of difference between the \nsubgroups.  \n  \n6. Video Manipulation Effect  (VME)  \nhttps://VideoManipulationEffect.com   \n \n One of the most troublesome forms of ephemeral content my \nassociates and I have studied over the years are the recommendati ons \nYouTube makes when someone types a search term into YouTube\u2019s \nsearch bar. (Trivia buffs might be interested in knowing that YouTube is \nthe second largest search engine in the world \u2013 second in size only to t he \nGoogle search engine itsel f (Ever s, 2020 ). Like Google search \nsuggestions and search results, those YouTube recommendations appear, \nimpact the user, and then disappear forever, leaving no paper trail. The \nrecommendation at the top of that list \u2013 the so -alled \u201cup -next\u201d video , is \nespecially  troublesome, because if the user does nothing after watching \na video, the up -next video \u2013 chosen by Google \u2013 plays automatically, and \nthe process continues indefinitely, sometimes leading users to watch \nhundreds of related videos in a row ( Solsman, 2018 ).  \n YouTube video sequences of this sort have been tied at times to cases \nof radi calization \u2013 even to vio lence ( Lewis, 2018; Matamoros -Fern\u00e1ndez \net al., 2021 ). The power of what we now call the \u201cvideo manipulation \neffect\u201d (VME) cannot be overstated, partly because videos, in general, \nimpact people\u2019s thinking more than textual or audio content does \n(Arruda, 2016; Wyzowl, 2023 ), but mainly because Google \nrepresentatives, a Google whistleblower, and outside researchers have  \nall concluded that about 70% of the time people are watching videos on \nYouTube, they are watching content that has been suggested by Google\u2019s \nrecommender algorithm ( Solsman, 2018 ). \n In 2024 , we published two controlled experiments showing  (a) that \npolitical bias in recommendations made on our You Tube simulator \nproduced VMPs between 51.5% and 65.6% , and (b) that masking our \nmanipulation reduced perception of bias from 33.0% (in Experiment 1) \nto 14.6% (in Experiment 2). We concluded, \u201c If the findings in the present \nstudy largely apply to YouTube, th is popular video platform might have \nunprecedented power to impact thinking and behavior worldwi de\u201d \n(Epstein & Flores , in press ). \nEPSTEIN  \n \n42 \n   \n7. Opinion Matching Effect (OME)  \n \n Of the 10 new forms of manipulation my team and I have discovered \nand studied over the yea r, the \u201copinion matching effect\u201d (OME) is the \nonly one that is not, at the moment, controlled exclusively by Big Tech \ncompanies  (Epstein et al., in press) . That said, Google, Facebook, and \nother major platforms have, at times, urged users to click on links  to \nvarious websites to get more information about politi cal candidates and \nissues  \u2013 among them, Google, Facebook, Snapchat,  Instagram, and  \nTwitter  \u2013 so they might be making extensive use of OME; that is a matter \nmy team and are currently investigating (see Part Three ). \n I began to speculate about the possibility of this technique when I ran \nacross the website http://ISideWith.com , which was founded in 2012 by \ntwo friends, Taylor Peck and Nick Boutelier , who claim on the ir website \nto be independent politically. At this writing (June 29, 2024), the website \nalso claims to have helped 79,688,669  people make up their minds about \nwhom to vote for in various elections in the US.  \n The method is simple \u2013 and easy to abuse. One s imply asks the user \nfor his or her opinions about some current political issues (abortion, gay \nmarriage, the border, etc.) and then use s a secret formula of some sort to \ninform the user which candidate is the better match for his or her \nopinions. Sometimes , users are shown percentage matches for each issue  \nand candidate . \n When, long ago, I stumbled upon ISideWith,  I could not  help but \nwonder whether the secret formula was biased in any way. The \nindividual user would have no way of knowing, of course. I also  \nwondered how much impact opinion matching could have on people\u2019s \nopinions and voting preferences.  \n I became increasingly intriguied by online opinion matching when it \ndawned on me that ISideWith \u2013 and many other websites, perhaps? \u2013 \nwould attract exactly  the right population of users that could have a \ndecisive impact on the outcome of elections: the undecided. Why would \nyou visit a website that exists to help you make up your mind about \npolitical candidates unless you were trying to do just that? My curio sity \nincreased when, in 2016, I noticed that not long before the November \nPresidential election, many websites  \u2013 WashingtonPost.com , \nWallStreet Journal.com , and even Tinder.com  \u2013 added opinion -matching \nTHE EVIDENCE  \n43 features to their websites to help  people decide between Hillary Clinton \nand Donald Trump. Tinder \u2013 until then  just a hookup  site for people \nlooking f or sexual partners \u2013 added a \u201cswipe -the-vote\u201d feature to the \nwebsite. Normally, they showed you photographs of potential partners, \nand you swiped right to like someone and left to reject them. With swipe -\nthe-vote, you were asked a small number of yes -or-no questions about \npolitical issues, and you swiped right for Yes and left for No \n(Klinkenberg, 2023 ) (Figure S7 ).  \n The problem is obvious: What if political bias was built into the \nalgorithm that did the matching? In fact, how would you even know \nwhether t he algorithm looked  at the answers you gave to the questions it \nposed? An acquaintance of mine who works in the world of online \nmarketing (and who won\u2019t allow me to use his name in this essay) told \nme that it was common practice among some companies to use  opinion \nmatching to help sell their products. If they were selling guitars, he said, \nthey simply set up multiple websites that \u201chelp ed people make up their \nminds\u201d about what guitar to buy. We have all seen such websites, and \nsome include quizzes. Those  quizzes, he said, \u201ccreate the impression the \nwebsite is helping you by collecting information about your \npreferences.\u201d Then, he said, it automatically recommends the guitar \nmanufactured by the company that set up the website. Your answers to \nthose questio ns, he said, \u201care completely ignored.\u201d  \n My associates and I explored these issues in two ways: First, we \nevaluated the possible bias of a number of different political websites \nthat offered opinion -matching quizzes. We did so by employing an \nalgorithm that  simulated human typing rhythms and that answered each \nquestion the website posed with a random answer. We generally did so \n300 times at each website. We made the reasonable assumption that if \nthe website recommended one political party or candidate repeatedly in \nresponse to our random answers, it was probably biased. We found two \nsuch website s among the first 10 we assessed (Epstein et al., in press ), \nwith the bias statistically significant at each site.  \n Second, we conducted a controlled experiment in which participants \n\u2013 all undecided voters \u2013 employed our simulation of the ISideWith \nwebsite (which we called DoodleMatch ), to help them decide  between \ntwo candidates running for Prime Minister of Australia. As you might \nexpect, we ignored their  answers and simply assigned them a t random to \none of three groups: Pro-Candidate -A, a Pro -Candidate -B, or control. As \nEPSTEIN  \n \n44 \n  in SEME studies (see Section #1 above), before participants took the \nquiz (the manipulation), they were given brief information about the \ncandidates and then asked eight questions: six opinion questions and two \nvoting -preferences questions. After the quiz, in the bias groups \nparticipants were told that their answers matched the policies of the \nfavored candidate 85% of the time and the policies  of the nonfavored \ncandidate 15% of the time. In the control group, participants were told \nthat their answers matched the policies of each candidate equally.  \n Two aspects of the results were startling. First, in one of the four \ngroups in this experiment we  obtained a VMP of 95.2% \u2013 the highest we \nhad ever seen. Second, not a single one of the 510 participants in the bias \ngroups in this experiment appeared to see any bias in the content we \nshowed them.  They had no way to see bias, of course, because they had  \nno way of knowing how our scoring was done. What\u2019s more, like those \nconsumers searching for guitars, the quiz created the false impression \nthat we cared about their opinions.  \n  \n8. Digital Personalization Effect (DPE)  \nhttps://DigitalPersonalizationEffect.com   \n \n For a decade or so, our studies alwa ys looked at the possible impact \nof biased content on people who were undecided about candidates or, in \nsome cases, about issues such as abortion or fracking. It wasn\u2019t until \n2023  or so that I finally decided to investigate another possible \ndimension of on line manipulation. Google, Facebook, Instagram, \nTikTok, and other online platforms presented personalized  content to \ntheir users \u2013 that is, content that was customized to match their interests \nbased on the vast amoun t of personal information these companie s \ncollected daily about their users ( Google Help Center, n.d .-c ). Google, \nin particular, had long touted it s ability to modify its content to meet the \nspecific needs of ea ch of its users ( Fredrick, 2022 ).  \n The primary motivation for personal ization is monetary: For Google, \nexposing users to opportunities to make purchases that meet their \nimmediate needs \u2013 even to predict  those needs before the user \nexperiences them \u2013 has generally accounted for more than 90% of the \ncompany\u2019s revenue since it first learned how to  monetize its search \nresults using  Google Ads (Coelen, 2023 ). Users generally like this \narrangement; my late wife called Google her \u201cpersonal shopper,\u201d since \nTHE EVIDENCE  \n45 it constantly showed her links or ads that connected her with exactly the \nclothes and other items she desired. For online platforms, personalization \nalso increases watchtime. On YouTube, recommending videos that \nconnect with people \u2019s interests and passions can  keep them watching for \nhours (Hao, 2019 ), and the longer peop le stay on  a platform, the more \nmoney that platform tends to make, mainly because, over time, users are \nmore likely to click on ads or links to ads.   \n Personalization also has a darker side. You can\u2019t personalize content \nunless you are aggressively surveilling users . The internet and related \ntechnologies have made it possible for private companies to surveil  us \nand our children 24 hours a day, often without our knowledge or \npermission. Google invented an entirely new business model that retired \nHarvard professor Sho shana Zuboff labeled \u201csurveillance capitalism\u201d i n \na recent book of that title ( Zuboff, 2019 ), and thousands of other \ncompanies now imitate that model. The methodology is simple and \nfundamentally deceptive, as I explained in a TIME  magazine article long \nago (Epstein, 2013): Users are enticed to use a wide variety of \u201cfree\u201d \nonline services, which, over time, get them to reveal a vast amount of \npersonal information about themselves. That information is then \nmonetized. On the surface, companies like Google and Facebook look \nlike kindly librarians at public libraries. On a deeper level, these \ncompanies are actually vast advertising companies , tricking people into \nrevealing personalized information that allows these companies to \nconnect vendors (their actual custo mers) with users (the products the Big \nTech companies sell) (Ball, 2023 ). \n These companies are insatiable when it comes to data, which is why \nGoogle purchased YouTube (to capture information about the videos \npeople watch), Fitbit (to track physiological da ta), and the Nest smart \nthermostat company (which allowed Google to record sounds in people\u2019s \nhome after they installed microph ones into some of Nest\u2019s products) \n(Kerns, 2022 ). Although people might be dimly aware that they are \nsurrendering personal information  to Google when they use Gmail, \nGoogle\u2019s email system, or Chrome, Google\u2019s browser, people have no \nawareness at all of the scale of Google\u2019s surveillance. Google actually \nmonitors our behavior over more than  200 different platforms, most of \nwhich people ha ve never heard of ( Masheshone, 2021 ; cf. Desjardins, \n2017; Liao, 2018; Nakashima, 2018; Weinberg, n.d. ). \nEPSTEIN  \n \n46 \n   And then there is even a darker side to personalization. As novelist \nTom Clancy once said, \u201c If you can control info rmation, you can control \npeople,\u201d a nd research suggests that the more information you have about \npeople, the  easier it is to control them (Matz et al. 2024; Selvarajah, \n2018 ). Yet we had never studied or tried to quantify online \npersonalization. How would we do so, I wondered, and what migh t \nexperiments on personalization teach  us? \n We presented the results of our first set of experiments on the \u201cdigital \npersonalization effect\u201d (DPE) at a scientific conference in April 2024 \n(Epstein et al., 202 4d). Using our simulated versions of two different \nplatforms \u2013 Google  and Twitter/X \u2013 we found evidence that \nmanipulations were far more powerful when they were personalized. In \nour Twiddler experiment, for example, the VMP for participants seeing \nbiased content was 21.8%; whereas the VMP for particiapnts seeing \npersonalized biased content was 71.9% \u2013 more than three times as great. \nIn our search engine experiment, the VMP for participants seeing biased \ncontent was 17.1%; whereas the VMP for particiapnts seeing \npersonalized biased content was 67.7% \u2013 nearly  four times as great. We \npersonalized content by altering it so it appeared to come from news \nsources people had told us they trusted.  \n If personalizing content on real on line platforms increases the impact \nof biased content anywhere near the extent it doe s in our experiments, \nthen, conceivably, over the past decade, my team and I have been greatly \nunderestimating  the possible impact of Big Tech platforms on people\u2019s \nthinking and behavior.  \n \n9. Multiple Exposure Effect (MEE)  \nhttps://Multiple ExposureEffect.com   \n \n One of the limitations on  the kinds of experiments we had been \nconducting was that we had no means of looking at the possible longterm \neffects of our manipulations. Perhaps the shifts in opinions and voting \npreferences we measured only lasted a few hours \u2013 maybe even just a \nfew mi nutes.  We could explore such possibilities by testing people hours \nor days after a manipulation, but I decided to pursue this type of concern \nin another way.  \n If an effect like SEME affects people for only a few minutes, that \nwould likely be the case not o nly in our experiments but also with \nTHE EVIDENCE  \n47 experiences on major online platforms. But onl ine platfoms can impact \npeople in ways my research team cannot. For one thing, a company like \nGoogle c an expose voters to similarly biased content content not just \nonce, as we do in our experiments, but dozens or hundreds of times in \nthe days leading up to an election.  \n We have so far explored this possibility in a relatively modest but \nsuggestive fashion. In controlled experiments we have conducted  using \nour simulations of t hree different platforms \u2013 again, Google, Twitter/X, \nand Alexa \u2013 we have looked at the VMP after one search on a platform, \nthen after exposure to similarly biased content on that platform, and then \nafter yet another exposure to similarly biased content on that platform. \nIn all three experiments, the impact of multiple exposure proved  to be \nadditive. In Experiment 1 (on  Kadoodle,  our Google simulator), the \nVMP increased with successive searches from 14.3% to 20.2% to 22.6%. \nIn Experiment 2 (on  Twiddler,  our Twitter/X simulator), the VMP \nincreased with successive exposures to biased tweets from 49.7% to \n61.8% to 69.1%. In Experiment 3 (on Dyslexa, our Alexa simulator), the \nVMP increase d with successive exposures to biased replies  from 72.1% \nto 91.2% to 98.6%. We also found c orresponding shifts for  how much \nparticipants reported  liking and trusting the candidates and for \nparticipants\u2019 overall impression of the candidates  (Epstein et al., 202 4c).  \n If repeated exposure to similarly biased content is additive on G oogle \nand other online platforms, then, conceivably \u2013 and once again \u2013 over \nthe past decade, my team and I might  have been underestimating the \nimpact of Big Tech platforms on people\u2019s thinking and behavior.  \n \n10. Multiple Platforms Effec t (MPE)  \nhttps://MultiplePlatformsEffect.com   \n \n Taking this logic to what is perhaps its ultimate applicatio n, at some \npoint it dawned on m e that if people are exposed to similarly biased \ncontent on different platforms \u2013 say, Google, then Facebook, then \nInstagram, then YouTube \u2013 the impact of those expos ures, even though \nthe types of content might differ greatly, might also be additive. That the \nbias on most of the major online platforms might be similar \u2013 bias \ninvolving politics or values, I thought \u2013 seemed highly likely given that \nroughly 95%  of politi cal donations from these companies and their \nEPSTEIN  \n \n48 \n  employees consistentl y went to one political party (Oberhaus, 2020 ; cf. \nKanter 2018 ). \n My associates and I recently completed an experiment that explo red \nthis issue in a straightforward manner: Participants were first exposed to \nbiased content on any one of the three platforms I mentioned earlier  \n(MEE Section  #9 above ); the platform was chosen at random by the \nalgorithm  that controlled the experiment. We then combined the data \nfrom all participants after tha t first exposure (in which people had been \nexposed to all three platforms) and measured the VMP, which turned out \nto be 42.4%. In the second part of the experiment, participants were \nexposed to a different platform \u2013 again, with the order of platforms \nrandomized \u2013 and we obtained a VMP of 56.5%. In the third part of the \nexperiment, people were exposed to the remaining platform, and we \nobtained a VMP of 66.7%  (Epstein  et al. , in press ).  \n  If repeated exposure to similarly biased content is additive acr oss \ndifferent platforms on the internet, then, conceivably \u2013 and yet again \u2013 \nover the past decade, my team and I might  have been greatly \nunderestimating the impact of Big Tech platforms on people\u2019s thinking \nand behavior.  \n The 10 effects we have in vestigated since 2013 are not the whole \nstory. In 2024 , I became aware of two manuscripts describing a form of \ninfluence that is distinctly different from the 10 I described above.  Each \npaper presented  controlled experiments  showing that when people used  \nonline writing tools that suggest ed text (\u201cpredictive text\u201d)  as people were \ntyping, when that text was  biased in some way, that bias impacted  the \ncontent of people\u2019s writing  in predictable ways  (Jakesch et al., 2023; \nWilliams -Ceci et al., 2024) . As early as  2018, a similar effect was shown \nfor how predictive text can impact the writing of restaurant reviews. \nAccording to the authors , \u201cWe demonstrate that in at least one domain\u2026, \nbiased system behavior leads to biased human behavior: People \npresented with phr asal text entry shortcuts that were skewed positive \nwrote more positive reviews than they did when presented with negative -\nskewed shortcuts\u201d (Arnold et al., 2018).  One might label  this new form \nof influence the \u201cpredictive text effect\u201d (PTE).   \n How many ne w forms of influence have the internet and related \ntechnologies made possible? I have no idea. But I have, in this essay, \ngone into some detail about the extent and magnitude of such techniques \nto try to bring the reader on the journey of discovery my asso ciates have \nTHE EVIDENCE  \n49 traveled over the past de cade. That journey has taught me that the \ninternet is so potentially dangerous as a tool of manipulation \u2013 especially \ngiven that almost all the techniques we have discovered are controlled \nexclusively by a small numb er of worldwide monopolies \u2013 that \naggressive and permanent systems must be developed and implemented \nas quickly as possible to protect elections, the easily influenced minds of \nchildren, and human autonomy itself. Without such systems \u2013 \nsupplemented, perha ps, by relevant laws and regulations (more about \nthat below) \u2013 we will be sleepwalking into a world that is largely \ncontrolled by a very small number of tech executives.  \n The third and final part of this essay summarizes efforts my team and \nI have been ma king since 2016 to develop and implement monitoring \nsystems that we believe will make Big Tech companies accountable to \nthe public, and, in so doing, will protect our democracy, our children, \nand our own minds  from Big Tech manipulation . \n \nPart Three: Development and Deployment of a \nNationwide System  for Preserving and Analyzing \nPerson alized Ephemeral Content  \n \n1. 2016 US Presidential Election  \n \n In the summer of 2015, I received a phone call from Jim Hood, then \nAttorney General (AG) of M ississippi. He was worried that Google \ncould interfere with his reelection as AG, and he asked me whether that \nwas possible. He was especially concerned because he had recently sued \nGoogle on behalf of Mississippi, and Google had responded by suing \nhim per sonally. I said yes and explained how Google might interfere, \nand he asked, \u201cBut how would we know  that they were doing these \nthings?\u201d  \n Law enforcement professionals, he said, would set up fake users \ncalled \u201csock puppets\u201d (now more commonly called \u201cbots\u201d),  and those \nfake users would access Google to see whether Google was somehow \nskewing content in a way that might turn voters away from Hood. I \nexplained to him (as I did in Part One of this essay) why that kind of \ninvestigation would be worthless. Google\u2019s algorithms would easily \nrecognize that those sock puppets were not real people, I told him, \nbecause Google had no profiles for them. In that case, Google would \nEPSTEIN  \n \n50 \n  likely sanitize the data it sent. To see the real content Google was \nsending, one would somehow have to look over the shoulders of real \nusers without Google\u2019s knowledge, I told him. That was the only way to \nknow what they were sending, I said, because Google content is \npersonalized.  \n Hood was agitated, and I was energized. How could I look over the \nshoulders of a large number of real voters and quickly aggregate and \nanalyze the content they were seeing on their screens? I had no idea.  \n I began addressing the problem in early 2016. I asked a friend (and \nformer government official) in Washington, D.C. h ow I might be able to \nfind funding to create a system t o monitor content Google and other \ncompanies were sending to voters, and he put me in touch with a \nmysterious individual in Central America. After I spoke with that \nmysterious fellow , money started to flow to the bank account of the \n501(c)(3) organization where I conducted my research. The donations \nwent through the charitable giving department of a New York bank and \nwere made anonymously. To this day, I have no idea where the money \ncame from or what th e motiv es of the donor might have been.  \n I assembled a team, and we got to work, having no idea where to \nstart. We tried a dozen different ways of recruiting voters who might let \nus monitor the political content they were receiving from  Google and \nother se arch engines; every method we tried failed. We eventually \nresorted to working with a \u201cblack hat\u201d group that found possible \nparticipants for us by running advertisements on Facebook that directed \nregistered US voters to the website of a fake company , where we \nsometimes were successful in getting their phone numbers. We then \ninterviewed and vetted each person, looking for a politically and \ngeographically diverse group of people who seemed willing  to let us \ninstall custom software on their computers. The softw are would, we \nhoped, preserve the content of web pages they visited when they \nconducted politically -related searches on Google, Bing, and Yahoo.  We \noffered to pay our participants about $25 per month  for their cooperation.  \n Through networking, we were also  eventually able to find a highly \nskilled  programmer  who had once spent time in federal prison for \nhacking. His job was to create custom software that we could install on \nthe computers of our participants \u2013 now called \u201cfield agents\u201d \u2013 software \nthat would , we hoped,  be invisible to tech companies and that would \nrapidly transmit search results to us in a secure fashion.  \nTHE EVIDENCE  \n51  In this first monitoring project, as in all the projects that followed, \nwe not only preserved the anonymity of the field agents, we also \ntransmitted data from their computers without including any information \nthat could identify them. We did, in effect, the opposite of what Google \nwas doing. We never violated the privacy of our field agents. We never \nexamined or preserved their search historie s, for example; we only \nanalyzed data in aggregate form. Google, of course, collects a nd \nmonetizes a massive amount of personal information each and every \nuser.  \n We were, in effect, setting up a Neilsen -type monitoring system. In \n1950, American entrepreneu r Arthur C. Neilsen began recruiting \nfamilies throughout the US who, for a token monthly fee, allowed him \nto install custom hardware in their homes that showed him what \ntelev ision sh ows they were watching (Neilsen  n.d.). The aggregated data \nbecame the basi s of the \u201cNeilsen ratings,\u201d which are used to this day to \ndetermine whether TV shows remain on the air or are canceled, and  are \nalso used to calculate the cost of advertising on those shows. This \nsystem, now greatly expanded in its capabilities, is now used by The \nNeils en Company  in over 100 countries (Statistica Research Department, \n2024 ). \n All told, by Election Day \u2013 November 8, 2016 \u2013 we had recruited a \npolitically diverse group of 95 field agents in 24 states, and during the 3 \nweeks leading up to the election, includ ing Election Day itself, we \npreserved 13,207 election -related searches \u2013 the roughly 10 search \nresults listed on the first page of search results on Google and Yahoo \u2013 \nalong with the 98,044 web pages to which the search results linked. Of \nspecial note: Alt hough we had begun with a list of 500 politically -related \nsearch terms  that we generated ourselves , we soon realized that many of \nthese terms were not neutral, and non -neutral search terms will \nnecessarily yield non -neutral (or \u201cbiased\u201d) se arch results  (Kulshrestha et \nal., 2019 ). So based  on ratings from independent raters, we rapidly \nreduced our initial list of search terms to 250 terms that produced mean \npolitical bias ratings of between -0.2 and + 0.2 on a scale from -5.0 (pro-\nClinton ) to +5.0 ( pro-Trump ) \u2013 in other words, terms that were, for all \npractical purposes, neutral. Over time, we had come to realize that the \nsearch results we wanted to examine were those produced not just by any \nsearch terms, but by politically neutral ones  exclusively . In an ide al \nworld, we reasoned, politically neutral search terms (say, \u201cHillary \nEPSTEIN  \n \n52 \n  Clinton\u201d rather than \u201cHillary Clinton is evil\u201d) should produce search \nresults that are politically neutral.  \n Over the next few months, we used crowd sourcing to rate the \npolitical bias of web pages on a numeric scale from pro-Clinton ( -5.0) to \npro-Trump (+5.0) . We now had all the information we needed to \ndetermine whether the search results on these three platforms were \npolitically biased \u2013 that is, to see whether high -ranking sear ch results \nfavored either Donald Trump or Hillary Clinton.  \n Among other things, we found  the following : \n (a) Political bias . Google search results had a strong and statistically \nsignificant pro-Clinton  bias ( p < .001)  (Figure 6), and this bias occurred \nin all 10 po sitions of the search results  (Figure 7).  \n (b) Possible impact on votes . Our experimental research on SEME \nsuggested that that degree of bias, if seen nationwide over time, could \nhave shifted between 2.6 and 10.4 million votes to Hillary Clinton \nwithout p eople knowing \u2013 and, normally, without leaving a paper trail.  \nTo see how we arrived at this estimate, see  S3 Text. \n (c) Google vs. Yahoo . The liberal bias we found on Google search \nresults (-.19) was significantly greater than the liberal bias we found on \nYahoo ( -.09, p < .001).  \n (d) Search bias by state . Google sent pro-Clinton content to voters in \nblue ( -.24), red (-.12), and swing  states  (-.10 ). Based on Google\u2019s public \npolicy \u2013 \u201cWith personalization, you get Google Search results tailored \nfor you base d on your activity \u201d (Google Search Help, n.d. ) \u2013 we had \nexpected Google to show conservatively biased content to users in red \nstates and, perhaps, unbiased c ontent to users in swing states, but that is \nnot what we found.  \n Could the search results we captur ed have been biased simply \nbecause people were using  biased search terms? On a scale from -5 (pro -\nClinton) to +5 (pro -Trump ), the average bias in people\u2019s search  terms \nwas slightly pro -Trump (+ 0.08). The search terms people used should \ntherefore have yield ed a pro -Trump bias in search results, but they did \nnot. \n I\u2019ll mention just one more finding from the 2016 monitoring project. \nWe had deliberately recruited field agents who did not use Gmail, \nGoogle\u2019s email system, thinking that this would make it more di fficult \nfor Google to identify our field agents . To test this idea, we deliberately \nrecruited a small number of Gmail users. The difference between the bias \nTHE EVIDENCE  \n53 in their search results ( -.03) and the bias in the search results seen by \nnon-Gmail users ( -.19, p < .001 ) was striking ( Figure 8).  Perhaps Google \nidentified our confidants through its email system and targeted them to \nreceive unbiased results; we have no way to confirm this, but it is a \nplausible explanation for the pattern of results we found.  \n For more det ails about our 2016 monitoring project, see Epstein et \nal. (2017) and my article, \u201cTaming Big Tech,\u201d published in the online \ntech magazine Hacker Noon in 2018 ( Epstein, 201 8c). \n Given our various stumbles over the nearly 10 months it took us to \nset up and debug our monitoring system \u2013 the first of its type in the world, \nas far as I know \u2013 my team and I were pleased that we had been able to \npreserve data that was normally lost forever and that we were quickly \ndeveloping ways to analyze those data meaningfull y. That said, I \nconsidered this first project to be little more than a proof of concept \u2013 a \ndemonstration, if you will. In my mind, I began to try to envision what a \nlarge -scale monitoring system would look like \u2013 even what a permanent, \nnationwide system w ould look like. In 2018, in the months leading up to \nthe midterm elections in the US, my team and I set about trying to build \na bigger and more sophisticated system than the one we had deployed in \n2016.  \n\nEPSTEIN  \n \n54 \n  Figure. 6. Mean political bias in Google search re sults in the days leading \nup the 2016 Presidential election . Note that the mean bias we found on each \nof these days favored candidate Hillary Clinton .FN  \nFigure 7. Political bias in search results by search posit ion. Google\u2019s search \nresults showed liberal bias in all 10 search positions on the first page of its \nsearch results .  \n2. 2018 US Midterm Elections  \n \n 2018 was a midterms election year in the US, which means no \nnational political offices were at stake. Instead, people voted for local, \ncounty, and stat e candidates \u2013 including governorships  in some states \u2013 \nas well as for state representatives to serve as members of the US Senate \nand the US House of Representatives \u2013 the two chambers  of the the US \nCongress.  \n The 2018 midterms presented us with some new c hallenges, not the \nleast of which was funding. Presidential elections in the US generate a \ngreat deal of interest, and they also inspire people, companies, and \norganizations to spend vast sums of money on campaigns (Bustillo, \n2023 ), as well as on \u201celection  integrity\u201d projects. My research on the \npossible impact that Big Tech companies might have on elections could \nbe considered one of the latter. Recall that in 2016 our funding came \nfrom one or more anonymous sources. The mysterious man who had \nhelped us th en was not willing to help in the midterms. The organization \nwhere I am based \u2013 a nonprofit, nonpartisan, 501(c)(3) public charity \n\nTHE EVIDENCE  \n55 called the American Institute for Behavioral Research and Technology \n(AIBRT) \u2013 depends on grants and tax -deductible donations  for its \nsurvival. My small staff and I sent off grant proposals, and I also solicited \ndonations through podcast and radio interviews, as well as through \narticles I wrote for mainstream media outlets, such as an article I \npublished in USA Today entitled, \u201c Not Just Conservatives: Google and \nBig Tech Can Shift Millions of Votes i n Any Direction \u201d (Epstein, \n2018b). It wasn\u2019t until late summer \u2013 less than two months before \nElection Day (November 8, 2016)  \u2013 that we had sufficient funds to set \nup our new montitori ng system. Fortunately, we had already developed \nthe most needed technology, and we were able to quickly bring together \nkey people from our 2016 team.  \n \n \n \nEPSTEIN  \n \n56 \n   \n \nFigure 8 . Bias in search res ults received on Google by non -Gmail users vs. bias received by Gmail \nusers.  Note that the pro -Clinton bias received by our field agents who did not use Google\u2019s email system \nwas significantly higher than the bias received in search results by our Gmail us ers (p < .001). In the latter \ngroup, the mean level of bias (.03) was only marginally different from 0 ( p = .014).  \n \nTHE EVIDENCE  \n57  We decided to focus our efforts on three Congressional districts in \nCalifornia \u2013 Districts 45, 48 , and 49  \u2013 each of which had long been \nRepublican bastions but where political pundits though that Democratic \ncandi dates had a chance of winning (Silver, 2018a , 2018b , 2018c ). In all, \nwe recruited a politically -balanced group of 1 25 registe red voters in \nthose districts to serve as our field agents.  We also recruited 31 field \nagents outside of California, but we didn\u2019t have enough people in any \none Congressional district to analyze the data we received from them.  \n The midterms presented us wi th another new challenge \u2013 what search \nterms to use. In 2016, we originally created our own list of about 500 \nsearch terms and then, as I noted, we eliminated half of them after we \nhad people who were not directly associated with our project or institute \nrate all the terms for political bias. In 2018, I asked Robert Schlessinger, \nformerly the Opinion Editor for U.S. News & World Report , to generate \na separate list s of neutral search terms for each of the three Congressional \ndistricts we were monitoring. This  required him to research the issues of \nlikely interest to voters in each district. Then, as before, we had \nindependent people rate those terms for political bias, and we kept only \nthose terms with ratings between -0.2 and +0.2  on a scale between -5.0 \n(liberal bias) and +5.0 (conservative bias). We ended up with  an average \nof 251  search  terms for each of the three Congressional districts  (see Text \nS4 for the search terms).  \n We instructed our field agents to conduct searches on Google, Bing, \nor Yahoo, at the ir discretion. We also gave them our list of neutral search \nterms relevant to their district. We collected and analyzed data data only \nwhen they chose to use terms from those lists.  \n By the end of Election Day, we had preserve d 47,294 searches on \nGoogle , Bing, and Yahoo , along with the 392,274 web pages to which \nthe search results linked  \u2013 more than three times the amount of ephemeral \ndata we had been able to preserve in 2016. We summarized our findings \nin a presentation at a scientific meeting in April, 20 19 (Epstein, 2019).  \nThe key findings , in which we focused our analysis on the data we had \nobtained during the 10 days leading up to and including Election Day  \u2013 \nOctober 28 to November 6, 2018 \u2013 were as follows:  \n (a) Political bias. Based on crowd -sourced b ias ratings, we found \nthat Google search results were significantly  more liberal than non -\nGoogle search results on all 10 days leading up to and including Election \nDay (Fig ure 9) and in all 10 positions of search results on the first page \nEPSTEIN  \n \n58 \n  of search results  (Figure 10). This finding was further supported by \ncalculating the political bias of the news sources used in the search \nresults, based on ratings of 976 online news sources published in 2017 \nby Harvard\u2019s Berkman Klein Center (Faris et al., 2017). On a sc ale from \n-1.00 ( liberal ) to +1.00 ( conservative ), the mean bias level of Google \nsearch results ( M = -0.14, n = 210,088) was significantly more liberal  \nthan the m ean bias level of non -Google results ( M = 0.13, n = 14,506, p \n< .001, d = 0.52).  \n \n \nFigure 9.  Political bias in search results by date, Google vs. non -Google \nsearch engines . On Election Day and the 9 days preceding that day in 2018, \nGoogle search results were consistently liberally biased each day. On Bing and \nYahoo combined, the bias generally lea ned conservative. Because most \nsearches are conducted on Google, however, Bing and Yahoo have little effect \non election outcomes.  \n\nTHE EVIDENCE  \n59  \nFigure 10. Political bias in search results by search position, Google vs. \nnon-Google search engines . Google\u2019s search result s showed liberal bias in all \n10 search positions on the first page of its search results; whereas, Bing and \nYahoo combined showed conservative -leaning results.  \n  \nb) Possible impact on votes . A computational SEME model suggests \nthat this level of bias in s earch results nationwide could have shifted \nupwards of 78.2 million votes toward Democratic candidates (spread \nacross hundreds of state and regional races) in 2018 without user \nawareness  (see S3 Text for details ). In the three Congressional districts \nwe mo nitored, we estimated that if all Google users were exposed to the \nlevel of bias we detected in Google search results, Google might have \nshifted between 4% and 16% of th e total votes cast in each district  (see \nS3 Text). \n (c) Outcomes in the three Congressi onal districts . All three districts \nflipped Democratic, with win margins within the possible margins that \nGoogle might have been able to control  with bias ed search results : In \nDistrict 45,  Katie Porter (D ) prevailed over Mimi Walters (R) by a \nmargin of 2.4 p oints. In District 48 , Harley Rouda (D ) prevailed over \nDana Rohrabacher  (R) by a margin of 5.8 points, and in District 49 , Mike \nLevin (D ) prevailed over Diane Harkey (R) by a margin in 10.8 points.  \n\nEPSTEIN  \n \n60 \n   (d) Search engine use . Our field agents  chose to conduct 92.1% of \ntheir searches on Google and only 7.9% of th eir searches on Bing and \nYahoo combined.  \n In 2018, another possible manipulation caught my eye. On Election \nDay (November 6th), Google posted colorful go -vote reminders on its \nhome page, which is likely  seen more than 900 million  times  a day  in the \nUS ( S1 Text). These reminders were praised by online users and a few \njournalists as a public service ( e.g., Steer, 2018 ). It occur red to me \nimmediately that this  reminder  could easily be acting  as a powerful vo te \nmanipulation .  \n In early January, 2019, I published an article in The Epoch Times  \nentitled,  \u201cHow Google Shifts Votes: A \u2018Go Vote\u2019  Reminder Is Not \nAlways What You Think It Is\u201d (Epstein, 2019 a). (The mainstream liberal \npublications where I normally publi shed refused to publish this  article \u2013 \na glimpse of my total ouster from mainstream media that would take \nplace a few months later.) In this article, I explained how partisan go -\nvote reminders could easily shift votes toward one candidate, and I also \nexpla ined that even if go -vote reminders were sent to everyone , they \ncould still shift votes mainly to one candidate \u2013 an effect , as I mentioned \nearlier in this essay,  I labeled the \u201cdifferential demographics effect\u201d \n(DDE , see Part One, Section #5 ). \n I explained the simple logic of the partisan manipulation earlier in \nthis essay in the section on \u201cThe Targeted Messaging Effect.\u201d \nUnfortunately, unless a whistleblower shows up, preferably  with some \nwritten documentation, there is no way for voters or authorities to know \nthat go -vote remind ers were sent out mainly to members of one party. \nYes, this kind of manipulation could be captured with a sophisticated \nmonitoring system, but in 2018, we were not looking for vote reminders  \non Google\u2019s home page, on Facebook, or anywhere else. As you will  see, \nwe rectified that mistake  in 2020, with troubling  results.  \n In The Epoch Times  article, I calculated that an extreme variant of a \npartisan go-vote manipulation could have shifted 841,000  votes ( spread \nacross hundreds of elections, large and small, nationwi de). I also \nexplained, as I mentioned earlier, that even if Google had sent those vote \nreminders to all of its users, it still would have given 49,500 more votes \nto Democrats than to Republicans that day  (see S2 Text for excerpts from \nThe Epoch Times  article). I still find this kind of manipulation to be \nespecially disturbing because it truly appears to be a well -intentioned \nTHE EVIDENCE  \n61 public service, and it is difficult to imagine any authorities ever  enacting \na law or regulation to prohibit it.  \n So in 2018, we got  better at monitoring and at generating search \nterms, and we also learned that we had to monitor more than just search \nresults. Well before the 2020 Presidential election, we set our sights on \nbuilding the first large -scale system for monitoring a variety o f \nephemeral content on multiple platforms.  \n \n3. 2020 US Presidential Election  & 2021 Georgia Senate Runoff \nElections  \n \n In 2020, three things happened that allowed us to expand and \nprofessionalize our monitoring efforts dramatically. First, we had access \nto a lot more money than we had had in previous projects. This was \nmainly because of a longtime talk -show host and author named Glenn \nBeck. H e had me as a guest on his various programs multiple times \u2013 \neven on a one -hour special \u2013 during which he insisted, ov er and over \nagain, that his audience fund my 2020 monitoring project, which he \nbelieved was essential for preventing Google and other tech companies \nfrom interfering with the free -and-fair election in the US.  \n As a result, more than 7,000 of his listeners and viewers made \ndonations of between $1 and $300,000. That big donation was made by \na businessman who had never heard of me before. His sister saw me on \none of Glenn\u2019s shows and insisted that he donate.  \n Second, a remarkable young woman whom I will call R uby Lyle (an \nalias) \u2013 one of my research interns in 2020 \u2013 heard that I was looking for \nsomeone to build a large team of field agents to help us monitor the \nupcoming Presidential election, and she walked over to my desk one day, \nsmiled broadly, and said, \u201cI can do that!\u201d \u201cHave you ever done anything \nlike this before?\u201d I asked, and she said, \u201cNo, but I know  I can do it.\u201d  \n I don\u2019t know  why I believed her, but I did, and, over the next few \nmonths, she assembled and trained a team of more than a dozen  \nrecruite rs, and they in turn recruited a politically -diverse group of 1,735 \nfield agents located primarily in four swing states  \u2013 Arizona, Florida, \nGeorgia and North Carolina . In the Presidential election, we aggregated \nand analyzed data from 732 field agents in Arizona, Florida, and North \nCarolina \u2013 455,107  ephemeral experiences in all.  Immediately following \nthe Presidential election, we focused on obtaining and analyzing data \nEPSTEIN  \n \n62 \n  from 1,003 field agents in Georgia \u2013 1,112,416  ephemeral experiences \nin all.  \n And third,  our secret hacker, with the help of some of his coder \nfriends, beefed up our monitorin g software so that we now could capture \nsearch suggestions (those phrases Google flashes at you while you are \ntype a search term), search results, answer boxes, images o n Google\u2019s \nhome page, content on Facebook home pages, YouTube videos  people \nwere watching  and the videos YouTube was recommending to users, and \nmore.  \n We also got more professional in the way we operated, in part \nbecause we realized that the more ephemeral data we collected, t he \ngreater the threat \u2013 at least potentially \u2013 we posed to Big Tech companies. \nRuby and other new staff used aliases and secure phones, and we located \nour staff and equipment in secure new office space.  \n 2020 Presidential election . Our 732 field agents in Arizona, Florida, \nand North Carolina were fairly well balanced by age (33.4% under age \n30), gender (55% female), and political leaning (29.1% liberal vs. 28.5% \nconservative) ( S8 Figure). Our major findings were as follows:  \n (a) As we had seen in previou s elections, political bias in Google \nsearch was significantly liberal , with bias in search results on Bing and \nYahoo closer to zero ( Figure 11). \nTHE EVIDENCE  \n63  \nFigure 11. 2020 Presidential election, October 29 to November 3, mean \nsearch engine bias by day.  Bias on the  Google search engine was substantially \nand significantly  greater than on Bing and Yahoo .  \n \n (b) Google sent liberally biased content to liberals, moderate, and \nconservatives in roughly equal proportions  (Figure 12 ). Notably, \nconservatives received a signi ficantly higher proportion of liberally \nbiased search results than liberals did ( U =3.59 x 107, p < .001 ). \n \n \n \n \n\nEPSTEIN  \n \n64 \n   \n \nFigure 12 . 2020 Presidential election, October 29 to November 3, mean bias \nby political leaning (Google only).  Google sent liberally biased se arch results \nto liberals, moderates, and conservatives. Notably, conservatives received a \nsignificantly higher proportion of liberally biased search results than liberals \ndid (U =3.59 x 107, p < .001) . \n \n (c) On Election Day in 2020, on Google\u2019s home page m oderates \nreceived the highest percentage of go -vote reminders (74 .7%), followed \nby liberals (53. 6%), followed by conservatives (42.6%)  (Figure 13) . The \ndifference in percentages between conservatives (42.6%) and liberals \nand moderates combined (59.4%) was highly statistically significant ( z \n= -8.94, p < .00 1). Too see what a go -vote reminder looked like on \nGoogle\u2019s home page in 2020 (sent to speakers of English in the US), see \nS6 Figure. \n \n \n \n \n \n\nTHE EVIDENCE  \n65  \n \nFigure 13. 2020 Presidential election, October 1  to November 3, G oogle \nhom e-page go -vote reminders by political leaning.  Google sent more of these \nreminders to moderates than to liberals, and more to liberals than to \nconservatives .  \n \n (d) On YouTube , 93.3% of the news videos  YouTube (owned by \nGoogle) recommended to users came fr om liberal news sources ( Figure \n14), and Google recommended videos from liberal news sources in equal \nproportions  to liberals , moderates, and conservatives ( Figure 15). A \nGoogle representative might claim in this situation that the algorithm \nwas simply rec ommending a representative sample of available videos. \nNews sources regularly examined by three nonpartisan ratings \norganizations , however  \u2013 Ad Fontes Media \n(adfontesmedia.com/interacti ve-media -bias-chart/ ), All Sides \nTechnology Inc. ( allsides.com/media -bias/ratings ), and  Media Bias/Fact \nCheck ( mediabiasfactcheck.com ) \u2013 suggest t hat the political leaning of \nnews sources available online is actual ly fairly well balanced across  \npolitical viewpoints (Figure 16). \n\nEPSTEIN  \n \n66 \n   2021 Georgia Senate runoff e lections . In Georgia, political activity \ndid not abate after the November 3, 2020 , Presidential  election. Georgia \nwas already gearing up for two runoff elections to determine who would \noccupy Georgia\u2019s seats in the U.S. Senate. We had already been \nrecruiting field agents in Georgia, because it was one of the key swing \nstates in the Presidential elec tion. Now we rapidly accelerated our \nrecruitment efforts there, ultimately having in place 1,003 field agents  \nthroughout the state. This allowed us to preserve 1,112,416 ephemeral \nexperiences on Google, Bing, Yah oo, the Google home page, YouTube, \nand Faceb ook. \n \n \nFigure 14. 2020 Presidential election: Political leaning of videos watched \non YouTube, October 27, 2020 to November 3, 2020 . All videos were \nrecommended by Google\u2019s up -next algorithm. Bias is shown only for videos \ncoming from news sources.  To compa re this distribution to the distribution of \nactual news sources online, view Figure 16. \n \n \n \n \n\nTHE EVIDENCE  \n67  \nFigure 15. 2020 Presidential election: Proportion of liberal -leaning videos \nbeing recommended  to our conservative, liberal, and moderal field agents \nin the days l eading up to Election Day in 2020.  \n \n The Georgia project proved to be especially informative  for us \u2013 \nperhaps a bellweather for what large -scale monitoring systems might be \nable to accomplish. On October 30, 2020, I sent a summary of our \nfindings up to th at time  in the Presidential election to the office of \nSenator Ted Cruz (R, Texas). I had had some contact with him in the \nsummer 2 019 when I testified before a subcommittee of the US Senate \nJudiciary (Epstein, 2019 c), so I sent the summary to my contact in  his \noffice. As a result, on November 5, 2020, Senator Cruz sent a letter co -\nsigned by Senator Ron Johnson (R, Wisconsin) and Senator Mike Lee \n(R, Utah) to Sundar Pichai, the CEO of Google. It said, among other \nthings, that the preliminary findings from my  2020 monitoring system \nsuggested that Pichai\u2019s own testimony before Congress that \u201cWe won\u2019t \ndo any work, you know, to politically tilt anything one way or the other\u201d \nwas not true.  \n\nEPSTEIN  \n \n68 \n   \n \nFigure 16. Distribution of rated news sources as of November, 2022 . Thes e \nnews sources are rated by three nonpartisan organizations: Ad Fontes Media, \nAll Sides Technology, and  Media Bias/Fact Check . Their ratings suggest that \na fair search engine would show people conservative, moderate, and liberal \nnews stories (whether in t ext, video, or other format s) in roughly equal \nproportions, or, possibly, in the proportions matching the political leanings of \ntheir users.   \n \n That night, and on the days that followed leading up to the January \n5, 2021 runoff elections in Georgia, my staf f and I noticed two abrupt \nchanges in the data we were collecting in Georgia:  \n First, beginning that night, none of our field agents in Georgia \nreceived go -vote reminders on Google\u2019s home page  (Figure 17); if that \nmeant that Google had stopped sending such  reminders to voters \nstatewide , it was presumably giving up one of its most powerful potential \nvote manipulations.  Second, and more striking, political bias i n Google \nsearch results disappeared. On Election Day (January 5, 2021) and the 5 \nprevious days \u2013 the 6-day period we focused on in our analyses \u2013 the \n\nTHE EVIDENCE  \n69 political bias in Google search results was close to  0 (Figure s 17 & 18). \nThis was \u2013 except for the graph I mentioned earlier that had come from \na small group of Gmail users in 2016 \u2013 the first time we ha ve ever seen \nunbiased Google search results.  \n \n \nFigure 17. Google home page go -vote reminders, 2020 Presidential election \nvs. 2021 Georgia Senate runoff elections . Each bar show the proportion of \nhome pages on which go -vote reminders  were shown during the 5 days leading \nup to each election (plus Election Day). The proportion was over 70% in the \nPresidential election; we detected no go -vote reminders in the Georgia elections  \n(z = 50.78 , p < .001).  \n \n \n  \n\nEPSTEIN  \n \n70 \n   \n \nFigure 18. 2021  Georgia Senate runoff election s, polit ical bias in search \nengines, December 31, 2020, to January 5, 2021 . On Election Day in Georgia, \nas well as on the 5 days leading up to the election, we found virtually no \npolitical bias in Google search results but some degree of conservati ve bias on \nthe B ing and Yahoo search engines.  (To se e these data in a bar graph, view  \nFigure S #) \n  \n \n\nTHE EVIDENCE  \n71  \n \nFigure 19. Google search engine bias, 2020 Presidential election vs. 2021 \nGeorgia Senate runoff elections.  Each bar represents mean bias on the first \npage of Google sear ch results during the 5 days leading up to each election, \nplus Election Day. We found substantial liberal bias in the Presidential election \nbut virtually no political bias in the Georgia elections. The bias in Georgia \nsearch results disappeared shortly aft er three US Senators sent a letter to the \nCEO of Google protesting possible election bias in Google search results. See \ntext for details.  \n \n4. 2022 US Midterm Election  \n \n In 2022, we built a bigger monitoring system, this time with 2,742 \nfield agents  locate d mainly in 10 swing states: Arizona, Florida, Georgia, \nMissouri, North Carolina, New Hampshire, Nevada, Ohio, \nPennsylvania, and Wisconsin. We preserved 2,549,544 ephemeral \nexperiences on Google, Bing, Googl e home page, YouTube, Twitter, and  \nFacebook , and we measured political bias on contact both by using \nratings from a custom machine learning algorithm we developed (which \nhad achieved 85% agreement with human raters who had helped train \nthe algorithm) and  by using ratings from those three nonpartisan \norganizati ons  that use various methods to rate the poli tical bias of news \nsources:  \n\nEPSTEIN  \n \n72 \n   (1) All Sides Technologies, Inc. ( https://allsides.com ), a public \nbenefit corporation that rates more than 1,400 media outlets and writers. \nIt relies on editorial reviews conducted by multipartisan panels of six -to-\nnine reviewers from the left, center, and right. Its motto is, \u201c Free people \nfrom filter bubbles so they can better understand the world \u2014 and each \nother. \u201d  \n (2) Ad Fontes Media ( https://adfontesmedia.com ), a public benefit \ncorporation the mission of which is \u201c to rate all the news to positively \ntransform society. \u201d It relies on 60 human analysts to examine news \nsources, subjecting each to perusal by  one liberal, one moderate, and one \nconservative analyst. At this writing, it has  rated more than 3,900 news \nsources.  \n (3) Media Bias Fact Check ( https://mediabiasfactcheck.com ), an \nindependent, donation suppo rted organization that relies on \u201c a collective \nof volunteers and paid contractors \u201d to rate the \u201cideological leanings and \nfactual accuracy\u201d of online content. It claims to have evaluated more than \n8,200 \u201c media sources, journalists, and politicians \u201d so far.    Its mission \nis \u201cto educate the public on media bias and deceptive news practices .\u201d \n The main way we rated news sources in our 2022 monitoring project \nwas to quickly rescale ratings from these three sources (or just two of \nthem, or even one in rare cases  in which one or more of these rating \nservices did not list the news source we were checking) from -1.0 (for \nliberal) to +1.0 (for conservative) and then to calculate the mean of the \nratings.  \n Our field agents were  fairly closely  balanced politically ( Figure 20), \nand our search terms were again rated as neutral by independent raters \u2013 \nbetween -0.2 and +0.2 on a political bias scale ranging from -5.0 to +5. 0. \nThe data we gathered from the computers of our field agents were \npreserved from searches they condu cted using more than 500  neutral \nsearch terms.   \n \n \nTHE EVIDENCE  \n73  \nFigure 20 . Pie chart of field agents by political leanings \nduring the 2022 presidential election . \n \n \nOur major findings were as follows:  \n (1) Focusing on the 5 days immediately preceeding Election Day \n(Nov ember 8, 2022), plus Election Day, we found, as we had in previous \nelections, that search results on Google were strongly liberally biased; \nsearch results on Bing were closer to zero (Fig ures 21 and 22 ). If seen \nnationally and shown to people for 6 months prior to the midterms, \npolitical bias in Google search could have shifted more than 80 million  \nvotes (spread across hundreds of elections nationwide).  \n \n \n \n \n \n\nEPSTEIN  \n \n74 \n   \nFigure 21. 2022 midterm elections, mean bias by search engine , days \nleading up to the elections . \n \n \n \nFigure 22. 2022 midterm elections, mean bias by search engine.  \n \n\nTHE EVIDENCE  \n75  2) Once again, we found that Google was showing liberally biased \nsearch results in roughly equal proportions to liberals, mode rates, and \nconservatives ( Figure 23). \n \n \n \n \nFigure 23. 2022 mi dterm elections, mean bias by the political leaning of \nour field agents.   \n \n (3) Again, as we have seen before, Google showed users liberally \nbiased search results in all 10 positions on the first page of their search \nresults (Figure 24). \n\nEPSTEIN  \n \n76 \n   \nFigure 24. 2022 midterm elections, mean bias by search position on Google . \n \n (4) In 2022, we also began tracking \u201cpolitical update\u201d tweets sent by \nthe Twitter company itself to our field agents, and we found that in the \ndays leading up to the elections, Twitter sent signi ficantly more of such \nupdates to our liberal field agents than to our conservative field agents \n(Figure 25). Note that these elections took place before Elon Musk took \ncontrol of the company and fired 80% of its employees, complaining that \nTwitter content was blatantly liberally biased ( Nava, 2023 ). \n \n \n\nTHE EVIDENCE  \n77  \nFigure 25. 2022 midterm elections, proportion of election \nupdates received by our conservative and liberal field \nagents in the days leading up to the elections.  \n \n (5) In 2022, our system became more proficie nt in preserving the \ncontent on Google\u2019s home page. In Florida in the days leading up to the \nelection, we found a large and statistically significant difference in the \nnumber of go -vote reminders Google sent to our liberal and conservative \nfield agents. Al l liberal field agents received them, but only 59% of \nconservatives did. This is the kind of blatant targeting t hat can shift large \nnumbers of v otes in a national election \u2013 even on Election Day itself \n(Bond et al. 2012; Epstein et al., 2023; Zittrain, 201 4). Nationwide \u2013 with \ndata mainly from 10 swing states, as I noted above \u2013 we again found that \n100% of our liberal field agents received go -vote reminders on Election \nDay on Google\u2019s home page and that the proportion of conservatives \nwho received such remi nders was significantly smaller ( p < .001) ( Figure \n26). \n \n\nEPSTEIN  \n \n78 \n   \n \nFigure 26. 2022 midterm elections, proportion of go -vote reminders \nreceived  in Florida  on Election Day by our conservative and liberal field \nagents.    \n \n \n \n\nTHE EVIDENCE  \n79  \nFigure 27. 2022 midterm elections, propo rtion of go -vote reminders \nreceived nationwide (mainly in 10 swing states) on Election Day by \nour conservative and liberal field agents.  \n \n (6) Finally, we found a liberal bias in the recommendations Google \nmade to our field agents on YouTube in the days l eading up to the \nelection  (Figure 28). 76.1% of those recommendations came from liberal \nnews sources, about twice as ma ny as one would expect by chance  (for \ncomparision, see Figure 16, which  shows a breakdown of all  US video \nnews sources ). \n  \n \n \n \n \n\nEPSTEIN  \n \n80 \n  Figure 28. 2022 midterm e lections, percentage of up -next videos \non YouTube, shown by political leaning.  \n \n5. 2023 -2024 Nationwide Monitoring System  \n \n After the 2022 midterm elections, I decided that the time had come \nto build a permanent, nationwide monitoring system. I was thinki ng \nalong these lines: We know from a decade of controlled studies that \nephemeral content can be used to shift people\u2019s thinking and behavior \ndramatically \u2013 typically without their knowledge and also without \nleaving a paper trail for authorities to trace. W e also know from leaks \nand whistleblowers that most Silicon Valley technology companies share \nstrong social values and are not shy about using the various powers they \nhave to advance their agendas \u2013 monetary, philosophical, and political .  \n We also know th at our leaders \u2013 at least the ones we\u2019re able to keep \nan eye on in Washington, DC \u2013 seem to be hobbled by partisan bickering \nmost of the time. They hold hearings occasionally in which they \nbrowbeat Big Tech executives, but they seem incapable of protecting  the \n\nTHE EVIDENCE  \n81 American public from the three big threats that Big Tech companies pose \nto our citizenry and our society: the massive surveillance, the censorship , \nand the manipulations . \n My team  and I did not have solutions to these problems, but we had \ndeveloped a way of making Big Tech companies accountable to the \npublic \u2013 specifically, by tracking, capturing, and analyzing the actual \npersonalized content these companies were sending to childre n, teens, \nand adults 24 hours a day.  \n As I mulled over these issues, I finally concluded that a permanent, \nnonpartisan, nationwide monitoring system \u2013 or perhaps several such \nsystems \u2013 had to be established in the US \u2013 and perhaps elsew here \naround the wor ld \u2013 for two reasons. First, if no such system exists, we \nwill have no idea how tech companies, now and in the future, might be \nusing or misusing ephemeral content . Manipulations that could \nsubstantially shift the thinking and behavior of most people in th e world \nwould be entirely invisible both to users and to authorities.  \n And second, if, by some miracle, world leaders enact laws and \nregulations that protect users from possible manipulations by technology \ncompanies \u2013 as EU leaders have been doing aggressi vely since they \npassed the General Data Protection Regulation (GDPR) in 2016 \u2013 they \nwill have no way of measuring compliance with these new laws and \nregulations unless large -scale monitoring systems are in place . The \nEuropean Commission  \u2013 the EU  agency tha t has been investigating Big \nTech companies since 2014  \u2013 recentl y admitted that Big Tech companies \nhave \u201cfall [en] short of effective compliance\u201d with the restrictions the EU \nhas put in place in recent years ( CBS News, 2024 ; cf. Bergen, 2015; \nHirst, 2016; R ankins, 2016 ; Sterling, 2019 ).  \n For credibility, I speculated that our new  system had to recruit a \npolitically -balanced group of registered voters in each US state \u2013 our \n\u201cfield agents\u201d (FAs) \u2013 and that the number of FAs in each state had to \nexceed some mi nimum  so that data collected from the computers of those \nindividuals  might  be court admissible in state court s. To achieve this, I \ndid some rough calculations regarding the minimum proportion of \nrepresentative registered voters we needed in each state to al low us to \nmake statistically significant predictions about the entire population of \nregistered voters in that state. I cannot, alas, make those numbers public \n\u2013 at least not at this time in this essay \u2013 just as the Neilsen Company \ncannot release details ab out the families who help it compute the Neilsen \nEPSTEIN  \n \n82 \n  ratings. To do so would be to give the companies we monitor guideposts \nfor identifying our FAs.  \n My team was now proficient in recruiting field agents in a way that \nprotected the integrity of our data. This  meant, first and foremost, that \nwe could not ask for volunteers. If we did, we would run the risk that \nGoogle would send us thousands \u2013 not from among their employees, \nperhaps (because we could probably identify that connection), but from \namong their more  than 12 0,000  \u201ctemporary, vendor and contract \nworkers \u201d (Moreno, 2019 ) \u2013 people whom we would have trouble \nassociating with the company. That issue aside, we have found ways of \napproaching registered voters  directly, of vetting them, having  them sign \nnon-disclosure agreemen ts, equipping their computers and mobile \ndevices with \u201cpassive\u201d monitoring software that Google could not easily \ndetect, training them, and protecting their identities. Although this was \nan expensive and labor -intensive process, by late 202 2, we knew how to \ndo it efficiently . \n We had also learned over the years how to monitor an increasingly \nwider and more diverse range of online ephemeral content \u2013 even those \nsearch suggestions Google flashes at you while you are typing a search \nterm. We ha d learned how to capture search suggestions by the millions.  \n We had also made progress in protecting our data, in recovering \nrapidly from attacks (yes, there have been many), and in analyzing the \nwealth of data we were preserving.  \n I now believe that moni toring  systems are essential for protecting \ndemocracies around the world.  As a parent, I also have become \nincreasingly concerned about the many ways in which online content is \nadversely impacting our children ( Common Sense, 2022;  Izundu, 2015; \nYbarra, 2016 ). Over the years, it had also crossed my mind that the \nvarious new types of manipulation I had been discovering and \nquantifying would probably work even better on children than on adults. \nMight Google or other companies deliberately use such techniques to  \nalter the thinking and behavior of children around the world \u2013 to \n\u201cindoctrinate\u201d chidren, as some might say? Even if that possibility was \nhighly unlikely, it seemed to me that a permanent monitoring system had \nto be able to capture ephemeral content being  sent to the mobile devices \nof children (with parental permission, needless to say).  \n Shortly after the 2022 midterms, I began seeking  support  that would \nallow us to expand the system we had already built, and by late May,  \nTHE EVIDENCE  \n83 2023 , we were able to build a s ystem that included just over 8,000 field \nagents in 49 US  states. By May 31st , we had also preserved 16,520,793  \nephemeral experiences on multiple plat - forms.  \n Toward the end of November, we built and deployed what for me \nwas a dream come true: a real -time dashboard for what w e now  called \n\u201cAmerica\u2019s Digital Shield\u201d ( ADS; you can view  the dashboard at \nhttps://AmericasDigitalShield.com ), and I began to talk about it in \nlectures and interviews. The dashboard showed, in real time, the numb er \nof ephemeral experiences we had preserved, disturbing images we had \npreserved from videos YouTube was recommending to children and \nteens, political bias in search results on Google, Bing, and Yahoo, and \npolitical bias in recommended videos on YouTube  (Figure 29).  \n As a result, I was soon invited to testify again before Congress, this \ntime on December 13, 2023, before the United States Senate Judiciary \nSubcommittee on Competition Policy, Antitrust, and Consumer Rights , \nchaired by Amy Klobuchar (D, MN). M y written testimony, which \nincluded peer -reviewed publications and manuscripts still under review \nby scientific journals, was 480 pages in length ( Epstein, 2023 ). \n As of this writi ng (September 4 , 2024) , we have preserved more than \n99 million ephemeral exper iences \u2013 data that are normally lost forever \u2013 \nand we have preliminary evidence suggesting that our monitoring system \nis having a mitigating effect on political bias in Google search results. \nSpecifically, since we went public with ADS in November, 2023, w e \nhave seen a slow and steady decrease in liberal bias in their search results \n(Figure 30). This decrease could also mean that Google is rigging the \ngame: reducing overall political bias while focusing its manipulations  \nnarrowly on the races it is most con cerned about.  \n And, of course, this decrease could also mean that Google is \ngradually identifying our field agents, and , with every identification, is \nsending out sanitized content, as we saw with Gmail users in 2016. Over \nthe years, however, we have deve loped increasingly sophisticated ways \nof determining whether any of our field agents are receiving suspicious \ndata. When we are in doubt, we remove those people from service .  \n For this tracking system to produce valid numbers, secrecy is \nessential. With billions  of dollars in production costs and advertising \nrevenues on the line, imagine the lengths to which interested parties \nmight go to influence a N ielsen family\u2019s viewing habits \u2013 or, for that \nmatter, to tamper with those black box es. Over time, Neilsen  has \nEPSTEIN  \n \n84 \n  developed increasingly sophisticated ways of detecting when incoming  \ndata was suspect, and so have we.  \n Here is a sampling of findings from the massive and ever -growing \ndatabase of ephemeral content we have been building over the past year \nor so:  \n  \nTHE EVIDENCE  \n85 https://AmericasDigitalShield.com   \n \nFigure 29. America\u2019s Digital Shield. The graphs above show statistically significant \nliberal bias in both Google and Bing search results (the blue shaded areas below each \nx-axis) as of May 13, 2024. The image in the lower right shows that Google is sending \nliberally biased content to registered voters in all 50 states \u2013 to liberals, moderates, and \nconservatives. As of September 3 , 2024, the total number of ephemeral events we h ave \npreserved has passed 99 million.   \n\nEPSTEIN  \n \n86 \n  Figure 30. Data from the public dashboard of America\u2019s Digital Shield (ADS) . \nThis graph, which is updated every 5 min online at https://AmericasDigitalShield.com , \nsugg ests a slow and steady decline in liberal bias in Google search results beginning in \nmid-November, 2023, which is when we launched and went public with ADS. This \ncould mean that ADS is having a mitigating effect on political bias in Google search \nresults, although other explanations are possible.   \n \n \n(1) Search engine bias . Above, I have already shown you a graph \nfrom the ADS real -time dashboard suggesting that (a) the liberal \nbias in Google search results since we began monitoring in 2016 \nis still going st rong, and (b) that Google might have begun to \ngradually reduce this bi as since we went public with ADS  in \nNovember, 2023. We are seeing the opposite trend on  Bing and \nYahoo, however (Fig s 31 and 32). Because Bing and Yahoo \ncombined attract only about 3% of  search traffic in the US, bias \nin their search results presents little threat to the free -and-fair \nelection here . That said, why, over the years, Bing and Yahoo \nhave each  drifted away from showing a small conservative bias to \nan increasingly large r libera l bias is a matter that should be \ninvestigated. Relevant here is the fact that Yahoo has not been a \ntrue search engine (that is, a company that aggressively crawls  the \n\nTHE EVIDENCE  \n87 internet to update its index of internet content) since roughly 2002  \n(Kratz, 20 09). Since 2015 , Yahoo has been drawing most or \nperhaps all of its search results from Google ( Fingas, 2015). Less \nclear is whether a secret \u201cpact\u201d that was signed between Google \nand Microsoft (owner of Bing) in early 2016  gave Bing the right \nto begin draw ing s earch information from Google . At the \nmoment, I regard the shift in political bias in Bing and Yahoo \nsearch results to be a mystery.  \n(2) Targeting individuals . Here are three examples of how our data \ncan b e used to investigate how content on Google or oth er \nplatforms might be putting certain individuals in either a positive \nor negative light. In the first, we see that users seeking \ninformation about Ken Paxton \u2013 the conservative Republican \nattorney general  of Texas who has sued Google more than once \u2013 \non the Google search engine are receiving search results that are \nhighly liberally biased ( Figure 3 3) \u2013 in other words, probably \nhostile toward Paxton . In the second, we see that users seeking  \ninformation about liberal Senator Elizabeth Warren \u2013 the rare \nDemoc rat who has called repeatedly for Google\u2019s breakup \n(Warren, 2019; c f. Epstein, 2019 b) \u2013 are receiving highly \nconservative  search results ( Figure 34).  In each case, the same \nlevel of bias is being sent to our liberal, moderate, and \nconservative field agents . In both cases, it would appear that \npolitically biased search results are being used to vilify these \nindividuals. Finally, in our third example, we show a steady \nincrease in liberal bias between March and July 2024 when people \nsearched Google for informa tion about Vice President Kamala \nHarris ( Figure 35).  So our system can not only detect whether  \nGoogle or other search engines favor certain candidates (or \ncauses, brands, historical figures, or religions), it can also show \nhow such bias changes over time.   \n(3) Targeted messages . I mentioned earlier that on Election Day in \nFlorida in 2022, Google appeared to be sending substantially \nmore go -vote reminders to liberals than to conservatives. At this \nwriting (August 26, 2024) \u2013 and this observation is subject to \nchange \u2013 it appears that our liberal FAs nationwide are getting \nregister -to-vote reminders on Google\u2019s home page at two -and-a-\nhalf times the rate that our conservative FAs are getting such \nEPSTEIN  \n \n88 \n  reminders. It is not inconceivable that those messages could turn \ninto partisan mail -in-your-ballot reminders, which could  then \nbecome partisan go -vote reminders. Recall that a simple \nextrapolation from the study that Facebook published in Nature  \nin 2012 with faculty members at the University of California San \nDiego suggests  that a partisan go -vote reminder on Election Day \ncould give the favored candidate at least 450,000 additional votes \nthat day.  \n(4) State -by-state differences.  Currently on the ADS dashboard \u2013 \nagain, this is subject to change \u2013 we are showing two maps of the \nUS. The first shows the usual red and blue states, with just seven \n\u201cswing\u201d states (shown in purple) in which the winners determine \nwho wins the 2024 Presidential election (Figure 29).  At this \nwriting (August 17, 2024), it appears that Kamala Harris\u2019 entry \ninto the fray has turned one of those swing states blue. We are also \ndisplaying a map that uses a range of color shades from dark red \n(signifying strong conservative bias) to dark blue (signifying \nstrong liberal bias) to show, in real time, the mean bias in the \nsearch results Google is showing to our FAs state -by-state. As of \nthis writing (August 17, 2024), 49 of the 50 states are various \nshades of blue. The only red state is Alaska, in which the \nconservative bias is a paltry .03 on a scale from -1.00 to +1.0 0. \nOver the past year that we have been tracking state -by-state bias in \nGoogle search, we have only found a few days when any states \nturned red (never more that five), and we have never  seen one of \nthe swing states turn red.  \n  \nTHE EVIDENCE  \n89 Figure 31. Political bias in  Bing search results.   \n \n       Figure 32 . Political Bias in Yahoo search results.  \n \n \n\nEPSTEIN  \n \n90 \n   \nFigure 33. Political bias in Google searches for Attorney G eneral \nKen Paxton of Texas.  \n \n \nFigure 34. Political bias in Google search es for Senator Elizabeth \nWarren (D, MA).  \n\nTHE EVIDENCE  \n91  \n \nFigure 35. Political bias  is Google searches for Vice President \nKamala Harris . \n \n \n(5) Political bias on YouTube . We are also capturing hundreds of \nthousands of images and videos from YouTube, guided entirely by \nrecommendations Google\u2019s up -next algorithm  makes. One way we \nhave analyzed this content so far is to look at the political bias in \nnews sources of the recommended videos. Once again, we have \nfound liberal bias fairly consistently (Figure 36). \n(6) Disturbing content going to children and teens . Current ly on ADS, \nwe are also showing some of the gruesomely  violent and sexually \nexplicit  content we have preserved on videos that are being \nrecommended to children and teens on YouTube. We offer a few \nexamples in the Supplementary Materials ( S9 to S15  Figures). \n \n By the way, if my graphs don\u2019t impress you, please bear in mind \nwhat these data say about the actual content  people are seeing. When \n\nEPSTEIN  \n \n92 \n  someone clicks on a high -ranking search results shown in searches about \nRepublican Attorney General Ken Paxton, he or she is li kely to be \nbrought to a web page containing a headline such as, \u201cReports of Texas \nAG\u2019s Attempts to Collect Data on Trans Adults Are a Terrifying \nOverstep Into the Lives and Privacy of LGTBQ+ People\u201d (Figure 37). \nAnd when someone clicks on a high -ranking se arch result shown in \nsearches about Democratic Senator Elizabeth Warren, he or she is likely \nto be brought to a web page containing a headline such as, \u201cThe Socialist \nMoment Hasn\u2019t Passed. It\u2019s Yet to Come \u201d (Figure 38).  This is the type \nof ephemeral conten t we are preserving; we make our graphs by \naggregating thousands of examples like these.  \n We will soon be adding to the dashboard figures that summarize \nsome of the content we have been collecting on other platforms, among \nthem TikTok, Facebook,  Instagram,  and X.  \n The findings I have summarized above are not all the interesting \nthings that ADS can show us. These are simply some of the main findings \nwe are currently displaying on our public dashboard, and these findings \nare updated every 5 min, which means t hey are likely to change over \ntime.  \n Rather than show you other findings, below, in the final section of \nthis essay, I will offer  you a vision \u2013 an idea of the possibilities. We have \nthus far preserved terabytes of data, including tens of millions of search \nsuggestions, millions of YouTube recommen dations (which are \nephemeral, of course), plus millions of images and HTML files from \nTikTok, Instagram, Facebook, Twitter, and other platforms.  What other \ninformation might we preserve ? \n An old professor of mine used to say that \u201crepetition is the mother of \nwisdom,\u201d so I am going to repeat, yet a gain, that all of these data are \nephemeral \u2013 data that impact people and then disappear \u2013 normally \nforever. What are these data telling us at the moment, and what might \nthey tell us moving forward? And where we have detected what appear \nto be signs of poli tical bias in the content we have preserved, should we \nbe concerned? If so, what actions might we take to mitigate such bias?  \nTHE EVIDENCE  \n93  \nFigure 36 . Political  bias in  Youtube\u2019s recommended video \ncontent . \n \n  \n\nEPSTEIN  \n \n94 \n  Figure 37. Webpage vilifying AG Ken Paxton from high ranki ng \nsearch results on Google.  \n \n \nFigure 38. Wepage vilifying Senator Elizabeth Warren from high \nranking search results on Google.  \n  \n\nTHE EVIDENCE  \n95  \nSummary  and Conclusions  \n \n The internet has made it possible for a small number of technology \ncompanies to dominate the think ing, behavior, and votes of more than 5 \nbillion people worldwide using new subliminal techniques. We have \ndiscovered and quantified 10 of these techniques in controlled \nexperiments we have been conducting and publishing since 2013, and , \nin 2016, we develop ed technology that allowed us to preserve search \nresults on multiple search engines. Search results, like newsfeeds and \nvideo sequences, are types of  ephemeral content that influence thinking \nand behavior and then disappear, leaving no paper trail for auth orities to \ntrace. In 2018, 2020, and 2022, we improved and  expanded our \nmonitoring system  to preserve a wide variety of online content in the \ndays leading up to elections  held in the US in those years .  \n We build our systems by recruiting real voters aroun d the U.S. \u2013 in \n2016, just 95 voters in 24 states \u2013 and, with their permission, installing \ncustom software on their computers that allows us to stream the political \ncontent they see to our servers, where we quickly aggregate and analyze \nthe data. In our sm all 2016 project, we preserved 13,207 politically -\nrelated searches, along with the 98,044 web pages to which the search \nresults linked. W e found substantial political bias on the most popular \nsearch engine (Google), sufficient to have shifted at least 2.6 million \nvotes in the Presidential election that year. In 2022, through the \ncomputers of a politically -balanced group of 2,742 registered voters, we \npreserved more than 2.5 million ephemeral experiences on multiple \nplatforms, which tended, once again, to be  highly biased politically. In \nlate 2022, we began to build a permanent nationwide monitoring system \n\u2013 our \u201c America\u2019s Digital Shield\u201d project. As of this writing (August 17, \n2024), we have preserved more than 97  million ephemeral experiences \non multiple pl atforms through the computers of a politically -balanced \ngroup of more than 15 ,000 registered voters in all 50 states, with the \nsystem growing larger each day. This system has the potential to make  \nBig Tech companies accountable to the public for the first time and for \nthe foreseeable future, forcing them to constrain their algorithms so that \nthey do not interfere with our free -and-fair elections, the impressionable \nminds of our children, and our own  autonomy.  \nEPSTEIN  \n \n96 \n   Imagine the possibilities . What might one do w ith the wealth of \nephemeral content we have been preserving? The tantalizing truth is that \nI don\u2019t know . I can only tell you two things I know for sure: In our \nnational system, we have now preserved more than a year\u2019s worth of data  \non multiple platforms, w ith the database growing every day. So one thing \nwe can do is to look back in time  at ephemeral content \u2013 something that \nhas never been possible before. There is practically no limit to the kinds \nof patterns and trends one might search for.  \n I find the se cond possibility even more intriguing. We frequen tly \nadjust and expand the parame ters we use to track and preserve data, \nlooking at more kinds of data on more platforms. Again, there is \npotentially no limit to the kind of data we can preserve. We also have  \nbuilt devices that will allow us to preserve the spoken answers home \nsurveillance devices such as Alexa and Google Home give to users. Over \ntime, our expanding pool of FAs will be equipped with these d evices, \nand we will be parsing these answers, looking for content that might \nimpact people\u2019s thinking and behavior in any number of ways.  \n We are also beginning to collect more and more of the content being \ngenerated by AIs such as ChatGPT. In a few short years, search engines \nwill likely be used only by scho lars and scientists. The vast majority of \nhumankind will simply ask questions of their devices, and they will \nlikely believe the answers they read or hear (see the section in Part Two \nof this essay on the Answer Bot Effect [ABE]). Unfortunately, nearly all  \nthe content being generated at the moment by answerbots is \u2013 that\u2019s right \n\u2013 ephemeral . Without sophisticated monitoring systems in place to \ncapture such content, no one will know what that content is or how it \nmight be affecting elections, our children, a nd you and me.  \n What \u2019s more, our ability to preserve and analyze  content from \ngenerative AIs means that we can, going forward, track the extent to \nwhich AIs present serious threats to humanity . Real-time m onitoring \nsystems can p rovide active threat assessment ; they might  prove to be \nhumanity \u2019s best defense against the threats AI pose.  \n The good news here is that monitoring is tech , and it can, with \nadequate resources, move as fast as the tech industry does. Laws and \nregulations move at Turtle Speed; whereas technology moves at, well, \nLudicrous Speed (to borrow a phrase fr om the classic film, \u201cSpaceballs\u201d \n\u2013 a term that was also borrowed by Tesla to describe its fastest \nacceleration rate ). I don\u2019t think we should give up on our legislators and \nTHE EVIDENCE  \n97 regulators, but given the accelerating  rate at which  technology is \nchanging (speak ing of acce leration), I think we need to keep our \nexpectations low. Ideally, data from monitoring systems might help our \ngovernment officials to craft more meaningful and effective laws and \nregulations to keep tech in check.  \n That question about censorship . This brings me to the question I \nraised at the very beginning of this essay. Is Google suppressing \nconservative content? I summarized three studies that concluded that \nGoogle was unbiased in this regard ( Economist, 2019; Lewis et al., 2023; \nMetaxa et al. , 2019 ), and I argued that none of those studies had been \nconducted with designs adequate to support their conclusions.  \n Here are  some of  the problems I pointed out in those studies, and here \nis how our monitoring system s have  overcome those problems:  \n The bot problem. First, in both The Economist study and the Stanford \nstudy, content was being collected by computer programs that had \nclearly and unequivocally identified themselves a s bots \u2013 in other words, \nas nonhumans. That is a fatal flaw in both studies . As The Economist \nauthors admitted, \u201cOur study does not prove Google is impartial. In \ntheory, Google could serve un -biased links only to users without a \nbrowsing history.\u201d  \n We solved this problem by recruiting real people using their own \ncomputers \u2013 peopl e who presumably were known intimately by Google\u2019s \nalgorithms. One must collect ephemeral content through the computers \nof real people because (a) such content is frequently personalized , and \nyou cannot see personalized content without looking over the sho ulders \nof real persons , and (b) as I have shown you with our own data (see \nabove for content we tracked during the 2016 and 2020  monitoring \nprojects, for example), Google turns off bias  in search results whenever \nit pleases. There is only one way to see th e actual content Google is \nsending to real users, and that is to recruit a large, representative sample \nof people, install passive monitoring software on thei r computers, protect \nthe identit ies of those individuals (just as the Neilsen company protects \nthe identities of its families), and then aggregate, preserve, and analyze \nthe data. That is what my team and I have been doing with increasing \nexpertise and efficiency in multiple monitoring projects since 2016.  \n The search term problem . Second,  in those stu dies I critiqued, the \nresearchers used a small number of search terms: 31 in The Economist \nstudy and 4 in the Lewis study. In the Stanford study, only the names and \nEPSTEIN  \n \n98 \n  states of political candidates were used as search terms, and those same \nnames were used in  searches on simulated computers every day for 6 \nmonths. In all three studies, it would have been a simple matter for \nGoogle to identify those unusual searches and to reply with atypical data. \nMoreover, n one of these studies attempted to evaluate the polit ical \nneutrality of their search  terms; as we have noted, non -neutral search \nterms will, by necessity, produ ce non -neutral search result s (Kulshrestha \net al., 2019 ).  \n We avoid  such difficulties by using long lists of tren ding search \nterms that have  been ra ted as neutral by independent raters. In our 2016 \nmonitoring project, we ultimately used 250 such terms , and in our \nsubsequent projects, we have used at least 500 such terms for each \nelection we have monitored . For security reasons, we don\u2019t reveal the \nexact number of terms we are using for America\u2019s Digital Shield.  \n The Google problem . Third,  I noted above that The Economist  was \nclosely associated with Google at the time they published their study. \nThe Metaxa et al. (2019) study also seemed tainted becaus e of Stanford\u2019s \nclose ties with Google . In contrast, AIBRT, the institute where I conduct \nmy research, is a nonprofit, nonpartisan, 501(c)(3) public charity. More \nimportant, we do not accept restricted gifts . This is important, because it \nmeans that donors  cannot tell us how to use their donations. Perhaps even \nmore important, as I disclosed at the beginning of this essay, I lean left \npolitically; if anything, I should be praising Big Tech companies for their \npolitical bias. I don\u2019t  praise them because I lo ve my country and our \nsystem of government more than I love any particular party or candidate, \nand my 12 years of rigorous research on online manipulation has \nconvinced me that Eisenhower\u2019s 1961 prediction has come true: The \ntechnological elite are now in control . They have undermined the \nintegrity of the free -and-fair election, which is a cornerstone of \ndemocracy , and U.S. authorities have not constrained the conduct of \nthese companies in any way through laws and regulations . In addition, \nthe data we are n ow collecting from the devices of children and teens \n(with their parents\u2019 permission) is telling me that Big Tech companies \nmay be indoctrinating our children \u2013 at the moment, in ways I do not \nfully understand.  \n \n Here, then, to conclude this essay, are the  reasons I believe that a \npermanent, large -scale , multipartisan, passive  monitoring system must \nTHE EVIDENCE  \n99 be established quickly in the US and, almost certainly, in other \ndemocracies around the world:  \n 1) Online ep hemeral conten t is unprecedented in its ability to \nmanipulate people\u2019s thinking  and behavior.  The research my associates \nand I have been conducting since 2013 shows unequivocally that Google, \nand, to a lesser extent, other tech companies, are currently using \nephemeral content \u2013 fleeting data that impacts pe ople and then \ndisappears, normally leaving no paper trail \u2013 to impact the thinking and \nbehavior of most of the people on earth, probably every day. Even when \nthese manipulations are not deliberate on the part of tech company \nemployees or executives, their algorithms  are producing these changes. \nAs I explained in  the section on SEME in Part Two  of this essay, search \nengines are inherently biased ; they always  filter and order the answers \nthey show you, and that changes the thinking o f people who haven\u2019t yet \nmade up  their minds. The search engine is the most powerful mind \ncontrol machine ever invented, and its recent integration with generative \nAIs will greatly increase its power in the very near future. The output of \nsearch engines \u2013 search suggestions, answer  boxes, and search results \u2013 \nis all ephemeral. If we don\u2019t capture this fleeting content, we will have \nno idea why people are thinking and behaving the way they do . We will \nalso have no idea about how tech companies are impacting our elections, \nand democra cy will be little more than  an illusion.   \n 2) Ephemeral content is controlled mainly by monopolies . If \nthousands of companies were competing against each other to get our \nattention using ephemeral content, such content would not present a \nserious threat. A fter all, that is exactly how news organizations operate. \nAs a whole \u2013 at least in a free democracy \u2013 they tend to cancel each other \nout. But, as a federal court recently ruled in the case U.S. Department of \nJustice vs. Google  (Roller, 2024 ), Google is a v ast monopoly, controlling \nabout 92% of search worldwide. In an article I published in Bloomberg \nBusinessweek  in 2019  (Epstein, 201 9c), I proposed an easy way to end \nGoogle\u2019s search monopoly, and that  was by declaring its \u201cindex\u201d \u2013 the \never-expanding databa se it uses to generate search results \u2013 to be a public \ncommons. This is a regulatory maneuver that has been practiced for \ncenturies, brought to bear whenever a service or commodity becomes a \nnecessity: think water, gasoline, or telephone communications. Wi th the \nentire world having access to Google\u2019s index, thousands of competing \nsearch engines will soon be established, each serving niche audiences \nEPSTEIN  \n \n100 \n  and each vying for our attention \u2013 exactly as the news organizations do \nnow. Search would become competitive a gain \u2013 as it was when Google \nwas founded \u2013 and it would also become innovative  again. There has \nbeen no innovation in search ever since Google began to dominate that \nindustry 20 years ago. In the meantime, because Google still has so much \npower, we must  track and preserve its output in order to understand what \nit\u2019s doing.  \n 3) Nearly all of the largest information -handling companies in the \nworld , along with the generative AIs they have developed,  share the same \npolitical values.  As I noted earlier, various s tudies have confirmed tha t \n95% of donations from Silicon Valley tech companies go to Democrats  \n(Oberhaus, 2020 ). Their employees share similar values, and this \nhomogeneity gets expressed in the algorithms they write, as well as in \nthe frequent manual adjus tments they make to how their algorithms \nwork. As I noted in the section on the multiple platforms effect (MPE) \nin Part Two of this essay, our latest research shows that when people are \nexposed to similarly biased content on different platforms, the net ef fect \nis additive. Because the content on these platforms is ephemeral, we must \nmonitor, preserve, and analyze this content in order to understand how \nthese companies are impacting our society.  \n 4) Without monitoring systems in place, we will have no idea h ow \ntech companies are using ephemeral content to manipulate our society, \nour elections, and our people.  Please forgive my redundancy here, but \nimagine moving forward with no monitoring systems in place. That will \nmean that the profound impact that tech com pany algorithms are having \non voters, on children, and even on ourselves, will remain a complete \nmystery. Moreover, if you wanted to know how tech companies might \nhave interfered with a past election, it would take a time machine to find \nout.  \n 5) The data  obtained from monitoring systems will help government \nofficials to craft more meaningful and effective laws and regulations to \nkeep tech companies in check.  Perhaps the main reason Big Tech \ncompanies are so entirely out of control these days is because ou r \nlegislators and regulators never envisioned the kind of threats they now \npose. If anything, with the passage of Section 230 of the \nCommunications  Decency Act  of 1996, Big Tech companies have been \nshielded from litigation ( Electronic Frontier Foundation, n.d.). In other \nwords, they are not only unregulated, they also are protected from legal \nTHE EVIDENCE  \n101 actions that might be filed against them. Many of our lawmakers are \nstruggling now to figure out what kinds of laws are needed to constrain \nthe tech companies. Without  the data from monitoring systems, they will \nbe effectively blinded  in their efforts to create effective laws.  \n 6) Without monitoring systems in place, our public officials will have \nno accurate way to measure compliance with laws or regulation they \nenact  to prevent tech companies from interfering with our elections and \nchildren, and with human autonomy itself.  As officials in the EU have \ndiscovered in recent years, monitoring systems are essential tools for \ntracking a tech company\u2019s compliance with new la ws and regulations \nintended to constrain their activities. If Amazon were prohibited by law \nfrom listing its own knockoff products ahead of competing products in \nits product listings, a monitoring system would detect violations of that \nlaw immediately.  \n 7) Real-time monitoring systems can provide active threat \nassessment of the potentially existential threats that generative AI \nsystems pose to humankind.  \n 8) Monitoring systems have the potential to make Big Tech \ncompanies accountable to the public.  The tech companies are private  \ncorporations , accountable at the moment only to their shareholders. They \nalso tend to be highly secretiv e (Carter, 2021; Lima, 2022 ; Pegoraro, \n2019 ). Monitoring systems have the potential to make  these companies \naccountable to the public for the first time, especially if the findings from \nsuch systems are made available to the public on real -time dashboards  \n(see https://AmericasDigitalShield.com ).  \n Supreme Court Justice Louis B. Brandeis is often remembered for a \nstatement he made about sunlight a century ago. We remember the \nstatement, inaccurately, as \u201cSunlight is the best disinfecta nt.\u201d What he \nactually wrote is even stronger: \u201cIt is said that sunlight is the best of \ndisinfectants, and streetlamps the best policemen\u201d ( Brandeis, 1913 ). \nWhere emerging technologies have the potential to harm our society and \nour children, monitoring syst ems are as essential as sunlight.  \n \n  \nEPSTEIN  \n \n102 \n  Funding  \n \nNo external funding was provided.  \n \nEthics Approval a nd Consent t o Participate  \n \nApproval to conduct the studies described in this essay was granted by \nthe Institutional Review Board (IRB) of the American Institu te for \nBehavioral Research and Technology (AIBRT). AIBRT is registered \nwith the HHS Office for Human Research Protections (OHRP) under \nIORG0007755. AIRBT\u2019s  IRB is registered with OHRP under number \nIRB00009303, and the Federalwide Assurance number for our I RB is \nFWA00021545.  \n \nConflict o f Interest  \n \nThe author  declare s no conflict of interest, financial or otherwise.  \n \nAcknowledgements  \n \nThis essay  is based i n part on a paper s presente d at the annual meeting \nof the Association for Psychological Sciences in 2013,  at the \nInternational Convention of Psychological Science  in 2017, and at the \nannual  meeting s of the Western Psychological Association  between \n2015 and 2024 . It is also based on testimony before Congressional \ncommittee s in 2019 and 2023. The author thanks the nearly 100  staff \nmembers and interns \u2013 most of them volunteers \u2013 who have helped him  \nconduct research  on online manipulation since 2013. He is also grateful \nto the tens of thousands of people who have made donations, large and \nsmall, to AIBRT , which  have supported  the basic research studies \nreported in this essay  and that have made it possible for AIBRT\u2019s Tech \nWatch team  to build our country\u2019s first nationwide digital monitoring \nsystem. The author is especially grateful to Phillip W. Dyck, a research \nintern at AIBRT , who spent many hours helping me to prepare this essay.  \n \n \nBiography  \n \nTHE EVIDENCE  \n103 ROBERT EPSTEIN is Senior Research Psychologist at the American \nInstitute for Behavioral Research and Technology (AIBRT) and the \nformer editor -in-chief of Psychology Today magazine. A Ph.D. of \nHarvard University, where he was the last doctoral student of pioneering \npsychologist B. F. Skinner, Dr. Epstein has published 15 books on stress \nmanagement, motivation, artificial intelligence, creativity, and other \ntopics, as well as more than 300 scientific and popular articles. \nBeginning in March 2020, he published a series of articles proposing a \nsimple and economical way to eradicate the novel coronavirus without \nlockdowns or vaccines (see https://CarrierSeparationPlan.com ). He is \nalso a pioneer in the study of online manipulation. His 2015 report in the \nProceedings of the National Academy of Sciences  on the \u201cSearch Engine \nManipulation Effect\u201d (SEME, pronounced \"seem\") \n(https://SearchEngineManipulationEffect.com ) describes one of the \nmost powerful types of influence ever discovered in the behavioral \nsciences, and because SEME leaves no paper trail and is invisible to \nusers, it  is especially dangerous. Dr. Epstein's research suggests that \nSEME and a dozen other new methods of online influence he has \ndiscovered pose a serious threat to democracy, free speech, our children, \nand human autonomy. In July 2019, Dr. Epstein testified b efore a \nCongressional committee about his research on online manipulation (7 -\nmin. video here: https://EpsteinTestimony.com ). In December 2023, Dr. \nEpstein testified again before Congress, this time reporting on his \nsuccess in creating the world's first large -scale system for preserving and \nanalyzing the ephemeral content Big Tech companies might be using to \ninfluence elections, children, and the adult human mind. His new \ntestimony is accessible at https://2023EpsteinTestimony.com , and a \ndashboard that summarizes the data his new monitoring system is \ncollecting can be viewed at https://AmericasDigitalShield.com .   \nEPSTEIN  \n \n104 \n   \nReferences  \n \nAdams, J. (1780) From John Adams to Jonathan Jackson, 2 October 1780. \nFounders Online. https://founders.archives.gov/documents/Adams/06 -\n10-02-0113  \nAgudo, U., & Matute, H.  (2021). The influence of algorithms on political and \ndating decisions. PLOS ONE, 16 (4), e0249454. \nhttps://doi.org/10.1371/journal.pone.0249454   \nAkbar, A., Caton, S., & Bierig, R. (2023). Persona lised Filter Bias \nwith Google and  DuckDuckGo: An Exploratory Study. In: Longo, L., \nO\u2019Reilly, R. (eds) Artificial Intelligence and Cognitive Science. AICS \n2022. Communications in Computer and Information Science, vol \n1662. Springer, Cham. https://doi.org/10.1007/978 -3-031-26438 -2_39  \nAmiri, F. (2023). GOP subpoenas tech CEOS as part of probe into \ncensorship . AP News. https://apnews.com/article/technology -us-\nrepublican -party -tim-cook -business -sundar -pichai -\nce0e00d7ddeff8c08a95a7fbf01080f8  \nAngwin, J. & Faturechi, R. (2014). Stanford Promises Not to Use Google \nMoney for Privacy Research . ProPublica. \nhttps://www.propublica.org/article/stanford -promises -not-to-use-\ngoogle -money -for-privacy -research  \nBall, J. ( 2023). Online Ads Are About to Get Even Worse . \nhttps://www.theatlantic.com/technology/archive/2023/06/advertising -\nrevenue -google -meta -amazon -apple -microsoft/674258/  \nBarrett, P. M., & Sims, J. G. (2021). False Accusation: The unfounded claim \nthat social media companies censor conservatives . NYU Stern Center \nfor Business and Human Rights. Retrieved from: \nhttps://static1.squarespace.com/static/5b6df958f8370af3217d4178/t/6\n0187b5f45762e708708c8e9/1612217185240/NYU+False+Accusation\n_2.pdf  \nBerger, J. (2016). Invisible Influence: The Hidden Forces That Shape \nBehavior . Simon & Schuster.  \nBerners -Lee, Tim. (2018, Mar 12). The web can be weaponised \u2013 and we \ncan't count on big tech to stop it: It\u2019s dangerous having a handful of \ncompanies control  how ideas and opinions are shared. A regulator \nmay be needed.  The Guardian. Retrieved from: \nhttps://www.theguardian.com/commentisfree/20 18/mar/12/tim -\nberners -lee-web-weapon -regulation -open -letter  \nBlock, M. (2021, December 23).  The clear and present danger of Trump\u2019s \nenduring \u2018Big Lie.\u2019 NPR. Retrieved from: \nTHE EVIDENCE  \n105 https://www.npr.org/2021/12/23/1065277246/trump -big-lie-jan-6-\nelection   \nBlodget, H. (2012). Google Caught Secretly Hacking Apple Software To \nTrack Apple iPhone and Mac Users . Business Insider. \nhttps://www.businessinsider.com/google -tracking -apple -users -2012 -\n2?IR=T  \nBogert, E., Schecter, A., & Watson, R. T. (2021). Humans rely more on \nalgorithms than social influence as a task becomes more difficult. \nScientific Reports , 11. https://doi.org/10.1038/s41598 -021-87480 -9  \nBolyard, P. (2018, August 25). 96 percent of my Google search results for \n\u2018Trump\u2019 news were from liberal media outlets . PJ Media. Retrieved \nfrom: https://pjmedia.com/news -and-politics/paula -\nbolyarda/2018/08/25/google -search -results -show -pervasive -anti-\ntrump -anti-conservative -bias-n60450  \nBond, R. M., Fariss, C. J., Jones, J. J., Kramer, A. D. I.; Marlow, C., Settle, J. \nE., Fowler, J. H. (2012). A 61 -million -person experiment in social \ninfluence and political mobilization. Nature, 489(7415), 295 \u2013298. \ndoi:10.1038/nature11421  \nBurell, I. (2019). Are Google and Facebook killing advertising?. Raconteur. \nhttps://www.raconteur.net/marketing -sales/google -facebook -duopoly  \nBusti llo, X. (2023). It takes lots of money to win elections. Here's what you \nneed to know . NPR. \nhttps://www.npr.org/2023/11/01/1205728664/c ampaign -finance -\ndonations -election -fec-fundraising -ad-spending  \nCarlson, M. (2018).  Facebook in the News. Digital Journalism, 6(1), 4 \u2013\n20. doi:10.1080/21670811.2017.1298044   \nChen, W., Pacheco, D., Yang, K. C., & Menczer, F. (2021). Neutral bots \nprobe politi cal bias on social media.  Nature communications , 12(1), \n5580. https://doi.org/10.1038/s41467 -021-25738 -6 \nCheng, J. (2023). The Race to Build a Chat GPT Powered Search Engine . \nWired. https://www.wired.com/story/the -race-to-build -a-chatgpt -\npowered -search -engine/  \nChimielewski, D. (2016). A chart of lobbyists\u2019 White House visits reveal its \nclose ties with Goog le. recode. \nhttps://web.archive.org/web/20160525162743/https://www.recode.net\n/2016/4/26/11586424/google -white -house -visits  \nCoelen,  J. (2023). How Google struggled to monetize its search monopoply . I \nWant Product Market -fit. \nhttps://iwantproductmarketfit.substack.com/p/how -google -struggled -\nto-monetize  \nEPSTEIN  \n \n106 \n  Common Sense. (2022). Teens and Pornography . \nhttps://www.commonsensemedia.org/sites/default/files/research/repor\nt/2022 -teens-and-pornography -final-web.pdf  \nConstine, J. (2014). Leaked Documents Show How Yelp Thinks It\u2019s Getting \nScrewed By Google . Tech Crunch. \nhttps://techcrunch.com/2014/07/09/yelp -googl e-anti-trust/  \nCutts, M. (2011). Google search and search engine spam. Google Blog Spot. \nhttps://web.archive.org/web/ 20110121212328/http://googleblog.blogs\npot.com/2011/01/google -search -and-search -engine -spam.html  \nDame, N. (2015). They Fooled Us All: Why Google May No Longer \nAnnounce Major Algorithm Updates. Search Engine Land . \nhttps://searchengineland.com/fooled -us-google -no-longer -announces -\nmajor -algo-updates -217494  \nDavies, C. (2009). Google Blacklists Entire Internet. The Guardian. \nhttps://www.theguardian.com/technology/2009/jan/31/google -\nblacklist -internet  \nDayen, D.  (2016). Google\u2019s unusually close relationship with the White \nHouse raises lots of questions . Mashable. \nhttps://mashable.com/article/google -relationship -obama -white -house  \nDe Witte, M. (2019, November 26). Search results not biased along party \nlines, Stanford scholars find. Stanford News Service. Retrieved from: \nhttps://news.stanford.edu/pr ess-releases/2019/11/26/search -media -\nbiased/  \nDean, B. (2023). Here\u2019s What We Learned About Organic Click Through \nRate. BackLinko. https://backlinko.com/google -ctr-stats.  \nDesjardins, J. (2017). How Go ogle Tracks You \u2013 And What You Can Do \nAbout it . Visual Capitalist. https://www.visualcapitalist.com/how -\ngoogle -tracks -you/ \nD\u2019Onfro, J. (2018, December 11). Google\u2019s Sundar Pichai was g rilled on \nprivacy, data collection, and Chine during congressional hearing . \nCNBC. Retrieved from: https://www.cnbc.com/2018/12/11/google -\nceo-sundar -pichai -testifies -before -congress -on-bias-privacy.html  \nD\u00f6pfner, M. (2014, Apr 17). Why we fear Google. Frankfurter Allgemeine \nZeitung . http://www.faz.net/aktuell/feuilleton/debatten/mathias -\ndoepfner -s-open -letter -to-eric-schmidt -12900860.html  \nD\u2019Souza, D., Engelbrecht, C., & Phillips, G. (Producers), & D\u2019Souza, D., \nD\u2019Souza, D., & Schooley, B. (Directors). (2022). 2000 Mules  \n[Motion Picture]. United States: D\u2019Souza Media.  \nDuffy, C. (2021, October 4). Facebook whistleblower revealed on \u201960 \nMinutes,\u2019 says the company prioritized profit over public good.  CNN. \nRetrieved from: https://edition.cnn.com/2021/10/03/tech/facebook -\nwhistleblower -60-minutes/index.html   \nTHE EVIDENCE  \n107 Dunn, J. (2016). The tech industry\u2019s major players are firmly behind Hillary \nClinton. Business Insider . https://www.businessinsider.com/tech -\ncompany -donations -clinton -vs-trump -chart -2016 -11 \nThe Economist Group. (n.d.). About Us.  Retrieved from: \nhttps://www.economistgroup.com/about -us  \nThe Economist. (2019, June 8). Google rewards reputable reporting, not left -\nwing politics.  Retrieved from: https://www.economist.com/graphic -\ndetail/2019/06/08/google -rewards -reputable -reporting -not-left-wing -\npolitics   \nEdelman, B. (2011). Bias in search results? Diagnosis and response. Indian \nJournal of Lar and Technology , 7, 16 -32. Retrieved from: \nhttps://www.ijlt.in/journal/bias -in-search -results%3F%3A -diagnosis -\nand-response   \nEisenhower, D. D. (1961, January 17). Military -industrial complex speech. \nYale Law School. Retrieved from: \nhttps://avalon.law.yale.edu/20th_century/eisenhower001.asp   \nEpstein, R. (2012 a, September 5). Google: The Case For Hawkish \nRegulation . The Kernel. \nhttps://web.archive.org/web/20121016112820/http://www.kernelmag.\ncom/features/report/3281/g oogle -the-case-for-hawkish -regulation/  \nEpstein, R. (2012 b, October 23). Why Google Should Be Regulated (Part 1).  \nHuffington Post. https://www.huffpost.com/entry/google -\nprivacy_b_196282 7 \nEpstein, R. (2012 c, October 31). Why Google Should Be Regulated (Part 2).  \nHuffington Post. https://www.huffpost.com/entry/online -\nprivacy_b_2013583  \nEpstein, R. (2012 d, November 2). Why Google Should Be Regulated (Part 3) . \nHuffington Post . https:// www.huffpost.com/entry/why -google -should -\nbe-regu_b_2054111  \nEpstein, R. (2012 e, November 5). Why Google Should Be Regulated (Part 4) . \nHuffington Post. https://www.huffpost.co m/entry/why -google -should -\nbe-regu_b_2069223  \nEpstein, R., & Robertson, R. E. (2015). The search engine manipulation effect \n(SEME) and its possible impact on the outcomes of elections. \nProceedings of the National Academy of Sciences USA, 112 (33), \nE4512 -E452 1. ttps://doi.org/10.1073/pnas.1419828112  (forwarding \nlink: https://SearchEngineManipulationEffect.com )  \nEpstein, R. (2016 a, June 22). The Ne w Censorship . U.S News & World \nReport. \nhttps://web.archive.org/web/2024050600224 0/http://www.usnews.co\nm/opinion/articles/2016 -06-22/google -is-the-worlds -biggest -censor -\nand-its-power -must -be-regulated  \nEPSTEIN  \n \n108 \n  Epstein, R. (2016 b, September 6). Free Isn\u2019t Freedom: How Silicon Valley \nTricks Us . Vice. https://www.vice.com/en/article/8q8vav/free -isnt-\nfreedom -epstein -essay  \nEpstein, R. (2018 a). How Major News Organizations, Universities and \nBusinesses Surrender Their Privacy to Google . Daily Caller. \nhttps://dailycaller.com/2018/08/27/surrender -privacy -google/  \nEpstein, R. (2018b). Not Just Conservatives: Google and Big Tech Can Shift \nMillions of Votes in Any Direction . USA Today. \nhttps://eu.usatoday.com/story/opinion/2018/09/13/google -big-tech-\nbias-hurts -democracy -not-just-conservatives -column/12650200 02/ \nEpstein, R. (2019a, January 2). How Google Shifts Votes: A \u2018Go -\nVote\u2019Reminder Is Not Always What You Think It Is. Epoch Times. \nhttps://www.theepochtimes.com/opinion/another -way-google -\nmanipulates -votes -without -us-knowing -a-go-vote-reminder -is-not-\nwhat -you-think -it-is-2754073  \nEpstein, R. (2019b, March 22). Google, Facebook, Amazon: Warren's \ntoothless break -up plan ignores real Big Tech threats: Surveillance \nand manipulation are the real threats these companies pose. They'd \nstill have more unchecked power than any dictator under Warren's \nplan. USA Today. \nhttps://www.usatoday.com/story/opinion/2019/03/22/elizabeth -\nwarren -plan-misses -dangers -facebook -amazon -google -surveillance -\ncolumn/320545100 2/ \nEpstein, R. (2019 c, July 15). To break Google\u2019s monopoly on search, make its \nindex public. Bloomberg Businessweek. \nhttps://aibrt.org/downloads/EPSTEIN -15July2019 -\nBUSINESSWEEKTo_Break_Googles_Monopoly_on_Search.pdf \n(forwarding link: https://EpsteinInBus inessWeek.com) (also behind \npaywall at https://www.bloomberg.com/news/articles/2019 -07-15/to -\nbreak -google -s-monopoly -on-searchmake -its-index -public) (audio \nversion: https://app.newsoveraudio.com/articles/robert -epstein -\ntobreak -googles -monopoly -on-search -make-its-index -public -981325)  \nEpstein, R. (2019 d, July 16). Why Google poses a serious threat to \ndemocracy, and how to end that threat (written testimony). Before the \nUnited States Senate Judiciary Subcommittee on the Constitution, \nCongressional Record. \nhttps://www.judiciary.senate.gov/imo/media/doc/Epstein%20Testimo\nny.pdf  \nEpstein, R. (2020). The technological elite are now in control. Chapter to \nappear in Vol. 49 of Champi ons of Freedom . Hillsdale College Press. \nRetrieved from:  https://aibrt.org/downloads/EPSTEIN_2020 -\nThe_Technological_Elite_Are_Now_in_Control.pdf  \nTHE EVIDENCE  \n109 Epstei n. R. (2023, December 13). America\u2019s Digital Shield: A new online \nmonitoring system will make Google and other tech companies \naccountable to the public. Testimony before the United States Senate \nJudiciary Subcommittee on Competition Policy, Antitrust, and \nConsumer Rights (written testimony, 480 pp.). Congressional Record.  \nhttps://aibrt.org/downloads/EPSTEIN_2023 -\nAmericas_Di gital_Shield -Written_Testimony -Senate_Judiciary -\n13December2023 -g.pdf  \nEpstein, R., Lee, V., Mohr, R., Jr., & Zankich, V. R. (2022). The Answer Bot \nEffect (ABE): A powerful new form of influence made possible by \nintelligent personal assistants and search en gines. PLOS ONE.  \nhttps://AnswerBotEffect.com   \nEpstein, R., Tyagi, C., & Wang, H. (2023, July 27). What would happen if \nTwitter sent consequential messages to only a strategically important \nsubset of users? A quan tification of the Targeted Messaging Effect \n(TME). PLOS ONE . https://doi.org/10.1371/journal.pone.0284495  \n(forwarding link: https://TargetedMess agingEffect.com , preprint \nposted to SSRN, September 1, 2022, \nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4207187 )  \nEpstein, R., & Flores, A. (in press). The Video Manipula tion Effect (VME): A \nquantification of the possible impact that the ordering of YouTube \nvideos might have on opinions and voting preferences. PLOS ONE . \n(Preprint posted on SSRN, July 31, 2023, \nhttp://dx .doi.org/10.2139/ssrn.4527207 , forwarding link: \nhttps://VideoManipulationEffect.com )  \nEpstein, R., Huang, Y., Megerdoomian, M., & Zankich, V.R. (in press). The \nOpinion Matching Effect (OME): A subtle but  powerful new form of \ninfluence that is apparently being used on the internet. PLOS ONE.  \n(Preprint posted on SSRN, August 4, 2023, \nhttps://dx.doi.org/10.2139/ssrn.4532141 , forwarding link: \nhttps://OpinionMatchingEffect.com )  \nEpstein, R., & Li, J. (2024, March 26). Can biased search results change \npeople\u2019s opinions about anything at all? A close replication of the \nSearch Engine Manipulation Effect (SEME) . *PLOS ONE*. \nhttps://doi.org/10.1371/journal.pone.0300727 (forwarding link: \nhttps://MultipleTopicsResearch.com, preprint posted on SSRN, \nOctober 10, 2023, https://dx.doi.org/10.2139/ssrn.4597654 ) \nEpstein, R., Aries, S., Grebbien, K., Salcedo, A.M., & Zankich, V.R. (2024 a, \nJuly 6). The Search Suggestion Effect (SSE): How search suggestions \ncan be used to impact opinions and votes. *Computers in Human \nBehavior*. https://doi.org/10.1016/j.chb.2024.108342  (Preprint \nposted on SSRN, August 8, 2023, \nEPSTEIN  \n \n110 \n  https://dx.doi.org/10.2139/ssrn.4535163, forwarding link: \nhttps://SearchSuggestionEffect.com)  \n Epstein, R., Lothringer, M., & Zankich, V.R. (2024b , January). How a daily \nregimen of operant conditioning might expla in the power of the \nSearch Engine Manipulation Effect (SEME). *Behavioral and Social \nIssues, 33*, 82 \u2013106. https://doi.org/10.1007/s42822 -023-00155 -0 \nEpstein, R., N ewland, A., & Tang, L. Y. (2024c , August 2). The \"multiple \nexposure effect\" (MEE): How multiple exposures to similarly biased \nonline content can cause increasingly larger shifts in opinions and \nvoting preferences. Available at SSRN: \nhttps:// ssrn.com/abstract=4914689  \n Epstein, R., N ewland, A., & Tang, L. Y. (2024d , August 10). The \"digital \npersonalization effect\" (DPE): A quantification of the extent to which \npersonalizing content can increase the impact of online \nmanipulations. Available at SSRN: https://ssrn.com/abstract = \nEpstein, R., Parsick, T., Sha nkar, P., & Zankich, V.R. (2024e , April). The \nDifferential Demographics Effect (DDE): Post hoc analyses of \nmultiple datasets show the power of a new and  invisible form of \nmanipulation made possible by the internet. Paper presented at the \n104th annual meeting of the Western Psychological Association, San \nFrancisco, CA. https://DifferentialDemographicsEffect.com  \nEvers, A. (2020). How Youtube became an inter net giant . CNBC. \nhttps://www.cnbc.com/2020/11/14/how -googles -youtube -became -an-\ninternet -video -giant.html  \nE-Ventures Worldwide, LLC v. Google, Inc., No. 2:2014cv00646 - Document \n104 (M.D. Fla. 2016). \nhttps://digitalcommons.law.scu.edu/cgi/viewcontent.cgi?article=2410\n&context=historical  \nFarley, N. (2022, N ov 17). Is Google Search Getting Worse? Search Engine \nland. https://searchengineland.com/is -google -search -getting -worse -\n389658  \nFlaherty, S. (2017). Search Engine Manipulatio n Fuels Content Creators \nFears . BrandContent. https://brandcontent.co.uk/outfoxed/search -\nengine -manipulation -fuels -content -creator -fears/  \nFeeney, M.  (2020) Conservative Big Tech Campaign Based on Myths and \nMisunderstanding. Cato Institute. \nhttps://www.cato.org/commentary/conservative -big-tech-campaign -\nbased -myths -misunderstanding  \nFernholz, T. (2016). Hacked emails show Eric Schidmt played a crucial role \nin Team Hillary\u2019s election tech . Quartz. https://qz.com/823922/eric -\nschmidt -played -a-crucial -role-in-team -hillarys -election -tech \nTHE EVIDENCE  \n111 Fingas, J. (2015). Google will provide some of Yahoo\u2019s search results. Yahoo \nNews. https://www.yahoo.com/news/2015 -10-20-google -yahoo -\nsearch -deal.html  \nFlores, R. (2016 ). Hillary Clinton Google Suggestions Accused of Favoring \nCandidate  . CBS News. https://www.cbsnews.com/news/hillary -\nclinton -google -suggestions -accused -favoring -candidate -election -\n2016/  \nForbes. (2024). Why Big Tech Will Remain Beyond Government Control . \nhttps://www.forbes.com/sites/hecparis/2024/01/12/why -big-tech-will-\nremain -beyond -government -control/  \nFredrick, B. (2022, September). Google Announce More Personalized Search \nResults at Search  on Event . Search Engine Journal. \nhttps://www.searchenginejournal.com/google -announces -more -\npersonalized -search -results -at-searc h-on-event/466428/  \nHalliday, J. (2011). Google demotes \u2018low quality\u2019 websites in search \noverhaul. The Guardian. \nhttps://www.theguardian.com/technology/2011/feb/25/goog le-search -\nresults  \nHao, K. (2019). Youtube is experimenting with ways to make its algorithm \neven more addictive . MIT Technology Review. \nhttps://www.tec hnologyreview.com/2019/09/27/132829/youtube -\nalgorithm -gets-more -addictive/  \nHeaven, W. D. (2021). Language models like GPT -3 could herald a new type \nof search engine . MIT Technology Review. \nhttps://www.technologyreview.com/2021/05/14/1024918/language -\nmodels -gpt3-search -engine -google/  \nHindman, M. (2009). The Myth of Digital Democracy. Princeton University \nPress.  \nHorling, B. & Kulick, M. (2009). Personalized Search for everyone . Google. \nGoogle Blogspot. \nhttps://googleblog.blogspot.com/2009/12/personalized -search -for-\neveryone.html?m=1  \nHusz\u00e1r, F., Ktena, S. I., O'Brien, C., Belli, L., Schlaikjer, A., & Hardt, M. \n(2022). Algorithmic amplification of politics on Twitter. Proceedings  \nof the National Academy of Sciences of the United States of America, \n119(1), e2025334119. https://doi.org/10.1073/pnas.2025334119  \nIzundu, C. C. (2015, March 10). Children as young as six \u201cuploading s exually \nexplicit content of themselves .\u201d BBC News. \nhttps://www.bbc.com/news/newsbeat -31710517  \nEPSTEIN  \n \n112 \n  Jaitain, V. (2022). The walled gardens of ad tech, explained . \nBlockthrough.  https://blockthrough.com/blog/the -walled -\ngardens -of-the-ad-tech-industry -explained/  \nKratz, T. (2009). Yahoo ponders the meaning of search.  CNET. \nhttps://www.cnet.com/culture/yahoo -ponders -the-meaning -of-\nsearch/  \nKanter, J. (2018, Dec) This graph shows 90% political donations from big \ntech workers went to the democrats, with Googlers leading the \ncharge . Business Insider. https://www.businessinsider.in/this -graph -\nshows -90-political -donations -from -big-tech-workers -went -to-the-\ndemocrats -with-googlers -leading -the-\ncharge/articleshow/67010538.cms  \nKerns, T. (2022). Fitbit and Google Fit stats spotted on Nest Hub smart \ndisplay . Android Police. https://www.androidpolice.com/fitbit -\ngoogle -fit-stats-nest-hub-displays/  \nKlinkenberg, B. (2023). Tinder Adds \u201cSwipe The Vote\u201d So You Can Hook Up \nWith Candidates . BuzzFeed News. \nhttps://www.buzzfeednews.com/article/brendanklinkenberg/tinder -\nwants -you-tovote#.ep8lDQxX4o  \nGatewood, C. & O\u2019Connor C. (2020). Disinformation Briefing: Narratives \naround Bla ck Lives Matter and voter fraud . Institute for Strategic \nDialogue. https://www.isdglobal.org/isd -publications/disinforma tion-\nbriefing -narratives -around -black -lives -matter -and-voter -fraud/  \nGilder, G. (2018). Life after Google: The fall of big data and the rise of the \nblockchain economy. Regnery Gateway.  \nGogarty, K., Silva, S., & Evans, C. (2020). A New Study Finds That Fac ebook \nis not Censoring Conservatives . Media Matters. \nhttps://www.mediamatters.org/facebook/new -study -finds -facebook -\nnot-censoring -conservatives -despite -their-repeated -attacks  \nGonz\u00e1lez -Bail\u00f3n, S., d'Andrea, V., Freelon, D., & De Domenico, M. (2022). \nThe advantage of the right in social media news sharing. PNAS \nnexus, 1(3), pgac137. doi: 10.1093/pnasnexus/pgac137.  \nGoogle. ( n.d.-a). About Google: How our business works . Retrived July 8, \n2024 from https://about.google/how -our-business -works/   \nGoogle . (n.d. -b). Personalization & Google Searc h results - Google Search \nHelp. https://support.google.com/websearch/answer/12410098?hl=en  \nGoogle. (n.d. -c). Why is my site labeled as dangerous in Google Search?.  \nRetrieved May 9, 2024 from \nhttps://support.google.com/webmasters/answer/6347750?hl=en  \nHardy, Q. (2016, June 7). The web\u2019s creator looks to reinvent it.  New York \nTimes. Retrieved from: \nTHE EVIDENCE  \n113 https://www.nytimes.com/2016/06/08/technology/the -webs -creator -\nlooks -to-reinvent -it.html    \nHawking, S. (2018). Brief Answers To The Big Questions . John Murray.  \nHoward, J. J., & Rabbitt, L. R. , & Sirotin, Y. B. (2020). Human -algorithm \nteaming in face recognition: How algorithm outcomes cognitively \nbias human decision -making. PLOS ONE , 15(8), e0237855. \nhttps://doi.org/10.1371/journal.po ne.0237855  \nLahey, C. & Skopec, C. (2024, Jan 11). 34 Eye -Opening Google Search \nStatistics for 2024 . Semrush Blog. \nhttps://www.semrush.com/blog/google -search -statistics/  \nLangley, H. (20 23). Google recently cut 'people' from its Search guidelines. \nNow, website owners say a flood of AI content is pushing them down \nin search results. https://www.businessinsider.com/google -search -\nhelpful -content -update -results -drop-ai-generated -2023 -\n9?r=US&IR=T  \nLee, A. (2011). Google Takes Down Over 11 Million CO.CC Sites. \nHuffington Post. https://www.huffpost.com/entry/google -\ncocc_n_891696  \nLeith, Douglas. (2023). What Data Do the Google Dialer and Messages Apps \non Android Send to Google?. 10.1007/978 -3-031-25538 -0_29.  \nLewis, P. (2017). 'Our minds can be hijacked': th e tech insiders who fear a \nsmartphone dystopia\u2019.  The Guardian. \nhttps://www.theguardian.com/technology/2017/oct/05/smartphone -\naddiction -silicon -valley-dystopia .  \nLevy, Ari. (2020). Here\u2019s the final tally of where tech billionaires donated for \nthe 2020 election. CNBC. https://www.cnbc.com/2020/11 /02/tech -\nbillionaire -2020 -election -donations -final-tally.html  \nLiao, S. (2018). Google still tracks you through the web if you turn off \nLocation History . The Verge. \nhttps://www.theverge.com/2018/8/13/17684660/google -turn-off-\nlocation -history -data \nLoftus, E. F. (1975).  Leading questions and the eyewitness report. Cognitive \nPsychology, 7(4), 560 \u2013572. doi:10.1016/0010 -0285(75)90023 -7  \nLohr, S. (2021). He Created T he Web. Now He\u2019s Out to Remake the Digital \nWorld. New York Times. \nhttps://www.nytimes.com/2021/01/10/technology/tim -berners -lee-\nprivacy -internet.html  \nLomas,  N. (2016). Why did ProtonMail vanish from Google search results for \nmonths?  Tech Crunch. https://techcrunch.com/2014/07/09/yelp -\ngoogle -anti-trust/  \nEPSTEIN  \n \n114 \n  Jatain, V. (2022). The walled gard ens of ad tech, explained. BlockThrough. \nhttps://blockthrough.com/blog/the -walled -gardens -of-the-ad-tech-\nindustry -explained/  \nJefferson, T. (1789 ). From Thom as Jefferson to Francis Hopkinson, 13 March \n1789 . Founders Online, National Archives. \nhttps://founders.archives.gov/documents/Jefferson/01 -14-02-0402  \nThe Index Project (n.d.). CrowdFlower . Retrieved April 2, 2024, from \nhttps://theindexproject.org/post/crowdflower  \nKanai, R., Feilden, T., Firth, C., & Rees, G. (2011). Political orientations are \ncorrelated with brain stru cture in young adults.  Current biology : \nCB, 21(8), 677 \u2013680. https://doi.org/10.1016/j.cub.2011.03.017  \nKnapp, A. (2018, December 11). Google CEO Sundar Pichai answered \ncongressional questions on Chi na, privacy. political bias and more. \nForbes. Retrieved from: \nhttps://www.forbes.com/sites/alexkn app/2018/12/11/follow -along -\nwith-google -ceo-sundar -pichais -testimony -before -congress -here-\n1/?sh=1b78eb8d3272   \nKulshrestha, J., Eslami, M., Messias, J., Zafar, M. E., Ghosh, S., Gummadi, \nK. P., Karahalios, K. (2019). Search bias quantification: investigati ng \npolitical bias in social media and web search. Inf Retrieval J 22, 188 \u2013\n227 (2019). https://doi.org/10.1007/s10791 -018-9341 -2 \nKulwin, N. (2018, April 16). The internet apologizes.  Intelligencer. R etrieved \nfrom: https://nymag.com/intelligencer/2018/04/an -apology -for-the-\ninternet -from -the-people -who-built-it.html   \nLanier, J. (2018). T en Arguments for Deleting Your Social Media Accounts \nRight Now. Henry Holt & Co. \nhttps://www.theguardian.com/books/2018/ma y/30/ten -arguments -\ndeleting -your-social -media -accounts -right -now-jaron -lanier  \nLanum, N. (2022, March 29). Twitter, Facebook, Google have repeatedly \ncensored conservatives despite liberal doubts . Fox News. Retrieved \nfrom: https://www.foxnews.com/media/twitter -facebook -google -\ncensored -conservatives -big-tech-suspension  \nLazer, D., Robertson, R. E., & Wilson, C. (2018). Auditing the \nPersonalization a nd Composition of Politically -Related Search \nEngine. In WWW 2018: The 2018 Web Conference, April 23 \u201327, \n2018, Lyon, France. ACM, New York, NY, USA, 11 pages.. \nhttps://doi.org/10.1145/3178876.3186143   \nLewis, S. C., Nechushtai, E., & Zamith, R. (2023): More of the Same? \nHomogenization in News Recommendations When Users Search on \nGoogle, YouTube, Facebook, and Twitter. Mass Communication and \nSociety.  https://doi.org/10.1080/15205436.2023.2173609  \nTHE EVIDENCE  \n115 Lipsitz, K. (2009). The Consequences of Battleground and \u201cSpectator\u201d State \nResidency for Political Participation. , 31(2), 187 \u2013209. \ndoi:10.1007/s11109 -008-9068 -7 \nLocklear, M. (2017). Google accidenta lly broke the internet throughout Japan. \nEngadget. https://www.engadget.com/2017 -08-28-google -\naccidentally -broke -internet -japan.html .  \nLowenstein, M. (2016) . Vox. \nhttps://www.vox.com/2016/5/30/11800532/google -consumer -facing -\ncustomer -service  \nLogg, J. M., Minson, J. A., & Moore, D. A. (2018, October 26). Do people \ntrust algorithms more than companies realize? Harvard Business \nReview. Retrieved from: https://hbr.org/2018/10/do -people -trust-\nalgorithms -more -than-companies -realize   \nMaheshone, M. (2021). Over 271 Google & Products You Probably Don\u2019t \nKnow . Masheone. https://maheshone.com/google -products -and-\nservices/  \nMatz, S. C., Teeny, J. D., Vaid, S. S., Peters, H. , Harari, G. M., & Cerf, M. \n(2024). The potential of generative AI for personalized persuasion at \nscale. Scientific reports, 14(1), 4692. https://doi.org/10.1038/s41598 -\n024-53755 -0 \nMayer, W. G. (20 08). The Swing Voter In American Politics. Brookings \nInsititution Press.  \nMcDool, E., Powell, P., Roberts, J., & Taylor, K. (2020). The internet and \nchildren\u2019s psychological wellbeing. Journal of Health Economics, 69. \nhttps://doi.org/10.1016/j.jhealeco.2019.102274   \nMcGee, M. (2012). Google Search Quality Raters Instructions Gain New \n\u201cPage Quality\u201d Guidelines. Search Engine Land. \nhttps://searchengineland.com/google -search -quality -raters -\ninstructions -gain-new-page -quality -guidelines -132436  \nMcKinnon, J. D., & MacMillan, D. (2018, September 20). Google workers \ndiscussed tweak ing search function to counter travel ban: Company \nsays none of proposed changes to search results were ever \nimplemented. The Wall Street Journal. Retrieved from: \nhttps://www.wsj.com/articles/google -workers -discussed -tweaking -\nsearch -function -to-counter -travel -ban-1537488472   \nMcNamee, R. (2020). Zucked: Waking Up To The Facebook Catastrophe \n(Rev.ed). Penguin Books.  \nMetaxa, D., Park , J. S., Landay, J. A., & Hancock, J. (2019). Search media \nand elections: A longitudinal investigation of political search results \nin the 2018 U.S. elections. Proceedings of the ACM on Human -\nComputer Interaction, 3 (CSCW), 1 -17. \nhttps://doi.org/10.1145/3359231   \nEPSTEIN  \n \n116 \n  Moon, H. & Pariseau, G. (2023). Alarming Election Interference! Big Tech \nCensors Biden Opponents 162 times . Censor Track. \nhttps://censortrack.org/alarming -election -interference -big-tech-\ncensors -biden -opponents -162-times  \nMoreno, J. (2019). Google Follows A Growing Workplace Trend: Hiring \nMore Contractors Than Employees . Forbes. \nhttps://www.forbes.com/sites/johanmoreno/2019/05/31/google -\nfollows -a-growing -workplace -trend -hiring -more -contractors -than-\nemployees/  \nMorrison, R. (2022). Companies back \u2018open, private\u2019 internet in face of Big -\nTech \u2018surveillance capitalism\u2019. Tech Monitor. \nhttps://techmonitor.ai/polic y/privacy -and-data-protection/open -\ninternet -big-tech. \nMowery, D.C. & Simcoe, T. (2001). Is the internet a US invention? \u2013 An \neconomic and technological history of computer networking. \nResearch Policy, 31 (8-9), 1369 -1987. https://doi.org/10.1016/S0048 -\n7333(02)00069 -0 \nMullins, B. & Nicas, J. (2017). Paying Professors: Inside Google\u2019s Academic \nInfluence Campaign . Wall Street Journal. \nhttps://www.wsj.com/articles/paying -professors -inside -googles -\nacademic -influence -campaign -1499785286  \nMurphy, K. (2011). Google dumps all 11 + million .co.cc sites from its \nresults. The Register.  \nhttps://web.archive.org/web/20110707043119/http://www.theregister.\nco.uk/2011/07/06/google_cans_11m_dot_co_dot_cc_sites/  \nNakashima,  R. (2018 a). AP Exclusive: Google tracks your movements, like it \nor not . AP News. \nhttps://apnews.com/article/828aefab64d4411bac257a07c1af0ecb  \nNate Silver. (2018 b). California 45th \u2013 2018 House forecas t. FiveThirtyEight. \nhttps://projects.fivethirtyeight.com/2018 -midterm -election -\nforecast/house/california/45/  \nNate Silver. (2018 c). California 48th \u2013 2018 house forecast . FiveThirtyEight. \nhttps://projects.fivethirtyeight.com/2018 -midterm -election -\nforecast/house/california/48/  \nNate S ilver. (2018). Calif ornia 45th \u2013 2018 House forecast . FiveThirtyEight. \nhttps://projects.fivethirtyeight.com/2018 -midterm -election -\nforecast/house/califor nia/45/  \nNewton, C. (2018, May 10). Google\u2019s new focus on well -being started five \nyears ago with this presentation. The Verge. Retrieved from: \nhttps://www.wsj.com/articles/google -workers -discussed -tweaking -\nsearch -function -to-counter -travel -ban-1537488472   \nTHE EVIDENCE  \n117 The Nielsen Company. (n.d.). Solutions: Audience Measurement . Retrieved \nApril 1, 2024, from https://www.nielsen.com/solutions/audience -\nmeasurement/  \nNolan, L. (2020, September 22). RealClearPolitics: Google suppresses \nconservative media in search results.  Breitbart. Retrieved from: \nhttps://www.breitbart.com/tech/2020/09/22/realclearpolitics -google -\nsuppresses -conservative -media -in-search -results/  \nNyguen, G. (2021). How Google and Yelp  handle fake users and policy \nviolations . Search Engine Land. https://searchengineland.com/how -\ngoogle -and-yelp-handle -fake-reviews -and-policy -violations -374071  \nNylen, L. (2024). Google\u2019s Payments to Apple Reached $20 Billion in 2022, \nAntitrust Court Documents Show. Bloomberg. \nhttps://www.bloomberg.com/news/articles/2024 -05-01/google -s-\npayments -to-apple -reached -20-billion -in-2022 -cue-says \nOberhaus, D. (2020).  Silicon Valley Opens Its Wallet for Joe Biden . Wired. \nhttps://www.wired.com/story/silicon -valley -opens -wallet -joe-biden/  \nO\u2019Connor, R. (2012). Google Is Evil . Wired. \nhttps://www.wired.com/2012/06/opinion -google -is-evil/ \nO\u2019Neil, C. (2016). Weapons of Math Destruction: How Big Data Increases \nInequality and Threatens Democracy . Crown Publishing Group.  \nOrenstein, D. (2011). Google and Stanford\u2019s relationship provides answers to \ntough problems . Standford University. \nhttps://engineering.stanford.edu/magazine/google -and-stanfords -\nrelationship -provides -answers -tough -problems  \nPassifiume, B. (2022) \u2018Twitter  Files\u2019 explained, and what they revealed about \ntech censorship . National Post. https://nationalpost.com/news/twitter -\nfiles-explained -and-what -they-revealed -about -tech-censorship  \nPegoraro, R. (2022). The little -known reason why competing with google is \nso hard. Fast Company. https:// www.fastcompany.com/90709672/the -\nlittle-known -reason -why-competing -with-google -is-so-hard \nProlific (n.d.). Solutions: Research and Development. Retrieved April 2, \n2024, from https://www.prol ific.com/research -and-development  \nQuan, S. (2024, Jan 23). Top Google Searches . ahrefsblog. \nhttps://ahrefs.com/blog/top -google -searches/  \nReneau, A. Expert on tyranny explains the Trump \u2018Big Lie\u2019  you\u2019ve been \nhearing so much about lately . Upworthy. Retrieved from: \nhttps://www.upworthy.com/the -trump -big-lie-explained -historian   \nReuters Staff. (2021, January 12). Father of  the Web Tim Berners -Lee \nprepares \u2018do -over.\u2019  Reuters. Retrieved from: \nhttps://www.reuters.com/article/uk -tech-bernerslee -interview -\nidUSKBN29H1JG   \nEPSTEIN  \n \n118 \n  Riehm, K. E., Feder , K. A., Tormohlen, K. N., Crum, R. M., Young, A. S., \nGreen, K. M., Pacek, L. R., et al. (2019). Associations between time \nspent using social media and internalizing and externalizing problems \namong US youth. JAMA Psychiatry, 76 (12), 1266 -1273. \ndoi:10.1001 /jamapsychiatry.2019.2325  \nRoller, A. (2024). Google antitrust case explained: What's next? . TechTarget. \nhttps://www.techtarget.com/whatis/feature/Google -antitrust -case-\nexplained -Whats -next \nRomm, T. (2020). The Justice Department is suing Google \u2014 but it\u2019s the \ngovernment\u2019s power to police big tech that\u2019s on trial. \nhttps://www.washingtonpost.com/technology/2020/10/21/justice -\ndepartment -google -lawsuit -congress/  \nRushe, D. (2014). US government requests for Google user data jumps 120% \nsince 2009. The Guardian. \nhttps://www.theguardian.com/technology/2014/mar/27/us -\ngovernment -requests -google -user-data \nSandler, R. (2020). Republicans Use Tech Anti -trust hearing to rail against \n\u201cAnti -Conservative Bias \u201d. Forbes.    \nhttps://www.forbes.com/sites/rachelsandler/2020/07/29/republicans -\nuse-tech-antitrust -hearing -to-rail-against -anti-conservative -bias/  \nSawyer, P. (2022, April 23). Big Brother (sorry, Big Person) is correcting you \non Google. The Telegraph. Retrieved from: \nhttps://www.telegraph.co.uk/news/2022/04/23/big -brother -sorry -big-\nperson -correcting -google/   \nSchmidt, D.C. (2018). Google Data Collection. Digital Content Next. \nhttps://digitalcontentnext.org/  \nSchw artz, B. (2010, September 8). Google Instant: Predictions Were Right . \nSearch Engine Roundtable. \nhttps://www.seroundtable.com/archives/022883.html  \nSchwartz, B. (2017, October 3). Google autoc omplete showing 10 \nsuggestions again . Search Engine Round Table. \nhttps://www.seroundtable.com/google -autocomplete -10-suggestions -\n24560.html  \nSchwartz, B. (2022, Augus t 18). Google\u2019s new helpful content update targets \nsites creating content for search engines first. Search Engine Land . \nhttps://searchengineland.com/googles -new-helpful -content -update -\ntargets -sites-creating -content -for-search -engines -first-387237  \nSchwartz, B. (2024). Report: Google Sending More Discover Traffic \nBut Less Search Traffic But Less Search Traffic To Publi shers . \nSearch Engine Roundtable. \nTHE EVIDENCE  \n119 https://www.seroundtable.com/google -traffic -discover -search -\nnewzdash -37879.html  \nSelvarajah, M. (2018). Business are using micro  personalization to \ntarget customers . CBC News. \nhttps://www.cbc.ca/news/canada/businesses -are-using -micro -\npersonalization -to-target -custo mers -1.4832247  \nShuman, V., Sander, D., & Scherer, K. R. (2013). Levels of valence. \nFrontiers in Psychology , 4, 261. \nhttps://doi.org/10.3389/fpsyg.2013.00261  \nSingel, R. (2011). Google: We're Working to Help Good Sites Caught by Spam \nCleanup. Wired. https://www.wired.com/2011/03/google -spam -side-\neffects/   \nSmith, J. (2020). Facebook and Twitter employees have given $2.7million to \nDemocrats in 2020 - ten times the amount they have donated to \nRepublicans - as big tech faces more accusations of political bias . \nDaily Mail. https://www.dailymail.co.uk/news/article -8848313/90 -\nFacebook -Twitter -employee -political -donations -Democrats.html  \nSmyrnaios, N., & Rebillard, F. (2019). How infomediation platforms took \nover the news: A longitudinal perspective.  \nSoloman, M. (20 14). Google\u2019s Customer Service Steps Into The Spotlight . \nForbes. \nhttps://www.forbes.com/sites/micahsolomon/2014/05/05/google/  \nSolsman JE. (2018, Jan). YouTube\u2019s AI is the puppet m aster over most of \nwhat you watch.  CNET. https://www.cnet.com/tech/services -and-\nsoftware/youtubeces -2018 -neal-mohan/  \nStandford Law School. (2006). Google Inc. Pledg es $2M to Stanford Law \nSchool Center for Internet and Society. \nhttps://law.stanford.edu/press/google -inc-pledges -2m-to-stanford -\nlaw-school -center -for-internet -and-society/  \nStatistica Research Department. (2024). Nielsen Holdings - statistics & facts . \nhttps://www.statista.com/topics/5067/nielsen -holdings/  \nStatt, N. (2018, Dec 4). Google personalizes search results even when you\u2019re \nlogged out, new study suggests claim. The Verge. \nhttps://www.th everge.com/2018/12/4/18124718/google -search -\nresults -personalized -unique -duckduckgo -filter -bubble  \nStoker \u2013Walker, C. & Nature Magazine (2023). AI Chatbots Are Coming to \nSearch Engines . Can You Trust Them?. Scientific American. \nhttps://www.scientificamerican.com/article/ai -chatbots -are-coming -\nto-search -engines -can-you-trust-them/  \nEPSTEIN  \n \n120 \n  Sullivan D. (2018, Jan). A reintroduct ion to Google\u2019s featured snippets . \nGoogle Blog. https://blog.google/products/search/reintroduction -\ngoogles -featured -snippets/  \nTech Transparency Project. (2017). G oogle Academics Inc. \nhttps://www.techtransparencyproject.org/articles/google -academics -\ninc \nThompson, N. (2018). We Need To Have An Honest Talk About Our Data. \nWired. https://www.wired.com/story/interview -with-jaron -lanier/  \nWakabayashi, D. (2018). Leaked Google Video After Trump\u2019s Win Adds to \nPressure From Conservatives . New York Times. \nhttps://web.archive.org/web/20180913012639/https://www.nytimes.c\nom/2018/09/12/technology/leaked -google -video -trump.html  \nWaikar, S. (202 1). Why a Plan to Encourage Search -Engine Competition \nFailed. Stanford Business. \nhttps://www.gsb.stanford.edu/insights/why -plan-encourage -search -\nengine -competition -failed  \nWashington, G. (1793 ) George Washington Papers, Series 2, Letterbooks -\n1799: Letterbook 24, April 3, 1793 - March 3, 1797. April 3, - March \n3, 1797 . [Manuscript/Mixed Material] Retrieved from the Library of \nCongress, https://www.loc.gov/item/mgw2.024/  \nWarren, Elizabeth. (2019, Mar 8). Here's how we can break up Big Tech. \nMedium. https://medium.com/@teamwarren/heres -how-we-can-\nbreak -up-big-tech-9ad9e0da324c  \nWarriner, A. B., Kuperman, V.,  & Brysbaert, M. (2013). Norms of valence, \narousal, and dominance for 13,915 English lemmas. Behavior \nResearch Methods , 45, 1191 \u20131207. https://doi.org/10.3758/s13428 -\n012-0314 -x  \nWeinberg, G. (n.d.).  How does Google track me even when I'm not using it?.  \nDuckDuckGo. https://spreadprivacy.com/how -does-google -track -me-\neven -when -im-not-using -it/ \nWoods, H. C., & S cott, H. (2016). #Sleepyteens: Social media use in \nadolescence is associated with poor sleep quality, anxiety, depression \nand low self -esteem. Journal of Adolescence, 51, 41 -49. \nhttps://doi.o rg/10.1016/j.adolescence.2016.05.008  \nW\u00f6stmann, M., St\u00f6rmer, V. S., Obleser, J., Addleman, D. A., Andersen, S. \nK., Gaspelin, N., Geng, J. J., Luck, S. J., Noonan, M. P., Slagter, H. \nA., & Theeuwes, J. (2022). Ten simple rules to study distractor \nsuppressio n. Progress in neurobiology, 213, 102269. \nhttps://doi.org/10.1016/j.pneurobio.2022.102269  \nYbarra, M. (2016). Is Sex In The Media Related to Sexual Behavior Among \nTeens . Psychology Today. \nTHE EVIDENCE  \n121 https://www.psychologytoday.com/us/blog/connected/201606/is -sex-\nin-the-media -related -sexual -behavior -among -teens  \nZaiceva, A. (2022).  What is a Walled Garden in AdTech Industry? Setupad \nBlog. https://setupad.com/blog/walled -garden/  \nZittrain, J. (2014). Facebook Could Decide an Election Without Anyone Ever \nFinding Out.  New Republic.  \nhttps://newrepublic.com/article/117878/information -fiduciary -\nsolution -facebook -digital -gerrymandering  \nZuboff, S. (2019). The Age of Surve illance Capitalism: The Fight for a \nHuman Future at the New Frontier of Power. PublicAffairs.   \nEPSTEIN  \n \n122 \n   \n \nThis is an advance copy of The \nEvidence  dated September 4, 2024. The \ncontent may contain errors and is \nsubject to change. A complete and \ncorrected version of this monograph has \nbeen submitted for peer review and \npublication with an academic publisher. \nThe full version includes footnotes and \nmore than 100 pages of supplementary \nmaterials. If you are interested in \nobtaining the final version o f this \nmonograph after it has been accepted \nfor publication, please inquire at \ninfo@aibrt.org . This document is \ncopyrighted 2024 by the American \nInstitute for Behavioral Research and \nTechnology ( https://AIBRT.org ).  \n      \n \u00a92024 AIBRT. 9 -4-24 rev.  \nAIBRT  \nAmerican Institute for Behavioral Research and Technology  his monograph  describe s the development and deployment of a \nnationwide  system for preserving and analyzing online ephemeral \ncontent being sent to Americans  by technology companies, 24 hours \na day. Online ephemeral content has been shown in controlled studies to \nhave unprecedented power to shift people\u2019s thinking and behavior without \ntheir awareness. Normally, because such content is ephemeral, it gives tech \ncompanies the ability to influence people without leaving a paper trail for \nauthorities to trace; hence, the importance of building systems for \npreserving such content. This essay also addresses an important public \npolicy issue: To what extent, if any, hav e tech companies been using \nephemeral content for political purposes? Dr. Epstein  address es this issue  \nby summarizing and critiquing three recent studies that have defended the \ntech companies. He show s that two of these studies  have  ties to t he tech \ncompan ies and that all of them  have fatally flawed methodology. He  argue s \nthat because ephemeral content is highly personalized, the only way we can \nget an accurate picture of how such  content is being employed is by  \n\u201clook ing over the shoulders\u201d of a large, repr esentative sample of real users \nas they are receiving such content, and then aggregating and analyzing the \ncontent, much as the Neilsen company does worldwide to rate TV \nviewership. The monitoring system he has  built aggregates and ana lyzes \nsuch content in  real time, and it has repeatedly identified politically  biased \ncontent sufficient to have shifted millions of votes in national elections in \nthe US. Epstein  conclude s that large -scale monitoring systems must \nbecome a permanent feature of the internet to protect our democracy, our \nautonomy, and the minds of our children from potentially profound \nmanipulations by the algorithms of Big Tech companies, both now and in \nthe foreseeable future.   \nR. ROBERT EPSTEIN is Senior Research Psychologist at the \nAmerican Institute for Behavioral Research and Technology and \nthe former editor -in-chief of Psychology Today  magazine. A Ph.D. \nof Harvard University, he is a pioneer in the stu dy of new forms of \nmanipulation that have been made possible by the internet. He has testified \ntwice before Congress about his research in this area . His latest  \nCongressional testimony is at https://2023Epst einTestimony.com  (6-min. \nvideo), and a dashboard that summarizes the data his new monitoring \nsystem is collecting is at https://AmericasDigitalShield.com .  T \nD ", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Evidence", "author": ["R Epstein"], "pub_year": "2024", "venue": "NA", "abstract": "This essay contains three sections following this Introduction. In Part One, I will explain why  several widely cited studies that claim to show that Big Tech platforms are politically unbiased"}, "filled": false, "gsrank": 485, "pub_url": "https://techwatchproject.org/wp-content/uploads/2024/09/EPSTEIN-THE_EVIDENCE-monograph-advance_copy-9-4-2024.pdf", "author_id": [""], "url_scholarbib": "/scholar?hl=en&q=info:ePv-Fd5djCgJ:scholar.google.com/&output=cite&scirp=484&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D480%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=ePv-Fd5djCgJ&ei=XbWsaM-eLcDZieoPqdqh8QU&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:ePv-Fd5djCgJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://techwatchproject.org/wp-content/uploads/2024/09/EPSTEIN-THE_EVIDENCE-monograph-advance_copy-9-4-2024.pdf"}}, {"title": "What's in a Label? Propaganda Labels and User Sharing Behavior on Social Media Platforms", "year": "2025", "pdf_data": "What\u2019s in a Label? Propaganda Labels and User Sharing Behavior on Social\nMedia Platforms\nJulia Jose, Chris Geeng, Kediel O Morales, Damon McCoy, Rachel Greenstadt\nDepartment of Computer Science and Engineering, New York University, New York, NY , USA\n{jj3545, cg4247, km5655, mccoy, rg195}@nyu.edu\nAbstract\nAuthentic information is vital for a society\u2019s ability to make\nrational decisions. Fabricated and manipulative information\ncan be harmful to society as seen in cases of threatening\nevents that were consequences of foreign propaganda and\nradical ideologies. While past research has studied dis- and\nmisinformation on social media platforms, the study of pro-\npaganda has received much less attention. This study explores\nthe sharing intentions of propaganda on social media plat-\nforms and develops an intervention to help detect it. In a ran-\ndomized controlled trial setting, we added indicators to so-\ncial media posts that used propaganda techniques to advance\nan agenda, including techniques that rely on fallacious rea-\nsoning, emotional rather than logical reasoning, etc. We then\nasked our participants (n=1,187) about their intention to en-\ngage with these posts. We found that participants were sig-\nnificantly (2.4 times) less likely to share these posts with in-\ndicators. We also found that participants\u2019 political affiliation\nmoderated their sharing intentions. We believe our findings\nprovide valuable insights for the study of propaganda on so-\ncial media platforms.\nIntroduction\nJowett & O\u2019Donnell (2006) define propaganda as \u201cthe delib-\nerate, systematic attempt to shape perceptions, manipulate\ncognitions, and direct behavior to achieve a response that\nfurthers the desired intent of the propagandist \u201d. Propaganda,\nalong with disinformation and misinformation, has prolifer-\nated on social media platforms, becoming a great source of\nconcern for truth and democracy (Guess and Lyons 2020).\nWhile researchers and platforms have explored providing in-\ndicators that a post is false or lacks context to combat misin-\nformation and disinformation (Geeng et al. 2020; Roth and\nPickles 2020; Morrow et al. 2022; Janmohamed et al. 2021;\nYaqub et al. 2020; Papakyriakopoulos and Goodman 2022),\nthe usage of indicators to combat propaganda is less studied.\nPrior research on tackling propaganda, in general, sug-\ngests improving propaganda literacy (Graham 1939; Booth;\n1940; Hollis; 1939). This can be achieved by teaching peo-\nple to recognize common propaganda techniques like name-\ncalling (\u201cgiving an idea a bad label and therefore rejecting\nand condemning it without examining the evidence\u201d), and so\nCopyright \u00a9 2025, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.on (Lee and Lee 1939). In this study, we design three indi-\ncators that inform users of propaganda techniques associated\nwith a social media post and examine their impact on users\u2019\nsharing intentions.\nThis paper considers the following research questions:\n\u2022RQ1: Do propaganda indicators on social media posts\naffect information-sharing behavior? Through a ran-\ndomized controlled trial experiment, we examined if\nusers in the treatment group (group of people exposed\nto propaganda indicators) showed different sharing inten-\ntions than users in the control group (group of people not\nexposed to propaganda indicators).\n\u2022RQ2: How does revealing the rhetorical devices of\npropaganda used in posts affect information-sharing\nbehavior compared to an indicator that does not? Re-\nsearch in the misinformation literature has shown that\nindicators that contained contextual information were\npreferable to generic indicators with little to no detail\nabout the tag (Sharevski et al. 2022; Epstein et al. 2022).\nWe designed three indicators to explore this phenomenon\nin the context of propaganda: a standard indicator with a\ngeneric propaganda warning text, a contextual indicator\nthat contains information on the rhetorical devices of pro-\npaganda used in the post, and a warning+contextual indi-\ncator that combined certain warning elements along with\nthe contextual information. We then analyzed the perfor-\nmance difference between the three treatment groups to\nunderstand if one was more effective than the other.\n\u2022RQ3: Do factors such as age, gender, political affil-\niation, and social media usage levels moderate user\nengagement? Past research has shown that factors such\nas political affiliation influence how individuals react to\ncredibility indicators (Pennycook et al. 2020). We in-\ncluded political affiliation as well as factors such as age,\ngender, and social media usage levels to see if these vari-\nables moderated user engagement intentions.\nTo answer these questions, we conducted an online ex-\nperiment with 1,187 human subjects recruited from Prolific,\nwhere they were randomly assigned to either a control con-\ndition or one of three treatment conditions that tested three\ntypes of propaganda indicators. Our findings indicate that\nexposure to propaganda indicators is effective in reducing\nusers\u2019 intention to share such posts on social media plat-\nProceedings of the Nineteenth International AAAI Conference on Web and Social Media (ICWSM 2025)\n918\nforms. We found that users were 2.4 times less likely to share\nsuch posts on social media when they came with an indica-\ntor. We also found that the impact of these indicators varies\nacross political groups as well as with changes in user con-\ncordance with the post\u2019s partisan slant. Our results show ev-\nidence that propaganda indicators can help tackle the spread\nof propaganda on social media platforms.\nRelated Work\nContent Labeling\nAs the circulation of problematic information, such as disin-\nformation, misinformation, and propaganda, increases daily\non social media platforms (Forum 2022), techniques such\nas content labeling are used by platforms to moderate the\ncirculation of such information. The concept of attaching la-\nbels to content for additional information has roots in infor-\nmation labeling practices, such as food labels, prescription\ndrug labels (Morrow et al. 2022), and privacy \u201cnutrition\u201d la-\nbels (Kelley et al. 2009).\nWhile various content labels exist, two of the most com-\nmon types are veracity labels and contextual labels (Morrow\net al. 2022). The former gives information on the veracity\nof the content. An example of this would be Twitter\u2019s labels\nfor misleading content (their labels for misleading/disput-\ned/unverified claims) (Roth and Pickles 2020). On the other\nhand, contextual labels are labels that give additional infor-\nmation about the context of the content and can be further\ncategorized into source labels, claim-specific labels, and so\non (Morrow et al. 2022). Examples of source-specific labels\ncan be seen on Twitter where they attach labels to govern-\nment accounts, state-affiliated media accounts, and individ-\nuals associated with state-affiliated media (Center 2020).\nEffects of Content Labeling Previous studies have ex-\namined the effect of some of these labeling techniques on\nuser engagement and news-sharing behavior on social me-\ndia platforms, particularly around COVID-19 misinforma-\ntion (Janmohamed et al. 2021). For example, Geeng et\nal. (Geeng et al. 2020) found that these warnings were more\nhelpful when post-specific messages (such as \u201cFalse Infor-\nmation\u201d) were used rather than generic messages such as\nTwitter\u2019s \u201cKnow the facts\u201d message or Instagram\u2019s \u201cHelp\nprevent the spread of Coronavirus\u201d message when searching\nfor COVID-19 terms.\nSimilarly, outside of the scope of COVID-19, Clayton\net al. (Clayton et al. 2020) studied the impact of general\nmisinformation messages such as Facebook\u2019s \u201ctips for spot-\nting fake news\u201d (A. 2016), versus specific warning messages\nsuch as \u201cDisputed\u201d or \u201cRated False\u201d on users\u2019 perceived ac-\ncuracy of social media posts. They found that specific warn-\ning messages significantly reduced perceived accuracy.\nIn another experimental study, Yaqub et al. (Yaqub et al.\n2020) studied the effects of credibility indicators on users\u2019\nnews-sharing behavior on social media. Also looking at\nthe effect of the entity disputing the post, they found that\ncredibility indicators were the most effective when a fact-\nchecking journalist disputed it. Being least effective when\nan AI system was involved in disputing it. However, in their\nwork on AI-crowd-generated credibility indicators, Epstein\n(a) Standard Propaganda Indicator\n(b) Contextual Propaganda Indicator\n(c) Warning+Contextual Propaganda Indicator\nFigure 1: The different propaganda indicators used in\nour study. (a) The standard indicator is a generic propa-\nganda warning text. (b) The contextual indicator reveals\nthe propaganda techniques used in the post. (c) The warn-\ning+contextual indicator adds warning elements to the con-\ntextual indicator.\net al. (Epstein et al. 2022) found that adding a more descrip-\ntive explanation of the process by which these indicators\nwere generated increased such indicators\u2019 effectiveness.\nPapakyriakopoulos et al. (Papakyriakopoulos and Good-\nman 2022) studied the effect of Twitter\u2019s veracity labels\nas well as contextual labels and found that contextual la-\nbels were useful especially when there was textual and top-\nical overlap between the label and the tweet. Sharevski et\nal. (Sharevski et al. 2022) similarly found that providing\nadditional contextual information to warning messages was\npreferable to generic warning indicators in the misinforma-\ntion context. Kreps et al. (Kreps and Kriner 2022) further\nstudied the same under the COVID-19 context and found\ngeneric warnings to be of limited efficacy. They found that\nadding more contextual information on why the post was\nflagged significantly helped alter beliefs as well as reduce\n919\nthe sharing of these posts.\nLooking at state-sponsored propaganda Liang et al.\n(Liang, Zhu, and Li 2022) found Twitter\u2019s practice of label-\ning state-affiliated media reduced the news-sharing intent as-\nsociated with labeled content on Twitter. On YouTube, Nas-\nsetta and Gross (Nassetta and Gross 2020) found similar re-\nsults for state-affiliated media when labels were noticeable.\nWe base our study on the understanding that users prefer\npost-specific and contextual information over generic warn-\ning messages.\nImpact of Cognitive Processes and Truth Discernment\nPennycook et al. (Pennycook and Rand 2019b) found that\nanalytical thinking skills have a direct impact on truth dis-\ncernment abilities. They found that lack of such skills made\npeople more susceptible to fake news than partisanship\nper se. Similarly, in another study of credibility indicators,\nArechar et al. (Arechar et al. 2022) looked at cognitive pro-\ncesses such as analytical thinking and their relationship to\naccuracy discernment. They showed that strong analytical\nskills enhance accuracy discernment, influencing both truth\ndiscernment and sharing behavior. However, there was a dis-\nconnect between truth discernment and sharing intentions\nand users needed to be reminded to consider accuracy when\npresented with an article to enhance sharing discernment.\nThey found that a digital literacy message with critical think-\ning tips also improved sharing discernment. These studies\ninformed our design of an indicator that highlights propa-\nganda techniques (a propaganda literacy message) used in\nposts to improve discernment and sharing behavior.\nPropaganda and its Devices\nAlthough traditional media sources like newspapers de-\nclined as propaganda tools after World War II (Jowett and\nO\u2019Donnell 2018), the digital age has created a new, eas-\nily accessible medium for spreading propaganda. Mare \u02c7s\nand Mlejnkov \u00b4a (Mare \u02c7s and Mlejnkov \u00b4a 2021) studied secu-\nrity threats that emerged from online propaganda, includ-\ning Russian interference in foreign elections and the global\nwhite nationalist movement and far-right-led attacks. The\nincreasing calls for research into countering propaganda\nand enhancing information integrity by government institu-\ntions (Committee 2017; IIRD IWG 2022) speaks volumes\nabout why this is an evolving security threat.\nPropagandists often use rhetorical devices based on log-\nical fallacies, emotional appeals, and psychological tactics,\nsuch as \u201cglittering generalities\u201d which involve associating a\nmessage with positive words to gain acceptance without ev-\nidence. During the \u201cgolden age\u201d of propaganda in the 1930s\nand 1940s, organizations like the Institute for Propaganda\nAnalysis (IPA) worked to educate the public on identifying\nthese techniques (Lee and Lee 1939).\nFollowing the works of (Lee and Lee 1939), Martino et\nal. (Da San Martino et al. 2019) derived 18 propaganda\ntechniques commonly found in modern journalistic articles.\nSome of these techniques include - \u201cbandwagon\u201d (when a\nperson tries to convince someone to accept or do some-\nthing simply because everyone else is doing it), and \u201cflag-\nwaving\u201d (statements that play on strong national feelings).They collaborated with media professionals to create an an-\nnotated dataset of these techniques and developed the Tanbih\nAPI(QCRI 2021), a tool capable of identifying propaganda\ntechniques in articles with moderate accuracy. In our study,\nwe use some of these techniques to highlight the use of pro-\npaganda in social media posts, aiming to provide contextual\ninformation to users.\nScience Denialism Research shows that technique rebut-\ntal, which refutes argument flaws and rhetorical techniques,\nis effective against science denialism (Schmid and Betsch\n2019). Science denialism shares traits with propaganda, as\nboth use tactics like false logic, fake experts, and appeals to\nfalse authorities.\nMethods\nProcedure\nParticipants were randomly assigned to one of four groups\n- one control and three treatment groups, with each treat-\nment group testing a different indicator type. Across all four\ngroups, we exposed participants to a series of both propa-\nganda and non-propaganda posts. There were 12 propaganda\nand 6 non-propaganda posts that were shown in all groups.\nParticipants in the control group saw propaganda posts\nwithout any propaganda indicators. Participants in treatment\ngroup 1 saw propaganda posts with a standard propaganda\nindicator (Figure 1a) that had a generic warning message.\nParticipants in treatment group 2 saw propaganda posts ac-\ncompanied by a contextual indicator (Figure 1b) that con-\ntained information on the rhetorical devices of propaganda\nused in the post. Participants in treatment group 3 saw pro-\npaganda posts with a warning+contextual indicator (Fig-\nure 1c) that combined certain warning elements along with\nthe rhetorical devices of propaganda used in the post.\nIn all four groups, the non-propaganda posts did not have\nan indicator. After each post, the participant was asked about\ntheir intention to share the post on social media platforms.\nThe order of the posts was randomized in all four groups.\nAt the end of the 18 posts, participants were asked ques-\ntions on demographics and political affiliation (See Ap-\npendix for questionnaire).\nMaterials\nPost Selection Our post collection included an even mix\nof posts that leaned toward Left, Independent, and Right\npolitical ideologies in the US since engagement intentions\nare influenced by how well the information aligns with the\nuser\u2019s beliefs (politically) (Kahan 2012; Pennycook et al.\n2020). We also included an even mix of well-known and\nlesser-known narratives given the influence of topic popu-\nlarity on sharing intentions (Wong and Burkell 2017).\nMedia Bias/Fact Check (MBFC) is an online platform\nthat rates over 5,500 news sources based on bias and fac-\ntual accuracy (Check 2022a). Its categories include pseu-\ndoscience conspiracy andquestionable sources among oth-\ners such as left-biased, right-biased, left-centered, and right-\ncentered biases. They define questionable sources as sources\nthat promote propaganda and/or conspiracy theories (Check\n920\n2022b). We studied the websites that were listed under this\ncategory such as occupydemocrats.com, naturalnews.com,\nrt.com, and so on.\nTo obtain politically-oriented excerpts (social media\nposts), we utilized MBFC\u2019s bias categories from within the\nquestionable source category i.e. to gather right-leaning pro-\npaganda posts, we looked at right-biased sources that were\nsimultaneously also under the questionable source category.\nWe then looked up the popularity of these narratives on\nGoogle to check if popular news websites such as CNN, Fox\nNews, and NY Times covered the same narrative, for pop-\nularity categorization purposes. We retained a post as long\nas it exhibited the use of at least two propaganda techniques\nmentioned in Martino et. al. (Da San Martino et al. 2019).\nFurthermore, we aimed to explore the potential interac-\ntion between misinformation content in some of these posts\nand how that could influence sharing intention. When ap-\nplicable, we looked up the veracity of the statements used in\nthese posts on Politifact, making sure to avoid unwanted bias\nand/or skewness towards either side of the political spec-\ntrum.\nFor non-propaganda excerpts (posts), we referred to web-\nsites with high scores on MBFC\u2019s factual reporting scale\nsuch as reuters.com, excluding those listed under question-\nable sources. Furthermore, three of the authors manually\nchecked these narratives to validate this categorization.\nWe accumulated over 42 such propaganda posts and 24\nnon-propaganda posts. We then ran a small-scale experiment\n(n=84) where we presented participants with a random set of\nposts and asked them about their sharing intention for each.\nThis was done in a stratified random manner ensuring that\neach post was seen by an equal number of participants across\nall 3 political subgroups. The most shared posts (12 propa-\nganda and 6 non-propaganda posts) among these were then\nshortlisted for use in the main experiment.\nWe acknowledge that real-world social media feeds typi-\ncally mix highly engaging content with less-viral posts from\nusers\u2019 local networks. Therefore, selecting only the most\nshared posts may not fully capture this diversity. However,\nour approach was intended to reduce experimental noise by\nchoosing to focus on posts that participants are relatively\nmore likely to share since posts with less shares provide lit-\ntle to no informative variation regarding the effects of our\ninterventions. Moreover, we also believe that this procedure\nmimics social media algorithms, which tend to amplify posts\nthat receive higher engagement.\nPretest Taking inspiration from Pennycook et al. (Pen-\nnycook et al. 2020), we conducted a pretest (n=193) on the\nshortlisted posts to validate their political orientation. Par-\nticipants were randomly assigned to one of two groups that\nsaw either propaganda or non-propaganda posts. In both\ngroups, they were asked to rate the posts on a scale of 1 to\n7 with 1 being Democrat-favorable and 7 being Republican-\nfavorable.\nFor the propaganda posts, the right-leaning posts (M=5.7)\ndiffered significantly in orientation from the left-leaning\nposts (M=1.9). Both right-leaning and left-leaning posts\nalso significantly differed from independent-leaning posts(M=3.9). Independent-leaning posts did not differ from the\ncenter of the scale (4). Furthermore, the right-leaning and\nleft-leaning posts were equally partisan (differed equally\nfrom the center of scale; p > 0.05).\nFor the non-propaganda posts, the right-leaning posts\n(M=4.5) differed significantly in orientation from left-\nleaning posts (M=2.3). Both right-leaning and left-leaning\nposts also significantly differed from independent-leaning\nposts (M=4.2). The independent-leaning posts, however, dif-\nfered from the center of the scale (p = 0.008), making it\nslightly skewed to the right.\nThis pretest validated our post-selection process with re-\nspect to political leaning. It also validated the inclusion of an\neven mix of posts corresponding to all political subgroups in\nthe US. We further controlled for user\u2019s partisan lean versus\npost\u2019s partisan lean ((dis)concordance) by coding up a \u201ccon-\ncordance\u201d (political concordance) variable that had value=1\nif the participant\u2019s partisan lean matched the post\u2019s partisan\nlean and 0 otherwise.\nIndicator Design We designed three indicators to under-\nstand the dissemination of propaganda on social media.\nFor the first type, a standard indicator, we replicated the\ngeneric warning message used by Yaqub et al. (Yaqub et al.\n2020), where a claim is accompanied by a broad statement\nsuch as \u201cdisputed by fact-checkers\u201d (A. 2016). Platforms\nlike Facebook and Twitter frequently used such warnings be-\ntween 2016 and 2020 to curb misinformation. Several stud-\nies (Yaqub et al. 2020; Pennycook et al. 2020; Sharevski\net al. 2022; Pennycook and Rand 2019b; Clayton et al. 2020)\nhave since explored their effects on sharing behavior and\nperceived accuracy, highlighting issues such as the backfire\neffect (Nyhan and Reifler 2010) and the implied truth ef-\nfect (Pennycook et al. 2020).\nOn the other hand, to make warning messages more in-\nformative and interpretable, our second indicator, a contex-\ntual indicator, draws inspiration from Twitter\u2019s Commu-\nnity Notes (Center 2022) feature, providing detailed con-\ntext about a post. Contextual indicators are generally well-\nreceived (Sharevski et al. 2022; Geeng et al. 2020; Kreps and\nKriner 2022). We designed this indicator to appear below the\npost in a non-disruptive manner, with a bold title and a gray\nbackground. To enhance clarity and help participants visu-\nally distinguish among the eight propaganda techniques, we\nrandomly assigned each technique one of eight colors and\nensured consistent technique-color pairings in this group.\nGiven that minimal warnings can go unnoticed (Kaiser\net al. 2021), we designed a third, more explicit indicator\nwhich we call our warning+contextual indicator. While in-\nterstitial warnings may cause user friction (Kaiser et al.\n2021), they significantly improve noticeability, which is key\nto effective intervention (Nassetta and Gross 2020; Kaiser\net al. 2021). This indicator type included a warning sign, the\nphrase \u201dPropaganda Warning\u201d and a color palette incorpo-\nrating red to signal danger (Silver et al. 2002). Because the\ngoal here was to improve noticeability, we deliberately used\nthe color red to highlight one randomly chosen technique in\neach post. For the remaining techniques, we selected from\ngreen and blue to provide additional contrast.\n921\nTo control for the effect that the source or publisher infor-\nmation would have on users\u2019 engagement intentions, these\nwere redacted in all the posts across all groups. However,\nwe acknowledge that in real-world settings, source informa-\ntion can influence engagement decisions (Thompson, Wang,\nand Daya 2019).\nPropaganda Techniques For the contextual and warn-\ning+contextual indicator, we used 8 of the 18 propaganda\ntechniques mentioned in Martino et al. (Da San Martino\net al. 2019). These include:\n1. Name-Calling: \u201cLabeling the object of the propaganda\ncampaign as either something the target audience fears,\nhates, finds undesirable or otherwise loves or praises\u201d\n2. Loaded Language: \u201cUsing words or phrases with strong\nemotional implications to influence an audience\u201d\n3. Doubt: \u201cQuestioning the credibility of someone or some-\nthing\u201d\n4. Appeal to Fear: \u201cSeeking to build support for an idea\nby instilling anxiety and/or panic in the population to-\nwards an alternative, possibly based on preconceived\njudgments\u201d\n5. Flag-Waving: \u201cPlaying on strong national feeling (or\nwith respect to a group, e.g., race, gender, political pref-\nerence) to justify or promote an action or idea\u201d\n6. Black-and-white fallacy: \u201cPresenting two alternative op-\ntions as the only possibilities, when in fact more possi-\nbilities exist\u201d\n7. Bandwagon: \u201cAttempting to persuade the target audience\nto join in and take the course of action because \u2018everyone\nelse is taking the same action\u2019 \u201d\n8. Causal oversimplification: \u201cAssuming one cause when\nthere are multiple causes behind an issue OR the transfer\nof blame to one person without investigating the com-\nplexities of an issue\u201d\nAnnotation To annotate the techniques present in the so-\ncial media posts, we used a multi-stage annotation process\nwhere two authors annotated techniques and a third au-\nthor served to mitigate disagreements. Similar to Ogren et\nal. (Ogren et al. 2008), we subdivided a larger set of posts\ninto trial and experimental sets where the trial set was used\nas an exercise tool for the annotation task and the experi-\nmental set contained posts for the main experiment.\nIn stage 1, the trial set and an initial annotation guide-\nline were used by the two annotators to annotate posts in-\ndependently. At the end of stage 1, the annotators came to-\ngether to resolve disagreements and to update the annotation\nguideline for further clarity. The Inter-Annotator Agreement\n(IAA) score was used as a metric for annotation reliabil-\nity. Two types of IAA scores were calculated (F-measure\nand token-level Cohen\u2019s Kappa (Deleger et al. 2012)) be-\ncause of the inherent difficulty in calculating Cohen\u2019s Kappa\nfor entity recognition tasks (Hripcsak and Rothschild 2005;\nDeleger et al. 2012).\nIn stage 2, the annotators annotated the experimental set\nusing the updated guideline, reaching an IAA score of 0.63\nfor F-measure and 0.52 for token-level Kappa, indicatingmoderate agreement. At the end of stage 2, the annotations\nwere consolidated by the third author who helped establish\nthe final set of annotations that were used for the main ex-\nperiment. For the final annotations, annotations where both\nannotators agreed (both span and technique), were consid-\nered and only three techniques were shown per post. In the\ncase of disagreements in the technique, the consolidator\u2019s\njudgment was considered. The annotated propaganda posts\nas well as the non-propaganda posts used in this study can\nbe found in our repository.1\nRecruitment and Ethical Considerations\nThroughout the study, ethical standards were in place to pro-\ntect the rights of the research participants. An informed con-\nsent form was provided to participants at the beginning of\neach experiment. The informed consent form contained de-\ntails about the study, estimated time, compensation, and con-\nfidentiality of data. No personally identifiable information\nwas collected.\nParticipants were recruited via Prolific, limited to U.S.-\nbased individuals aged 18+, with 50+ completed surveys and\na 95% approval rating. The post-selection and pretest stud-\nies lasted 10 minutes with $1.50 compensation, and the main\nexperiment took 12 minutes with $1.80 compensation (both\n$9/hr). To ensure data quality, we used reCaptcha, fraud/du-\nplicate detection, commitment requests, attention checks,\nand time-out mechanisms. The study was approved by our\ninstitution\u2019s Institutional Review Board (IRB).\nParticipant Demographics We recruited 1,290 partici-\npants for the main experiment and 305 participants for post-\nselection and pretest experiments. Participants who com-\npleted the study too slowly/quickly (outside 3 standard devi-\nations), failed to consent or did not meet initial commitment\nrequest were asked to return the study. Data was discarded\nfor failing attention checks, bot/duplicate checks, or incom-\nplete surveys. The final sample included 1,187 participants\nfor the main experiment and 277 for sub-experiments.\nOur sample was representative of the social media pop-\nulation (OBERLO 2023; Dixon 2023). 32% of the partic-\nipants said they identified as Democrats, 30% as Republi-\ncans, and 38% as Independents. The gender group had 47%\nfemales, 51% males, and 2% who marked their gender as\n\u201cOther\u201d. The participant age group had a mean of 41 and a\nmedian age of 38. 52% of them said they used social me-\ndia for less than 2 hours, while 48% said they used it for\nover 2 hours. 20% had no college degree, 64% had an asso-\nciate\u2019s/undergraduate degree, 14% had a graduate-level de-\ngree and 2% preferred not to say.\nGroup sizes for the main experiment: control (299), treat-\nment with standard indicator (294), treatment with contex-\ntual indicator (295), and treatment with warning+contextual\nindicator (299).\nAnalysis\nAcross all four groups, we exposed participants to propa-\nganda and non-propaganda social media posts and asked\n1Repository containing social media posts https://doi.org/10.\n6084/m9.figshare.24274639\n922\nFigure 2: Propaganda and non-propaganda post shares (non-\npropaganda posts had no indicator)\nabout their intention to share the post (yes/no) on social\nmedia. Given the nature of this experiment, we used a bi-\nnary mixed effects logistic regression model to model our\ndata. Our independent variables included participant\u2019s polit-\nical affiliation, political concordance, age, gender, and social\nmedia usage levels and our dependent variable was the shar-\ning intention (dichotomous). To account for repeated mea-\nsures, we added participant ID and post ID as random effects\nto the model.\nWe used a step-wise approach to add variables to the\nmodel to systematically evaluate which demographic and\ncontextual variables significantly contributed to the model\u2019s\nexplanatory power. We used the likelihood ratio test using\nchi-square for significance testing and we retained a vari-\nable as long as it showed significance. We acknowledge that\nstepwise procedures are sensitive to the order in which the\nvariables are added. To address this issue, we tested differ-\nent orders of entry to confirm that variables retained in the\nfinal model showed consistent performance without any sig-\nnificant deviations in results (see Table 3 in Appendix for re-\nsults of each step of the step-wise model). We also tested for\nrelevant interactions in each step. To comprehend the signif-\nicant interaction effects between the independent variables,\nwe used Tukey\u2019s Honest Significance Test (HSD) as a post-\nhoc test. Tukey\u2019s HSD allows for pairwise comparisons of\ndifferences in means (Stoll 2017) and returns corrected p-\nvalues (Dillon 2016).\nFor RQ1, we hypothesized that propaganda indicators do\naffect information-sharing behavior on social media. We\ncompared differences in sharing intentions between con-\ntrol and all the treatment groups combined. For RQ2, we\nintended to understand how the three types of indicators\ncontrasted with each other so we compared sharing inten-\ntions between control and treatment groups independently as\nwell as differences between the three treatments. For RQ3,\nwe analyzed the demographic/contextual variables, checked\nfor significant interaction effects, and used Tukey\u2019s HSD to\ncomprehend these effects.\nFigure 3: Odds ratio of sharing intentions across indicator\ngroups by political affiliation (higher odds ratio indicates\ngreater indicator impact)\nResults\nNon-Propaganda Posts\nIn all four groups, we exposed participants to non-\npropaganda posts. Research suggests that there exists an\nimplied truth effect wherein the presence of indicators on\nposts causes people to think of the posts without indica-\ntors as more true in comparison to a control (Pennycook\net al. 2020). However, as seen in Figure 2, we did not ob-\nserve significant differences in sharing intentions for non-\npropaganda posts across the control and treatment groups\ncombined (\u03b2 = 0.078, SE=0.135, p=0.564) as well as con-\nsidered individually - control and standard (\u03b2 =\u22120.039,\nSE=0.166, p=0.816), control and contextual (\u03b2 = 0.034,\nSE=0.165, p=0.837), and control and warning+contextual\n(\u03b2= 0. 232, SE=0.164, p=0.158). Similar patterns were also\nfound in studies exploring credibility indicators (Yaqub et al.\n2020; Clayton et al. 2020).\nPropaganda Posts\nOur study found that exposing participants to propaganda\nindicators had an impact on their propaganda-sharing be-\nhavior. As seen in Figure 2, participants in the treatment\ngroup (all groups combined) shared 13.8% of the propa-\nganda posts, whereas participants in the control group shared\n22.4% of the propaganda posts. As seen in Table 1, partici-\npants in the treatment group (all groups combined) were 2.4\ntimes less likely to share propaganda posts when compared\nto the control condition (p < 0.0001).\nAnalysis of the treatment (indicator) groups showed sim-\nilar reductions in sharing intentions. Participants exposed to\nthe standard indicator were 2.6 times less likely to share pro-\npaganda posts (p < 0.0001), those shown the contextual in-\ndicator were 2.3 times less likely (p < 0.0001), and those\nshown the warning+contextual indicator were 2.4 times less\nlikely (p < 0.0001) to share propaganda on social media.\nHowever, the differences between the indicator groups were\ninsignificant (corrected p= 0.875 for standard vs. con-\ntextual, p= 0.878 for standard vs. warning+contextual,\n923\nVariable Reference Level Odds Ratio Lower CI Upper CI p\nGroup Control Indicator 0.417 0.323 0.539 2.167e-11***\nGroup Control Standard 0.384 0.278 0.530 6.132e-09***\nGroup Control Contextual 0.443 0.322 0.608 5.114e-07***\nGroup Control Warning+Contextual 0.425 0.310 0.584 1.284e-07***\nConcordance Discordant Concordant 2.900 2.597 3.238 9.573e-80***\nPolitical Affilia-\ntionDemocrat Independent 1.025 0.770 1.364 0.865\nPolitical Affilia-\ntionDemocrat Republican 1.718 1.279 2.308 0.000327***\nSocial Media Us-\nageLow High 1.826 1.449 2.302 3.434e-07***\nGender Female Male 1.952 1.539 2.475 3.385e-08***\nGender Female Other 1.257 0.511 3.093 0.619\nSignificance codes: *** p <0.001, ** p <0.01, * p <0.05\nTable 1: Logistic Regression model showing the odds of sharing a propaganda post\np= 1. 0for contextual vs. warning+contextual), as indicated\nby the overlapping confidence intervals in Table 1.\nPolitical Affiliation Our study found that participant\u2019s\npolitical affiliation moderated sharing intentions. As seen\nin Table 1, Republicans were 1.7 times more likely than\nDemocrats and 1.5 times more likely than Independents to\nshare propaganda (p < 0.0001 andp= 0. 007, respectively),\naligning with previous misinformation research (Yaqub et al.\n2020).\nUpon analyzing the impact of the indicators (all groups\ncombined), Republicans were 2.7 times less likely to\nshare propaganda, Independents 2.4 times less likely, and\nDemocrats 1.9 times less likely (all p < 0.01). As seen\nin Figure 3, Republicans exposed to the standard indicator\nwere 3.9 times less likely to share these posts whereas Inde-\npendents who saw this indicator were 2.8 times less likely to\nshare these (both p < 0.0001). This effect was insignificant\nfor Democrats (p = 0. 514).\nFor the contextual indicator, Democrats were 2.5 times\nless likely to share these posts (p = 0.007), Republi-\ncans 2 times less likely, and Independents 2.2 times less\nlikely (p = 0.038andp= 0.008respectively). The warn-\ning+contextual indicator made Republicans 2.5 times less\nlikely (p = 0.004) and Independents 2.3 times less likely\n(p= 0.004), but the effect was insignificant for Democrats\n(p= 0. 083). Thus, the standard indicator reduced sharing\nfor Republicans and Independents, while adding context re-\nduced sharing across all groups. Including a warning ele-\nment further reduced sharing for Republicans and Indepen-\ndents.\nPolitical Concordance Previous studies suggest that po-\nlitical concordance (alignment of the post\u2019s political leaning\nwith the user\u2019s) impacts the effectiveness of credibility indi-\ncators (Pennycook and Rand 2019a; Pennycook et al. 2020).\nWe modeled political concordance as a binary variable witha value 1 if the post\u2019s leaning matched the user\u2019s, and 0 oth-\nerwise. For Independents, concordance was 1 if the post had\na neutral rating (4) on the pretest.\nAs seen in Table 1, participants were 2.9 times more likely\nto share politically concordant propaganda posts than dis-\ncordant posts (p < 0.0001). Both Democrats and Republi-\ncans shared concordant posts significantly more than discor-\ndant posts (p < 0.0001) (See Table 4 in Appendix). Inde-\npendents, however, shared discordant posts 2.35 times more\nthan concordant posts (p < 0.0001, Table 4).\nFigure 4 illustrates a significant three-way interaction\namong concordance, political affiliation, and treatment. The\nstandard indicator significantly reduced sharing intentions in\nboth cases of dis/concordance for Independents and Repub-\nlicans (all p < 0.05), but was insignificant for Democrats\n(discordant p= 0.309, concordant p= 0.816). The con-\ntextual indicator significantly reduced sharing intentions in\nboth cases of dis/concordance for Independents (p = 0.018\nandp= 0.023 respectively) and for discordant posts\namong Democrats and Republicans (p < 0.0001 andp <\n0.05). The warning+contextual indicator significantly re-\nduced sharing in all groups except concordant Democrats\n(p= 0.296). Table 2 summarizes the findings of the three-\nway interaction visually. See Appendix for Tukey\u2019s HSD test\nexploring the three-way interaction in detail.\nGender We included Gender in the final model because\nit has been shown to influence sharing decisions (Yaqub\net al. 2020). As seen in Table 1, men were 2 times more\nlikely to share propaganda posts than women (p < 0.0001).\nAll three indicators significantly reduced sharing intentions\nacross both groups. The percentage change (reduction) in\nsharing for men was 44% for standard indicator, 33% for\ncontextual indicator, and 39% for warning+contextual indi-\ncator (all p < 0.0001). The percentage change (reduction) in\nsharing for women was 32% for standard indicator, 41% for\n924\n(a) Concordant Propaganda\n (b) Discordant Propaganda\nFigure 4: Odds Ratio of sharing intentions across indicator and political subgroups with respect to concordance (higher odds\nratio indicates greater indicator impact)\ncontextual indicator, and 35% for warning+contextual indi-\ncator (all p < 0.05).\nSocial Media Usage We included Social Media usage in\nthe final model because it has been shown to influence shar-\ning decisions and engagement levels (Yaqub et al. 2020).\nAs seen in Table 1, participants with higher social me-\ndia usage were 1.8 times more likely to share propaganda\nposts than participants with lower levels of social media\nusage (p < 0.0001). All three indicators significantly re-\nduced sharing intentions across both groups. The percentage\nchange (reduction) in sharing for the low usage group was\n44% for standard indicator, 34% for contextual indicator,\nand 36% for warning+contextual indicator (all p < 0.01).\nThe percentage change (reduction) in sharing for the high\nusage group was 34% for standard indicator, 41% for con-\ntextual indicator, and 38% for warning+contextual indicator\n(allp < 0.0001).\nDiscussion\nThrough this study, we established a comprehensive un-\nderstanding of how propaganda indicators affect users\u2019\ninformation-sharing behavior on social media. Unlike many\nfact-checking studies, our research delves into the nu-\nanced nature of propaganda. Evidence of backfire effects\nin the misinformation and fact-checking literature (Nyhan\nand Reifler 2010; Flynn, Nyhan, and Reifler 2017) and\nthe promising potential of contextual indicators (Kreps and\nKriner 2022; Sharevski et al. 2022) prompted us to design\ninformative indicators. Our indicators highlight specific pro-\npaganda characteristics, aligning with Spradling et al.\u2019s call\nfor more descriptive labeling practices to combat misinfor-\nmation (Spradling, Straub, and Strong 2021). These indica-\ntors equip users with detailed content information, enabling\nthem to make more informed decisions.\nOur study found that propaganda indicators significantly\nimpact users\u2019 information-sharing behavior on social me-\ndia platforms (RQ1). Overall, participants were significantly\n(2.4 times) less likely to share propaganda posts when ex-\nposed to propaganda indicators, with all three types effec-\ntively reducing sharing. We found no evidence for back-fire effects at the indicator, concordance, or political affili-\nation levels. To that extent, our study adds to the growing\nevidence for the lack of backfire effects (Wood and Porter\n2019; Schmid and Betsch 2019), wherein the presence of\nindicators caused users to share (and believe) these posts\nmore (Nyhan and Reifler 2010).\nWhile RQ2\u2013how does revealing the rhetorical devices of\npropaganda used in posts affect information-sharing behav-\nior compared to an indicator that does not?\u2014remains in-\nconclusive, we found insightful observations into how dif-\nferent political subgroups responded to each indicator, en-\nhancing our understanding of political reactions to propa-\nganda interventions. We believe these insights fill a signif-\nicant gap in existing research, as there is limited work ex-\nploring how diverse groups react to these interventions with\nthis level of detail in the complex landscape of propaganda.\nFor RQ3, we examined demographic factors such as age,\ngender, and social media use. We found that men were more\nlikely to share propaganda than women, aligning with find-\nings from (Yaqub et al. 2020), possibly due to men\u2019s ten-\ndency to share more political news (Fractl 2016).\nIn this study, we used 18 posts as stimuli similar to past re-\nsearch studying the effect of fact-checking and misinforma-\ntion warnings (Pennycook et al. 2020; Sharevski et al. 2022;\nPennycook and Rand 2019b). This number was chosen for\nseveral reasons. First, it helps to minimize information over-\nload and participant fatigue, ensuring that the provided re-\nsponses are reliable and of high quality. Second, utilizing\na controlled number of stimuli allows for a more focused\nexamination of the immediate effects of exposing users to\nthese propaganda indicators. To understand the generaliz-\nability of our results, we tested for interaction effects be-\ntween post ID and indicator conditions. We did so to under-\nstand if \u2014 a) our main effect was being driven by a specific\npost (or a subset of them) and b) the observed effects would\nstill hold in an experiment with a larger sample of stimuli.\nThe interaction term turned out insignificant (see Appendix\nfor details on the statistical analysis), suggesting that the ef-\nfect of the indicator on the outcome does not differ signifi-\ncantly across posts. Notably, this insignificant interaction be-\ntween post ID and treatment group supports the robustness\n925\nAffiliationT1 (Std) T2 (Ctx) T3 (W+C)\nDis. Con. Dis. Con. Dis. Con.\nDemocrats \u2717 \u2717 \u2713\u2217\u2217\u2217\u2717\u2713\u2217\u2717\nIndependents \u2713\u2217\u2217\u2217\u2713\u2217\u2713\u2217\u2713\u2217\u2713\u2217\u2713\u2217\u2217\nRepublicans \u2713\u2217\u2217\u2217\u2713\u2217\u2217\u2217\u2713\u2217\u2717\u2713\u2217\u2217\u2713\u2217\nSignificance codes: *** p <0.001, **p <0.01, *p <0.05\nTable 2: Three-way Interaction between Political Affiliation,\nConcordance (Con./Dis.), and Indicator Condition (T1\u2013T3).\nA check mark indicates a significant effect (reduced sharing\nintention); a cross mark indicates no effect. Std (Standard),\nCtx (Contextual), W+C (Warning+Contextual).\nof using this limited set of stimuli in our study, by show-\ning that the results are not dependent on the particular set of\nposts used in this study. This supports the notion that if we\nwere to use a larger set of posts, the overall effect is likely\nto hold. However, future research could extend our study by\nusing a larger set of stimuli in a more ecologically valid,\nreal-world environment to validate our findings.\nImplications\nOur study aligns with Sharevski et al.\u2019s findings that\nDemocrats prefer contextual indicators, while Republicans\n\u201cprefer minimum intervention and distraction\u201d (Sharevski\net al. 2022). The standard indicator had the most impact\non Republicans (both concordant and discordant posts),\nwhile the contextual indicator had the greatest impact on\nDemocrats, particularly for discordant posts. However, un-\nlike Sharevski et al., who found that adding a \u201cred flag\u201d\nto the contextual indicator further minimized the indicator\u2019s\nimpact for Republicans, our warning+contextual indicator,\nwhich included a threat sign, did not reduce its effect on Re-\npublicans (see Table 2).\nUsers\u2019 propaganda-sharing behavior when exposed to the\nstandard propaganda indicator slightly differed from Yaqub\net al.\u2019s (Yaqub et al. 2020) findings on how users react to\nthese in the fake-news setting. Similar to our finding, they\nfound that Republicans and Independents were more likely\nto share these in the first place compared to Democrats.\nThey observed that such indicators were most effective for\nDemocrats, then Independents, and least for Republicans. In\nour study, generic indicators worked best for Republicans,\nfollowed by Independents and Democrats, possibly because\nRepublicans prefer minimal intervention (Sharevski et al.\n2022). This difference may stem from our study\u2019s tighter\ncontrol (and rightly so) over ensuring the inclusion of posts\nrepresenting different political leanings.\nOverall, the indicators in our study were equally effec-\ntive on both concordant and discordant posts, unlike Penny-\ncook et al.\u2019s (Pennycook et al. 2020) finding that warnings\nwork better on politically concordant fake news or with the\npostulate that identity-protective cognition plays a key role\nin how people process (mis)information (Kahan 2017). To\nthat extent, our results align with the findings of Clayton et\nal. (Clayton et al. 2020), where political congeniality does\nnot interact with indicator conditions in such settings.\nWhile many studies focus on dis- and misinformation,our study examines propaganda, which is a more nuanced\npart of the disinformation landscape. Although these terms\nare often used interchangeably due to their similarities, they\nhave key differences, especially in intent and the veracity\nof claims being made (Libraries 2023). Misinformation is\nthe unintentional spread of false information, disinforma-\ntion is the intentional spread of fake news, and propaganda\nis a deliberate attempt to mislead, using statements that may\nor may not be based on facts (Libraries 2023). Therefore,\nresponses to indicators targeting these forms may differ.\nDecoupling these forms can help in understanding partic-\nipant responses; however, this becomes challenging when\none form is used as a tool for another (e.g., using disinforma-\ntion as propaganda). Establishing the veracity of propaganda\nis more difficult since it may contain factual elements while\nstill relying on misleading, emotional reasoning.\nA key question is whether labeling content as \u201cpropa-\nganda\u201d is effective at scale, given the challenge of identify-\ning propaganda techniques across numerous posts. By shift-\ning the focus from \u201ctrue vs. false\u201d to analyzing propaganda\ntechniques shows a meaningful evolution in warning design\nby highlighting why something has been flagged as mis-\nleading, rather than simply flagging it. Our findings show\nthat incorporating propaganda techniques in the indicator\nwas effective across all three major U.S. political subgroups,\nwhereas the lack of such techniques was received negatively\nby Democrats. This result supports the notion that moving\naway from black-and-white fact-checking practices toward\nproviding more context on the flagged content can improve\nthe effectiveness of such warnings in reducing the spread\nof propaganda online (Kreps and Kriner 2022; Sharevski\net al. 2022; Spradling, Straub, and Strong 2021; Epstein\net al. 2022). Platforms such as Twitter have turned to using\nsuch nuanced approaches to increase indicator effectiveness\nthrough their Community Notes feature (Center 2022). In\nour study, we systematically investigate how different polit-\nical subgroups respond to such indicators especially when\npresented with political content. We offer insights into what\nworks for whom and provide an explanatory framework for\nintervention. Our results suggest that, besides flagging mis-\ninformation and debunked posts, social media platforms can\nalso flag propaganda to reduce its spread. While our results\ndo not point to a definitive best indicator to use for flagging\npropaganda, we present multiple options based on prior re-\nsearch on contextual and standard warning indicators. Now\nthat we know these approaches can have tangible effects,\ncollaborations with content moderation teams could use this\nknowledge to integrate rhetorical cues at scale, potentially\nthrough NLP and machine-learning systems, such as those\ndemonstrated in SemEval Task 11 (2020) for propaganda\ndetection (Da San Martino et al. 2020).\nWhile designing measures to counter propaganda, it is\ncrucial to respect freedom of expression. With censorship\nand removal of content, press freedom can take a hit. Pro-\nponents of the Counterspeech Doctrine argue that the best\nresponse to negative news is to counter it with positive\nnews (Hudson 2009). Since propaganda lacks a universally\nagreed-upon definition (Laskin 2019), a logical course of ac-\ntion then would be to explain why the content was flagged as\n926\npropaganda which would in turn help enhance transparency\naround content moderation practices. Contextual indicators\nlike ours provide users the agency to make informed de-\ncisions. While counterspeech might not always be suitable\n(e.g., in cases of incitement to violence), it remains a valu-\nable supplemental technique.\nLimitations\nPropaganda favors one side of an argument and political po-\nlarization in the US amplifies its impact. However, two-fifths\nof the US population do not align with the two dominant po-\nlitical parties in the US (Anonymous 2019), suggesting that\nthe influence of politically motivated speech may be less for\nthis group. Studying the impact of propaganda interventions\non Independents is challenging. We assumed that Indepen-\ndents would prefer non-polarizing content regarding the two\nmain parties but might engage with polarizing content re-\nlated to national, foreign, or bipartisan issues. The propa-\nganda posts that received a rating of \u201c4\u201d (neither Democrat\nnor Republican favorable) on a scale of 1 to 7 on the pretest,\nwere coded as politically concordant for Independents.\nWe acknowledge that this assumption can be contested,\ngiven that a majority of Independents do in fact lean towards\neither one of the parties (Anonymous 2019) leaving poten-\ntial implications unaccounted for. This is partly reflected in\nour results where Independents were 2.35 times more likely\nto share right/left-leaning posts than the \u201cneutral\u201d posts (p <\n0.0001). However, they shared significantly fewer right/left-\nleaning posts compared to Republicans/Democrats (Table 4\nin the Appendix with p < 0.0001). Future research could ex-\nplore more systematic ways to model political concordance\nfor Independents.\nIn this study, the visual presentation of the propaganda\ntechniques was an important aspect of the intervention.\nHowever, one design feature that requires further discus-\nsion is the role of color in the presentation of these tech-\nniques. In the contextual indicators case, the eight tech-\nniques were highlighted using eight randomly selected col-\nors, with within-group technique-color consistency. In the\nwarning+contextual case, however, because the goal was to\nenhance the prominence of the propaganda message, red was\ndeliberately introduced (as explained in the methods section)\nto highlight one random technique per post, alongside two\nadditional colors (green and blue). We acknowledge that this\ndifference in color assignment between the two groups intro-\nduces a variable that may affect outcomes beyond textual or\ncontextual information alone. Future studies could system-\natically manipulate colors to disentangle the effect of colors\nfrom those of textual information.\nAlthough using a limited set of only 18 posts aligns\nwith prior work on fact-checking and misinformation warn-\nings (Pennycook et al. 2020; Sharevski et al. 2022; Penny-\ncook and Rand 2019b) and our analysis showed an insignif-\nicant interaction effect between post ID and indicator group,\nour study focused on immediate effects. In real-world sce-\nnarios, as social media users get exposed to a vast array of\ninformation and repeated interventions, these repeated ex-\nposures could lead to habituation, potentially diminishing\nthe effectiveness of such indicators. Future studies shouldtherefore extend our work by using a larger set of stimuli to\ninvestigate the longitudinal effects of using such indicators.\nFurthermore, while contextual indicators show promise,\nthey may lead to information fatigue. Future work could\nexplore designs that minimize overload, such as by using\nhover-over elements or click-through expanded texts.\nAs is the case with online survey experiments, our Pro-\nlific sample may not capture the full variety of social media\nusers. Future work could expand on these findings through\nfield experiments to enhance ecological validity. We also ac-\nknowledge that our study focuses on the US political land-\nscape which may or may not map directly to other sociopo-\nlitical contexts. While cultural differences exist in the inter-\npretation of \u201cpropaganda\u201d, our study shows that describing\nthe strategies behind misleading and persuasive content can\nbe beneficial. Replicating this study in different sociopoliti-\ncal and cultural environments will be valuable in establish-\ning broader applicability.\nOn a related note, our Prolific study description specifi-\ncally mentioned the term \u201cpropaganda\u201d, which likely made\nparticipants more vigilant. A future study without such\ncues would better mimic real-world scenarios, though evi-\ndence suggests that such demand effects are often exagger-\nated (Mummolo and Peterson 2018). Our investigation into\ndemand effects yielded insignificant results (see Appendix\nfor details).\nFinally, our study does not account for the impact of social\ndynamics or peer influence (such as exposure to celebrities\u2019\nor close friends\u2019 sharing behavior) on sharing intentions.\nFurthermore, it is also possible that the impact of the indi-\ncator depends on the source of the post. Future work could\nintegrate these factors to better understand how these cues\nshape sharing intentions.\nConclusion\nThis study demonstrates that propaganda indicators effec-\ntively reduce the sharing of propaganda on social media. In-\ndicators revealing rhetorical devices of propaganda used in\nposts led to decreased sharing, with effects moderated by\nuser partisanship and post concordance. Our findings sup-\nport efforts to develop detection systems for propaganda\ntechniques (Da San Martino et al. 2019, 2020; Gupta et al.\n2019), highlighting the importance of countering propa-\nganda as a key challenge for social media platforms today.\nAcknowledgements\nWe thank our anonymous reviewers for their useful sugges-\ntions. We also gratefully acknowledge the contributions of\nour colleagues and collaborators who helped revise and re-\nfine several drafts of this paper. This work was supported\nby the National Science Foundation under grant number\n1940713.\nReferences\nA., M. 2016. Addressing Hoaxes and Fake News.\nhttps://about.fb.com/news/2016/12/news-feed-fyi-\naddressing-hoaxes-and-fake-news/, as of February 12,\n2024.\n927\nAnonymous. 2019. Political Independents: Who\nThey Are, What They Think. Pew Research Center.\nhttps://www.pewresearch.org/politics/2019/03/14/political-\nindependents-who-they-are-what-they-think/, as of August\n30, 2023.\nArechar, A. A.; Allen, J. N. L.; Cole, R.; Epstein, Z.;\nGarimella, K.; Gully, A.; Lu, J. G.; Ross, R. M.; Stagnaro,\nM.; Zhang, J.; et al. 2022. Understanding and reducing on-\nline misinformation across 16 countries on six continents.\nhttps://psyarxiv.com/a9frz/. Presented as SOUPS Keynote.\nBooth;, G. C. 1940. Can Propaganda Analysis Be Taught?\nJunior College Journal, 310\u2013312.\nCenter, T. H. 2020. About government and state-affiliated\nmedia account labels on Twitter. https://help.twitter.com/en/\nrules-and-policies/state-affiliated, as of February 15, 2023.\nCenter, T. H. 2022. About Community Notes on Twit-\nter. https://help.twitter.com/en/using-twitter/community-\nnotes, as of February 15, 2023.\nCheck, M. B. 2022a. https://mediabiasfactcheck.com/, as of\nFebruary 15, 2023.\nCheck, M. B. 2022b. Questionable Sources. https:\n//mediabiasfactcheck.com/fake-news/, as of February 15,\n2023.\nClayton, K.; Blair, S.; Busam, J. A.; Forstner, S.; Glance,\nJ.; Green, G.; Kawata, A.; Kovvuri, A.; Martin, J.; Morgan,\nE.; et al. 2020. Real solutions for fake news? Measuring\nthe effectiveness of general warnings and fact-check tags in\nreducing belief in false stories on social media. Political\nbehavior, 42: 1073\u20131095.\nCommittee, H. A. S. 2017. Crafting an Information Warfare\nand Counter-Propaganda Strategy for the Emerging Security\nEnvironment. https://irp.fas.org/congress/2017 hr/counter-\nprop.pdf. Hearing before the Subcommittee on Emerging\nThreats and Capabilities of the H.A.S.C. No. 115-116.\nDa San Martino, G.; Barr \u00b4on-Cede \u02dcno, A.; Wachsmuth, H.;\nPetrov, R.; and Nakov, P. 2020. SemEval-2020 Task 11:\nDetection of Propaganda Techniques in News Articles. In\nProceedings of the Fourteenth Workshop on Semantic Eval-\nuation, 1377\u20131414. Barcelona (online): International Com-\nmittee for Computational Linguistics.\nDa San Martino, G.; Seunghak, Y .; Barr \u00b4on-Cedeno, A.;\nPetrov, R.; Nakov, P.; et al. 2019. Fine-grained analysis\nof propaganda in news article. In Proceedings of the 2019\nconference on empirical methods in natural language pro-\ncessing and the 9th international joint conference on natural\nlanguage processing (EMNLP-IJCNLP), 5636\u20135646. Asso-\nciation for Computational Linguistics.\nDeleger, L.; Li, Q.; Lingren, T.; Kaiser, M.; Molnar, K.;\nStoutenborough, L.; Kouril, M.; Marsolo, K.; Solti, I.; et al.\n2012. Building gold standard corpora for medical natural\nlanguage processing tasks. In AMIA Annual Symposium\nProceedings, volume 2012, 144. American Medical Infor-\nmatics Association.\nDillon, B. W. 2016. ANOV A comparisons. https://people.\numass.edu/bwdillon/LING609/Section3/Lecture15.html, as\nof February 15, 2023.Dixon, S. J. 2023. Distribution of Facebook users\nworldwide as of January 2023, by age and gen-\nder. https://www.statista.com/statistics/376128/facebook-\nglobal-user-age-distribution/, as of October 3, 2023.\nEpstein, Z.; Foppiani, N.; Hilgard, S.; Sharma, S.; Glass-\nman, E.; and Rand, D. 2022. Do explanations increase the\neffectiveness of AI-crowd generated fake news warnings? In\nProceedings of the International AAAI Conference on Web\nand Social Media, volume 16, 183\u2013193.\nFlynn, D. J.; Nyhan, B.; and Reifler, J. 2017. The nature\nand origins of misperceptions: Understanding false and un-\nsupported beliefs about politics. Political Psychology, 38:\n127\u2013150.\nFORCE11. 2020. The FAIR Data principles. https://force11.\norg/info/the-fair-data-principles/, as of February 15, 2023.\nForum, W. E. 2022. Disinformation is a growing cri-\nsis. Governments, business and individuals can help stem\nthe tide. https://www.weforum.org/agenda/2022/10/how-to-\naddress-disinformation/, as of February 15, 2023.\nFractl. 2016. Average Facebook User Sharing Habits Study.\nhttps://www.frac.tl/work/marketing-research/facebook-\nuser-sharing-habits-study/, as of February 15, 2023.\nGebru, T.; Morgenstern, J.; Vecchione, B.; Vaughan, J. W.;\nWallach, H.; Iii, H. D.; and Crawford, K. 2021. Datasheets\nfor datasets. Communications of the ACM, 64(12): 86\u201392.\nGeeng, C.; Francisco, T.; West, J.; and Roesner, F. 2020. So-\ncial media COVID-19 misinformation interventions viewed\npositively, but have limited impact. arXiv preprint\narXiv:2012.11055.\nGraham, M. M. W. 1939. Analyzing Propaganda. Proceed-\nings of the National Education Association, 423\u201331.\nGuess, A. M.; and Lyons, B. A. 2020. Misinformation, disin-\nformation, and online propaganda. Social media and democ-\nracy: The state of the field, prospects for reform, 10.\nGupta, P.; Saxena, K.; Yaseen, U.; Runkler, T.; and Sch \u00a8utze,\nH. 2019. Neural Architectures for Fine-Grained Propaganda\nDetection in News. In Proceedings of the Second Work-\nshop on Natural Language Processing for Internet Freedom:\nCensorship, Disinformation, and Propaganda, 92\u201397. Hong\nKong, China: Association for Computational Linguistics.\nHollis;, E. V . 1939. Antidote for Propaganda,. School and\nSociety, 50:449\u2013453.\nHripcsak, G.; and Rothschild, A. S. 2005. Agreement, the f-\nmeasure, and reliability in information retrieval. Journal of\nthe American medical informatics association, 12(3): 296\u2013\n298.\nHudson, D. L. 2009. Counterspeech Doctrine.\nhttps://www.mtsu.edu/first-amendment/article/940/\ncounterspeech-doctrine, as of February 15, 2023.\nIIRD IWG, N. S. 2022. ROADMAP FOR RE-\nSEARCHERS ON PRIORITIES RELATED TO INFOR-\nMATION INTEGRITY RESEARCH AND DEVELOP-\nMENT. https://www.whitehouse.gov/wp-content/uploads/\n2022/12/Roadmap-Information-Integrity-RD-2022.pdf, as\nof February 15, 2023.\n928\nJanmohamed, K.; Walter, N.; Nyhan, K.; Khoshnood, K.;\nTucker, J. D.; Sangngam, N.; Altice, F. L.; Ding, Q.; Wong,\nA.; Schwitzky, Z. M.; et al. 2021. Interventions to mitigate\nCOVID-19 misinformation: a systematic review and meta-\nanalysis. Journal of Health Communication, 26(12): 846\u2013\n857.\nJowett, G. S.; and O\u2019Donnell, V . 2018. Propaganda & Per-\nsuasion. Sage publications.\nKahan, D. M. 2012. Ideology, motivated reasoning, and\ncognitive reflection: an experimental study (SSRN Schol-\narly Paper ID 2182588). Social Science Research Network.\nhttps://papers. ssrn. com/abstract, 2182588.\nKahan, D. M. 2017. Misconceptions, misinformation, and\nthe logic of identity-protective cognition.\nKaiser, B.; Wei, J.; Lucherini, E.; Lee, K.; Matias, J. N.; and\nMayer, J. R. 2021. Adapting Security Warnings to Counter\nOnline Disinformation. In USENIX Security Symposium,\n1163\u20131180.\nKelley, P. G.; Bresee, J.; Cranor, L. F.; and Reeder, R. W.\n2009. A\u201d nutrition label\u201d for privacy. In Proceedings of the\n5th Symposium on Usable Privacy and Security, 1\u201312.\nKreps, S. E.; and Kriner, D. L. 2022. The COVID-19 info-\ndemic and the efficacy of interventions intended to reduce\nmisinformation. Public Opinion Quarterly, 86(1): 162\u2013175.\nLaskin, A. V . 2019. Defining propaganda: A psychoanalytic\nperspective. Communication and the Public, 4(4): 305\u2013314.\nLee, A.; and Lee, E. B. 1939. The fine art of propaganda.\nLiang, F.; Zhu, Q.; and Li, G. M. 2022. The Effects\nof Flagging Propaganda Sources on News Sharing: Quasi-\nExperimental Evidence from Twitter. The International\nJournal of Press/Politics, 19401612221086905.\nLibraries, J. H. S. 2023. EV ALUATING INFORMA-\nTION: Propaganda, Misinformation, Disinformation.\nhttps://guides.library.jhu.edu/evaluate/propaganda-vs-\nmisinformation, as of February 12, 2024.\nMare \u02c7s, M.; and Mlejnkov \u00b4a, P. 2021. Propaganda and Disin-\nformation as a Security Threat. Challenging Online Propa-\nganda and Disinformation in the 21st Century, 75\u2013103.\nMorrow, G.; Swire-Thompson, B.; Polny, J. M.; Kopec, M.;\nand Wihbey, J. P. 2022. The emerging science of con-\ntent labeling: Contextualizing social media content moder-\nation. Journal of the Association for Information Science\nand Technology, 73(10): 1365\u20131386.\nMummolo, J.; and Peterson, E. 2018. Demand effects in sur-\nvey experiments: An empirical assessment (SSRN Scholarly\nPaper No. ID 2956147). Rochester, NY: Social Science Re-\nsearch Network.\nNassetta, J.; and Gross, K. 2020. State media warning labels\ncan counteract the effects of foreign misinformation. Har-\nvard Kennedy School Misinformation Review.\nNyhan, B.; and Reifler, J. 2010. When corrections fail: The\npersistence of political misperceptions. Political Behavior,\n32(2): 303\u2013330.\nOBERLO. 2023. SOCIAL MEDIA USAGE STATISTICS\nBY AGE. https://www.oberlo.com/statistics/social-media-\nusage-statistics-by-age, as of October 3, 2023.Ogren, P. V .; Savova, G. K.; Chute, C. G.; et al. 2008. Con-\nstructing Evaluation Corpora for Automated Clinical Named\nEntity Recognition. In LREC, volume 8, 3143\u20133150.\nPapakyriakopoulos, O.; and Goodman, E. 2022. The Impact\nof Twitter Labels on Misinformation Spread and User En-\ngagement: Lessons from Trump\u2019s Election Tweets. In Pro-\nceedings of the ACM Web Conference 2022, 2541\u20132551.\nPennycook, G.; Bear, A.; Collins, E. T.; and Rand, D. G.\n2020. The Implied Truth Effect: Attaching Warnings to a\nSubset of Fake News Headlines Increases Perceived Accu-\nracy of Headlines Without Warnings. Management Science,\n66(11): 4944\u20134957.\nPennycook, G.; and Rand, D. G. 2019a. Fighting misinfor-\nmation on social media using crowdsourced judgments of\nnews source quality. Proceedings of the National Academy\nof Sciences, 116(7): 2521\u20132526.\nPennycook, G.; and Rand, D. G. 2019b. Lazy, not biased:\nSusceptibility to partisan fake news is better explained by\nlack of reasoning than by motivated reasoning. Cognition,\n188: 39\u201350.\nQCRI. 2021. Tanbih API. https://app.swaggerhub.com/apis/\nyifan2019/Tanbih/0.8.0/, as of February 15, 2023.\nRoth, Y .; and Pickles, N. 2020. Updating our ap-\nproach to misleading information. Twitter Blog. https:\n//blog.twitter.com/en us/topics/product/2020/updating-our-\napproach-to-misleading-information, as of February 15,\n2023.\nSchmid, P.; and Betsch, C. 2019. Effective strategies for\nrebutting science denialism in public discussions. Nature\nHuman Behaviour, 3(9): 931\u2013939.\nSharevski, F.; Devine, A.; Jachim, P.; and Pieroni, E. 2022.\nMeaningful Context, a Red Flag, or Both? Preferences for\nEnhanced Misinformation Warnings Among US Twitter\nUsers. In Proceedings of the 2022 European Symposium\non Usable Security, 189\u2013201.\nSilver, N. C.; Drake, K. L.; Niaghi, Z. B.; Brim, A. C.; and\nPedraza, O. 2002. The effects of product, signal word, and\ncolor on warning labels: Differences in perceived hazard. In\nProceedings of the Human Factors and Ergonomics Society\nAnnual Meeting, volume 46, 735\u2013739. SAGE Publications\nSage CA: Los Angeles, CA.\nSpradling, M.; Straub, J.; and Strong, J. 2021. Protection\nfrom \u2018fake news\u2019: the need for descriptive factual labeling\nfor online content. Future Internet, 13(6): 142.\nStoll, A. 2017. Post hoc tests: Tukey honestly significant\ndifference test. The SAGE encyclopedia of communication\nresearch methods, 1306\u20131307.\nThompson, N.; Wang, X.; and Daya, P. 2019. Determinants\nof news sharing behavior on social media. Journal of Com-\nputer Information Systems.\nWong, L. Y .; and Burkell, J. 2017. Motivations for sharing\nnews on social media. In Proceedings of the 8th Interna-\ntional conference on social media & society, 1\u20135.\nWood, T.; and Porter, E. 2019. The elusive backfire effect:\nMass attitudes\u2019 steadfast factual adherence. Political Behav-\nior, 41: 135\u2013163.\n929\nYaqub, W.; Kakhidze, O.; Brockman, M. L.; Memon, N.;\nand Patil, S. 2020. Effects of credibility indicators on social\nmedia news sharing intent. In Proceedings of the 2020 chi\nconference on human factors in computing systems, 1\u201314.\nPaper Checklist\n1. For most authors...\n(a) Would answering this research question advance sci-\nence without violating social contracts, such as violat-\ning privacy norms, perpetuating unfair profiling, exac-\nerbating the socio-economic divide, or implying disre-\nspect to societies or cultures? Yes. The research ques-\ntions answered in this paper advance research in con-\ntent moderation and labeling, without violating social\ncontracts.\n(b) Do your main claims in the abstract and introduction\naccurately reflect the paper\u2019s contributions and scope?\nYes\n(c) Do you clarify how the proposed methodological ap-\nproach is appropriate for the claims made? Yes\n(d) Do you clarify what are possible artifacts in the data\nused, given population-specific distributions? Yes. We\ndiscuss population sample in the paper.\n(e) Did you describe the limitations of your work? Yes\n(f) Did you discuss any potential negative societal im-\npacts of your work? Yes. We do not create any artifacts\nthat can be used outside of this work negatively.\n(g) Did you discuss any potential misuse of your work?\nYes. We do not release any data that can potentially be\nmisused.\n(h) Did you describe steps taken to prevent or mitigate po-\ntential negative outcomes of the research, such as data\nand model documentation, data anonymization, re-\nsponsible release, access control, and the reproducibil-\nity of findings? Yes. See methods section.\n(i) Have you read the ethics review guidelines and en-\nsured that your paper conforms to them? Yes\n2. Additionally, if your study involves hypotheses testing...\n(a) Did you clearly state the assumptions underlying all\ntheoretical results? Yes\n(b) Have you provided justifications for all theoretical re-\nsults? Yes\n(c) Did you discuss competing hypotheses or theories that\nmight challenge or complement your theoretical re-\nsults? Yes. See discussion section.\n(d) Have you considered alternative mechanisms or expla-\nnations that might account for the same outcomes ob-\nserved in your study? Yes. See discussion section.\n(e) Did you address potential biases or limitations in your\ntheoretical framework? Yes. See discussion section.\n(f) Have you related your theoretical results to the existing\nliterature in social science? Yes. We do this for each\nresult in the discussion section.\n(g) Did you discuss the implications of your theoretical\nresults for policy, practice, or further research in thesocial science domain? Yes. We have an implications\nsection discussing this.\n3. Additionally, if you are including theoretical proofs...\n(a) Did you state the full set of assumptions of all theoret-\nical results? NA\n(b) Did you include complete proofs of all theoretical re-\nsults? NA\n4. Additionally, if you ran machine learning experiments...\n(a) Did you include the code, data, and instructions\nneeded to reproduce the main experimental results (ei-\nther in the supplemental material or as a URL)? NA\n(b) Did you specify all the training details (e.g., data splits,\nhyperparameters, how they were chosen)? NA\n(c) Did you report error bars (e.g., with respect to the ran-\ndom seed after running experiments multiple times)?\nNA\n(d) Did you include the total amount of compute and the\ntype of resources used (e.g., type of GPUs, internal\ncluster, or cloud provider)? NA\n(e) Do you justify how the proposed evaluation is suffi-\ncient and appropriate to the claims made? NA\n(f) Do you discuss what is \u201cthe cost\u201c of misclassification\nand fault (in)tolerance? NA\n5. Additionally, if you are using existing assets (e.g., code,\ndata, models) or curating/releasing new assets, without\ncompromising anonymity...\n(a) If your work uses existing assets, did you cite the cre-\nators? Yes\n(b) Did you mention the license of the assets? Yes. The\ndatasets are publicly available.\n(c) Did you include any new assets in the supplemental\nmaterial or as a URL? Yes. We include the social me-\ndia posts used in this study as url.\n(d) Did you discuss whether and how consent was ob-\ntained from people whose data you\u2019re using/curating?\nNA\n(e) Did you discuss whether the data you are using/cu-\nrating contains personally identifiable information or\noffensive content? NA\n(f) If you are curating or releasing new datasets, did you\ndiscuss how you intend to make your datasets FAIR\n(see FORCE11 (2020))? NA\n(g) If you are curating or releasing new datasets, did you\ncreate a Datasheet for the Dataset (see Gebru et al.\n(2021))? NA\n6. Additionally, if you used crowdsourcing or conducted\nresearch with human subjects, without compromising\nanonymity...\n(a) Did you include the full text of instructions given to\nparticipants and screenshots? Yes. See Appendix.\n(b) Did you describe any potential participant risks, with\nmentions of Institutional Review Board (IRB) ap-\nprovals? Yes. See methods section.\n930\n(c) Did you include the estimated hourly wage paid to\nparticipants and the total amount spent on participant\ncompensation? Yes. See methods section.\n(d) Did you discuss how data is stored, shared, and dei-\ndentified? Yes. No PII was collected.\nAppendix\nSurvey Instrument\nFollowing the consent form and prolific ID collection (for\ncompensation purposes), participants in all groups were pre-\nsented with a commitment request. If the answer \u201cNo, I will\nnot\u201d was selected, the participant was redirected to the end\nof the study.\n1. We care about the quality of our survey data. For us to\nget accurate measures of your information sharing be-\nhavior on social media platforms, please respond to the\nquestions as you would in real life. Do you commit to\nresponding to this survey thoughtfully?\n\u2022 Yes, I will\n\u2022 No, I will not\n\u2022 I can\u2019t promise either way\nParticipants were then presented with 18 posts (12 pro-\npaganda and 6 non-propaganda). While the posts remained\nthe same across all four groups, participants in the treatment\ngroups saw posts with corresponding indicators and partic-\nipants in the control group saw posts without any indicator.\nUnder each of the 18 posts, the following questions were\nasked.\n1. Would you share this post on social media?\n\u2022 Yes\n\u2022 No\nIf \u2018Yes\u2019 was selected, the following question was shown\n1. Why did you choose to share the post?\n\u2022 I agree with the contents of the post\n\u2022 I find it interesting\n\u2022 The information in the post is true\n\u2022 Other. Please specify:\nIf \u2018No\u2019 was selected, the following question was shown\n1. Why did you choose NOT to share the post?\n\u2022 I don\u2019t agree with the contents of the post\n\u2022 I don\u2019t find it interesting\n\u2022 The information in the post is not true\n\u2022 Other. Please specify:\nAn attention check question was also asked.\n1. Please click \u2019strongly agree\u2019 to show you are paying at-\ntention to this question.\n\u2022 Strongly Agree\n\u2022 Agree\n\u2022 Disagree\n\u2022 Strongly DisagreeParticipants were finally asked questions on demograph-\nics.\n1. What is your year of birth?\n2. Which is your gender identity?\n\u2022 Female\n\u2022 Male\n\u2022 Other. Please Specify:\n3. Generally speaking, do you consider yourself a Republi-\ncan, a Democrat, or an Independent?\n\u2022 Democrat\n\u2022 Independent with a lean toward the Democratic party\n\u2022 Independent\n\u2022 Independent with a lean toward the Republican party\n\u2022 Republican\n4. What is the highest level of education you have com-\npleted? (If currently enrolled, highest degree received.)\n\u2022 Less than high school\n\u2022 Some college\n\u2022 Prefer not to say\n\u2022 Professional degree after college (e.g., law or medical\nschool)\n\u2022 V ocational training\n\u2022 High school graduate\n\u2022 Doctoral degree\n\u2022 High school diploma\n\u2022 College graduate (B.S., B.A., or other 4 year degree)\n\u2022 Master\u2019s degree\n\u2022 Other. Please specify:\n5. How much time do you spend on social media per day?\n\u2022 Less than 2 hours\n\u2022 More than 2 hours\nSocial Media Posts\nThe posts can be found at:\nhttps://doi.org/10.6084/m9.figshare.24274639.\nDemand Effects\nTo test for the effects of demand characteristics in our pri-\nmary experiment, we conducted a short experiment that ma-\nnipulated the indicators. In a randomized controlled trial\nexperiment, we assigned participants to either a control or\ntreatment group. The control group (n=199) saw 1 propa-\nganda post with an indicator and 1 non-propaganda post\nwithout an indicator, randomly from a pool of 6 propaganda\nand 6 non-propaganda posts. The treatment group (n=200),\non the other hand, saw 1 propaganda post without an indi-\ncator and 1 non-propaganda post with an intentionally false\nindicator, again, randomly from a pool of 6 propaganda and\n6 non-propaganda posts. Our hypothesis was that, if the in-\ndicator effect was driven by demand effects, then we would\nexpect to see a similar effect size (reduction in sharing) for\nthe non-propaganda posts that were falsely flagged as pro-\npaganda.\n931\nWe analyzed sharing behavior for propaganda posts us-\ning a logistic regression model comparing \u201cindicator\u201d vs.\n\u201cno indicator\u201d (coefficient=0.365, SE=0.372, p=0.327) cor-\nresponding to an odds ratio of 1.440 (95% CI [0.694,2.988]).\nSince p > 0.05, this effect size is insignificant. We then\nrepeated this analysis for non-propaganda posts compar-\ning \u201cfalse indicator\u201d vs. \u201cno indicator\u201d (coefficient=-0.544,\nSE=0.358, p=0.129) corresponding to an odds ratio of 0.580\n(95% CI [0.287,1.172]). Since p > 0.05, this effect size was\nalso insignificant. Hence, in both cases, we observed no sta-\ntistically significant reduction in sharing under the presence\nof an indicator. Due to this lack of significance, the results\nremain inconclusive: it is possible that no meaningful de-\nmand effect exists, or that our study did not have enough\npower to detect an effect. Had both effects been significant,\nwe would have compared their magnitudes to understand if\nthey were similarly reducing shares, indicating a demand ef-\nfect. However, the insignificant findings precluded this step.\nWe believe that a more thought-out experiment such as the\none suggested in (Mummolo and Peterson 2018) would be\nmore appropriate to investigate this, even though the very\nsame study proves that most studies are robust to such de-\nmand effects.\nPost-Specific Variation in Indicator Effects\nIn this study, we used 18 posts (12 propaganda and 6 non-\npropaganda posts) to understand the effectiveness of the dif-\nferent indicator types used in this study. This sample size of\nstimuli helped us minimize participant fatigue, thereby en-\nsuring that responses were of high quality, a practice also\nobserved in prior fact-checking and misinformation warn-\ning literature (Pennycook et al. 2020; Sharevski et al. 2022;\nPennycook and Rand 2019b). We acknowledge that, in real-\nworld settings, social media users encounter far more con-\ntent, leading to \u201cinformation and intervention fatigue\u201d where\nrepeated exposure to these can influence people\u2019s sharing in-\ntentions. While our smaller set of posts cannot fully model\nthis complexity, it helped us investigate the immediate ef-\nfects of these indicators.\nTo determine if any single post or a subset of posts drove\nour main effect, we examined the indicators\u2019 effectiveness\nacross posts. First, we fitted a random-slopes model (where\neach post has a different slope for the different indica-\ntor groups) and compared this model to a simple random-\nintercepts model (which assumes uniform indicator effect\nacross posts). To test for significance, we used the likelihood\nratio test which gave us \u03c72= 9.11,p=NaN , and showed\nsigns of overfitting (singular estimates), indicating that the\nrandom-slopes model was too complex for the given data.\nMoreover, the random-slope model\u2019s AIC value (10772) was\nhigher than the random-intercepts model (10763), indicat-\ning poor overall fit. Consequently, we removed the random-\nslopes term and used the random-intercepts model instead\nto better explain the data, suggesting no strong evidence that\nthe indicators\u2019 effects varied across posts.\nTo further strengthen this analysis, we investigated a\nfixed-effects interaction approach where we analyzed the\ninteraction term, GroupxPost ID. The likelihood ratio test\ngave us \u03c72= 44.58,p= 0.085, again indicating no statis-tically significant interaction between specific posts and in-\ndicator groups. In other words, within the set of propaganda\nposts that we used, there is no strong evidence that the effect\nof the indicators depends on any specific post among these.\nWe believe that these analyses support the conclusion that\nour indicators are effective independent of the posts being\nflagged, even though we acknowledge that a larger sample\nof posts could further strengthen the ecological validity.\nEthical Considerations\nThis study was carried out with the intent of designing ef-\nfective indicators to curb propaganda on social media plat-\nforms and to increase transparency regarding how these in-\ndicators affect sharing intentions online. All annotated posts\nincluded in the study were collected from publicly available\nwebsites. We provide access to these via a link to facilitate\ntransparency and further research. However, we acknowl-\nedge that the techniques, assets, and findings might be mis-\nused, for example, to manipulate public opinion. We there-\nfore emphasize that the insights from this study be used eth-\nically and in ways that encourage informed public discourse\nrather than serve as tools for political manipulation.\n932\nStep V\nariable Reference Level Odds Ratio Lower CI Upper CI p\nStep 1:\nGroupGroup Control Standard 0.394 0.290 0.535 2.480e-09***\nGroup Control Contextual 0.445 0.328 0.602 1.5714e-07***\nGroup Control Warning+Contextual 0.444 0.328 0.601 1.377-07***\nStep 2:\nConcordance Group Control Standard 0.377 0.274 0.519 2.405e-09***\nGroup Control Contextual 0.428 0.312 0.587 1.5344e-07***\nGroup Control Warning+Contextual 0.427 0.586 0.601 1.307-07***\nConcordance Discordant Concordant 2.864 2.581 3.178 0.0***\nStep 3:\nP\nolitical AffiliationGroup Control Standard 0.382 0.278 0.524 2.565e-09***\nGroup Control Contextual 0.433 0.316 0.593 1.725e-07***\nGroup Control Warning+Contextual 0.43 0.315 0.588 1.263-07***\nConcordance Discordant Concordant 2.877 2.592 3.192 0.0***\nPolitical Affiliation Democrat Independent 1.093 0.831 1.439 0.524\nPolitical Affiliation Democrat Republican 1.794 1.349 2.387 5.898e-05***\nStep 4:\nSocial\nMedia UsageGroup Control Standard 0.387 0.283 0.53 3.151e-09***\nGroup Control Contextual 0.435 0.319 0.594 1.585e-07***\nGroup Control Warning+Contextual 0.431 0.316 0.587 1.000-07***\nConcordance Discordant Concordant 2.877 2.592 3.193 0.0***\nPolitical Affiliation Democrat Independent 1.169 0.89 1.537 0.262\nPolitical Affiliation Democrat Republican 1.912 1.439 2.539 7.670e-06***\nSocial Media Usage Low High 1.718 1.373 2.151 2.260e-06***\nStep 5:\nGenderGroup Control Standard 0.384 0.278 0.530 6.132e-09***\nGroup Control Contextual 0.443 0.322 0.608 5.114e-07***\nGroup Control Warning+Contextual 0.425 0.310 0.584 1.284e-07***\nConcordance Discordant Concordant 2.900 2.597 3.238 9.573e-80***\nPolitical Affiliation Democrat Independent 1.025 0.770 1.364 0.865\nPolitical Affiliation Democrat Republican 1.718 1.279 2.308 0.000327***\nSocial Media Usage Low High 1.826 1.449 2.302 3.434e-07***\nGender Female Male 1.952 1.539 2.475 3.385e-08***\nGender Female Other 1.257 0.511 3.093 0.619\nNote: Significance codes: *** p <0.001, **p <0.01, *p <0.05.\nTable 3: Step-wise logistic regression results showing the odds of sharing a propaganda post. The table shows the sequential\naddition of predictors, starting with the treatment groups in step 1 and subsequently adding concordance, political affiliation,\nsocial media usage, and gender to illustrate how each variable contributes to the overall model.\n933\nGroup 1 Group 2 Odds Ratio Lower CI Upper CI p-value\nDemocrat, Discordant Democrat, Concordant 0.077926 0.052707 0.115095 0.000***\nDemocrat, Concordant Independent, Discordant 0.371948 0.233167 0.593333 0.000***\nDemocrat, Discordant Independent, Concordant 0.493615 0.307586 0.792154 0.000***\nIndependent, Discordant Independent, Concordant 2.356082 1.614459 3.438379 0.000***\nRepublican, Discordant Republican, Concordant 0.137106 0.093201 0.201493 0.000***\nRepublican, Concordant Independent, Discordant 0.265272 0.165630 0.424433 0.000***\nRepublican, Discordant Independent, Concordant 1.217744 0.767974 1.930927 0.828\nSignificance codes: *** p <0.001, ** p <0.01, * p <0.05\nTable 4: Tukey\u2019s HSD test showing interaction effects between Political Affiliation and Concordance\nGroup 1 Group 2 Odds Ratio Lower CI Upper CI p-value\nControl, Democrat, Discordant Control, Democrat, Concordant 0.108392 0.051767 0.226955 0.000***\nT1, Democrat, Discordant T1, Democrat, Concordant 0.082413 0.037328 0.181954 0.000***\nControl, Democrat, Discordant T1, Democrat, Discordant 1.743684 0.760332 3.998823 0.313\nControl, Democrat, Concordant T1, Democrat, Concordant 1.325779 0.580422 3.028296 0.817\nT2, Democrat, Discordant T2, Democrat, Concordant 0.039203 0.015252 0.100761 0.000***\nControl, Democrat, Discordant T2, Democrat, Discordant 4.957988 2.007721 12.25580 0.000***\nControl, Democrat, Concordant T2, Democrat, Concordant 1.793197 0.779580 4.124728 0.272\nT3, Democrat, Discordant T3, Democrat, Concordant 0.078316 0.034218 0.179424 0.000***\nControl, Democrat, Discordant T3, Democrat, Discordant 2.437566 1.047074 5.668928 0.034*\nControl, Democrat, Concordant T3, Democrat, Concordant 1.761208 0.767206 4.043053 0.297\nControl, Independent, Discordant Control, Independent, Concordant 2.401275 1.225072 4.702056 0.001**\nT1, Independent, Discordant T1, Independent, Concordant 1.624175 0.742301 3.550186 0.843\nControl, Independent, Discordant T1, Independent, Discordant 3.511348 1.643783 7.500727 0.000***\nControl, Independent, Concordant T1, Independent, Concordant 2.372632 1.087629 5.181010 0.023*\nT2, Independent, Discordant T2, Independent, Concordant 2.452235 1.185304 5.073343 0.002**\nControl, Independent, Discordant T2, Independent, Discordant 2.272771 1.104066 4.683286 0.018*\nControl, Independent, Concordant T2, Independent, Concordant 2.323327 1.084371 4.972884 0.023*\nT3, Independent, Discordant T3, Independent, Concordant 3.257630 1.548056 6.855148 0.000***\nControl, Independent, Discordant T3, Independent, Discordant 2.127612 1.030455 4.392946 0.037*\nControl, Independent, Concordant T3, Independent, Concordant 2.886371 1.331092 6.265134 0.002**\nControl, Republican, Discordant Control, Republican, Concordant 0.158341 0.078787 0.318223 0.000***\nT1, Republican, Discordant T1, Republican, Concordant 0.103312 0.044600 0.239308 0.000***\nControl, Republican, Discordant T1, Republican, Discordant 5.328128 2.341988 12.10961 0.000***\nControl, Republican, Concordant T1, Republican, Concordant 3.476409 1.502304 8.044591 0.001**\nT2, Republican, Discordant T2, Republican, Concordant 0.114406 0.052392 0.249573 0.000***\nControl, Republican, Discordant T2, Republican, Discordant 2.549762 1.139968 5.708750 0.015*\nControl, Republican, Concordant T2, Republican, Concordant 1.842273 0.799315 4.246096 0.237\nT3, Republican, Discordant T3, Republican, Concordant 0.159613 0.074125 0.343695 0.000***\nControl, Republican, Discordant T3, Republican, Discordant 2.664456 1.208041 5.870853 0.008**\nControl, Republican, Concordant T3, Republican, Concordant 2.685857 1.171166 6.153371 0.012*\nSignificance codes: *** p <0.001, ** p <0.01, * p <0.05\nTable 5: Tukey\u2019s HSD test showing interaction effects between Political Affiliation, Concordance, and Treatment\n934", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "What's in a Label? Propaganda Labels and User Sharing Behavior on Social Media Platforms", "author": ["J Jose", "C Geeng", "KO Morales", "D McCoy"], "pub_year": "2025", "venue": "Proceedings of the \u2026", "abstract": "Authentic information is vital for a society's ability to make rational decisions. Fabricated and  manipulative information can be harmful to society as seen in cases of threatening events"}, "filled": false, "gsrank": 486, "pub_url": "https://ojs.aaai.org/index.php/ICWSM/article/view/35853", "author_id": ["0Tfk-VsAAAAJ", "sIKmv4UAAAAJ", "m2q5JJ0AAAAJ", "pT8-2f0AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:vQg6p6_LAYEJ:scholar.google.com/&output=cite&scirp=485&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D480%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=vQg6p6_LAYEJ&ei=XbWsaM-eLcDZieoPqdqh8QU&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:vQg6p6_LAYEJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://ojs.aaai.org/index.php/ICWSM/article/download/35853/38007"}}, {"title": "Automatically Estimating the Trustworthiness of Wikipedia", "year": "NA", "pdf_data": "Friedrich-Schiller-Universit\u00e4t Jena\nInstitute of Computer Science\nDegree Programme Computer Science, B.Sc.\nAutomatically Estimating the\nTrustworthiness of Wikipedia\nArticles\nBachelor\u2019s Thesis\nLuca-Philipp Grumbach\n1. Referee: Prof. Dr. Matthias Hagen\n2. Referee: Jan Heinrich Merker, M.Sc.\nSubmission date: February 21, 2025\nZusammenfassung\nWikipedia ist eine der meistgenutzen Informationsquellen im Internet und um-\nfasst mehrere Millionen von Artikeln \u00fcber eine Vielzahl an Themen. Diese\nArtikel werden von freiwilligen und teilweise anonymen Autoren erstellt und\nbearbeitet. Als Folge dieser offenen Struktur ist es jedoch m\u00f6glich, Wikipedia\nzu missbrauchen um zum Beispiel Falschinformation und Propaganda zu ver-\nbreiten. Um die manuelle \u00dcberpr\u00fcfung der enormen Anzahl von Artikeln zu\nunterst\u00fctzen, widmen wir uns in dieser Arbeit einem automatischen Verfahren\nzur Einsch\u00e4tzung der Vertrauensw\u00fcrdigkeit von Wikipedia-Artikeln.\nWir pr\u00e4sentieren ein Modell, welches auf Basis von manuell annotierten\nWikipedia-Artikeln,dieVertrauensw\u00fcrdigkeitvonexternenQuelleneinsch\u00e4tzen\nsoll. Dazu analysieren wir wie oft eine externe Quelle in Wikipedia-Artikeln\nreferenziert wurde, in denen entweder ein Problem mit der Zuverl\u00e4ssigkeit fest-\ngestellt oder ein zuvor festgestelltes Problem gel\u00f6st wurde. Aus der H\u00e4ufigkeit\nder jeweiligen Vorkommen sollen R\u00fcckschl\u00fcsse auf einen positiven oder nega-\ntiven Einfluss der Quelle auf die Vertrauensw\u00fcrdigkeit von neuen Wikipedia-\nArtikeln gezogen werden. Unser Ziel ist es, basierend auf den externen Quellen\ndie ein Wikipedia-Artikel zitiert, vorherzusagen ob in diesem Artikel ein Prob-\nlem mit der Zuverl\u00e4ssigkeit vorliegt oder nicht. Erste Experimente zeigen\njedoch, dass unser Modell die Vertrauensw\u00fcrdigkeit von Wikipedia-Artikeln\nnoch nicht zuverl\u00e4ssig einsch\u00e4tzen kann. Als Gr\u00fcnde f\u00fcr unsere Resultate\nidentifizieren wir sowohl Defizite in den zugrundeliegenden Daten f\u00fcr die Ein-\nsch\u00e4tzung der Vertrauensw\u00fcrdigkeit von externen Quellen, als auch in der\nvereinfachtenModellarchitektur. Abschlie\u00dfendf\u00fchrenwireineDiskussion\u00fcber\nm\u00f6gliche Verbesserungen und zuk\u00fcnftige Forschungsrichtungen.\nAbstract\nWikipedia has emerged as one of the most used sources of information on the\ninternet, with millions of articles spanning a wide range of topics. Its collabo-\nrative nature, where content is contributed by volunteers worldwide, allows for\nrapid updates but also creates the possibility of misuse, for example by spread-\ning misinformation and propaganda. In order to support the manual review\nof the vast number of articles, we explore a possible method for automatically\nestimating the trustworthiness of Wikipedia articles.\nWe present a model to assess the trustworthiness of external sources based\non manually annotated Wikipedia articles. To do so, we analyze how often an\nexternal source was referenced in Wikipedia articles in which either a problem\nwith reliability was identified or a previously identified problem was solved.\nFrom the frequency of the respective occurrences, we aim to draw conclusions\nabout a positive or negative influence of the source on the trustworthiness of\nnew Wikipedia articles. For this, we use the external sources referenced in\na Wikipedia article to predict whether the article contains a reliability issue\nor not. First experiments show that our model is not able to reliably assess\nthe trustworthiness of Wikipedia articles yet. As reasons for our results, we\nidentify shortcomings in the underlying data for assessing the trustworthiness\nof external sources as well as in our simplified model architecture. Finally, we\ndiscuss possible improvements and future research directions.\nContents\n1 Introduction 1\n1.1 Structure of Wikipedia . . . . . . . . . . . . . . . . . . . . . . . 2\n1.2 Use of External Sources on Wikipedia . . . . . . . . . . . . . . . 3\n1.3 Trustworthiness and Reliability . . . . . . . . . . . . . . . . . . 3\n1.4 Outline of the Approach . . . . . . . . . . . . . . . . . . . . . . 5\n2 Related Work 7\n3 Revision Extraction 9\n3.1 Selection of Reliability Templates . . . . . . . . . . . . . . . . . 9\n3.2 Parsing Wikipedia Dumps . . . . . . . . . . . . . . . . . . . . . 11\n3.3 Train and Test Sets . . . . . . . . . . . . . . . . . . . . . . . . . 12\n4 Trustworthiness Estimation of External Sources 14\n4.1 Trustworthiness Estimation Process . . . . . . . . . . . . . . . . 14\n4.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n5 Trustworthiness Estimation of Articles 20\n5.1 Template Prediction Process . . . . . . . . . . . . . . . . . . . . 20\n5.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n5.2.1 Template: Unreliable Sources . . . . . . . . . . . . . . . 23\n5.2.2 Template: Dubious . . . . . . . . . . . . . . . . . . . . . 24\n6 Discussion 26\n6.1 Error Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n6.1.1 Model Constraints . . . . . . . . . . . . . . . . . . . . . 26\n6.1.2 Complexities in Template Identification . . . . . . . . . . 27\n6.1.3 Data Deficiencies . . . . . . . . . . . . . . . . . . . . . . 29\n6.2 Possible Real World Applications . . . . . . . . . . . . . . . . . 31\n6.3 Conclusion and Future Work . . . . . . . . . . . . . . . . . . . . 32\nBibliography 33\ni\nChapter 1\nIntroduction\nWikipedia has emerged as one of the most used sources of information on\nthe internet,1with millions of articles spanning a wide range of topics.2Its\ncollaborative nature, where content is contributed by volunteers worldwide,\nallows for rapid updates but also creates the possibility of misuse, for example\nby spreading misinformation and propaganda.\nAs a platform commonly used for education, work, and personal decision-\nmaking,asshownbyLemmerichetal.[2018],ensuringthereliabilityofWikipedia\narticlesiscrucial. WhileKr\u00e4enbringetal.[2014]haveconcludedthatWikipedia\ncan be as accurate as traditional encyclopedias, concerns about its reliability\npersist. In the medical field for example, Azer [2014] and Phillips et al. [2014]\nhave highlighted inaccuracies in Wikipedia articles, emphasizing the need for\ncaution when using it as a source of information.\nAlthough Wikipedia employs mechanisms such as editorial oversight, ci-\ntation requirements, and vandalism detection,3the vast amount of articles\nrenders manual verification intensive. One way of supporting this process\nis through automated tools, that can assist in evaluating article quality and\ntrustworthiness. Research in this field has primarily focused on author repu-\ntation, article quality, and edit history. Notable works include the papers of\nAdler et al. [2008], Moturu and Liu [2009], and Suzuki and Yoshikawa [2012],\nwho have developed models to predict article trustworthiness based on various\nfeatures automatically.\nEstimating the trustworthiness of Wikipedia articles based on their exter-\nnal sources has not been studied extensively. We believe this aspect deserves\nfurther consideration because Wikipedia articles strongly depend on references\nto external sources. This is because Wikipedia itself is not a source of original\n1https://www.semrush.com/website/top/\n2https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia\n3https://en.wikipedia.org/wiki/Wikipedia:Editorial_oversight_and_control\n1\nCHAPTER 1. INTRODUCTION\nthought but rather a collection of existing knowledge.4Consequently, we as-\nsume that the trustworthiness of a Wikipedia article is heavily influenced by\nthe trustworthiness of the sources it references. To examine this further, we\npropose a model that first estimates the trustworthiness of external sources\nusing manually annotated problematic articles and then leverages this infor-\nmation to estimate the trustworthiness of new Wikipedia articles.\n1.1 Structure of Wikipedia\nBefore we go into the details of our approach, it is important to understand\nthe structure of Wikipedia. At the top level, Wikipedia consists of separate\nprojects with their own communities, policies, and guidelines for each lan-\nguage. In this thesis, we focus on the English Wikipedia, the largest and most\nactive project.5Wikipedia is organized into pages, each having a unique ID,\na title, and a namespace. The namespace groups pages by their type, with\neach namespace serving a specific purpose. Some examples include articles,\ncategories, templates, user pages, and talk pages.6In our analysis, we only\nestimate the trustworthiness of articles, as they are the main encyclopedia\nentries providing detailed information on specific topics.7\nArticles consist of revisions, with each revision representing a snapshot at\na particular point in time. When a user makes an edit, the updated content\nis published as a new revision with a unique and monotonically increasing ID.\nThis is also the case for reverts, which are revisions that undo the changes\nmade in a previous revision.\nThe second type of page that we intensively use in our analysis are tem-\nplates. These are pages that are designed to be included in other pages, pro-\nvidingawaytoreusecontentacrossmultiplearticles. Theyareoftenused with\ncustomizable input and for various purposes, such as navigation, formatting\nor to display messages for users.8In order to include a template in an article,\neditors place the template\u2019s name enclosed by double curly braces within the\narticle text, for example:{{Template name}}. When the article is rendered,\nthe template name is replaced with the actual content.9\n4https://en.wikipedia.org/wiki/Wikipedia:What_Wikipedia_is_not\n5https://en.wikipedia.org/wiki/English_Wikipedia\n6https://en.wikipedia.org/wiki/Wikipedia:Namespace\n7https://en.wikipedia.org/wiki/Wikipedia:What_is_an_article\n8https://en.wikipedia.org/wiki/Help:A_quick_guide_to_templates\n9https://en.wikipedia.org/wiki/Help:Template\n2\nCHAPTER 1. INTRODUCTION\n1.2 Use of External Sources on Wikipedia\nExternal sources are citations of books, articles, websites, and other materials\nthat provide information on a topic while not being part of Wikipedia itself.\nIn the context of Wikipedia, they are officially categorized inExternal Links\nandReferences.\nExternal Links are hyperlinks to websites that provide additional infor-\nmation on a topic. These hyperlinks should be placed in theExternal Links\nsection, which is located near the bottom of the article. External Links are\npurely optional and must not be used to verify any content. However, users\nmay follow external links for further reading or meaningful, relevant content\nthat is not deemed suitable for inclusion in the article itself.10\nReferences on the other hand are citations to external sources that verify\nthe information presented in an article. References are placed within the text\nitself and immediately after the statement which they support. On the ren-\ndered page, references are displayed as superscript numbers, which link to the\nfull citation in theReferencessection at the bottom of the article. This sec-\ntion is created automatically by Wikipedia and contains a list of all references\nused throughout the article. The references are listed in the order they appear\nin the article, with each reference having a unique number that corresponds\nto the superscript number in the text. This system allows readers to easily\nverify the information presented in the article by checking the corresponding\nsource.11\nGiven that external links are optional and not used for verification, we will\nonly consider references in our analysis. Accordingly, whenever we refer to\nexternal sources, we mean sources cited by references. If external links are\nintended, we will explicitly state so.\n1.3 Trustworthiness and Reliability\nTrustworthiness has various definitions and interpretations, especially in the\nworld of computer science and information systems, as discussed by Viljanen\nViljanen [2005]. In order to perform objective estimations, we will use this\nsection to ensure a common understanding first.\nIn the context of Wikipedia, trustworthiness is closely linked to reliability,\nwhere a reliable article is one that provides accurate and unbiased information.\nToupholdthisstandard, Wikipedia\u2019scontentpoliciesrequireallinformationto\n10https://en.wikipedia.org/wiki/Wikipedia:External_links\n11https://en.wikipedia.org/wiki/Help:External_links_and_references\n3\nCHAPTER 1. INTRODUCTION\nFigure 1.1:Example of a Wikipedia article with theUnreliable sourcestemplate.\nbe verifiable, meaning it must be supported by reliable sources.12This is also\nreflected in the citation guidelines, which recommend citing reputable sources\nto ensure the reliability of the information presented.13One effective method\nfor assessing article reliability has been described by Wong et al. [2021], which\ninvolves filtering revisions to identify the use of templates maintained by the\nWikiProject Reliability.\nWikiProjects are groups of editors, working together with the goal of im-\nproving Wikipedia.14The WikiProject Reliability aims to improve article reli-\nability by ensuring adherence to Wikipedia\u2019s content policies and encouraging\nrobust sourcing practices.15To achieve this, the project maintains a collection\nof templates that editors can use to flag articles with reliability issues. These\ntemplates are designed to highlight specific problems, such as dubious state-\nments or references to unreliable sources.16Note that in all further parts of\nthis work, we refer to the templates maintained by the WikiProject Reliabil-\nity asreliability templates. When an article contains a reliability template, a\nwarning is shown at the top of the article, providing a clear signal to readers\nand editors that the article may have issues. An example of theUnreliable\nsourcestemplate is shown in Figure 1.1, where a text box at the top of the\narticle can be seen.\nOur definition of trustworthiness uses these reliability templates as indi-\ncators of editorial assessments. Theadditionof a reliability template to an\narticle is seen as a negative signal, indicating that the article contains an issue\nregarding its reliability. We say that articles containing reliability issues are\nless trustworthy. Conversely, theremovalof a reliability template is seen as a\n12https://en.wikipedia.org/wiki/Wikipedia:Core_content_policies\n13https://en.wikipedia.org/wiki/Wikipedia:Citing_sources\n14https://en.wikipedia.org/wiki/Wikipedia:WikiProject\n15https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Reliability\n16https://en.wikipedia.org/wiki/Template:Citation_and_verifiability_\narticle_maintenance_templates\n4\nCHAPTER 1. INTRODUCTION\npositive signal, indicating that the article has been improved and is now more\nreliable. We say that articles where reliability issues have been resolved are\nmore trustworthy. Although the presence of such a template is a strong signal\nfor reliability issues, its absence does not necessarily imply that an article is\ntrustworthy. Articles containing reliability issues may not have been flagged\nyet, or the issue may have been overlooked. To address this, we focus on re-\nvisions where templates were deliberately added and then removed by editors,\nas this reflects conscious human evaluation. Additionally, for the purpose of\nthis thesis, we analyze reliability templates independently, as evaluating their\ninterconnections lies beyond the scope of this work.\nFor our analysis, it is reasonable to prioritize templates related to relia-\nbility issues of referenced sources, since our approach is fundamentally based\non evaluating the connection of sources with these templates. Although the\nWikiProject Reliability offers a wide range of source-related templates,17we\nare interested in the commonly used templates to ensure a sufficient num-\nber of data points for our analysis. The more often a template is used, the\nmore unique external sources we can analyze and the more information we can\ngather about the trustworthiness of these sources.\nFor that reason, we narrowed down our selection to two common reliability\ntemplates concerning issues with the reliability of referenced sources. Spe-\ncifically, we focus on the templatesUnreliable sourcesandDubious. The\nUnreliable sourcestemplate is used to flag articles where some of the refer-\nenced sources are assumed to be of questionable reliability, recommending to\nlook for better and more reliable sources.18TheDubioustemplate is used to\nmark statements that are doubtful or questionable and require verification. It\nmay also be used to raise questions on the veracity, accuracy, or methodology\nemployed by a given source.19\n1.4 Outline of the Approach\nThe first step of our approach is to extract data from a Wikipedia dump,\nparsing revisions to identify the presence of theUnreliable sourcesand the\nDubioustemplates. Note that all steps are performed separately for each\ntemplate. We select revisions where a template was added and then search for\nthe first subsequent revision where that template is no longer present. This\nprocess results in revision pairs, that capture the states of articles when they\n17https://en.wikipedia.org/wiki/Wikipedia:Template_index/Cleanup/\nVerifiability_and_sources\n18https://en.wikipedia.org/wiki/Template:Unreliable_sources\n19https://en.wikipedia.org/wiki/Template:Dubious\n5\nCHAPTER 1. INTRODUCTION\nwere considered problematic and the following state when the reliability issue\nwas resolved. For both revisions in a pair, we extract the referenced external\nsources and store them for further analysis.\nIn the second step, we estimate the trustworthiness of referenced sources.\nWe analyze whether a source is more likely to be present during the addition or\nduring the removal of a reliability template. In practice, this is done by count-\ning how often each source is used in a revision where a template was added and\nhow often in a revision where a template was removed. We are left with two\ncounts for each source, which we use to compute the respective probabilities\nby dividing each count by the sum of both counts. These probabilities serve as\na means of trustworthiness, indicating how likely a source is to be associated\nwith the addition or removal of a reliability template. We say that a source is\nmore trustworthy if it is more likely to be present when a reliability template\nis removed, as this indicates that the source could be associated with an im-\nprovement in reliability. Conversely, we say that a source is less trustworthy\nif it is more likely to be present when a reliability template is added, as this\nindicates that the source could be associated with a reliability issue.\nIn the third step, we estimate the trustworthiness of Wikipedia articles.\nGiven a revision of an article, we look up the sources referenced in the revision\nin our trustworthiness estimates and for each source retrieve the probabilities\nof being present during the addition or the removal of a reliability template.\nNext, we average the probabilities of all referenced sources. We are left with\ntheprobabilityoftherevisionbeingassociatedwiththeadditionofareliability\ntemplate and the probability of the revision being associated with the removal\nof a reliability template. Similarly to the trustworthiness estimation of refer-\nenced sources, we say that a revision is more trustworthy if it is more likely to\nbe associated with the removal of a reliability template, and less trustworthy\nif it is more likely to be associated with the addition of a reliability template.\n6\nChapter 2\nRelated Work\nSubstantial research has been conducted on the automatic estimation of trust-\nworthiness in Wikipedia articles, with most approaches relying on a combina-\ntion of textual and user-related data. One example is the research done by\nMoturu and Liu [2009], who define trust as a combination of quality and cred-\nibility. Quality is derived from content-related metrics, such as the proportion\nof paragraphs with citations and the overall size of the article. Credibility on\nthe other hand is determined by user behavior, including editing patterns and\nthe article\u2019s development history.\nAnother example is the work of Adler et al. [2008], who propose a mecha-\nnism that assigns a value of trust to each word based on the word\u2019s survival\nratio and the reputation of the editor who contributed the word. The survival\nratio describes how many revisions of an article a word has endured without\nbeing altered or removed. Words that survive more revisions are seen as more\ntrustworthy. The reputation of the editor is based on the survival ratio of the\nwords they contribute. Editors who consistently add words that remain in an\narticle over time see their reputation increase, while those whose contributions\nare frequently edited or removed experience a decline in reputation.\nThe combination of survival ratio and editor reputation has been explored\nfurther by Suzuki and Yoshikawa [2012], who propose a method designed to\nbe more resistant to vandalism. Their approach evaluates text quality based\non both the survival ratio of words and the reputation of the editors. The\nreputation of an editor is calculated as the average quality of the text they\ncontribute. Initially, this creates a challenge since text quality and the editor\u2019s\nreputation are interdependent. To address this, the process begins with a\nfixedvalueforeditorreputation, whichisthenusedtocalculatetextquality. In\nsubsequent steps, the values for text quality and editor reputation are updated\niteratively until they converge.\nAn example of a more recent and innovative approach is the research of\n7\nCHAPTER 2. RELATED WORK\nWong et al. [2021]. Similarly to our work, their objective is to predict the pres-\nence of reliability templates. To achieve this, they compile a dataset consisting\nof articles where a selected template was added, paired with the corresponding\nrevisions where it was later removed. For each pair, they also extract var-\nious features, such as the number of words, images, citations, and external\nreferences. Using these features, they then train machine learning models to\npredict whether the template would be present or not. However, their results\nfall short of expectations, with the best-performing model achieving an accu-\nracy of 62%. They conclude that while the metadata features they used offer\nsome predictive value, the task is inherently challenging and demands further\nresearch.\n8\nChapter 3\nRevision Extraction\nBeforeestimatingthetrustworthinessofWikipediaarticlesandtheirreferenced\nsources, we need to obtain data to work with. In this chapter, we will describe\nthe necessary steps such as choosing reliability templates, parsing a Wikipedia\ndump, and extracting relevant revisions with their referenced sources.\n3.1 Selection of Reliability Templates\nWhen selecting reliability templates, the first consideration is the relation of\nthe template to referenced sources. Since our approach is based on associating\nexternal sources with additions and removals of reliability templates, we seek\ntemplatesthatarestronglyrelatedtothetrustworthinessofreferencedsources.\nWe assume that source-related templates have a stronger connection to the\nused references than other templates that question the structure or tonality of\nthe article. However, templates such asUnreferencedorCitation needed\nare not considered, as they only criticize the quantity of references and not\ndirectly the trustworthiness of the referenced sources.\nThe second consideration is how commonly the template is used. While\nthe WikiProject Reliability maintains a variety of templates related to the\nreliability of the referenced sources, for most of them we extracted only a few\nhundredrevisionpairs. Sometemplatesthatweexcludedduetoextractingless\nthan 200 revision pairs are listed in Table 3.1. We excluded these templates\nbecause we assume that our model benefits from larger amounts of data to\nlearn from to estimate the trustworthiness of new Wikipedia articles more\naccurately. The more often a template is used, the more revision pairs we\ncan extract. This in turn can allow us to estimate the trustworthiness of\na wider range of external sources, which is crucial for the reliability of our\nmodel. With each additional revision pair, we might not only identify new\nunique sources but also increase the number of data points used to estimate\n9\nCHAPTER 3. REVISION EXTRACTION\nTemplate Description Number of\nrevision pairs\nBetter\nsources\nneededUsed to flag articles that need better or\nmore reliable citations.a21\nCircular Used if a referenced source previously got\nits information from Wikipedia.b128\nIndependent\nsourcesUsed to flag articles that rely on sources\ntoo close to the subject, thus likely being\nbiased.c65\nNo reliable\nsourcesUsed to flag articles where all of the refer-\nenced sources are considered unreliable.d9\nUser-\ngeneratedUsed to flag articles where multiple refer-\nenced sources are user-generated content,\nmeaning the content was written and pub-\nlished by random members of the public.e86\nTable 3.1:Description of the excluded reliability templates.\nahttps://en.wikipedia.org/wiki/Template:Better_sources_needed\nbhttps://en.wikipedia.org/wiki/Template:Circular\nchttps://en.wikipedia.org/wiki/Template:Independent_sources\ndhttps://en.wikipedia.org/wiki/Template:No_reliable_sources\nehttps://en.wikipedia.org/wiki/Template:User-generated\nthe trustworthiness of already extracted external sources. If a source is not\nreferenced at all during the addition or removal of a reliability template, we\ncannot make any assumptions about the trustworthiness of the source, as the\narticle that references it might not have been checked for reliability issues by\neditors yet. We believe that any referenced source in a Wikipedia article for\nwhich we have no trustworthiness estimate increases the uncertainty of the\narticle\u2019s trustworthiness estimate. As a result, we aim to retrieve as many\nrevision pairs as possible to estimate the trustworthiness of as many external\nsources as possible.\nThe templates we chose to work with areUnreliable sourcesandDubi-\nous. Both are widely used with over 12,000 and 43,000 revision pairs respec-\ntively. TheUnreliable sourcestemplate is used to flag articles where some of\nthe referenced sources may be unreliable,1while theDubioustemplate is used\n1https://en.wikipedia.org/wiki/Template:Unreliable_sources\n10\nCHAPTER 3. REVISION EXTRACTION\nto mark statements or alleged facts that seem dubious despite being sourced.\nTheDubioustemplate may also be used to question the accuracy or method-\nology used by a given source.2Since both templates challenge the reliability\nof the referenced sources, we consider them suitable for our purposes.\n3.2 Parsing Wikipedia Dumps\nTo gain as much information on reliability templates and external sources as\npossible, we use the full revision history of Wikipedia articles. The Wikimedia\nFoundation, which is the organization behind Wikipedia, provides regular data\ndumps of the entire Wikipedia database.3A full dump contains all revisions\nof all pages, which expand to multiple terabytes of text when decompressed.\nThe full dump holds an extensive amount of data, including all templates\nand references that have ever been used in Wikipedia articles. Fortunately,\nthis data is provided in multistream XML format, which allows for efficient\nparallel processing.\nIn our case, we already have access to previously downloaded Wikipedia\ndumps on a computer cluster. Being the most recent, we work with the\nWikipedia dump from September 2022 in all our experiments. The bzip24\ncompressed XML data is dispersed among 770 multistream files which sum\nup to around 1.3 terabytes in size. To handle this amount of data, we use\nApache Spark, a distributed computing framework that enables efficient pro-\ncessing of large datasets.5We leverage Apache Spark\u2019s capabilities using Scala\nand multiple jobs that run on a Kubernetes cluster. Each job is a piece of\ncode, designed to perform a specific task, such as extracting revisions from a\nWikipedia dump or estimating the trustworthiness of an article\u2019s referenced\nexternal sources. This allows us to scale our processing power according to the\nsize of the data we are working with in each step.\nThe job responsible for parsing the Wikipedia dump is the first one we run.\nIt streams Wikipedia\u2019s XML data and decompresses it on the fly, filtering out\nall Wikipedia pages that are not main articles. Redirects are also ignored, as\ntheydonotcontainanyinformationthemselvesandautomaticallynavigatethe\nuser to another page. Next, we use both XML parsers and regular expressions\nto filter revisions for the presence of a selected reliability template. Further\ninformation on the template such as the date or an editor\u2019s note is ignored.\nMuch like Wong et al. [2021] has done, we extract revisions where a reliability\n2https://en.wikipedia.org/wiki/Template:Dubious\n3https://dumps.wikimedia.org\n4https://sourceware.org/bzip2/\n5https://spark.apache.org\n11\nCHAPTER 3. REVISION EXTRACTION\ntemplate was added and pair it with the first subsequent revision in which that\ntemplate is no longer present. This results in a balanced dataset, where for\neach revision in which a reliability issue was detected, we have a corresponding\nrevision in which the issue was resolved. The revisions are later saved as pairs,\nso we can easily compare the state during the addition of the template and the\nstate during the removal of the template. However, most of the content of a\nrevision is not of interest to us, since we are only concerned with the external\nsources that are referenced in the article. Therefore, for each revision in a\npair, we only store the ID for later identification and the referenced external\nsources.\nUsing pattern matching mechanisms, which look for reference tags and ci-\ntation templates we can find the majority of the referenced external sources of\nany Wikipedia revision. It does not suffice to simply scan a revision for URLs\nor ISBNs since we intend to avoid external links (see 1.2). Next, we clean the\nextracted sources by removing unnecessary characters such as extra braces or\ncommata, which are occasionally present, presumably due to syntax errors by\neditors. To further ensure that we can compare sources across revisions, we\nalso have to normalize URLs by only storing their registrable domains. Sim-\nply put, we save the highest-level domain that is controlled by a single entity.\nFor example, the registrable domain ofhttps://en.wikipedia.org/wikiis\nwikipedia.org. This is done by using the public suffix list, which is a collec-\ntion of all top-level domains and their subdomains.6Normalized URLs allow\nfor an evaluation on a broader scale instead of considering specific links. We\nassume that if a domain such as a news website is trustworthy, then all pub-\nlished articles from that domain are likely to be trustworthy as well, even if\nthey come from different authors or even different subdomains. For books,\nthis is much simpler, as we can just store the ISBN to identify books even\nwhen their citations are formatted differently. Lastly, all data is stored in Par-\nquet7files, which is a columnar storage format that is optimized for big data\nprocessing.\n3.3 Train and Test Sets\nTo analyze the effectiveness of our model, we reserve some data to compare\nthe trustworthiness estimates of unknown articles against ground truth values.\nThis requires splitting the dataset of revision pairs into training and test sets:\nwe estimate the trustworthiness of external sources using the larger training\nset and then make predictions for unseen revisions in the smaller test set.\n6https://publicsuffix.org\n7https://parquet.apache.org\n12\nCHAPTER 3. REVISION EXTRACTION\nTemplate Training Set Test Set\nUnreliable sources 9,942 2,485\nDubious 34,708 8,677\nTable 3.2:Number of revision pairs in the training and test sets for each reliability\ntemplate.\nFor each revision in the test set, we have known labels indicating whether a\nreliability template was added or removed. These labels are unknown to our\nmodel because they were not used for trustworthiness estimations of external\nsources. Our goal is to predict these labels based on the external sources\nreferenced by the revisions of the test set and to evaluate the effectiveness of\nour model by comparing the predictions to the actual labels.\nTo split our dataset, we use an 80/20 ratio, meaning 80% of the revision\npairs are used for training and 20% for testing. This provides a good balance\nbetween having sufficient data to train on while still having a substantial test\nset. Typically, the data that will be used for training and the data that will\nbe used for testing is selected randomly from the whole dataset. In our case,\nwe aim to replicate a more realistic scenario. To achieve this, we sort the\nrevision pairs in our dataset by the revision ID of the first revision in each pair\nand use the first 80% for training. Since revision IDs generally increase over\ntime, this approach allows us to model a real-world situation where the model\nis trained on past, already evaluated Wikipedia revisions and tested on more\nrecent, unevaluated revisions. To provide some information on the size of the\ndatasets we used, we list the number of revision pairs in the training and test\nsets for each template in Table 3.2.\n13\nChapter 4\nTrustworthiness Estimation of\nExternal Sources\nAfter extracting revision pairs for a given reliability template and determining\na test split, we estimate the trustworthiness of referenced sources. The goal\nof this step is to assess the likelihood that a source is associated with the\naddition or the removal of a reliability template. Note that similarly to the\nother steps of our approach, the estimation of source trustworthiness is done\nfor one reliability template at a time.\n4.1 Trustworthiness Estimation Process\nWe begin by loading all revision pairs from the training set and iterate over\nthe referenced external sources of both revisions in the pair. To explain which\nreferenced sources we consider in our analysis and how we estimate their trust-\nworthiness, we will use the following example. LetT Additionbe a revision where\na reliability template was added andT Removalthe first subsequent revision of\nthe same Wikipedia article where the template was removed. For each external\nsource, we determine whether it was referenced inT Addition,TRemoval, or both.\nWhile doing so, we can distinguish between the three scenariosS 1,S2, andS 3,\nas displayed in Figure 4.1.\nS1describes an external source referenced in the revision where the relia-\nbility template is added, but not in the revision where the template is later\nremoved. We say that this source is likely to be associated with theaddition\nof the reliability template and is by our definition of trustworthiness less trust-\nworthy.S 2describes an external source referenced in the revision where the\nreliability template is removed, but not in the revision where the template was\nadded. Wesaythatthissourceislikelytobeassociatedwiththeremovalofthe\nreliability template and is by our definition of trustworthiness more trustwor-\n14\nCHAPTER 4. TRUSTWORTHINESS ESTIMATION OF EXTERNAL\nSOURCES\nTime\nTAddition TRemovalS1\nS2\nS3\nFigure 4.1:Timeline illustrating the time spans for sourcesS 1,S2,S3relative to\nthe eventsT Addition andT Removal .\nthy.S 3describes an external source referenced in both the revision where the\nreliability template is added and the revision where the template is removed.\nSources such asS 3are ignored in our analysis. We assume thatS 3could not\nhave been responsible for the addition of the template, as it was still present\nwhen the template was removed. Similarly, we assume thatS 3could not have\nbeen responsible for the removal of the template, as it was already present\nwhen the template was added. Furthermore, our program cannot distinguish\nwhether a reliability template applies to the entire article, a specific section, or\neven a single statement. While one might assume that any referenced source\nthat survived the removal of a reliability template is not problematic, this re-\nmains speculative. In larger articles with numerous references, it is likely that\nsources referenced in sections unrelated to the template were not checked by\nthe editor who added or removed a reliability template. By considering only\nreferenced sources that were added to or removed from the article, we aim to\ncapture those sources that are most relevant to the template change, as we\nknow that they were actively considered by an editor.\nTo perform the trustworthiness estimation of referenced external sources\nacross all revision pairs, we count how often each source appears in scenarioS 1\nand how often each source appears in scenarioS 2. This results in two counts\nfor each source, for which we compute probabilities by dividing each count by\nthe sum of both counts. These probabilities indicate the likelihood of a source\nbeing associated with the addition or the removal of a reliability template.\nThe limitation of this approach is that we cannot distinguish whether a source\nwas referenced in many revisions but mostly stayed unaffected by a reliabil-\nity template or if a source was referenced rarely but often associated with a\nreliability template. In the first case, the source might have been commonly\nreferenced but played no crucial role in the addition or the removal of a relia-\nbility template. There is no trivial method of using this source for predicting\n15\nCHAPTER 4. TRUSTWORTHINESS ESTIMATION OF EXTERNAL\nSOURCES\nthe presence of reliability templates, as it seems to have no strong connection\nto the addition or the removal of a reliability template. In the second case,\nthe source might have been referenced rarely but mostly in connection with a\nreliability template. Here we should be cautious of noise such as vandalism,\nwrong evaluation by an editor, or simply because the reference to the source\nwas added or removed for reasons unrelated to the reliability template. This\nis because when relying on a few data points, the trustworthiness estimate of\na source is easily skewed by outliers and noise. We think that for a simplified\napproach like ours, the distinction between these two cases is not necessary.\nIn both cases, the source is not a reliable indicator for the addition or removal\nof a reliability template and should be treated with caution.\nLastly, we store the registrable domain or ISBN of each external source,\nalong with the computed probabilities of being associated with a template\naddition or a template removal and the source\u2019s number of occurrences. The\nnumber of occurrences describes how often a source was referenced during the\naddition (S 1) or the removal (S 2) of a reliability template but not in both (S 3).\nStoring the number of occurrences ensures that we can later filter out sources\nthat have rarely been associated with a template addition or removal, as they\nmight be less reliable indicators for the presence of a reliability template. It\nalso opens up the possibility of weighting sources by their occurrences when\nestimating the trustworthiness of Wikipedia articles, which we will discuss in\nChapter 5.\n4.2 Results\nFor theUnreliable sourcestemplate, we computed trustworthiness estimates\nfor 22,278 unique sources. To better understand our model\u2019s results, we manu-\nallyverifytheresultsofthetop10sources(byoccurrence), asseeninTable4.1.\nUnless specified otherwise, the manual evaluations are taken from Wikipedia\u2019s\nlist of frequently discussed sources by Wikipedia contributors [2025]. Here we\nobserve that all sources generally considered reliable have a higher probabil-\nity of being referenced during the removal of theUnreliable sourcestemplate.\nFurthermore, the sources generally considered unreliable have a higher prob-\nability of being referenced during the addition of theUnreliable sourcestem-\nplate. This is a strong indicator that our model is working as intended. For\na brief manual evaluation of the top sources, please refer to Table 4.2. Note\nthat in the presented tables,P(T Addition )andP(T Removal )are the probabilities\nof a source being present during the addition or the removal of a specified\ntemplate, respectively.\nLooking at the trustworthiness estimates of the same 10 sources using the\n16\nCHAPTER 4. TRUSTWORTHINESS ESTIMATION OF EXTERNAL\nSOURCES\nDubioustemplate, we see different results (see Table 4.3). Although all sources\nthataregenerallyconsideredreliablearemoreoftenassociatedwithatemplate\nremoval, the probabilities are not always as clear-cut as with theUnreliable\nsourcestemplate. For examplenytimes.comhas no strong association with\neither the addition or the removal of theDubioustemplate. Sources that are\ngenerally considered unreliable on the other hand are not always more likely\nto be present during template additions. Examples of this areyoutube.com,\nimdb.com, andwordpress.com, which are all present more often during the re-\nmoval of theDubioustemplate than its addition. This trend may be explained\nby the fact that while they are generally unreliable due to user-generated\ncontent, they may also host credible material in certain contexts. Authori-\ntative sources might publish high-quality content on YouTube or WordPress,\nand IMDb includes structured, verified data on film credits and production\ndetails.1The presence of these sources inDubioustemplate removals might\nsuggest that Wikipedia editors sometimes accept authoritative content from\nthese platforms to replace other questionable sources. Another explanation is\nthat theDubioustemplate itself leaves room for interpretation, as it may be\nused in a variety of cases concerning specific statements or alleged facts that\nare sourced but seem dubious. When an author misinterprets a source or uses\nit out of context, the source itself might not be questionable, but the template\nmay still be used.2\nThis highlights the complexity of estimating the trustworthiness of sources\nbased on their association with reliability templates. While theUnreliable\nsourcestemplate allows for a clearer distinction between reliable and unreli-\nable sources, the evaluation of sources becomes more nuanced when templates\nsuch asDubiousallow for a wider range of interpretations. This is an impor-\ntant aspect to consider when interpreting the results of our trustworthiness\nestimation, as the computed estimates should always be seen in the context of\nthe specific reliability template they were computed for.\n1https://help.imdb.com/article/imdb/general-information/\nwhere-does-the-information-on-imdb-come-from\n2https://en.wikipedia.org/wiki/Template:Dubious\n17\nCHAPTER 4. TRUSTWORTHINESS ESTIMATION OF EXTERNAL\nSOURCES\nSourceP(T Addition )P(T Removal )Occurrences\narchive.org 0.1515 0.8485 931\ngoogle.com 0.2627 0.7373 571\nyoutube.com 0.5674 0.4326 527\nimdb.com 0.6893 0.3107 338\nnytimes.com 0.2862 0.7138 269\nfacebook.com 0.6653 0.3347 245\ntwitter.com 0.6000 0.4000 215\nwikipedia.org 0.7011 0.2989 184\ntheguardian.com 0.1548 0.8452 168\nwordpress.com 0.5897 0.4103 156\nTable 4.1:Top 10 sources with their probabilities and occurrences for theUnreliable\nsourcestemplate\nSource Manual Evaluation\narchive.org Mostly reliable and factual. It hosts books, papers, and\nother documents without containing original thoughts.a\ngoogle.com DifficulttoevaluatedirectlyasGoogleitselfisnottypically\na direct source of information. Possible subdomains such\nas Google Books and Google Scholar are generally reliable.\nyoutube.com Generally unreliable. The videos are mostly anonymous,\nself-published, and unverifiable.\nimdb.com Unreliable due to user-generated content.\nnytimes.com Generally reliable.\nfacebook.com Generally unreliable due to self-published content.\ntwitter.com Generally unreliable due to self-published content.\nwikipedia.org Unreliable due to self-published content.\ntheguardian.com Generally reliable.\nwordpress.com Generally unreliable due to self-published content.\nTable 4.2:Manual evaluation of the top 10 sources for theUnreliable sources\ntemplate\nahttps://mediabiasfactcheck.com/internet-archive-bias/\n18\nCHAPTER 4. TRUSTWORTHINESS ESTIMATION OF EXTERNAL\nSOURCES\nSourceP(T Addition )P(T Removal )Occurrences\narchive.org 0.2124 0.7876 1803\ngoogle.com 0.3282 0.6718 1868\nyoutube.com 0.3898 0.6102 449\nimdb.com 0.4471 0.5529 208\nnytimes.com 0.4882 0.5118 805\nfacebook.com 0.5441 0.4559 68\ntwitter.com 0.5077 0.4923 65\nwikipedia.org 0.5885 0.4115 243\ntheguardian.com 0.1526 0.8474 308\nwordpress.com 0.4170 0.5830 223\nTable 4.3:10 selected sources with their probabilities and occurrences for the\nDubioustemplate\n19\nChapter 5\nTrustworthiness Estimation of\nArticles\nAfter estimating the trustworthiness of external sources, we estimate the trust-\nworthiness of Wikipedia articles. Specifically, we predict for any revision in\nour test set whether a template was added or removed. To make these pre-\ndictions, we use the previously computed probabilities of the external sources\nbeing associated with a reliability template addition or a reliability template\nremoval.\n5.1 Template Prediction Process\nAt first, we load all revision pairs of the test set and iterate through the\nreferenced external sources of each revision. For each source, we look up the\npreviously computed probabilities of being associated with a template addition\nor a template removal. If a source is not found, it has not been used in the\ntraining data. These sources are referred to as unknown sources, to which we\nassign default probabilities of 0.5 for being associated with a template addition\nor a template removal. The probabilities of 0.5 indicate that the source is\nneutral and does not provide any information on the addition or removal of a\nreliability template. Furthermore, we set the source\u2019s occurrences to zero. By\nstill including the source instead of ignoring it completely, we can decide in\nthe next step whether to include the source in our computations, allowing for\nexperimentation with different approaches.\nAfter loading the probabilities for all external sources referenced by a re-\nvision from a data file, we first combine the probabilities of the sources being\nassociated with a template addition and then combine the probabilities of the\nsources being associated with a template removal. We do this to estimate\nthe probability of the revision being associated with a template addition and\n20\nCHAPTER 5. TRUSTWORTHINESS ESTIMATION OF ARTICLES\nto estimate the probability of the revision being associated with a template\nremoval. The consolidation of multiple probabilities can be done in different\nways. We have chosen to use a weighted average, where the weight of a source\nis determined by its number of occurrences. The weight can be interpreted as\na measure of confidence that the estimated trustworthiness of a source repre-\nsents the true trustworthiness. In statistical analysis, the amount by which\nan estimate might deviate from the true value is called themargin of error.\nTherefore, the smaller the margin of error for a trustworthiness estimate, the\nmore confident we are that the estimate is accurate. The margin of error is\ndetermined by the number of data points, where more data points result in a\nsmaller margin of error. Based on this concept, we assume that the more often\na source occurs in our training data, the more accurate the trustworthiness es-\ntimate of the source is. If a source\u2019s trustworthiness estimate is based on only a\nfew data points, we say that it is more likely to be inaccurate and consequently\nshould have less influence on the trustworthiness estimate of the article that\nreferences the source. However, the relationship between the number of data\npoints and the margin of error is not linear but rather follows a curve. This\nis because the margin of error is inversely proportional to the square root of\nthe number of data points. As summarized by Hunter [2025], this means that\nthere is a point of diminishing returns, where the margin of error decreases\nonly slightly with each additional data point. In our case, when a source\nrarely occurs during template additions or template removals, the confidence\nin the trustworthiness estimate of that source increases significantly with each\nadditional occurrence. However, after we have reached a certain confidence\nin a source\u2019s trustworthiness estimate, the confidence further increases only\nslightly with each additional occurrence. To represent this relationship in our\nmodel, we use a function that starts steep and flattens out as the number of\noccurrences increases. Specifically, we use the function shown in Figure 5.1. It\ncan be seen that the curve limits the weight to a maximum of 100, meaning\nthat all weights are normalized to the range [0, 100]. A weight of 0 indicates\nthat we have no confidence in the trustworthiness estimate of the source, while\na weight of 100 indicates that we are very confident in the trustworthiness\nestimate of the source. Note that although both the weight limit and the\nsteepness of the function are tunable parameters, we use fixed values due to\ntime constraints.\nUsingtheweights, wecalculatethefinalprobabilitiesofatemplateaddition\nand a template removal for the whole revision by taking the weighted average\nof the probabilities of the referenced external sources. To do this, we first\nnormalize the weights by dividing each weight by the total weight sum. We\nthen multiply each probability by the corresponding normalized weight and\nsum up the results to calculate the probability of a template addition and\n21\nCHAPTER 5. TRUSTWORTHINESS ESTIMATION OF ARTICLES\nFigure 5.1:Plot of the weight function:weight= 100\u2212100\u2217e\u22120.05\u2217occurrences\nthe probability of a template removal. Note that the two probabilities are\ncomplementary, as they sum up to 1.0.\nOur test set contains only revisions that can be assigned to one of two\nclasses. The first class consists of revisions where a reliability template was\nadded, while the second class contains revisions where a reliability template\nwas removed. The distinction between the two classes is signaled by the label\nTAddedwhich is set to 1.0 if a template was added and 0.0 if not, and the label\nTRemovedwhich is set to 1.0 if a template was removed and 0.0 if not. To predict\nif a template was added in a revision, we compare the computed probability\nof a template addition with a threshold. If the probability is greater than the\nthreshold, we predict theT Addedlabel to be 1.0, otherwise we predict it to be\n0.0. The same procedure is applied for theT Removedlabel, where we predict if\na template was removed in a revision. Lastly, we evaluate the effectiveness of\nour predictions by comparing the predicted labels with the actual labels.\n5.2 Results\nTo present the effectiveness of our model, we useReceiver Operating Charac-\nteristic(ROC) curves. A ROC curve is a graphical representation of the true\npositive rate (TPR) against the false positive rate (FPR) at various threshold\nsettings. The area under the curve (AUC) is a measure of how well the model\ncan distinguish between classes. An AUC of 1.0 indicates a perfect model,\nwhile an AUC of 0.5 indicates a model that is no better than random guessing.\n22\nCHAPTER 5. TRUSTWORTHINESS ESTIMATION OF ARTICLES\nFigure 5.2:ROC curves for the predic-\ntion of anUnreliable sourcestemplate ad-\ndition\nFigure 5.3:ROC curves for the predic-\ntion of anUnreliable sourcestemplate re-\nmoval\nIn the graphs we present, the random classifier is represented by the dotted\ndiagonal line. The closer the ROC curve is to the upper left corner, the better\nthe model can distinguish between the two classes.\n5.2.1 Template: Unreliable Sources\nFigure 5.2 shows the ROC curves for the prediction of whether anUnreliable\nsourcestemplate was added in a revision. The solid curve describes our stan-\ndard model, where unknown sources are assigned default probabilities of 0.5.\nThis approach performs only marginally better than random guessing (diago-\nnal dotted line), which is also reflected in the AUC of 0.57. We can see that for\nmost thresholds, the number of correctly identified revisions where a template\nwas added is only slightly higher than the number of falsely identified revisions\nwhere a template was not added.\nFigure5.3showstheROCcurvesforthepredictionofwhetheranUnreliable\nsourcestemplate was removed in a revision. Here too, the solid curve describes\nour standard model, where unknown sources are assigned default probabilities\nof 0.5. Note that the solid curve is flipped along the diagonal, as we are\nnow predicting the removal of a template instead of the addition. This is\nexpected, as our test set only contains two classes where for each revision,\nthe binary labelsT AddedandT Removedare always the opposite of each other.\nFurthermore, the probabilities of a template addition and a template removal\nare complementary, as they sum up to 1.0. Consequently, we can observe that\nthe number of correctly identified revisions where a template was removed is\nonly slightly higher than the number of falsely identified revisions where a\ntemplate was not removed.\n23\nCHAPTER 5. TRUSTWORTHINESS ESTIMATION OF ARTICLES\nOne possible reason for the poor performance of our model is that the\nmajority of revisions in our test set have unknown sources. This means that\nthe model has to rely on default probabilities for a large amount of the refer-\nenced sources because we have no computed trustworthiness estimate for these\nsources based on the training data. To examine this further, we conduct a sec-\nond experiment: We predict labels only for a subset of the test set, where we\nhave computed a trustworthiness estimate for all referenced external sources.\nThe ROC curves for this subset are also shown in Figure 5.2 and Figure 5.3 as\ndashed lines. We observe that the model performs significantly better when\nall sources have a trustworthiness estimate: the curve reaching higher into the\nupper left corner in Figure 5.2 indicates that the model correctly identified\nmore revisions where a template was added as in the first experiment. In\nFigure 5.3, the curve reaching higher into the upper left corner indicates that\nthe model correctly identified more revisions where a template was removed\nas in the first experiment. This is also reflected in the AUC of 0.74, which is a\nsignificant improvement over the AUC of 0.57 when using default probabilities\nfor unknown sources. However, we should mention that of the original 4,970\nrevisions in the test set, only 357 revisions were used in this experiment, as\nthey were the only ones where we had computed trustworthiness estimates for\nall sources.\n5.2.2 Template: Dubious\nFigure 5.4 shows the ROC curves for the prediction of whether aDubious\ntemplate was added in a revision. Figure 5.5 shows the ROC curves for the\nprediction of whether aDubioustemplate was removed in a revision. Similarly\nto theUnreliable sourcestemplate, the ROC curve of the standard model\nfor the removal of theDubioustemplate is flipped along the diagonal when\ncompared to the ROC for the addition of theDubioustemplate. The AUC in\nboth cases of the standard model is 0.52, which is about as good as random\nguessing. This indicates that our standard model cannot distinguish between\nrevisions where aDubioustemplate was added or removed.\nAs with theUnreliable sourcestemplate, we also conduct a second experi-\nment where we predict labels only for a subset of the test set, where we have\ncomputed a trustworthiness estimate for all referenced external sources. This\ntime, only 702 of the original 17,354 revisions in the test set remained. The\nROC curves for this subset are shown in Figure 5.4 and Figure 5.5 respectively,\nand are portrayed as dotted lines. We observe that for most thresholds, the\nmodel performs significantly better when all sources have a trustworthiness\nestimate. However, for some thresholds, the model performs worse than a ran-\ndom classifier. We believe that this is due to the small number of revisions\n24\nCHAPTER 5. TRUSTWORTHINESS ESTIMATION OF ARTICLES\nFigure 5.4:ROC curves for the predic-\ntion of aDubioustemplate addition\nFigure 5.5:ROC curves for the predic-\ntion of aDubioustemplate removal\nused in the test set of this experiment, which makes the results less reliable and\nmore prone to random fluctuations and outliers. The AUC of 0.60 indicates\nthat the model has a slight advantage over random guessing, but is still not\nable to reliably distinguish between revisions where aDubioustemplate was\nadded or removed.\n25\nChapter 6\nDiscussion\nIn this chapter, we discuss the results of our experiments and analyze the\nlimitations of our approach. We explore potential solutions to the identified\nissues and discuss possible real-world applications of our model.\n6.1 Error Analysis\nError analysis is crucial, particularly when results are not as expected, as it\nhelps to identify the limitations of the current approach and guides future im-\nprovements. This section explores the factors contributing to the shortcomings\nof our approach and discusses potential solutions.\n6.1.1 Model Constraints\nIn our current model, we only consider the template-added and template-\nremoved states of a Wikipedia article. This means that to estimate the trust-\nworthiness of external sources, our model solely relies on the data that is\nretrieved when a template is added and later removed. Therefore, any ad-\nditions or removals of referenced sources before a template was added and\nafter a template was removed are ignored. While this allows for a much sim-\npler approach, it brings significant limitations. To analyze the impact of this\nconstraint, we will consider the scenarios shown in Figure 6.1. It models a\ntimeline of a Wikipedia article with the event of a template additionT Addition\nand the eventT Removalof the removal of that template. On the timeline there\nare three sourcesS 4,S5, andS 6which are referenced in the article at different\ntimes.S 4is referenced before the template is added,S 5is referenced while the\ntemplate is present andS 6is referenced after the template is removed. Note\nthat none of these sources are being referenced at the time of the template\u2019s\n26\nCHAPTER 6. DISCUSSION\nTime\nTAddition TRemovalS4 S5 S6\nFigure 6.1:Timeline illustrating the time spans for sourcesS 4,S5,S6relative to\nthe eventsT Addition andT Removal .\naddition or removal, which is why they are not included in our source extrac-\ntion process and are not considered in our model. While we assume that it\nis safe to ignoreS 4as there is no indication that it was the reason for the\ntemplate addition,S 5andS 6might contain valuable information that we miss\nout on.S 5describes any source, which is referenced while the template was\npresent but removed before the template was removed. This could be a source\nthat was tested for reliability but was found not reliable enough to keep in\nthe article. One could argue that this source should be considered when esti-\nmating the trustworthiness of external sources, as it could be marked to be of\nquestionable reliability. An even more critical scenario however is the addition\nof a source afterT Removal, such asS 6. An editor might remove complete sec-\ntions of a Wikipedia article, prioritizing displaying less information over the\nrisk of spreading unreliable or misleading content. In this case, the template\nis removed and the article is later rebuilt using more reliable sources, which\nwe do not capture in our approach. Although it is uncertain how often this\nscenario occurs, we are missing out on valuable data that could improve the\ntrustworthiness estimates of external sources.\nTo solve the mentioned limitations, one would need to consider a model\nthat takes more than just the template-added and template-removed states\ninto account. This would include scanning revisions in between the template\naddition and the template removal, as well as scanning consequent revisions\nafter the template was removed. Additional data could be retrieved, such as\nexternal sources that were tested while the template was present or possibly\nreliablesourcesthatwereaddedimmediatelyafterthetemplatewasremovedto\nrebuildthearticle. Thisapproachcouldpavethewayforamorecomprehensive\ntrustworthiness estimation of external sources.\n6.1.2 Complexities in Template Identification\nAnother factor contributing to the model\u2019s limitations is the complexity of\nidentifying templates. One reason why we only consider pairs of revisions\nwhere a reliability template was added and later removed is that we do not\n27\nCHAPTER 6. DISCUSSION\nknow which parts of a Wikipedia article are challenged by a reliability tem-\nplate. When we filter for the presence of reliability templates, we assume that\nthe templates refer to the entire article and we therefore extract all referenced\nexternal sources from the article. By then comparing the referenced sources of\nthe article when the template was added and when the template was removed,\nwe identify only the external sources that were changed, with the assumption\nthat these sources are generally related to the template\u2019s addition or removal.\nIn reality, however, this is not the case. TheUnreliable sourcestemplate for\nexample is used to mark entire articles, but the template directly states that\nonlysomeof the referenced sources in the article might be unreliable. The\nDubioustemplate on the other hand is not used to mark an entire article as\nunreliable, but rather to highlight specific sections that need improvement. In\nboth cases, any references that are added to or removed from sections that\nare not questioned by the template are falsely brought into association with\nthe template. Specifically, references that are removed from sections unrelated\nto the template before the template\u2019s removal are falsely marked as unreli-\nable, while references added to sections unrelated to the template after the\ntemplate\u2019s addition are falsely marked as reliable. These issues are not only\npresent when considering theUnreliable sourcesandDubioustemplates, but\nalso for many other reliability templates. Our original assumption was that\nthis noise would be negligible when using a large enough dataset, but we have\nfound that in reality, our data is very susceptible to noise, as shown in Sec-\ntion 6.1.3.\nIdentifying the exact parts of an article that are challenged by a reliability\ntemplateisnotatrivialtask. Thisisbecausetemplatesarenotalwaysusedina\nconsistentmanner, whichmakesitdifficulttoaccuratelyidentifythecontextof\na template using automatic methods. TheDubioustemplate for example uses\nareasonparameter, such as{{Dubious|reason=What the problem is}}to\nspecify the reason why the information is considered dubious.1This allows\neditors to mark specific parts of the article that are considered dubious, rather\nthan the entire article. For our automatic template filter mechanism, this is\nan issue, as it is a complex task to understand the context of thereason\nparameter and to identify the references connected to it. Furthermore, for\nsome templates, the context is not necessarily specified as a comment, but\nrather through another version of the template which focuses on a specific\nsection. For example, instead of theUnreliable sourcestemplate which refers\nto entire articles, theUnreliable sources sectiontemplate marks specific sec-\ntions that contain possibly unreliable sources.2In our experiments filtering for\nsection-specific templates, we encountered difficulties in accurately extracting\n1https://en.wikipedia.org/wiki/Template:Dubious\n2https://en.wikipedia.org/wiki/Template:Unreliable_sources_section\n28\nCHAPTER 6. DISCUSSION\nthe referenced external sources related to the template. We assume this is\nbecause section-specific templates and headlines are not always used consis-\ntently. Even slight variations in formatting or placement can strongly increase\nthe complexity of our pattern-matching process.\nToimprovetheeffectivenessofourmodel, onewouldneedtodevelopamore\nsophisticated approach to identify templates in revisions. This could involve\na more complex template filter mechanism that can identify the context of a\ntemplate,suchasthereasonparameteroftheDubioustemplate. Additionally,\nthe filter mechanism would need to identify section-specific templates. Using\nthese templates, one could also consider revisions where a template was added\nbut not yet removed, to identify specific references that the template was\nadded for, instead of considering all references of an article. Furthermore, we\nassume that it would be possible to reduce noise in the extracted referenced\nsources by only considering the references marked by the template. We think\nthat this could allow for extracting more accurate information on referenced\nsources and their association with reliability templates, which could improve\nthe trustworthiness estimates of external sources.\n6.1.3 Data Deficiencies\nThe data used for training and evaluating our model is a crucial factor that\ninfluences the model\u2019s effectiveness. In our datasets, we have identified sig-\nnificant deficiencies that help explain the issues with our model\u2019s ability to\naccurately predict whether a template was added or removed in a revision.\nOne of the main deficiencies is that despite having estimated the trustwor-\nthiness of thousands of external domains and ISBNs, the majority of revisions\nin the test set contain references to unknown sources, by which we refer to\nsources that did not occur in the training data and consequently have no com-\nputed trustworthiness estimate. Specifically, around 87.75% of the revisions\nin theUnreliable sourcestest set contain a reference to at least one unknown\nsource. Among these 87.75% of revisions, on average 49% of the referenced ex-\nternal sources are unknown. For theDubioustest set, 92.28% of the revisions\nreference at least one unknown source, with on average 44% of the referenced\nexternal sources being unknown. Essentially, this means that a large portion\nof the data the model relies on for predictions are simply default probabilities\nof 0.5 for unknown sources being associated with a template addition or a tem-\nplate removal. We assume that because so much of the data lacks predictive\nvalue, the model\u2019s predictions are largely random, resulting in performance\nonly marginally better than a random classifier. We have outlined the impact\nof this limitation when evaluating the model\u2019s effectiveness in Section 5.2.\nThe second important deficiency is that the trustworthiness estimate for\n29\nCHAPTER 6. DISCUSSION\nFigure 6.2:Cumulative distribution function of occurrences (number of revision\npairs used to compute the trustworthiness estimate) of external sources for theUn-\nreliable sourcestemplate.\nthe majority of sources is based on very few data points. When storing the\ntrustworthiness estimates of external sources, we also store the number of\nrevision pairs that were used to compute the trustworthiness estimate for each\nsource, also referred to as thesource occurences. In Figure 6.2 we plot the\ncumulative distribution function of the occurrences of the external sources\nthat were extracted from revision pairs where theUnreliable sourcestemplate\nwas added and later removed. We can see that almost 80% of the external\nsources have a trustworthiness estimate based on one revision pair and around\n92.5% of the referenced external sources have a trustworthiness estimate based\non a maximum of 3 revision pairs. This is a significant concern because it\nindicates that the trustworthiness estimates of the majority of the sources are\nbased on very little data. Noise which is introduced by issues with template\nidentification, vandalism, or simply wrongful template usage, can therefore\nhave a significant impact on the trustworthiness estimates of most sources.\nWe assume that this is a major factor contributing to the model\u2019s inability\nto accurately predict whether a template was added or removed in a revision,\neven when we only made predictions for revisions containing known sources.\nOverall, we conclude that the majority of the trustworthiness estimates\nfor the external sources are based on too little data to be reliable. Further-\nmore, the high number of unknown sources in the test set exaggerates this\nissue, as the model cannot make accurate predictions using these sources. To\ncombat these issues, one would need a more extensive dataset to increase the\n30\nCHAPTER 6. DISCUSSION\nnumber of known sources and the number of revisions used to compute the\ntrustworthiness estimates of the external sources. This could be done using\na more sophisticated model that can combine the data from multiple reliabil-\nity templates to extract a higher number of revision pairs. Alternatively, one\ncould consider a model that can compute trustworthiness estimates of external\nsources with improved accuracy for a single template. This could be achieved\nby reducing noise due to the usage of section-specific templates, as discussed\nin Section 6.1.2.\n6.2 Possible Real World Applications\nWe believe that our model has the potential to be used in real-world appli-\ncations. Specifically the trustworthiness estimation of external sources using\ntheUnreliable sourcestemplate showed promising results (see Section 4.2).\nCombined with the computationally efficient nature of our model, we believe\nthat it could be used in a variety of applications. The only computationally\nheavy workload is parsing Wikipedia data and filtering revisions containing\nspecific templates. While this task requires significant storage and paralleliza-\ntion capabilities, it suffices to do this occasionally. It is unnecessary to parse\nthe entire Wikipedia database each time a trustworthiness estimate for an ex-\nternal source is needed. We assume that this task could be done on a monthly\nor even yearly basis, to ensure that a significant number of new revisions con-\ntaining reliability templates can be extracted. Once all revision pairs for a\nreliability template have been extracted, all subsequent steps are principally\nlightweight and can be performed on standard hardware. Additionally, apart\nfrom the original Wikipedia dump, our model is very memory-friendly. After\nthe trustworthiness estimation process for external sources is complete, the\nresulting CSV file containing information on external sources is only a few\nmegabytes in size.\nThis opens up many possibilities for real-world applications. To provide a\nspecificexample, ourmodelcouldbeusedinsideabrowserextensionthatscans\nany open Wikipedia article for its referenced external sources and looks up\ntheir previously computed trustworthiness estimates. The browser extension\ncould then highlight sections of the text that are supported by sources that\nwere strongly associated with additions or removals of theUnreliable sources\ntemplate in the past. This would allow users to quickly assess whether they\ncan rely on the information provided in the article or whether they should be\ncautious.\nWhile the idea of such a browser extension is appealing, it is important to\nnote that the model\u2019s performance is not yet sufficient for such an application.\n31\nCHAPTER 6. DISCUSSION\nAs discussed in Section 6.1.3, the trustworthiness estimates of most external\nsources are based on too little data to be reliable. If the model were to be\nused in a real-world application, it would be crucial to reliably estimate the\ntrustworthiness of a wide range of external sources. Furthermore, identifying\nthe exact parts of an article that are supported by an external source is not\na trivial task, as discussed in Section 6.1.2. If the browser extension were to\nhighlight sections of the text that are supported by particularly reliable or\nparticularly unreliable external sources, it would need to accurately identify\nthe statements backed by these sources.\n6.3 Conclusion and Future Work\nIn our approach, we first created a dataset of Wikipedia revision pairs contain-\ning revisions where a reliability template was added and the first subsequent\nrevision where that template was removed. We then extracted the referenced\nexternal sources from these revisions and computed trustworthiness estimates\nfor each source. This was done by computing probabilities of the sources being\nassociated with the addition or the removal of a reliability template. While\nwe could manually verify the trustworthiness estimates of a small subset of\nsources for theUnreliable sourcestemplate, the manual evaluation of the same\nsources for theDubioustemplate required speculative reasoning, highlighting\nthat the trustworthiness estimates for templates with a wider range of use\ncases are more difficult to interpret manually. Future work could involve the\nanalysis of the trustworthiness estimates of external sources for a wider range\nof reliability templates, to better understand the potential of our model.\nUsing the computed trustworthiness estimates of the external sources, we\nthen predicted whether a reliability template was added or removed for a\nselected test set of revisions. We found that the model\u2019s performance was only\nmarginally better than a random classifier, which we mostly attribute to the\nhigh number of unknown sources in the test set and the low number of data\npoints used to compute the trustworthiness estimates of the majority of the\nexternalsources. Futureworkcouldinvolvethedevelopmentofamorecomplex\nmodel, combining the data from multiple reliability templates to increase the\nnumber of known sources and data points for each source. Alternatively, we\npropose that to compute more accurate trustworthiness estimates of external\nsources for a single template, one could try to reduce noise in the model by\nusing section-specific templates to identify the references challenged by the\nreliability template more accurately.\n32\nBibliography\nB. Thomas Adler, Krishnendu Chatterjee, Luca de Alfaro, Marco Faella, Ian\nPye, and Vishwanath Raman. Assigning trust to wikipedia content. In\nProceedings of the 4th International Symposium on Wikis, WikiSym \u201908,\nNew York, NY, USA, 2008. Association for Computing Machinery. ISBN\n9781605581286. doi: 10.1145/1822258.1822293. URLhttps://doi.org/\n10.1145/1822258.1822293.\nSamy A. Azer. Evaluation of gastroenterology and hepatology articles on\nwikipedia: Are they suitable as learning resources for medical students?\nEuropean Journal of Gastroenterology & Hepatology, 26(2):155\u2013163, Febru-\nary 2014. doi: 10.1097/MEG.0000000000000003.\nPamela Hunter. Margin of error and confidence levels made simple.Retrieved\nJanuary 21st, 2025.\nJona Kr\u00e4enbring, Tika Monzon Penza, Joanna Gutmann, Susanne Muehlich,\nOliver Zolk, Leszek Wojnowski, Renke Maas, Stefan Engelhardt, and Anto-\nnio Sarikas. Accuracy and completeness of drug information in wikipedia:\na comparison with standard textbooks of pharmacology.PLOS ONE, 9(9):\ne106930, 2014. doi: 10.1371/journal.pone.0106930. URLhttps://doi.org/\n10.1371/journal.pone.0106930. Epub 2014 Sep 24.\nFlorian Lemmerich, Diego S\u00e1ez-Trumper, Robert West, and Leila Zia. Why\nthe world reads wikipedia: Beyond english speakers, 2018. URLhttps:\n//arxiv.org/abs/1812.00474.\nSai T. Moturu and Huan Liu. Evaluating the trustworthiness of wikipedia ar-\nticles through quality and credibility. InProceedings of the 5th International\nSymposium on Wikis and Open Collaboration, WikiSym \u201909, New York, NY,\nUSA, 2009. Association for Computing Machinery. ISBN 9781605587301.\ndoi: 10.1145/1641309.1641349. URLhttps://doi.org/10.1145/1641309.\n1641349.\n33\nBIBLIOGRAPHY\nJennifer Phillips, Connie Lam, and Lisa Palmisano. Analysis of the accuracy\nand readability of herbal supplement information on wikipedia.Journal of\nthe American Pharmacists Association, 54(4):406\u2013414, July\u2013August 2014.\ndoi: 10.1331/JAPhA.2014.13181.\nYuSuzukiandMasatoshiYoshikawa. Mutualevaluationofeditorsandtextsfor\nassessing quality of wikipedia articles. InProceedings of the Eighth Annual\nInternational Symposium on Wikis and Open Collaboration, WikiSym \u201912,\nNew York, NY, USA, 2012. Association for Computing Machinery. ISBN\n9781450316057. doi: 10.1145/2462932.2462956. URLhttps://doi.org/\n10.1145/2462932.2462956.\nLeaViljanen. Towardsanontologyoftrust. InSokratisKatsikas, JavierL\u00f3pez,\nand G\u00fcnther Pernul, editors,Trust, Privacy, and Security in Digital Busi-\nness, pages 175\u2013184, Berlin, Heidelberg, 2005. Springer Berlin Heidelberg.\nISBN 978-3-540-31796-8.\nWikipedia contributors. Wikipedia:reliable sources/perennial sources,\n2025. URLhttps://en.wikipedia.org/w/index.php?title=Wikipedia:\nReliable_sources/Perennial_sources&oldid=1275862533. [Online; ac-\ncessed 19-February-2025].\nKayYen Wong, Miriam Redi, and Diego Saez-Trumper. Wiki-reliability: A\nlarge scale dataset for content reliability on wikipedia. InProceedings of\nthe 44th International ACM SIGIR Conference on Research and Devel-\nopment in Information Retrieval, SIGIR \u201921, page 2437\u20132442. ACM, July\n2021. doi: 10.1145/3404835.3463253. URLhttp://dx.doi.org/10.1145/\n3404835.3463253.\n34\nDeclaration\nUnless otherwise indicated in the text or references, this thesis is\nentirely the product of my own scholarly work.\nJena, February 21, 2025\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nLuca-Philipp Grumbach", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Automatically Estimating the Trustworthiness of Wikipedia", "author": ["LP Grumbach"], "venue": "NA", "pub_year": "NA", "abstract": "Wikipedia has emerged as one of the most used sources of information on the internet, with  millions of articles spanning a wide range of topics. Its collaborative nature, where content is"}, "filled": false, "gsrank": 490, "pub_url": "https://downloads.webis.de/theses/papers/grumbach_2025.pdf", "author_id": [""], "url_scholarbib": "/scholar?hl=en&q=info:W72wJmctBnoJ:scholar.google.com/&output=cite&scirp=489&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D480%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=W72wJmctBnoJ&ei=XbWsaM-eLcDZieoPqdqh8QU&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:W72wJmctBnoJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://downloads.webis.de/theses/papers/grumbach_2025.pdf"}}, {"title": "The emergence and public perception of misinformation: a study on fake news", "year": "2024", "pdf_data": " \n \n \nDejan Leskovar  \nThe Emergence and Public Perception of \nMisinformation: A Study on Fake News  \nMaster's thesis  \nPojav in javno dojemanje dezinformacij: \n\u0161tudija la\u017enih novic  \nMagistrsko delo  \n \nMaribor,  december  2024 \n\n \n \n \nDejan Leskovar  \nThe Emergence and Public Perception of \nMisinformation: A Study on Fake News  \nMaster's thesis  \nPojav in javno dojemanje dezinformacij: \n\u0161tudija la\u017enih novic  \nMagistrsko delo  \n \nMaribor, december  2024  \n\n \nThe Emergence and Public Perception of \nMisinformation: A Study on Fake News  \nMaster's thesis  \nPojav in javno dojemanje dezinformacij: \n\u0161tudija la\u017enih novic  \nMagistrsko delo  \n \n \n \n\u0160tudent:   Dejan Leskovar  \n\u0160tudijski program:  Magistrski \u0161tudijski program  \nSmer:    Pou\u010devanje angle\u0161\u010dine  \nMentor:   doc. dr. Janko Trupej  \nSomentor:   doc. dr. Katja Plemenita\u0161  \nLektor ici:                       Mojca P. Vaupoti\u010d, lekt. za slov. jezik; dr. Alenka \u010cu\u0161, univ. dipl. slov.\n   \nI \n  \n \n \n \nAcknowledgements  \n \nI would like to express my sincere gratitude  to my mentors, doc. dr. Janko Trupej and doc. \ndr. Katja Plemenita\u0161, for enabling me to research such an interesting topic.  \nI would also like to thank my parents for supporting me through all the ups and downs.  \nThanks also go to everyone who completed and shared my survey.  \n \n \n \nZahvala  \n \nNajve\u010dja zahvala gre mentor jema  doc. dr. Janku Trupeju in doc. dr. Katji Plemenita\u0161, da sta \nmi omogo\u010dila raziskovati tako zanimivo temo.  \nZahvalil bi se rad star\u0161ema, da sta me podpirala ob vseh vzponih in padcih.  \nZahvala gre tudi vsem , ki so re\u0161ili in delili mojo anketo.  \nII \n Pojav in javno dojemanje dezinformacij: \u0161tudija la\u017enih \nnovic  \n \nKlju\u010dne besede: la\u017ene novice, \u0161irjenje , identifikacija , mal -informacija  \nUDK:   \nPovzetek:  \nMagistrsko delo obravnava temo la\u017enih novic. La\u017ene novice so izraz , ki ga dandanes sli\u0161imo \npovsod. Zamisel o oblikovanju la\u017enih informacij sam a po sebi ni nov a, podrobno pr ou\u010devanje \nteh novic pa je dokaj nova tema, ki je za\u010dela  svojo  prepoznavnost dobivati z volitvami v \nZdru\u017eenih dr\u017eavah Amerike leta 2016.  \nV literaturi lahko zasledimo ve\u010d razli\u010dnih definicij la\u017enih novic. Te d efinicije se v ve\u010dini \nujemajo v tem, da  la\u017ene novice opredeljujejo kot popolnoma izmi\u0161ljene  ali prilagojene \ninformacije z namenom \u0161irjenja politi\u010dne agende ali denarnega dobi\u010dka. Poleg  la\u017enih novic \nobstajajo tudi  oblike , ki so la\u017enim novicam sorodne, kot so recimo parodije in satire, \nizmi\u0161ljene informacije , ki nimajo slabih namenov, saj je njihov namen zabava nje bralc ev. \nGlede na ve\u010dino definicij pod la\u017ene novice ne spadajo novice , pri katerih gre za napako  brez \nneposrednega namena zavajanja ali dobi\u010dka.  \nK temi la\u017enih novic spada tudi  manipulacija vsebine, ki zajema prilagajanje statisti\u010dnih \npodatkov ter urejanje fotografij in videoposnetkov. V zadnjih nekaj letih je zaradi napredka \ntehnologije manipulacija vsebine postala la\u017eja, s tehnologijo pa je mogo\u010de posneti tudi la\u017ene \ngovore javnih oseb. S tem namenom se je pove\u010dala raba umetne inteligence in vloga te pri \nkreaciji la\u017enih novic. Umetna inteligenca se lahko uporabi za kreacijo slik, ki zavajajo publiko \nglede nekega dogodka. Na osnovi posnetkov govora n eke osebe lahko ustvarimo posnetke, \nna katerih ta oseba govori, karkoli ho\u010demo. S pomo\u010djo \u00bbdeepfake\u00ab tehnologije lahko \nmanipuliramo posnetke slavnih oseb, da govorijo in po\u010dnejo, kar ho\u010demo.  \nDru\u017ebeni mediji so  prav tako  tesno povezani z obstojem la\u017enih novic, saj so gl avna lokacija  \nnjihovega deljenja in \u0161irjenja. Med te\u017eave dru\u017ebenih medijev spadajo ljudje , ki se pretvarjajo , \nda so druga oseba s slabimi namerami in t . i. \u00bbecho chambers\u00ab oziroma zaprte skupine ljudi \nz enakimi interesi, ki sprejemajo in delijo zgolj  novice , ki so  skladne z njihovimi  mnenji . Zelo \nvelika te\u017eava na dru\u017ebenih medijih, predvsem na Twitter -ju/X-u, so t. i. \u00bbboti\u00ab  \u2013 programi , \n\nIII \n ki posnemajo ljudi na dru\u017ebenih omre\u017ejih in delijo povezave, pogosto  kot la\u017ene novice. \nNekatere \u0161tudije (Shao, et al., 2018)  navajajo , da so ti roboti krivi za ve\u010dino deljenih la\u017enih \nnovic, medtem ko druge (Vosoughi, Roy, & Aral, 2018)  ka\u017eejo,  da so ljudje in roboti skoraj \nenako krivi za \u0161irjenje la\u017enih novic . Ljudje  imajo  celo ve\u010dji dele\u017e pri deljen ju novic, ki so la\u017ene, \nsaj imajo la\u017ene novice ve\u010dji \u010dustveni vpliv na \u010dloveka, medtem ko roboti enako pogosto delijo \nla\u017ene in resni\u010dne novice.  \nDosedanje raziskave se ukvarjajo z razli\u010dnimi vidiki  la\u017enih novic. Prva od teh se je ukvarjala \nz mnenjem odraslih prebivalcev  v Zdru\u017eenih dr\u017eav ah. Raziskava o mnenju odraslih \nprebivalcev ZDA o la\u017enih novicah  (Mitchell, Gottfried, Walker, Fedeli, & Stocking, 2019)  \nugotavlja , da polovica vseh vpra\u0161anih la\u017ene novice pojmuje  kot eno najve\u010djih od te\u017eav \ndr\u017eave, ki negativno vpliva na zaupanje v vlado in ostale prebivalce. Vpra\u0161ani so za najve\u010dji \nvir la\u017enih novic pojmovali  politike, bili pa so mnenja , da so za re\u0161itev te\u017eav la\u017enih novic \nnajbolj odgovorni mediji. Ve\u010dina vpra\u0161anih je izjavila , da se sre\u010dujejo z la\u017enimi novicami, pri \n\u010demer sta  glavna razloga  \u0161irjenje agend in dobi\u010dek. Polovica vpra\u0161anih je priznala , da so \u017ee \ndelili la\u017ene novice, vendar je le desetina vedela , da so novice la\u017ene , preden jih je delila, \nmedtem ko so ostali to izvedeli pozneje. Nekatere \u0161tudije se ukvarja jo s \u0161irjenjem la\u017enih novic \nna spletu. Pri eni  od nji h (Allcott, Gentzkow, & Yu, 2018)  so ugotovil i, da je deljenje la\u017enih \nnovic na Facebooku upadlo , medtem ko na Twitterju nara\u0161\u010da. Pri d rugi (Guess, Nagler, & \nTucker, 2019)  so ugotovil i, da \u0161tevilo deljenih la\u017enih novic nara\u0161\u010da s starostjo uporabnika. \nUgotovljeno je bilo  prav tako , da uporabniki , ki objavljajo najve\u010d povezav , objavijo manj \nla\u017enih novic kot tisti , ki objavljajo manj povezav . Iz tega je mogo\u010de  sklepa ti, da za \u0161irjenje \nla\u017enih novic morda niso krivi tisti , ki najbolj pogosto delijo . Pri eni od raziskav (Loos & \nNijenhuis, 2020)  so ustvarili stran z la\u017enimi novicami, ki so jih uporabniki delili na Facebooku. \nUgotovili so, da starej\u0161i uporabniki veliko pogosteje kliknejo na tak\u0161ne novice oziroma jih \nkomentirajo. Poleg tega je ve\u010dina ljudi reagirala zgolj na osnovi naslova, ne da b i kliknila na \n\u010dlanek. Obstajajo tudi raziskave, ki primerjajo odnos mo\u0161kih in \u017eensk do la\u017enih novic. \nRezultati (Almenar, Aran -Ramspott, Suau, & Masip, 2021)  so pokazali, da so razlike med \nspoloma manj\u0161e, kot smo pri\u010dakovali. Starej\u0161e \u017eenske so pokazale nekoliko ve\u010djo \nzaskrbljenost glede la\u017enih novic od mo\u0161kih, medtem ko pri mlaj\u0161ih ni bilo razlik. \u017denske so za \nglavni vir \u0161irjenja la\u017enih novic krivile Facebook  in Instagram, mo\u0161ki pa Twitter. Mo\u0161ke so bolj \nzanimale politi\u010dne novice in so prebirali ve\u010d novic nasploh, vendar pa slednje ni privedlo do \nbolj\u0161ega prepoznavanja la\u017enih novic. Nekatere raziskave (van der Linden, Panagopoulos, & \nRoozenbeek, 2020)  primerjajo odnos do la\u017enih novic v Zdru\u017eenih dr\u017eavah na osnovi politi\u010dne \nopredelitve. Anketiranci so v veliki meri opazili novice organizacij nasprotne ideolo\u0161ke \nusmeritve kot la\u017ene novice; tri \u010detrtine republikancev je tako imelo CNN za vir la\u017enih novic,  \nmedtem ko je ve\u010d kot polovica liberalnih anketirancev pojmovala za la\u017ene novice Fox News. \nKonzervativci so manj kot liberalci zaupali popularnim medijem.  \nIV \n Zgodovina ka\u017ee, da  la\u017ene novic e obstajajo  \u017ee od \u010dasov pred na\u0161im \u0161tetjem, ko so kamnite \ntablice in drugi zapisi poveli\u010devali vodje in so tisti, ki so znali brati , imeli veliko mo\u010d nad \nostalimi. Z izumom tiska se je pismenost  pove\u010dala , s \u010dimer se je pove\u010dalo \u0161irjenje la\u017enih novic. \nTisti, ki so imeli politi\u010dno ali finan\u010dno mo\u010d , so lahko naro\u010dali tiska nje informacij , ki so jih \nnameravali \u0161iriti. \u017de v 18. stoletju se je omenjal vpliv la\u017enih novic in te\u017eave pri soo\u010danju z \nnjimi .  \nZ la\u017enimi novicami je tesno povezana propaganda, ki pa je zaradi druge svetovne vojne \ndobila zelo negativ en prizvok, pred tem pa ta termin ni imel povsem negativne konotacije . \nLa\u017ene novice so imele pomembno vlogo v obdobju pandemije COVID -19. Mnogi ljudje so \nzaradi spreminjajo\u010dih se navodil s strani vlade in spreminjajo\u010dih se stali\u0161\u010d s strani medijev \nizgubili zaupanje do novic. \u0160irile so se \u0161tevilne govorice, npr. o 5G stolpih k ot povzro\u010diteljih \nvirusa.  \nZa ilustracijo razpona la\u017enih novic smo izvedli primerjalno analizo dveh \u010dlankov z la\u017enimi \nnovicami,  ki sta se med seboj zelo razlikovala . Eden je bil \u017ee na videz neresen , saj je vseboval \nsenzacionalisti\u010dne in nestrokovne  izraze , medtem ko je bil drugi strokovno strukturiran ter \npotencialno zelo zavajajo\u010d. Primerjali smo vire, uporabljene slike, besede in struktur o spletne \nstrani obeh \u010dlankov.  \nV literaturi najdemo  osem postopkov identifikacije la\u017enih novic (IFLA, 2019) . Ti postopki \nzajemajo preverjanje strani, podrobno branje \u010dlanka, preverjanje avtorja, virov  in datuma, \nugotavljanje , ali je novica satir i\u010dna, preverjanje lastnih predpostavk in uporab o strani za \npreverjanje dejstev.  \nNa osnovi zaklju\u010dkov dosedanjih raziskav smo predlagali , da se la\u017ene novice uporab ljajo  kot \ntema pri pou\u010devanju  \u0161olskega predmeta  angle\u0161\u010din a, predvsem ko gre za tuj jezik, da bodo \nu\u010denci v prihodnosti la\u017eje identificirali la\u017ene novice in zmanj\u0161ali njihovo \u0161irjenje. \nIzpostavljeno je  \u0161e bilo , da nekatere \u0161ole \u017ee pou\u010dujejo medijsko pismenost z namenom boja \nproti la\u017enim novicam.  \nEmpiri\u010dni del je temeljil na anket ah, izvedenih med letoma 2018 in 2024 , ki smo jih  delili po \nspletu . Prvo anketo so re\u0161ili 104 posamezniki, drugo pa 150 posameznikov.  Zaradi ma jhnega  \nvzorca anket i nista popolnoma  primern i za posplo\u0161evanje na celotno populacijo ali na vse \nuporabnike spleta, ampak sta lahko uporabljen i kot izhodi\u0161\u010de  za nadaljnje raziskave tega \npodro\u010dja z ve\u010djim obsegom. Pri anketi iz leta 2018 so veliko odzivov prispevali mo\u0161ki in mladi, \nmedtem ko je bil pri anketi iz leta 2024 vzorec dokaj enakomerno razporejen. Pri obeh \nanketah so bili skoraj vsi anketiranci iz Evrope in Severne Amerike, zato rezultati ne morejo \nbiti po splo\u0161eni na druge celine.  \nV \n Ve\u010dina vpra\u0161anih je novice iskala po spletu . Radio, televizijo in tisk je  leta 2018 uporabljala \ntretjina , leta 2024 pa \u010detrtina  vpra\u0161anih. Najve\u010d vpra\u0161anih je zaupalo  medijem  BBC, New \nYork Times in CNN, medtem ko \u010detrtina  vpra\u0161anih  v obeh letih ni zaupala nobenemu \nponudniku novic. Hipoteza, da ljudje najbolj zaupajo velikim ponudnikom novic, je bila delno \novr\u017eena, saj je \u2013 z izjemo nekaterih velikih ponudnikov \u2013 zaupanje zelo nizko. Vpra\u0161ani imajo \npodobno zaupanje do lokaln ih in dr\u017eavn ih ali mednarodn ih novic, kar je ovrglo na\u0161o \nhipotezo, da ve\u010dina ljudi bolj zaupa dr\u017eavnim in mednarodnim novicam. Potrjena  je bila \nhipoteza, da dru\u017ebenim medijem vpra\u0161ani ne zaupajo kot viru novic, je pa veliko vpra\u0161anih \npojmovalo  dru\u017ebene medije za dober za\u010detek za prejemanje novic, ki jih nato dodatno \nrazi\u0161\u010dejo drugje. Zaupanje socialnim medijem kot viru novic se je med letoma 2018 in 2024 \nnekoliko pove\u010dalo. Ovr\u017eena je bila hipoteza, da ve\u010dina ljudi pregleda le en vir, saj je velika \nve\u010dina vpra\u0161anih trdila, da preverijo za do lo\u010deno temo vsaj dva vira. Ve\u010dina vpra\u0161anih v\u010dasih \nali redko  verjame novicam samo na osnovi naslova, ne da bi prebrali celoten \u010dlanek. Vs e \nvpra\u0161an e osebe se  sre\u010dujejo  z la\u017enimi novic ami. V  obeh letih je polovica anketirancev la\u017ene \nnovice sre\u010devala ob\u010dasno, tretjina pa pogosto. Potrjena  je bila hipoteza, da je politi\u010dna \nusmeritev pojmovana kot  najpogostej\u0161i razlog la\u017enih novic, saj sta leta 2018 dve tretjini  \nvpra\u0161anih izbrali ta odgovor , leta 2024 pa ve\u010d kot polovica . Ve\u010dina vpra\u0161anih se z la\u017enimi \nnovicami spopada tako, da preverja razli\u010dne vire. Skoraj vsi vpra\u0161ani so mnenja, da bi moral \nvir novic , ki objavi nekaj, kar se izka\u017ee za la\u017eno, objaviti popravek, medtem ko polovica meni , \nda bi se vir moral opravi\u010diti  in \u010dlanek odstraniti . \u010cetrtina anketiranih je podala mnenje, da \nje pandemija COVID -19 zelo zni\u017eala njihovo zaupanje novicam, medtem ko je tretjina izrazila \nnekoliko manj\u0161e zaupanje, kar potrjuje na\u0161o hipotezo o vplivu pandemije na zaupanje \nnovicam.  \nZadnjih osem vpra\u0161anj je anketirancem ponudilo naslov neke novice in jim zadalo nalogo , \nda na osnovi samega naslova opredelijo , ali je navedena novica la\u017ena ali resni\u010dna. Prvi \nnaslov je velika ve\u010dina ustrezno  ozna\u010dila za la\u017enega. Drugi naslov je ve\u010dina ozna\u010dila za \nla\u017enega, \u010detudi je bil resni\u010den. Tretji naslov je ve\u010dina pravilno ozna\u010dila za la\u017enega , enako \ntudi \u010detrtega.  Peti naslov je bil resni\u010den,  dele\u017e odgovorov za la\u017eno in resni\u010dno pa je bil \npribli\u017eno enak . Peti naslov je ve\u010dina ustrezno  ozna\u010dila za la\u017en ega. Pri sedmem  naslovu, ki je \nbi la\u017een, je bil  dele\u017e obeh vrst odgovorov pribli\u017eno enak. Tudi pri zadnjem  naslovu,  ki je bil  \nresni\u010d en, je bil  dele\u017e obeh vrst odgovorov pribli\u017eno enak. Pri nekaterih naslovih je pri\u0161lo do \nopaznih razlik med razli\u010dnimi skupinami, na splo\u0161no pa prepoznavnih razlik med razli\u010dnimi \nskupinami glede na spol, lokacijo, starost in izobrazbo ni bilo veliko, zato smo ovrgli \nhipote zi, da bodo Severni Ameri\u010dani in vi\u0161je izobra\u017eeni bolje prepoznali la\u017ene novice na \nosnovi naslova.   \nVI \n The Emergence and Public Perception of Misinformation: A \nStudy on Fake News  \n \nKeywords:  fake news, spread , identification , mal -information  \nUDC:   \nAbstract:  \nIn this thesis , we deal with fake news, its history  and identification combined with how \npeople perceive it. We discuss the effect of the COVID -19 pandemic on fake news and the \nrole artificial intelligence plays in manufacturing it. We look  at how people define fake news, \nhow misinformation  is identified and how it spread s. The theoretical portion analyses and \ncompares several definitions of fake news , in order  to define what fake news is. Several  \nexisting studies on the topic are analysed. Two fake news articles are analysed  and \ncompared  based on their structure . Ways of identifying fake news are presented , along with \na suggestion to use fake news a s a topic when teaching English.  \nThe empirical section is based on two online survey s in 2018 and 2024 . The results show that \npeople encounter fake news very often but are taking steps, such as viewing multiple \nsources, to identify fake news. The majority of respondents consider political bias or agenda \nto be the main reason behind fake news. Most responde nts get their news online but do not \ntrust social media as a source ; however,  many consider it a good starting point. No \nsignificant differences between various  demographics were found in  the ability to identify \nfake news. Despite the COVID -19 pandemic and its effects on fake news, the differences in \nresponses between the surveys are minimal.   \n\nVII \n Table of Contents  \nAcknowledgements  ................................ ................................ ................................ .................  I \nPojav in javno dojemanje dezinformacij: \u0161tudija la\u017enih novic  ................................ ................  II \nThe Emergence and Public Perception of Misinformation: A Study on Fake News  .............  VI \nTable of Contents  ................................ ................................ ................................ .................  VII \nTable of Figures  ................................ ................................ ................................ .......................  X \nTable of Graphs  ................................ ................................ ................................ ......................  XI \n1 Introduction  ................................ ................................ ................................ ....................  1 \n1.1 Satire or Parody  ................................ ................................ ................................ ........  4 \n1.2 Misleading Content  ................................ ................................ ................................ .. 5 \n1.3 Imposter Content  ................................ ................................ ................................ ..... 5 \n1.4 Fabricated Content ................................ ................................ ................................ ... 8 \n1.5 False Connection  ................................ ................................ ................................ ...... 8 \n1.6 False Context  ................................ ................................ ................................ ............  9 \n1.7 Manipulated Content  ................................ ................................ ...............................  9 \n1.8 Some Definitions of Fake News  ................................ ................................ .............  12 \n1.9 Social Media  ................................ ................................ ................................ ...........  15 \n1.10  Overview of Existing Research  ................................ ................................ ...........  19 \n2 History of Fake News  ................................ ................................ ................................ .... 30 \n3 Fake News and the COVID -19 Pandemic  ................................ ................................ ...... 33 \n4 The Role of Artificial Intelligence (AI) in Fake News  ................................ .....................  36 \n5 Perception of Fake News Survey  ................................ ................................ ..................  41 \n5.1 Research Questions  and Hypotheses  ................................ ................................ ..... 41 \nVIII \n 5.2 Survey  Sample Size, Target and Methodology ................................ .......................  42 \n5.3 Survey Structure  ................................ ................................ ................................ ..... 43 \n5.4 Demographics  ................................ ................................ ................................ ........  44 \n5.4.1  Gender  ................................ ................................ ................................ ............  44 \n5.4.2  Age ................................ ................................ ................................ ..................  45 \n5.4.3  Location  ................................ ................................ ................................ ...........  46 \n5.4.4  Education  ................................ ................................ ................................ ........  47 \n5.5 Results  ................................ ................................ ................................ ....................  47 \n5.5.1  News Sources  ................................ ................................ ................................ .. 47 \n1.1.1  Trusted News Organizations  ................................ ................................ ...........  51 \n5.5.2  Local vs National/International News Organizations  ................................ ..... 54 \n5.5.3  Social Media as a Source of News  ................................ ................................ .. 55 \n5.5.4  Number of News Sources  ................................ ................................ ...............  56 \n5.5.5  Sources of Different Political Leaning  ................................ .............................  57 \n5.5.6  Trust Based on Headlines  ................................ ................................ ...............  58 \n5.5.7  Frequency of Encountering Fake News  ................................ ..........................  59 \n5.5.8  Most Common Reason Behind Fake News  ................................ .....................  60 \n5.5.9  Method of I dentifying Fake News  ................................ ................................ .. 61 \n5.5.10  Actions of a News Publisher After Posting Fake News  ................................ ... 62 \n5.5.11  Impact of COVID -19 Pandemic on Trust in News Publishers ..........................  63 \n5.6 Identifying Fake News Based on Headlines Alone  ................................ .................  64 \n5.6.1  First Headline  ................................ ................................ ................................ .. 65 \n5.6.2  Second Headline  ................................ ................................ .............................  67 \nIX \n 5.6.3  Third Headline  ................................ ................................ ................................ . 70 \n5.6.4  Fourth Headline  ................................ ................................ ..............................  72 \n5.6.5  Fifth Headline  ................................ ................................ ................................ .. 73 \n5.6.6  Sixth Headline  ................................ ................................ ................................ . 75 \n5.6.7  Seventh Headline  ................................ ................................ ............................  76 \n5.6.8  Eighth Headline  ................................ ................................ ...............................  79 \n5.7 Hypotheses  ................................ ................................ ................................ .............  82 \n6 Analysis of Two Fake News Articles  ................................ ................................ ..............  85 \n7 Identifying Fake News  ................................ ................................ ................................ ... 89 \n7.1 Consider the Source  ................................ ................................ ...............................  90 \n7.2 Read Beyond  ................................ ................................ ................................ ..........  90 \n7.3 Check the Author  ................................ ................................ ................................ ... 90 \n7.4 Supporting Sources  ................................ ................................ ................................  90 \n7.5 Check the Date  ................................ ................................ ................................ .......  91 \n7.6 Is It a Joke?  ................................ ................................ ................................ .............  91 \n7.7 Check Your Biases ................................ ................................ ................................ ... 91 \n7.8 Ask the Experts  ................................ ................................ ................................ .......  91 \n8 Fake News as a Topic in English Teaching  ................................ ................................ .... 92 \n9 Conclusion  ................................ ................................ ................................ .....................  93 \nWorks Cited  ................................ ................................ ................................ ...........................  96 \nAppendix 1 \u2013 Survey  ................................ ................................ ................................ ...........  104 \nAppendix 2 \u2013 Analysed Articles  ................................ ................................ ...........................  113 \n \nX \n Table of Figures  \nFigure 1:  Types of false information  ................................ ................................ ......................  3 \nFigure 2: Removal of Nikola Yezhov  ................................ ................................ .....................  10 \nFigure 3: Comparison of graphs  ................................ ................................ ............................  11 \nFigure 4: Views of surveyed Americans on made -up news  ................................ ..................  19 \nFigure 5: Views of surveyed Americans on the creation of and responsibility for made -up \ninformation  ................................ ................................ ................................ ...........................  20 \nFigure 6: Sharing of made -up information  ................................ ................................ ...........  22 \nFigure 7: Facebook engagements for four types of content  ................................ ................  23 \nFigure 8: Twitter shares for four types of content  ................................ ...............................  24 \nFigure 9: Mean number of fake news in comparison to all links posted  .............................  26 \nFigure 10: Mean number of fake news shared per age group  ................................ .............  27 \nFigure 11: Associations with fake news by political ideology  ................................ ...............  29 \nFigure 12: Increase in searches for coronavirus and 5G between December 2019 and \nOctober 2020.  ................................ ................................ ................................ .......................  33 \nFigure 13: Image created using DALL\u00b7E mini ................................ ................................ .........  37 \nFigure 14: Image created using Canva Magic Media  ................................ ............................  38 \nFigure 15: Second picture used in the article from Thugify  ................................ .................  86 \nFigure 16: The picture used in the article from WorldTruth  ................................ ................  87 \nFigure 17: How to spot fake news  ................................ ................................ ........................  89 \n \n  \nXI \n Table of Graphs  \nGraph 1: Gender structure of the samples  ................................ ................................ ...........  44 \nGraph 2: Age structure of the samples  ................................ ................................ .................  45 \nGraph 3: Location structure of the samples  ................................ ................................ .........  46 \nGraph 4: Education structure of the samples  ................................ ................................ .......  47 \nGraph 5: News sources used by respondents  ................................ ................................ ...... 48 \nGraph 6: Comparison of news sources based on gender  ................................ .....................  49 \nGraph 7: Comparison of news sources based on age range  ................................ ................  50 \nGraph 8: Comparison of news sources based on location  ................................ ...................  51 \nGraph 9: News organizations trusted by respondents  ................................ .........................  52 \nGraph 10: Comparison of trusted news organizations based on location  ...........................  53 \nGraph 11: Trust in local vs (inter)national news organizations by respondents  ..................  55 \nGraph 12: Trust in social media by respondents  ................................ ................................ .. 56 \nGraph 13: Number of news sources used by respondents  ................................ ..................  57 \nGraph 14: Tendency to check news sources of different political leaning by respondents  58 \nGraph 15: Trust or distrust in news based on headline alone by respondents  ...................  59 \nGraph 16: Frequency of encountering fake news by respondents  ................................ ...... 60 \nGraph 17: The most common reason behind fake news encountered by respondents  ...... 61 \nGraph 18: Respondents' methods of identifying fake news  ................................ ................  62 \nGraph 19: Respondents' desired actions of a publisher after posting fake news  ................  63 \nGraph 20: Effect of COVID -19 pandemic of the respondents' trust in news organizations  . 64 \nGraph 21: Respondents' opinion on the truth of the first headline  ................................ ..... 66 \nGraph 22: Respondents' opinion on the truth of the second headline  ................................  67 \nGraph 23: Comparison of responses to the second headline based on gender  ..................  68 \nGraph 24: Comparison of responses to the second headline based on region  ...................  69 \nGraph 25: Comparison of responses to the second headline based on the response to a \nprevious question about headlines  ................................ ................................ ......................  69 \nGraph 26: Respondents' opinion on the truth of the third headline  ................................ ... 70 \nXII \n Graph 27: Comparison of responses to the third headline based on region  .......................  71 \nGraph 28: Comparison of responses to the third headline based on the response to a \nprevious question about headlines  ................................ ................................ ......................  71 \nGraph 29: Respondents' opinion on the truth of the fourth headline  ................................ . 73 \nGraph 30: Respondents' opinion on the truth of the fifth headline  ................................ .... 74 \nGraph 31: Respondents' opinion on the truth of the sixth headline  ................................ ... 75 \nGraph 32: Comparison of responses to the sixth headline based on the completed level of \neducation  ................................ ................................ ................................ ..............................  76 \nGraph 33: Respondents' opinion on the truth of the seventh headline  ..............................  77 \nGraph 34: Comparison of responses to the seventh headline based on the completed level \nof education  ................................ ................................ ................................ ..........................  78 \nGraph 35: Comparison of responses to the seventh headline based on the response to a \nprevious question about headlines  ................................ ................................ ......................  79 \nGraph 36: Respondents' opinion on the truth of the eighth headline  ................................ . 80 \nGraph 37: Comparison of responses to the eighth headline based on gender  ...................  81 \nGraph 38: Comparison of responses to the eighth headline based on the completed level of \neducation  ................................ ................................ ................................ ..............................  81 \n  \n1 \n 1 Introduction  \nThe goal of our study  is to explore how individuals engage with news media, with special \nattention to the question of fake news. For this purpose , we conducted two online surveys, \none in 2018 and one in 2024, designed to assess respondents' news consumption habits. \nMore specifically, we looked at the types of news sources  they depend on, the levels of trust \nthey place in different news organizations , and their general views about fake news and \ntheir opinions on those  behind its propagation.  In the 2018 survey, participants were  also \npresented with a series of headlines and asked to judge whether they thought they were \nreal or fake. The 2024 survey also asked  participants how they thought the COVID -19 \npandemic had affected their perceptions of misinformation. Responses collected were \ncompared between the two survey time frames and analy sed against chief demographic \nvariables.  \nBefore delving into a detailed analysis of the survey results, this thesis provides a \nfoundational discussion of fake news, including its characteristics, impact, and significance \nin contemporary discourse. Additionally, we review existing research to cont extualize our \nfinding s. \n\u201cFake news\u201d is a phrase that has become very commonplace nowadays , and we can hear it \nalmost daily. The concept of falsification in news is not new, but talking about it and \nspecifically studying it has only been present over the past decade . Fake news is closely \nrelated to politics and is intertwined with political  bias and thus a lot of articles and research \nabout it exists . In this thesis , we will attempt to stay as apolitical and non -biased as possible \nin our analysis of fake news and how people perceiv e it. \nFirstly, we have to define what fake news  is. Some say that only news  with the  intent to \nmislead count s as fake news. \u201cFake news is the deliberate presentation of (typically) false \nor misleading claims as news , where the claims are misleading by design \u201d (Gelfert, 2018 , p. \n108; emphasis in the original ). This implies that the fake news article, video or any other \n2 \n form of fake news was created specifically with intent to spread misinformation in order to \nachieve a goal. Gelfert claims that the term \u201cfake news\u201d should be reserved only for such \nintentional cases. In this thesis , we will broaden the view to all falsified information posted \nas news, which includes satirical news posted with no intent to harm, and articles that \nunintentionally spread false information due to a mistake or bad sources. In cases of the \nlatter, it is also very important to consider whether th e publisher of unintentionally false \nnews later posted a retraction or correction, because we can argue that an unintentionally \nfalse news article or video, which  is never corrected or retracted, can count as fake news, \nsince  the inaction on it can be considered as intent.   \nBaptista  and Gradim (2022, p. 640)  used 5 points to define fake news as :  \nA type of online disinformation (1), with (2) misleading and/or false statements that \nmay or may not be associated with real events, (3) intentionally created to mislead \nand/or manipulate a public (4) specific or imagined, (5) through the appearance of \na ne ws format with an opportunistic structure (title, image, content) to attract the \nreader\u2019s attention, in order to obtain more clicks and shares and, therefore, greater \nadvertising revenue and/or ideological gain.  \nThe third point once again classifies fake news as something intentionally created to mislead \nor manipulate. It also lists monetary gain alongside ideological gain as an incentive for the \ncreation of false news. If we consider an article of false information intentionally made for \nadvertising revenue or other form of monetary gain as fake news, satirical articles could be \nincluded.  \n3 \n  \nFigure  1:  Types of false information (Derakhshan & Wardle , 2017 ) \nIn figure 1.1 we can see how Derakhshan and Wardle categorize what they call \u201cinformation \npollution\u201d  (Derakhshan & Wardle, 2017) , with an emphasis on whether the intent was to \nharm or not. Mis -Information is, as was mentioned previously, incorrect information, which \nwas posted without an intent to cause harm , and is  a result of  bad sources, wrongly \nconnected events, rushing to be the first to post the news and various other non -malicious \nreasons. Mal -information is using true information with an intent to cause harm through \nmaking private information public. Dis -information is the true \u201cfake news\u201d, as it is either \nfabricated informa tion intended to cause harm or information manipulated through added \nmalicious context and editing information, for example, doctoring photos. We  will mostly \nfocus on mis-information and dis-information, as mal-information is simply the publishing \nof private information, which is a wholly separate issue.  \n\n4 \n Wardle categorizes mis-information and dis-information into the following seven \ncategories:  \n\u2022 Satire or parody \u2013 No intention to cause harm , but has potential to fool  \n\u2022 Misleading content \u2013 Misleading use of information to frame an issue or individual  \n\u2022 Imposter content \u2013 When genuine sources are impersonated  \n\u2022 Fabricated content \u2013 New content is  a 100% false, designed to deceive and do harm  \n\u2022 False connection \u2013 When headlines, visuals or captions do  not support the content  \n\u2022 False context \u2013 When genuine information is shared with false contextual information  \n\u2022 Manipulated content \u2013 When genuine information or imagery is manipulated to deceive  \n(Wardle, 2017)  \n1.1  Satire or Parody  \nSeveral news publishers and individuals do not hide the fact that their articles and videos \nare pure satire with the intent to entertain. These articles and videos tell a story either \nabout something on the border of being believable to make people questio n if it could \nhappen or something so mundane to make people question why someone would write an \narticle about it. Some people, however, might encounter these articles without checking up \non the publisher, or in a reposted form elsewhere, and consider them factual, potentially \neven spreading them further, which could, depending on the topic of the article, cause \nharm. For example, if it is a satirical article about a TV personality doing something \nridiculous, enough people could be led to believe it that it could have serious negative \nconsequences on that personality\u2019s career. A satirical article could also be harmful if it \nclaims that a person achieved something great by doing something dangerous, as some \ncould try replicating that dangerous act and suffer i njury. The most important part when it \ncomes to satire and parody in news is to keep in mind the negative effects your article could \nhave if taken seriously and to not hide the fact that it is a satire or parody.  \n5 \n There are several popular forms of satirical news. The Daily Show is an American late -night \ntalk show that includes satirical news reporting as part of its comedy. It started in 1996 and \nwas an experiment in journalism due to its intertwining of serious ne ws reporting with \nhumour and mockery. Several websites featuring written forms of satirical news have \nreached some level of popularity , most notably The Onion and The Babylon Bee. It is \nimportant to note that these websites, despite being satirical, still have political biases, \nwhere The Onion is reportedly liberal , while The Babylon Bee is reportedly conservative \n(Brugman, Burgers, & Konijn, 2022) . When a satirical news website is politically biased, it \noften creates articles that portray the opposing side in a negative light  by creating fake \nevents that would fit their political narrative and stereotypes they have about  their \nopponents.  \n1.2  Misleading Content  \nWardle states that misleading content is used to frame issues and individuals, but in figure \n1.1, Derakhshan and Wardle classify misleading content as false without intent to harm, so \nthere appears to be a discrepancy, as framing does imply intent. Nevertheless, this content \nmisleads the public, whether it is intentional or not, through false information.  \n1.3  Imposter Content  \nIn the age of social media and anonymity online, one can easily pretend to be someone else. \nIt is commonplace for people to pretend to be a public figure or a representative for a large \ncompany online. This does have a large overlap with the category of pa rody and satire , since  \nmany people only do it to entertain themselves and others with no intent to cause harm. \nThe most common way to do this is to use the name and image of a public figure on social \nmedia and post content as if they posted it. Social medi a websites combat this by verifying \npublic figures by adding a checkmark next to their name on their confirmed account, but \nthe issue is that not everyone knows about the verification system and could easily believe \nan imposter account to be real and potentially start spreading information posted by the \n6 \n imposter account as factual. Some imposters are more malicious and specifically target the \nreputation of the person or business they are posing as.  \nEnnis describes  the potential effects of imposter accounts on businesses:  \n\u2022 Impersonate influential individuals and criticise, mock or lie about company activities  \n\u2022 Pretend to be a company or employee and damage brands through unsavoury \ncomments or posts  \n\u2022 Create and exploit security vulnerabilities (gaining access to sensitive data or business \naccounts)  \n\u2022 Conduct phishing schemes or other scam activities  \n(Ennis, 2018)  \nA huge issue with impersonation arose on Twitter, now  known as  X, in 2022 , after Elon Musk \npurchase d the company. The social media website ha d used its \u201cblue checkmark\u201d system \nfor years before Musk\u2019s acquisition. This system gave public figures, politicians, company \naccounts, journalists and other notable individuals a blue icon with a checkmark next to \ntheir username to signify that they have v erified their identity. This would let Twitter users \nknow when a post made, for example, by a celebrity, is actually them  and not an impostor. \nAfter his acquisition of the company . Musk added the option  to pay a monthly fee of 8 US \ndollars to receive certain benefits, one of them being a checkmark next to a user\u2019s name \nwithout any verification, identical to the one used to verify notable accounts for years. This \nled to several cases of impersonation. Most cases of impersonation were satirical, where \nsomeone faked a public figure posting something embarrassing. Due t o the popularity of \ncryptocurrency, several cases of imperson ation also entailed crypto scams, making users \nbelieve that a certain celebrity was promoting a cryptocurrency, thus making it seem more \nlegitimate.  \nOne very prominent case of impersonation using this new system was a person pretending \nto be the pharmaceutical giant Eli Lilly and Co., a major provider of insulin in the United \n7 \n States. This person changed their name and profile picture to that of the company and paid  \nthe monthly fee to receive a checkmark. Twitter has a system where each user has a unique \nusername, and a display name that can be the same as other users. While this user set their \ndisplay name to the same one as the company, it was not possible to use th e same \nusername. While the company uses the username \u201c@LillyPad\u201d, this user set theirs to \n\u201c@EliLillyandCo\u201d. To those unfamiliar with the company, the impostor\u2019s user name would \nseem more genuine than that of the actual company. Using this fake identity, this person \nposted a tweet stating \u201cWe are excited to announce insulin is free now.\u201d This resulted in \nseveral people believing that the company would start distributing  the drug to those who \nneed it for free. While those who needed  the drug were excited, the shareholders of the \ncompany did not like the news, as insulin is a major part of the company\u2019s profits. The shares \nof Eli Lilly and Co. dropped over 6% the next day,  lowering the company\u2019s market cap by \nbillions. The company blamed Twitter for this ordeal and retaliated by pulling all advertising \nof their products from the platform. As Twitter, now X, largely depends on advertising, with \nEli Lilly and Co. being a large  company, this was a significant hit to the company (Adams, \n2022) . The company has since changed its official Twitter/X username to \u201c@EliLillyandCo\u201d, \nexactly what the impostor used, perhaps realizing that \u201c@LillyPad\u201d was not a suitable \nusername for such a company.  \nTwitter/X has since updated its verification system . There are now 3 different colo urs of the \nverification badge. Those who pay the monthly premium subscription still have the blue \nbadge. Verified political figures have a grey badge. Verified companies now have a gold \nbadge and a square profile picture instead of a round one. Those assoc iated with a certain \ncompany now have a square logo of that company next to their username (Clark & Peters, \n2022) . This new system still leaves out several notable people such as actors, athletes, \ninfluencers and journalists not affiliated with an individual media company . There are new \nrequirements in place to be verified as someone paying the monthly subscription. Accounts \nmust have a confirmed phone number and no recent changes to their profile photo, display \nname or username, and are checked for signs of deception (X Help Center, How to Get the \n8 \n Blue Checkmark on X, 2023) . Despite this, users can still find it hard to trust accounts of \nfamous individuals who are not a part of a company and, while rare, impersonation can still \noccur, resulting in scams or damage  to the  image of the person being impersonated.  \nPeople can also create imposter websites that host false information and potentially profit \nfrom them through advertisement revenue and other means when people visit them \nbelieving they are  the real site. Impersonating public figures can also lead to profit if people \nare led to believe they are donating to a real public figure.  \nA po tential goal of impersonation can be  to gain the trust of someone to draw private \ninformation from them and then use that as mal -information or dis -information.  \n1.4  Fabricated Content  \nFabricated content is designed from the ground up to be malicious. It is completely \nfabricated like satire or parody, but with specific intent to cause harm to an individual or a \ngroup. This is the narrowest definition of fake news.  \n1.5  False Connection  \nFalse connection is usually the result of lazy or sensationalist journalism. A common form \nof false connection is the use of stock photos or photos from different events, which can \nmake people think it is from the event described in the article. The profit ability of online \nnews relies heavily on advertisement revenue, so it is very important to get people to click \non the article and read it. To achieve that, some authors and websites use articles and \nthumbnails that draw attention, commonly referred to as c lickbait. These titles and images \nsometimes match the content of the article very barely or not at all. This can be problematic \nbecause some people end up only reading the title and seeing the picture without reading \nthe full article and can then get the wrong idea or spread mis -information based on those \nalone. False connection generally does not include an  intent to harm.  \n9 \n 1.6  False Context  \nGenuine information can be twisted using context. This can be done to various types of \ninformation and media , such as study results, images and quotes. Something that is \ngenerally neutral can be made to appear bad or good through added malicious context. \nSome information can be exaggerated or deemphasized. Context around a certain picture \nor recording can be completely falsified but is believed due to the picture being real.  \nA common occurrence of false context on social media is posting a video or image from a \npast event  during a current event, claiming that it happened recently , in order to push a \nfalse narrative. This is common during events such as riots or armed conflicts, where a video \nfrom a past event is posted in order to make a current event seem worse or appear in a \ndifferent context. During a riot, social media users can post videos of destruction or other  \nnegative actions from a different riot to potentially frame a peaceful riot as a violent one or \nworsen the public\u2019s opinion on those rioting. False context videos posted during active \nmilitary conflicts can sway the opinion of the public on either side. People can be made to \nbelieve that a certain side is doing bet ter or worse than it actually is, or that a certain side \nis committing heinous acts and war crimes during the conflict.  \nDuring the conflict between Russia and Ukraine, which started in 2022, several videos were \nposted to social media such as TikTok, Facebook and Twitter stating that they are recent \nvideos, but turned out to be older or completely false. Some videos were fro m the 2014 \nconflict, some were from military training or other conflicts, and some were even from \nrealistic military video games such as Arma 3 (Sardarizadeh, 2022) . \n1.7  Manipulated Content  \nClosely related to false context, manipulated content is taking factual media and \ninformation and editing it to deceive. A well -known way of manipulating content is \ndoctoring photos, nowadays commonly referred to as \u201cphotoshopping\u201d , named after \nAdobe Photoshop, a popular graphics editor program. This, however, predates the use of \ncomputers and a notable case of this occurred under Joseph Stalin. \u201cStalin used a large \n10 \n group of photo retouchers to cut his enemies out of supposedly documentary photographs. \nOne such erasure was Nikola Yezhov, a secret police official who oversaw Stalin\u2019s purges.\u201d  \n(Blakemore, 2018)  \n \nFigure 2: Removal of Nikola Yezhov (Blakemor e, 2018 ) \nAt the time, this image editing almost erased the people who had been removed from \nhistory.  \nAs technology advances, the quality of edited photos increase s, and it is becoming \nincreasingly difficult to spot doctored photos and harder for the victims of doctored photos \nto prove their fabrication, even when providing the original, unedited photo, claims can be \nmade that the original photo is the edited one.  \nPhotos, however, are not the only form of media that can be edited. The technology to \ndoctor video and sound has evolved significantly as well. While it is harder and more time \nconsuming than doctoring photos, it is still an option for malicious use.  \nA form of software that can edit video recordings of people speaking and make them say \nwhatever the editor wants is also rapidly advancing. Automated intelligence is used to scan \nthe facial movement and voice of a speaking person to then reproduce their speech . The \nmore footage of the person speaking is analysed, the better the result, which makes it \neffective on public figures. Stanford University, Max Planck Institute for Informatics, \n\n11 \n Princeton University and Adobe Research collaborated in the creation of such software, \nwhich was able to change several words spoken in a short video.  \nIn tests in which the fake videos were shown to a group of 138 volunteers, some 60 \npercent of participants though the edits were real. That may sound quite low, but \nonly 80 percent of that same group thought the original, unedited footage was also \nlegitima te. (The researchers note that this might be because the individuals were \ntold their answers were being used for a study on video editing, meaning they\u2019d \nbeen primed to look for fakes.) (Vincent, 2019)  \nAnother form of manipulated content is the manipulation of statistical data and study \nresults. This can be achieved by only taking specific pieces of information from a study result \nand, similarly to false context, interpreting them maliciously. Without re ading the full study, \nindividual results and statistics are simple to manipulate to show something in a more \nnegative or positive light than intended. Maliciously drawing causation from  correlation or \nextrapolating data from a very small sample to the whol e population are common. \nManipulation of graphs by can also affect  what the reader thinks, despite the information \nbeing correct, such as by making the  scale smaller to emphasise differences or cutting out \nonly a part of the graph.  \n \nFigure 3: Comparison of graphs (L ebied , 2018 ) \n\n12 \n These two graphs show how cutting out a part of a larger graph can change the information \nthe reader receives in order to mislead. \u201cIt is worth mentioning that 1998 was one of the \nhottest years on record due to an abnormally strong El Ni\u00f1o wind current.\u201d (Lebied, 2018)  \nManipulated content is potentially the most dangerous form of fake news , as it often draws \nfrom factual verifiable information or is hard to disprove.  \n1.8  Some Definitions of Fake News \nUp t o this point, we focused on the views on fake news from 3 sources . It is important to \nlook at how other entities, including dictionaries, define fake news.  \nLots of things you read online [,] especially in your social media feeds [,] may appear \nto be true, often is not  [sic]. Fake news is news, stories or hoaxes created to \ndeliberately misinform or deceive readers. Usually, these stories are created to \neither influence people\u2019s views, push a political agenda or cause confusion and can \noften be a profitable business for online  publishers. Fake news stories can deceive \npeople by looking like trusted websites or using similar names and web addresses \nto reputable new s organisations. (Webwise, 2018)  \nWebwise stresses that fake news deliberately misinforms  or deceive s, and brings up the \npoint of fake news being profitable for publishers , while also touching on impersonation \nmentioned earlier.  \nFake news is just as it sounds: news that is misleading and not based on fact or, \nsimply put, fake. Unfortunately, the literal definition of fake news is the least \ncomplicated aspect of this complex topic. Unlike satire news, which is often hosted \nby parod y websites (think The Onion) and makes light humor of current events and \npeople, fake news has the intention of disseminating false information, not for \ncomedy, but for consumption.  (Alvarez, 2017)  \nAlvarez defines fake news as \u201cnot based on fact\u201d. This could be somewhat disagreeable , as \nit could be  argue d that the most effective forms of misleading news are the ones that take \n13 \n factual content and manipulate it through context and other means ; for an average reader , \nit is harder to spot and fact check than  to believe. Purely made -up content can potentially \nbe disproven very quickly, while some well manipulated news needs  a fair amount of \nresearch to disprove and is therefore potentially more harmful. It seems appropriate to \ndraw  the line between parody news websites, which do it for comedy , and intentionally \nmisleading sites, but the word consumption does seem to minimize the nega tive impact \nthat some malicious fake news can have.  \nFake news can be as slippery to define as it is to pin down. Stories may be factually \ninaccurate and deliberately published to underscore a certain viewpoint or drive lots \nof visitors to a website, or they could be partially true but exaggerated or not ful ly \nfact-checked before publication. (Charlton, 2019)  \nThis definition does not strictly limit fake news to malicious intent , as it adds news that is \nnot fully fact -checked, which would fall under non -malicious mis -information. The \ndefinition also touches on the clickbait and profitability aspect of fake news. It is worth \nconsidering whether the sole intent of profitability without an intent to misinfor m is \nmalicious in itself, especially when no entity is targeted by the information.  \nThe official website of the European Commission features the following definition of \ndisinformation in their policy on tackling online disinformation, which is synonymous with \nfake news.  \nDisinformation is verifiably false or misleading information created, presented and \ndisseminated for economic gain or to intentionally deceive the public. It may have \nfar-reaching consequences, cause public harm, be a threat to democratic political \nand pol icy-making processes, and may even put the protection of EU citizens' health, \nsecurity and their environment at risk. (European Comission, 2019)  \nThe European Comission  further state s the effect of disinformation on the trust citizens \nhave in institutions and its negative effects on electoral systems and society. It consider s \n14 \n disinformation a significant enough issue to construct an action plan to counter it. As with \nsome other definitions, the aspect of profitability of disinformation is listed. This is \npotentially the most serious definition available, due to its emphasis on health, security and \nenvironment risks. As such, it could almost be considered somewhat fearmongering. \u201cFake \nnews is a form of propaganda that consists of deliberate misinformation, usually intended \nto smear someone's image. Typically, fake news stories ar e generated to sway people's \nviews, push a political agenda, or cause confusion around an important issue. \u201d (Your \nDictionary, 2018)  \nThis definition directly considers fake news a form of propaganda and focuses on the \npolitical aspect, therefore somewhat narrowing down the definition compared to others.  \nLike many other definitions,  it limits fake news to deliberately false and malicious content, \nexcluding satire and unintentional false information.  \nFake news, or hoax news, refers to false information or propaganda published under \nthe guise of being authentic news. Fake news websites and channels push their fake \nnews content in an attempt to mislead consumers of the content and spread \nmisinformation v ia social networks and word -of-mouth. (Stroud, no date)  \nThis web dictionary entry and its author also relate fake news to propaganda. This definition \ngives special mention to social media and word -of-mouth and emphasises the importance \nof media consumers, who spread the false information.  \nAndreau (2021)  analyses several authors\u2019 definitions of fake news to come up with a \ndefinitive definition of fake news. The resulting definition is \u201c Fake news is misleading \ninformation intentionally published and presented as news which has the function of \ndeliberately misleading its recipients about its status as news. \u201d Once again, this definition \nstresses the intentionality of publishing misleading information with the aim of appearing \nas genuine news. While fake news is often false, it does not have to be; its definin g feature \nis the deliberate intent to mislead about its newsworthiness.  This definition stresses that \nthe intention to mislead, rather than the content\u2019s falsity or audience reach, is central to \n15 \n fake news. It distinguishes between the action of misleading, which is necessary, and the \nproperty of being misleading, which is not.  \nAll but one of the definitions analysed consider fake news to be limited to deliberate, mostly \nmalicious disinformation , and several mention the profitability aspect of fake news. From \nthis, we can draw the consensus of what is considered fake news \u2013 a deliberately falsified \nor manipulated news article or other form of public post with the intent to sway public \nopinion or to profit monetarily.  \nRodr\u00edguez -Ferr\u00e1ndiz  (2023)  analyses 30 definitions of fake news from dictionaries, academic \nsources, news agencies and others. M ost de finitions  focus on fake news  being intentionally \nmisleading, with 19 out of 30 definitions emphasizing that fake news is  intentionally  \ndesigned to deceive through falsehoods or disinformation.  Fake news is frequently \ncharacterized by its resemblance to genuine news, which complicates efforts to distinguish \nit from real news. Additionally, many definitions emphasize the widespread dissemination \nof fake  news, particularly through social media, where its potential for virality often \novershadows concerns about its authenticity. This focus on shareability underscores the \nchallenge of addressing fake news in an environment where its impact is measurable, eve n \nif its veracity is contested.  \n1.9  Social Media  \nSocial media has a strong connection with fake news. Several news organisations and \nindividual journalists, especially the so -called non -mainstream news media use social \nnetworks to publish or share \u2013 link to \u2013 their news articles, videos and infographics. For \nmany people it is more convenient to consume news media on their social media feeds , as \nthey are already consuming other content present there, so this form of news posting can \npotentially reach more users than other forms of publishing , such as news websites, \ntelevision and newspapers. With the amount of content users consume on social networks, \nthey are potentially less likely to pay close attention  to the news they encounter in their \nfeeds, and thus more likely to take them as fact without additional consideration or \n16 \n research. As mentioned previously, social networks are also the prime targets of \nimpersonators. If a certain news publisher publishes a correction or retraction to an article, \nin which they unintentionally got some information wrong, the vast amount of inf ormation \npassing by each user makes it less likely for a user to see it to find out an article they had \nread was incorrect and thus making them more likely to spread misinformation.  \nAnother problem related to fake news are social media \u201cecho chambers\u201d. A study \nconducted on Facebook users from Italy and United States by Quattrociocchi, Scala and \nSunstein in 2016 found that Facebook users polarize into closed groups c entered on \nnarratives. The users in these groups tend to react to the same posts in similar ways and \ninteract only with people of similar views. These users seek out information that enforces \ntheir narrative and reject any information going against it.  (Quattrociocchi, Scala, & \nSunstein, 2016)  \nThe users in such echo chambers are also potentially more likely to share false information \nintentionally, and due to the nature of echo champers, other users are inclined to trust \nthem. This can create a cycle that pushes a group into strongly believing t heir own \npotentially false narrative.  \nAnother spreader of false information on social media are bot accounts. These bots are \nautomated accounts created to simulate human behaviour on social media and are capable \nof interacting with other users.  \nA study conducted over 10 months in 2016 and 2017 by Shao, Ciampaglia, Varol, Yang, \nFlammini and Menczer on Twitter discovered that bots have a huge impact on the spread \nof information from low -credibility sources, sometimes through interaction with popula r \nusers. \u201cOnly 6% of accounts in the sample are labelled as bots using this method, but they \nare responsible for spreading 31% of all tweets linking to low -credibility content, and 34% \nof all articles from low -credibility sources\u201d (Shao, et al., 2018, p. 3) . If a user sees the same \ninformation posted by multiple accounts whom that user considers as real people, the user \nis more likely to believe that information.  \n17 \n In contrast, a study conducted by Vosoughu, Roy and Aral on Twitter posts from 2006 to \n2017 claims that humans and robots spread false information at a similar rate.  \nWe found that false news was more novel than true news, which suggests that \npeople were more likely to share novel information. Whereas false stories inspired \nfear, disgust, and surprise in replies, true stories inspired anticipation, sadness, joy, \nand tru st. Contrary to conventional wisdom, robots accelerated the spread of true \nand false news at the same rate, implying that false news spreads more than the \ntruth because humans, not robots, are more likely to spread it. (Vosoughi, Roy, & \nAral, 2018, p. 1146)  \nA potential reason for this discrepancy might be the the fact that this research used data \nfrom 2006 to 2017, while the research conducted by Shao et al. used data from 2016 to \n2017, and due to advances in technology and changes in global political climate over ten \nyears might have been the reason for mo re bots existing in the 2016 to 2017 period, while \nin the ten years before that it was mostly real users reposting the information.  \nThis research makes it important to consider human impact on the spread of false news. \nUsers tend to share what they find surprising and the study claims falsified information \ninvokes surprise more commonly than factual information.  The study found that falsified \ninformation was shared or \u201cretweeted\u201d (as it was called on Twitter ) 70% more often than \nfactual information. Some of t his spreading of false information by users could be related \nto echo chambers, but that is not touched on in this research.  \nAnother interesting find of this research is that robots spread factual and falsified \ninformation at the same rate , which could imply that these robots are not created with the \nintent to spread false information, but potentially to spread news for monetary gain.  \nFacebook, Google and Twitter voluntarily promised to fight the spread of fake news on their \nservices , but the European Union is not satisfied with their progress. \u201cFacebook, Google and \nTwitter ramped up their efforts to fight fake news ahead of elections last month , but \u201cmore \n18 \n needs to be done\u201d in the face of ongoing threats from Russia, EU officials said Friday.\u201d  \n(Schulze, 2019)  \nTo combat misinformation, Twitter/X launched a feature called \u201cCommunity Notes\u201d.  They \ndefine the aim of the feature:  \nCommunity Notes aim to create a better informed world by empowering people on \nX to collaboratively add context to potentially misleading posts. Contributors can \nleave notes on any post and if enough contributors from different points of view \nrate that note  as helpful, the note will be publicly shown on a post.  (X Help Center, \n2024)  \nThis feature allows users to apply to become a contributor, unlocking a feature where they \ncan propose a \u201ccommunity note\u201d to any post on the social media website, and vote on other \nproposed community notes. The intent of this system is to add notes as quickly as possible \nto posts spreading misinformation on the website in order to prevent further spread of that \nmisinformation. Users with a premium subscripti on on the website receive payment for \ntheir posts based on ads viewed by those who look at them. In a n effort to prevent people \nfrom posting misinformation for monetary gain, posts that have a community note present \nare not eligible for monetization.  \nThis system does have its downsides. Those adding and voting on community notes can \nhave their own biases , and they can sometimes push through community notes, especially \nin smaller communities or \u201cecho chambers\u201d on the website, where contributors of an \nopposing view do not encounter the post. Disliked users sometimes get overly negative \ncommunity notes instead of  being neutral. Due to community notes removing \nmonetization, they are also often misused to intentionally prevent someone from \nmonetizing their p ost by adding additional context that was not entirely necessary or \ncorrecting an obviously satirical post.  \n19 \n 1.10 Overview  of Existing Research  \nPew Research Center conducted a survey of 6,127 United States adults on their views of \nfake news  in February and March 2019 .  \n \nFigure 4: Views of surveyed A mericans on made -up news (Mitchell, Gottfried, Walker, Fedeli, & Stocking, 2019)  \nThe study showed that those surveyed considered made -up news to be a bigger problem \nthan issues such as violent crime, climate change and terrorism, with half of them \nconsidering made -up news as a big problem. As many as 68% consider made -up news as a \nmajo r factor in the decrease of confidence in the government.  \nThe major rise in the popularity of fake news as a concept happened during the United \nStates 2016 pre -election period , so it is interesting to see that people still consider it as such \nan important issue almost three years later.  Another interesting statistic is that made -up \nnews and information have  such a large impact on the confidence Americans have in each \nother, with 54% of those surveyed stating its impact. The lack of trust for each other echoes \nthe impact social media has on the spread of fake news due to news there being reposted \nand shared by  regular  people.  \n\n20 \n  \nFigure 5: Views of surveyed Americans on the creation of and responsibility for made -up information (Mitchell, Gottfried, \nWalker, Fedeli, & Stocking, 2019)  \nPolitical leaders and staff , followed by activist groups, are most commonly considered the \ncreators  of false information. It is important to note that this question is about the creation \nand not the spread. As previously stated, the public, mainly on social media but also by \nword -of-mouth , serves as one of the greatest spreaders of false news but is not often \nconnected to the creation, so 26% could be considered a somewhat high percentage.  \nThere is a large discrepancy between who those surveyed consider the creator and who \nthey consider responsible for the reduction of false information. While it might seem \nshocking at first glance, there are possible explanations for it. People see journalists as those \nwhose job it is to tell people the truth and correct misinformation, while there is a general \nidea in Western  society that politicians lie, so it is normal for people to expect journalists to \ncorrect the made -up information shared by politicia ns.  \nThe survey also inquired how often people encountered false news and what actions they \ntook in response.  \n\n21 \n Almost four -in-ten Americans (38%) say they often come across made -up news and \ninformation, and another 51% say they sometimes do. Given their concerns about \nmade -up news, Americans have also changed their news and technology habits. \nAlmost eight -in-ten (7 8%) say they have checked the facts in news stories \nthemselves. Roughly six -in-ten (63%) have stopped getting news from a particular \noutlet, about half (52%) have changed the way they use social media and roughly \nfour -in-ten (43%) have lessened their overa ll news intake.\u201d (Mitchell, Gottfried, \nWalker, Fedeli, & Stocking, 2019, p. 5)  \nCombining both the 38% that encounter made -up information often and 51% that \nsometimes encounter it, 89% of Americans surveyed encounter fake news , which shows the \nsignificance of how widespread dis -information is. Surprisingly, 78% of those surveyed \nstated that they themselves check the validity of news, but the way this is phrased means \nthat anyone who has checked the validity of even a single new s article falls into this \ncategory; therefore, we cannot know how many people do it regularly, which would be a \nmore significant statistic. It does seem reasonable to expect almost everyone to check up \non a news article that appears outrageous at first gla nce, but based on the amount of \ninformation people consume online nowadays , it is hard to expect anyone to fact check \neverything regularly.  \nFully 86% of U.S. adults think the desire to push an agenda or viewpoint is a major \nreason why made -up news gets created; 71% say making money is a major reason. \nFewer, though still about half (49%), think fame is a major reason, while only 12% \nsay a major  motivator is the desire to produce humor. (Mitchell, Gottfried, Walker, \nFedeli, & Stocking, 2019, p. 17)  \nPushing an agenda as a major reason comes as not surprising, as it is potentially the main \nreason the term fake news itself was coined. As many definitions analysed earlier have \nmentioned, profitability is a major factor in the creation of fake news, and 71% of those \nsurveyed tend to agree. Almost half think that fame is the reason and fame can closely be \n22 \n connected to profit. 12% consider humo ur as a major motivator. This  refers to satire and \nparo dy, which are not usually regarded as fake news, but as this survey was asking about \nmade -up news, it does seem like a good inclusion.  \n \nFigure 6: Sharing of made -up information (Mitchell, Gottfried, Walker, Fedeli, & Stocking, 2019)  \nUnsurprisingly, probably due to the ease of casually sharing news online or moth -to-mouth, \nabout half of those surveyed shared made -up information unknowingly. The more \nsurprising result was the 10%  who knowingly shared false information, but we do have to \nconsider that nearly half of them did it with the intent of informing others of the inaccuracy. \nThe 18% who shared it to discuss fall into the same category , as long as the discussion \ninvolved the invalidity of the news. The 35% who posted it due to it be ing surprising or due \nto them liking it could be considered somewhat malicious, but it depends strongly on the \ntopic of the shared news.  \n\n23 \n \u201cAnd, whether they\u2019ve shared it or not, about six -in-ten Americans (61%) say they at least \nsometimes discuss the issue of made -up news with others, while 16% do so often.\u201d \n(Mitchell, Gottfried, Walker, Fedeli, & Stocking, 2019, p. 16)  \nAs 20% of those surveyed said that the public has the most responsibility to combat made -\nup news, it does appear that 77% of those surveyed are doing their part.  \nAs mentioned  before, Twitter and Facebook attempted to tackle fake news. A study \npublished  by Allcott, Gentzkow and Yu in 2018 analysed Facebook engagements and Twitter \nshares for four different types of content  over four years . \n \nFigure 7: Facebook engagements for four types of content (Allcott, Gentzkow, & Yu, 2018)  \n\n24 \n The decline in engagement with fake news sites does show that Facebook has improve d in \nreducing the spread of false information since 2017. The measures they took might have \nalso had an impact on small news sites and business and culture sites, as those appear to \nhave started their decline at a similar time ; small news sites, however, have started \nrecovering in the second quarter of 2018. Major news sites also appear to have been \naffected, but they started their recovery earlier.  It is important to note the  scale of each \ngraph and the number of sites. 38 major news sites peaked somewhere near 250 million \nengagements with their low point being around 150 million while 78 small news sites only \npeaked at around 5  million with their lowest point being at about 2 million. 570 fake news \nsites were analysed, significantly more than any other category, so the average engagement \nper site is lower than it might appear in comparison to other three categories, but the most \nimportant aspect is the decline of  engagement wi th fake news sites itself.  \n \nFigure 8: Twitter shares for four types of content (Allcott, Gentzkow, & Yu, 2018)  \n\n25 \n Twitter appears to have more problems with tackling fake news sites , as they have \nexperienced significant growth in shares in 2018. Major news sites are also stead ily gaining \npopularity , while small news sites , along with business and culture sites , appear to be in \ndecline, although small news sites did experience a short increase in shares at the start of \n2018.  \nIt is important to define what Facebook engagements and Twitter shares are exactly. \nEngagements on Facebook are any form of interaction with a post, meaning a like or \nanother reaction, a share or comment , while Twitter data is limited to sharing alone, which \nis part of the reason for the discrepancy between the numbers for each website.  \nA study by Guess, Nagler and Tucker analyses the sharing of fake news articles on Facebook \nduring the 2016 election. While the study focuses a lot on  the political leaning s of the people \nsharing the articles, we  will focus on the results regarding age and frequency of link posting \nin an effort to remain apolitical.  \nThe vast majority of Facebook users in our data did not share any articles from fake \nnews domains in 2016 at all . . . this is not because people generally do not share \nlinks: While 3.4% of respondents for whom we have Facebook profile data shared \n10 or fe wer links of any kind, 310 (26.1%) respondents shared 10 to 100 links during \nthe period of data collection and 729 (61.3%) respondents shared 100 to 1000 links. \n(Guess, Nagler, & Tucker, 2019, p. 1)  \nThis information is surprising , considering that around half of those surveyed by the Pew \nResearch Center shared made -up news, but Facebook obviously is not the only way of \nsharing news, so they probably used different methods, such as word -of-mouth. It would \nbe interesting to know wher e and whom they shared it with because the reach matters a \nlot when it comes to sharing information and sharing information with a group of friends \nhas different effects than sharing it publicly  online.  \n26 \n When it comes to sharing on Facebook, it is important to note that users have the ability to \nchoose who can see their posts, meaning that their shares can be confined to a defined \ngroup, friends, friends of friends, or completely public. An important note to add is that the \nnumber  of contacts or \u201cfriends\u201d people have on Facebook varies from person to person.  \n \nFigure 9: Mean number of fake news in comparison  to all links posted (Guess, Nagler, & Tucker, 2019)  \nThis graph shows us the number of links a user posted to Facebook compared to the mean \nnumber of fake news stories shared among them . Those who posted the most links were \nalso the least likely, on average, to post fake news, but it is important to note that those \nwho posted more than 2000 links were a very small portion of those surveyed , so the \ninformation is somewhat less representative.  \nThus, it is not the case that what explains fake news sharing is simply that some \nrespondents \u201cwill share anything.\u201d These data are consistent with the hypothesis \nthat people who share many links are more familiar with what they are seeing and \nare able to distinguish fake news from real news. (We note that we have no measure \nas to whether or not respondents know that what they are sharing is fake news.) \n(Guess, Nagler, & Tucker, 2019, p. 2)  \n\n27 \n It is important to note that the people who post 1001 -2000 links, which is the category \nsharing the most fake news, could also include a lot of people who \u201cwill share anything\u201d \nthey encounter , but do not spend as much time on social media. Those sharing more than \n2000 links, on the other hand, potentially dedicate a lot more time to social media and are \nperhaps more knowledgeable about and interested in news.  \n \nFigure 10: Mean number of fake news shared per age group (Guess, Nagler, & Tucker, 2019)  \nThose aged over 65 were the most likely to share fake news stories, more than twice as \nlikely as the 45 -65 age group below them, with the youngest group of 18 -29 being the least \nlikely to share fake news. This shows us that age does play an important facto r in the sharing \nof fake news, but it is important to note that age is connected to a plethora of different \nfactors, such as the difference in education, proficiency in using social networks and, as the \nauthors of the research point out, political leaning.   \nTwo researchers (Loos & Nijenhuis, 2020)  created a fake news website with 14 articles, then \nused the Facebook advertising feature to boost the articles for audiences in the UK and the \nUSA. Some of these articles were political, for example , \"Experts: Trump must pay royalties \nto the Chinese government for the \u2018Wall\u2019.\"  and \"Big Ben to be moved to Brussels because \nof Brexit?!\" , while others were more apolitical, for example \"BREAKING: this celebrity just \n\n28 \n got arrested for domestic violence!!\"  and \"Campaign to save the Pacific Northwest tree \noctopus is gaining momentum\" . \nThe goal of this research was to see how people interact with these articles, mainly in regard \nto age. Results showed that older age groups were more likely to engage with fake news, \nwith a higher reach among people aged 55 \u201364 (24.7%) and 65+ (21.5%). Only 12.7% of users \nclicked on the articles, which means that  most users reacted based solely on the headlines \nwithout fact -checking. Pro-Brexit and pro -Trump groups were more likely to believe and \nengage with the content than anti -Brexit and anti -Trump groups . The study highlighted the \nneed to target older generations in media literacy programs, as  currently , young users are \nthe main focus  (Loos & Nijenhuis, 2020) . \nA study conducted in Spain (Almenar, Aran -Ramspott, Suau, & Masip, 2021)  explored \ngender differences  related to the perception of fake news, seeking to understand whether \nmen and women differ in their ability to detect disinformation, their concerns about it, and \nthe types of fake news they encounter.  The research finds that contrary to expectations, \nthere are less significant gender differences in how men and women perceive or detect fake \nnews  than expected . Both genders face similar challenges in identifying false content, rely \non similar trustworthiness cues, and encounter f ake news primarily about politics.   \nAmong noticeable differences is the increased concern about the spread of fake news \namong women. This, however, only appears among older respondents, with younger \ngenerations showing equal concern. Women cited Facebook and Instagram as the main \ndistributors of fake news while men cite d Twitter. Men were generally more interested in \npolitical news and consumed more news overall, but this did not translate into better \ndetection of fake news (Almenar, Aran -Ramspott, Suau, & Masip, 2021) . \nTo analyse the effect of political bias on the perception of fake news, a sample of 1000 US \ncitizens was studied (van der Linden, Panagopoulos, & Roozenbeek, 2020) . Respondents \noften characterized media outlets as fake news based on political leaning. 75% of \nconservatives believed CNN is fake news, while 59% of liberals considered Fox News as fake \n29 \n news. Conservatives were  also more likely to distrust mainstream media and associate the \nterm \"fake news\" with the media itself, whereas liberals tie it more to politics, especially \nTrump.  The findings indicate that conservatives are generally more distrustful of the media \nand more inclined to believe in conspiracies . The authors emphasize that the political \nclimate in the United States is very polarized, and that the findings might not apply in other \ncountries, especially those with multi -party systems.  \n \nFigure 11: Associations with fake news by political ideology (van der Linden, Panagopoulos, & Roozenbeek, 2020)   \n\n30 \n 2 History of Fake News  \nFake news is nothing new. While fake news was in the headlines frequently in the \n2016 US election cycle, the origins of fake news date back to before the printing \npress. Rumour  and false stories have probably been around as long as humans have \nlived in groups where power matters.  (Burkhardt, 2017, p. 5)  \nFake news is commonly connected directly to the 2016  and 2020  United States presidential \nelections,  the COVID -19 pandemic,  social media and the internet. Going by most definitions \nof fake news, however, several actions throughout history count as the spreading of fake \nnews. The main two reasons for publishing fake news appear to be a political bias or agenda \nand monetary gain. As Burkhardt states, fake news in some form probably had an effect on \nhumans ever since groups with power structures started forming . Power in a social group is \ndirectly connected to political positions and wealth.  \nEven before the invention of the printing press, humans wrote stories on walls, stones, \npapyrus. Leaders in these times generally had a lot of power over their subjects and could \ndirectly influence what was written about them. \u201cOften these messages were re minders to \nthe common people that the leader controlled their lives. Others were created to ensure  \nthat an individual leader would be remembered for his great prowess, his success in battle, \nor his great leadership skills.\u201d (Burkhardt, 2017)  The people at the time had hardly any \nmeans to check the validity of these claims and sometimes, modern historians have no \nmeans to check them either if those writings are the only object found. A related issue at \nthe time was also the low level of litera cy among the people, so those who could read had \nthe power to influence those who could not through intentionally misrepresenting in \nwriting. \nWith the invention of the printing press , the literacy rate increased, but so did the spread \nof written works. \u201cThis made the ability to write convincingly and authoritatively on a topic \na powerful skill. Leaders have always sought to have talented writers in their employ and to \ncontrol what informati on was produced.\u201d (Burkhardt, 2017)  Those in power, either \n31 \n politically as a leader or financially as an owner of a printing press, had control over what \ninformation people received once again.  \nDiscussion regarding political lying or \u2013 as we would now call it \u2013 fake news with a political \nagenda existed even back then ; it resembl ed the discussion of fake news today. In the \n1710s,  Jonathan Swift wrote \u201cPolitical Lying\u201d which included the following:  \nFalsehood flies, and truth comes limping after it, so that when men come to be \nundeceived, it is too late; the jest is over, and the tale hath had its effect: like a man, \nwho hath thought of a good repartee when the discourse is changed, or the \ncompany par ted; or like a physician, who hath found out an infallible medicine, after \nthe patient is dead. (Swift, 1710s)  \nSwift shared the idea that even if the fake news is proven wrong, it is always too late as \npeople have already been convinced. This can still be applicable today, especially with the \nlarge amount of news encountered on a daily basis.  \nEven before the internet, there were people who wrote hoax stories just to see h ow many \npeople w ould fall for them, something we would now call trolling. The most famous one \namong them being Edgar Allan Poe \u201cDuring his lifetime Poe would attempt a total of six \ndifferent hoaxes. Most modern anthologies fail to acknowledge that these stories were \noriginally published as non -fiction.\u201d (Tavistock Books, 2014)  His most famous hoax was a \nstory about a balloonist who travelled from Europe to the United States in a balloon in only \nthree days.  \nThe Sun  published a series of stories in 1835 regarding creatures inhabiting the moon, using \na legitimate scientist as a source and the stories were successful. \u201cA ploy by at least one \nperson at The Sun  who later admitted it was an attempt to boost circulation for the fledging \nNew York -based paper, the sham worked.\u201d (Haire, 2017)  This is a good example of fake \nnews with the explicit purpose of monetary gain.  \n32 \n With the introduction of the radio and television, fake news gained an even larger reach. \nBurkhardt (2017 ) lists two examples of radio broadcasts that started with an explanation of \nthe story being fiction and then continued with a fictional story about an attack.  Those who \nstarted listening late and missed the introduction, however, had no idea that the broadcasts \nwere fictional and both broadcasts caused a minor panic.  The first broadcast was Father \nRonald Arbuthnott Knox \u2019s \u201cBroadcasting the Barricades\u201d o n BBC radio  in January 1926, \nfeaturing an attack on London by Communists. The second broadcast was Orson Welles\u2019 \n\u201cWar of the Worlds\u201d broadcasted in the United States in 1938, where some people believed \nthat humanity was actually under attack by the Martians.  \nA word closely related to fake news is propaganda. Merriam -Webster defines propaganda \nas \u201cthe spreading of ideas, information, or rumor for the purpose of helping or injuring an \ninstitution, a cause, or a person\u201d and \u201cideas, facts, or allegations spread de liberately to \nfurther one's cause or to damage an opposing cause\u201d (Merriam -Webster, no date)  \nPropaganda is usually used to refer to use of information in order to benefit in a war  or to \nstrengthen political power , but the word has origins in the Catholic Church. \u201cIn 1622, Pope \nGregory XV established the Congregatio de Propaganda Fide (Congregation for Propagating \nthe Faith) for the purpose of promoting the faith in non -Catholic countries. The group\u2019s \nname was ofte n informally shortened to \u201cpropaganda,\u201d and the name stuck.\u201d (Zoschak, \n2014)  What we now call propaganda existed before the name itself with several ancient \nleaders using propaganda to increase their power or to harm their opponents.  \nThe British were very adept at using propaganda during the First World War and their \npropaganda is credited with convincing the United States to join the war. Adolf Hitler \npersonally studied British propaganda and use d it with the help of Goebbels to justify \natrocities. \u201cWhen the true horrors of Nazi Germany came to light, the extreme power of \npropaganda was terribly apparent. The word \u201cpropaganda\u201d soon developed a negative \nconnotation, one that it still carries to this  day in the English -speaking world.\u201d (Zoschak, \n2014)   \n33 \n 3 Fake News and the COVID -19 Pandemic  \nWhile the 2016 US election put fake news in the limelight in the United States, and some \nevents like the Brexit vote in the United Kingdom did so in other countries, the COVID -19 \npandemic made fake news a prominent issue across the globe . The pandemic a ffect ed not \njust politics, but public health, safety, and trust in institutions globally . The pandemic's scale \nand the rapid spread of misinformation alongside it was unprecedented.  \nAs the world faced an unprecedented public health crisis, misleading claims about the \nvirus\u2019s origins, transmission, and treatment proliferated, creating confusion and distrust \namong the public. Various  theories, such as the claim that 5G technology caused the virus, \nor that the pandemic was a government hoax, gained traction and diverted attention from \ncredible scientific information. This wave of misinformation complicated efforts to control \nthe virus,  as it undermined public health messages and contribut ed to vaccine hesitancy, \nresistance to lockdown measures, and the rejection of mask mandates.  \n \nFigure 12: Increase in searches for coronavirus and 5G between December 2019 and October 2020 (Nsoesie, Cesare, \nM\u00fcller, & Ozonoff, 2022) . \n\n34 \n Public trust in governments and the media was severely undermined during the COVID -19 \nepidemic due to changing government policies and scientific data. \u201cDuring the early phase \nof the pandemic, positions on general mandated face mask use were highly divergent across \ncountries and subject to change within countries . Several countries discouraged the use of \nface masks due to a lack of evidence of its effectiveness  in preserv ing limited supplies for \nhealth care and due to concerns about risk compensation in the  form of lowering \ncompliance with other measures . In response to changes in advice from the WHO and with \nmore studies proving the effectiveness of masks, face mask regulations became more \nuniform and accepted during later phases of the COVID -19 pandemic \u201d (Wismans, et al., \n2022) . Although  this change was motivated by newly available information, some people \nwere sceptical because they saw it as an indication of inconsistency or ineptitude on the \nside of the authorities.  The media\u2019s role in spreading  these  messages often worsened  the \nsituation, as conflicting reports and headlines further muddied the waters, contributing to \nwidespread public disillusionment.  \nOne study (Rocha, et al., 2023)  analysed the impact of fake news, specifically on social \nmedia, on physical and mental health of the population. Misinformation about the disease \nled to widespread confusion and fear, contributing to social unrest, attacks on health \nprofessionals, and dangerous behavio urs such as methanol consumption, which resulted in \nnumerous deaths and hospitalizations. Studies reveal ed that people across the globe  were \naffected by this misinformation, with many trusting unverified online content . While some \nresearch su ggests that not all false news is harmful, the general impact has been negative, \nleading to increased anxiety, stress, and other psychological problems among people of all \nages.  \n97 articles published in 2021 on fake news during the pandemic were studied (Balakrishnan, \nNg, Soo, Han, & Lee, 2022) , including some discussing AI detection of fake news. The authors \nconcluded their research with five main findings. The number of studies about fake news \nincreased in line with the increase in fake news dissemination during the pandemic. Most \n35 \n of these studies administered online surveys, while others used AI -based tools. The key \nmotives for the spread of fake news, as reported by the analysed studies, were lack of \nknowledge, altruism, low trust in the government, personal gain, entertainment and \ninformation seeking. The main sociodemographic factors for increa sed fake news spreading \nwere younger age, being male, high internet and social media along with low education and \nincome. Most of the AI-based  studies focused on fake news detection using d eep learning \nmodels. The main topics in fake news during 2021 were all related to the pandemic, such \nas vaccine s, the virus, remedies and treatment s. \nIt is important to note that the COVID -19 pandemic coincided with three major events in \nthe United States, which were also closely intertwined with the spread of misinformation. \nThe first major event was the murder of George Floyd by police in Minneapolis,  which was \nfollowed over a year of protests across the United States. The second event was the 2020 \npresidential election, with the incumbent president Donald Trump losing to former vice \npresident Joe Biden. The outcome of the election resulted in the thir d major event, the \nprotests at the Capitol Building in January of the next year. These events elevated the \ngeneral distrust in news and spread of misinformation which was already present due to \nthe pandemic, mainly in the United States and to a lower extent throughout the re st of the \nworld.  \n \n \n \n  \n36 \n 4 The Role of Artificial Intelligence (AI) in Fake News \nWhile people have always been s ceptical of written text and second -hand information, they \ntend to trust photographs, sound recordings, and especially video recordings much more. \nThis trust in images has been declining for decades, starting with Stalin\u2019s era, when images \nwere altered to e rase people from historical records. This early form of photo manipulation \nshowed how images could be edited to fit a particular narrative.  The rise of digital tools like \nPhotoshop further eroded trust in photographs by maki ng image editing more accessible \nand sophisticated. Video and sound recordings, on the other hand, have received a high \ndegree of trust until recently as the only real way to misinform using these types of media \nwas by providing a false context or splicing  video and audio to make something sound better \nor worse.  With the fast -paced advances in Artificial Intelligence in the past couple of years, \nediting or completely fabricating video and audio to misinform has become monumentally \neasier and widely accessib le. \nTo demonstrate how fast-paced the advances in AI are, we created an image using the same \nprompt , \u201ccrowd watching a presidential speech USA color photograph ,\u201d using two different \nfree image generators. The first image was created using \u201c DALL\u00b7E mini \u201d, one of the first \nwidely available free image generators, created in 2021, still available at \nhttps://huggingface.co/spaces/dalle -mini/dalle -mini . The second image was generated \nusing \u201cMagic Media\u201d, a free up-to-date tool on the online graphic design platform Canva \n(https://www.canva.com/ai -image -generator ). These generators were chosen to compare \nwhat image generation was available for free to anyone in 2021 and what is available now.  \n37 \n  \nFigure 13: Image created using DALL\u00b7E mini  (https://huggingface.co/spaces/dalle -mini/dalle -mini ) \n\n38 \n  \nFigure 14: Image created using Canva Magic Media ( https://www.canva.com/ai -image -generator ) \n  \n\n39 \n The image created using the generator available in 2021 is easily dismissed as unrealistic, \nas it looks more like an oil painting and makes it hard to distinguish anything besides the \nlarge American flag. The image created using a more up -to-date tool stil l has severe flaws, \nespecially when looking closer at the crowd in the back. It is, however, convincing enough \nto be used for something smaller, like a thumbnail for a news article, where the details could \nnot be distinguished.  \nIt is important to keep in mind that this is a free tool created to generate images based on \nalmost any prompt. A more sophisticated tool, which is potentially created for specific \npurposes, can create much more convincing images. The Canva website describ es AI image \ngeneration process as follow s:  \nTo create AI -generated images, the machine learning model scans millions of images \nacross the internet along with the text associated with them. The algorithms spot \ntrends in the images and text and eventually begin to guess which image and text fit \ntogeth er. Once the model can predict what an image should look like from a given \ntext, it creates entirely new images for you to choose from . (Canva, 2024)   \nIf we wanted to create a perfect picture for our prompt, we would have to feed an AI image \ngenerator  thousands of images of crowds and political rallies. An image created in this way \ncould be used to convince the public that a certain event had more people in attendance \nthan it actually did.  \nSimilarly to how AI image generation works  by learning from thousands to millions of \nimages, AI can be used to learn from samples of someone speaking to learn their voice. This \ncan then be used to make audio recordings od that person saying anything we want them \nto say, even words which that person  has never uttered before. This technology has been \nrapidly advancing alongside image generation. AI -generated speech used to be \nmonotonous, have trouble pronouncing some words and have severe issues with inflecti on. \nNow AI-generated speech can be very smooth and hardly distinguishable from the real \nperson. To mimic someone\u2019s voice accurately used to require several minutes  of recording, \n40 \n limiting the use to those who often speak in public, such as politicians. Some AI voice cloning \nmodels now claim to only require seconds, rendering it usable on nearly everyone. \u201c OpenAI \nis offering limited access to a text -to-voice generation platform it developed called Voice \nEngine, which can create a synthetic voice based on a 15 -second clip of someone\u2019s voice. \nThe AI -generated voice can read out text prompts on command in the s ame language as \nthe speaker or in a number of other languages \u201d (David, 2024) . Such AI can be used to \nfabricate speeches, phone calls and secret recordings of politicians and other people, and \nconvince the public that they said something which never left their mouth s. \nThe main example of AI usage in video are so -called \u201cdeepfakes\u201d. A common form of \ndeepfakes is to replace the face of a person in a video with the face of another. This is used \nto place the face of a recognizable person, such as a politician, on the body of someone \npotentially doing something negative to frame the recognizable person. The technology to \nreplace a face in a video is also commonly used in pornography to replace the face of a porn \nactor with that of a celebrity in order to create an illusion th at the celebrity created \npornographic material. These pornographic deepfakes were prevalent as far back as 2019. \n\u201cThe AI firm Deeptrace found 15,000 deepfake videos online in September 2019, a near \ndoubling over nine months. A staggering 96% were pornographic and 99% of those mapped \nfaces from female celebrities onto  porn stars \u201d (Sample, 2020) .  \nDeepfake technology can also be used to edit the lip movements in a video to match a \ndifferent audio recording. This can be used in conjunction with AI-generated  cloned voice \nmentioned earlier to create a convincing video of a recognizable person, such as a politician, \nsaying something in order to misinform the public about them.  \nRecently , AI video generation has started to improve to the point where it can create basic \nvideos from text prompts similarly to AI image generation. As this technology improves, AI \ncould be used to create videos for misinformation from the ground up without requi ring \nany base video to edit.  \n  \n41 \n 5 Perception of Fake News Survey  \nFor the purpose of studying the perception of fake news by online users, two surveys were \ncarried out, one in 2018 and one in 2024.  \n5.1  Research  Questions  and Hypotheses  \nThe goal of the survey was to identify what sources people trust, how often they encounter \nfake news, how they deal with fake news, and how well they  can identify fake news.  \nThe research  questions for the survey are  the following : \n\u2022 How often do people encounter fake news?  \n\u2022 Where do people usually get their news?  \n\u2022 What news organisations do people trust?  \n\u2022 Do people trust local or national/international news more?  \n\u2022 How much do people trust social media?  \n\u2022 How many sources do people check?  \n\u2022 What do people consider as the main reason for fake news?  \n\u2022 How do people try to identify fake news?  \n\u2022 Are there any significant differences between regions when it comes to identifying fake \nnews?  \n\u2022 Are there any significant differences between levels of education when it comes to \nidentifying fake news?  \n\u2022 How has the COVID -19 pandemic affected people\u2019s trust in news?  \nFollowing  these questions , we can form  the following hypotheses:  \n\u2022 Hypothesis 1: Most people encounter fake news very often.  \n\u2022 Hypothesis 2: Most people get their news online.  \n\u2022 Hypothesis 3: People mainly trust large news organisations.  \n\u2022 Hypothesis 4: People trust national/international news more than local news.  \n42 \n \u2022 Hypothesis 5: People do not trust social media as a source of news.  \n\u2022 Hypothesis 6: People usually check only one source.  \n\u2022 Hypothesis 7: People consider political agendas as the main reason for fake news.  \n\u2022 Hypothesis 8: People mainly try to identify fake news by looking at other news websites.  \n\u2022 Hypothesis 9: North Americans are more capable of identifying fake news.  \n\u2022 Hypothesis 10: Higher education provides better capabilities for identifying fake news.  \n\u2022 Hypothesis 11: Trust in news and news organizations has generally lowered due to the \nCOVID -19 pandemic.  \n5.2  Survey  Sample Size, Target  and Methodology  \nTwo surveys were carried out in two different periods. The first survey was carried out \nonline using the SurveyMonkey service, the second survey was carried out online using the \nGoogle Forms service. Both were posted on Facebook and Twitter/X, where they were \nshared by several users. The surveys were also post ed on the political board of the website \n4chan and several political subreddits on Reddit along with subreddits dedicated to survey \nsharing. This was done in an attempt to have people of different poli tical leanings, age \ngroups and interest groups (who are, however, familiar with the use of internet and social \nmedia ) take the survey.  \nThe first survey was open and posted several times throughout April 2018.  \nThe second survey was open and posted several times throughout July 2024.  \nAs is common with online survey s that offer no reward, the response to the first survey was \nminimal , as there were only 104 usable responses. Thus, the second survey used an \nincentive of selecting 3 respondents at random to receive a \u20ac10 Paysafecard. This did little \nto aid the number of responses, as we received 150 usable responses. A potential reason \nfor a minimal increase in responses is the general disdain people developed in the 6 years \nbetween the surveys. Another potential reason is the rise in popul arity of services where \npeople can take surveys for a guaranteed payment instead of a having chance  to receive \n43 \n one. This low number of responses potentially means that the survey may not be \nstatistically significant or applicable to a larger population, but it can serve as a basis for a \nlarger -scale  survey and as a starting point  for discussion . This lower number of the \nrespondents  also means that in several cases , a statistical analysis of significance (e.g. , \nPearson\u2019s Chi -Square test) is not possible due to several groups not reaching the required \nnumber of responses . \nFor most of our statistical analysis we will be using Pearson\u2019s Chi -square Test for \nIndependence. This is a t est on \u201c how likely it is that an observed distribution is due to \nchance. It is also called a \u2018goodness of fit \u2019 statistic, because it measures how well the \nobserved distribution of data fits with the distribution that is expected if the variables are \nindependent \u201d (Ling 300, 2008) . We use this test for the inverse purpose to see how probable \nit is that our variables are dependent and have a potential correlation. The output of this \ntest we are most interested in is the p -value or probability value of statistical significance. \nThe cutoff value of dependence between two variables is 0.05. Anything below shows a \npotential correlation between the tested variables ; the lower the number , the higher the \nprobability of dependence.  \n5.3  Survey Structure  \nBoth surveys started with questions asking for gender, age, region and education level to \nestablish the demographics of those surveyed. All four demographic questions allowed the \nrespondents to select the \u201cI\u2019d rather not answer\u201d option. The main part of bo th surveys \nconsisted of eleven questions inquiring about the news sources respondents trust, how they \nengage with news and their opinions  about  fake news and those who publish it. The 2018 \nsurvey featured eight more questions, where the respondents were gi ven the title of a news \narticle and were asked to determine whether that news article was real or fake news based \non the headline alone. The 2024 survey ended with a question about the impact of the \nCOVID -19 pandemic on the trust in news publishers.  \n44 \n 5.4  Demographics  \n5.4.1 Gender  \n \nGraph 1: Gender structure of the samples  \nThe difference between the gender distribution in both surveys is very apparent. In 2018 , \nmost respondents were male , and no respondent selected \u201cother\u201d or \u201cI\u2019d rather not \nanswer\u201d. In 2024 , the number of male and female responses was almost even, with a small \nnumber of the respondents  selecting \u201cother\u201d or \u201cI\u2019d rather not answer\u201d as well. There are \nseveral potential reasons for this difference. The ratio of women on websites where the \nsurvey was posted grew, although not significantly. Reddit was reported to  have a 69% male \nuserbase in 2016  (Statista Research Department, 2016) , and a 63.6% male userbase in 2024 \n(Duarte, 2024) . Another possible reason is the difference in where the majority of responses \ncame from. While we did not directly track the source of each response, the survey was \nposted on different websites at different times , so it was somewhat possible to track how \nmany responses came from each post. In 2018 , a lot more responses came from 4chan, \nwhile in 2024 , a lot more responses came from social media sites. 4chan generally has a \nmore male -dominated  userbase compared to social media sites , as its page with 79; 76.0%25; 24.0%2018 Gender\nMale Female70; 46.7%\n71; 47.3%6; 4.0%3; 2.0%2024 Gender\nMale Female Other Rather not answer\n45 \n information  for advertisers states a 70% male demographic (4chan, 2024) , which  might be \neven more skewed on the political board this survey was posted on. The appearance of \nthose who answered \u201cother\u201d might be a result of gender becoming a more prominent topic \nof discussion in recent years . \n5.4.2 Age \n \nGraph 2: Age structure of the samples  \nThe majority of the respondents  in 2018 were aged 18 -24, while the distribution of age \ngroups in 2024 was more even. This might again be the result of where the individual \nresponses came from, as Facebook, which resulted in more responses in 2024, generally \ntends to have an older user b ase than 4chan, which resulted in more responses in 2018. In \n2024 , 30.8% of Facebook users belong to the 25 -34 age range, followed by 22.6% for 18 -24 \nand 20.2% for 35 -44, while 26.4% of Facebook users fall in the age range  above 45 (OBERLO, \n2024) . 4chan reports a userbase age of 18 -34 on their page  with information  for advertisers \n(4chan, 2024) . 8; 7.7%\n61; 58.7%22; 21.2%7; 6.7%3; 2.9% 3; 2.9%2018 Age\nUnder 18 18-24 25-30\n31-40 41-50 Over 5014; 9.3%\n35; 23.3%\n39; 26.0%28; 18.7%16; 10.7%16; 10.7%2; 1.3%2024 Age\nUnder 18 18-24\n25-30 31-40\n41-50 Over 50\nRather not answer\n46 \n 5.4.3 Location  \n \nGraph 3: Location structure of the samples  \nIn both cases , the largest group of  the respondents was European. In 2018 , the ratio of \nNorth American respondents was higher. This is potentially the result of the change of \nattitude towards news and fake news in the United States. Another possibility is once again \nthe source of individual responses, as websites like Reddit and  4chan , where the majority \nof responses came from the first time, are global and predominantly American, while social \nnetworks tend to connect you with people closer to you. The resp onses in 2024, however, \nfeatured more regions.  60; 57.7%41; 39.4%1; 1.0% 2; 1.9%2018 Location\nEurope\nNorth America\nMiddle East, North Africa, and Greater Arabia\nAustralia and Oceania99; 66.0%37; 24.7%1; 0.7%5; 3.3%3; 2.0% 1; 0.7%4; 2.7%2024 Location\nEurope\nNorth America\nMiddle East, North Africa, and Greater Arabia\nAustralia and Oceania\nSouth America\nSub-Saharan Africa\nAsia\n47 \n 5.4.4 Education  \n \nGraph 4: Education structure of the samples  \nThe distribution of education was leaning somewhat higher in 2024 compared to 2018. This \nis most likely the result of the higher average age of the respondents.  \n5.5  Results  \n5.5.1 News Sources  \nFor this question , the respondents were asked to select all applicable sources  from which  \nthey usually get their news. The options were \u201cNews websites\u201d, \u201cSocial media\u201d, \u201cTV\u201d, \n\u201cRadio\u201d, \u201cNewspapers, Magazines\u201d, \u201cForums, blogs, Reddit\u201d, \u201cYouTube, video sharing sites\u201d \nand the ability to select \u201cother\u201d and enter own answers.  7; 6.7%\n27; 26.0%\n28; 26.9%3; 2.9%28; 26.9%7; 6.7%1; 1.0% 3; 2.9%2018 Education\nLess than high school High school\nSome college Associate degree\nBachelor's degree Master's degree\nProfessional Degree Rather not answer10; 6.7%\n23; 15.3%\n23; 15.3%\n4; 2.7%\n41; 27.3%34; 22.7%3; 2.0%4; 2.7% 8; 5.3%2024 Education\nLess than high school High school\nSome college Associate degree\nBachelor's degree Master's degree\nProfessional Degree Rather not answer\nDoctorate\n48 \n  \nGraph 5: News sources used by respondents  \nIn 2018 , the most common response was \u201cForums, boards, blogs, Reddit\u201d. This was most \nlikely due to more responses from 4chan and Reddit compared to the 2024 survey, as both \nwebsites fall into that category. This was followed by news websites and social media, whic h \nwere the top choices in 2024. The popularity of YouTube (and other video -sharing  \nwebsites), radio and print ha ve somewhat significantly decreased, while the popularity of \nTV has slightly increased. The reason for the increase in the popularity of  TV is very likely \ndue to the increase in the average age of the respondents between 2018 and 2024. In both \nyears , respondents listed podcasts, news apps and word of mouth under \u201cother\u201d.  \nMore than half of the respondents usually get their news on social networks , which are \npotentially the most common medium spreading  fake news.  9.3%22.0%26.7%38.0%42.0%44.0%61.3%63.3%\n5.8%30.8%35.6%35.6%61.6%49.0%58.7%59.6%\n0.0% 10.0% 20.0% 30.0% 40.0% 50.0% 60.0% 70.0%OtherPrintRadioTVForums, blogsYouTubeSocial mediaNews websitesNews sources in % of responses\n2018 2024\n49 \n  \nGraph 6: Comparison of news sources based on gender  \nFor easier and more statistically significant gender -based analysis, the respondents who \nanswered \u201cother\u201d and \u201cI\u2019d rather not answer\u201d were removed fr om this step. Using \nPearson\u2019s Chi -Square test, we found a statistically significant association between gender \nand news sources in 6 cases. In 2018 , female respondents were more inclined to use social \nmedia as a source of news (\u03c72 = 4.083, p = 0.043 ). In 2018 , women were also more likely to \nlist TV as one of their news sources (\u03c72 = 3.873, p = 0.049). These two disparities did not \nreappear in 2024. In 2018 , male respondents used forums as their news source more often \n(\u03c72 = 4.277, p = 0.039). This is the only disparity to reappear in 2024 (\u03c72 = 10.026, p = 0.002). \nIn 2024 , female respondents were more likely to use news websites as their news source \n(\u03c72 = 3.966, p = 0.046). Men, however, were more inclined to use YouTube and other video \nsharing sites as a news so urce in 2024 (\u03c72 = 7.828, p = 0.005).  \nThe preference for forums, boards, blogs and Reddit among male respondents does seem \nto align with the predominantly male userbase of those websites.  54.3%48.1%55.7%57.0%52.9%67.1%37.1%30.4%64.3%53.2%\n31.0%52.1%71.8%68.0%26.8%44.0%39.4%52.0%57.7%76.0%\n0.0% 10.0% 20.0% 30.0% 40.0% 50.0% 60.0% 70.0% 80.0%2024 YouTube2018 YouTube2024 news websites2018 news websites2024 forums2018 forums2024 TV2018 TV2024 social media2018 social mediaGender comparison for news sources\nfemale male\n50 \n  \nGraph 7: Comparison of news sources based on age range  \nFor age analysis , we split the respondents in to groups of those under and over the age of \n30, while ignoring the respondents who declined to state their age range. Pearson\u2019s Chi -\nSquare tests were used to determine statistically significant differences between the two \nage groups. Despite TV, radio , and print being more popular among those over 30 and social \nmedia, forums and video sharing sites being more popular among those under 30, none of \nthe results from 2018 showed any statistical significance. This mi ght be in part due to a \nlower sample of the respondents over 30 in the 2018 survey. In 2024 , three news sources \nshowed a statistically significant difference between the two age ranges. Respondents \nunder 30 were much more likely to get their news from social media (\u03c72 = 17.83, p < 0.001) \nand YouTube along with other video sharing websites (\u03c72 = 7 .001, p = 0.008). On the other \nhand, respondents over 30 were more likely to use TV as one of their news sources (\u03c72 = \n5.630, p = 0.018).  44.6%33.3%47.3%47.6%33.8%42.9%\n78.4%53.0%28.4%32.5%55.4%62.7%\n0.0% 10.0% 20.0% 30.0% 40.0% 50.0% 60.0% 70.0% 80.0% 90.0%2024 YouTube2018 YouTube2024 TV2018 TV2024 social media2018 social mediaAge range comparison for news sources\nunder 30 over 30\n51 \n  \nGraph 8: Comparison of news sources based on location  \nDue to the small number of the respondents from other regions, we only used Europe and \nNorth America for this analysis. Pearson\u2019s Chi -Square test showed statistical significance in \n3 cases. In 2018, European respondents were much more likely to use TV as a news source \ncompared to those from North America (\u03c72 = 5.641, p = 0.018). This did not repeat in 2024 \nand both regions listed TV as a news source at about the same frequency. Those from North \nAmerica listed forums, boards and Reddit as one of their news s ources more often in 2018 \n(\u03c72 = 4.044, p = 0.044). North Americans were still more likely to use these as their news \nsource in 2024 but not to a statistically significant level. In 2024 , European respondents \nlisted YouTube and other video sharing sites as one of their news sources more often than \ntheir North American counterparts (\u03c72 = 4.268, p = 0.039). This difference was also present \nin 2018 but not to the same extent.  \n1.1.1 Trusted News Organizations  \nRespondents were asked to select which news organizations they f ound trustworthy. One \nof the options was \u201cI don\u2019 t trust any news organizations\u201d, while other options were a list of \nthe most popular news organizations, alongside with an option to select \u201cother\u201d and list any \nnews organizations not listed.  29.7%39.0%45.9%73.2%37.8%22.0%\n49.5%55.0%36.4%53.3%40.4%45.0%\n0.0% 10.0% 20.0% 30.0% 40.0% 50.0% 60.0% 70.0% 80.0%2024 YouTube2018 YouTube2024 forums2018 forums2024 TV2018 TVLocation comparison for news sources\nEurope North America\n52 \n  \nGraph 9: News organizations trusted by respondents  \nThe most popular choices for both years were BBC, CNN , and the New York Times . In 2018, \n24% of respondents stated that they d id not trust any news organizations. This increased to \nalmost 31% in 2024. The news organizations that saw a decrease in trust between 2018 and \n2024 were the New York Times , the Wall Street Journal , Time , and Fox News , with Time \u2019s \ntrust decrease being the largest , as they lost more than half.  One possible reason for the \ndecrease in the popularity of Time  could be the change in ownersh ip, as the magazine \nchanged its ownership in both 2017 and 2018 (Historic Newspapers, 2020) . The other \npossible reason could be Time \u2019s continued decline based on several articles dated between \n2013 and 2017 citing its inability to adapt, with one article stating \u201c one of the key reasons \nfor Time Inc.\u2019s decline was its inability to figure out the internet \u201d (Nocera, 2017) .  The news 6.0%7.3%9.3%14.7%15.4%22.0%24.7%25.3%29.3%30.7%30.7%42.0%46.7%\n4.8%7.7%19.2%7.7%19.2%17.3%22.1%18.3%31.7%24.0%26.0%36.6%39.4%\n0.0% 5.0% 10.0% 15.0% 20.0% 25.0% 30.0% 35.0% 40.0% 45.0% 50.0%Daily MailFox NewsTimeAl JazeeraWall Street JournalAssociated PressGuardianReutersNew York TimesNO TRUSTCNNOtherBBCTrusted news organizations in % of responses\n2018 2024\n53 \n organizations that saw the largest increase in trust were Al Jazeera , Reuters  and BBC. In \n2018 , three respondents listed the Washington Post  and another three listed CBC under \nother. In 2024 , the Washington Post  was added as one of the options with 29 respondents , \nor 19.3% , selecting it. For a simpler comparison to 2018, these were added to \u201cother\u201d. Most \nof the news organizations listed under \u201cother\u201d were national organizations of smaller \ncountries or local news organizations.  \n \nGraph 10: Comparison of trusted news organizations based on location  21.6%19.5%10.8%31.7%43.2%31.7%56.8%31.7%27.0%34.1%43.2%51.2%48.6%56.1%\n32.3%20.0%9.6%8.3%20.2%8.3%11.1%6.7%11.1%6.7%24.2%16.7%48.5%28.3%\n0.0% 10.0% 20.0% 30.0% 40.0% 50.0% 60.0%2024 NO TRUST2018 NO TRUST2024 Time2018 Time2024 Reuters2018 Reuters2024 AP2018 AP2024 WSJ2018 WSJ2024 NYT2018 NYT2024 BBC2018 BBCLocation comparison for trusted news organizations\nEurope North America\n54 \n Comparing the trust in news organizations between European and North American \nrespondents with the help of Pearson\u2019s Chi -Square test led to some interesting results. \nWhile Europeans were slightly more incline d to select \u201cI don\u2019t trust any news organizations\u201d \nin 2024, the difference was not statistically significant (\u03c72 = 1.486, p = 0.223). In 2018 , both \nregions opted for distrust in news organizations at almost the same rate. Despite the lack \nof significant difference in distrust, North American respondents showed significantly more \ntrust in several news organizations. While this makes more sense fo r the New York Times , \nthe Wall Street Journal , Associated Press , and Time , as those organizations are based in the \nUnited States, both BBC and Reuters  are based in the United Kingdom, yet North Americans \nstill trust them more often than Europeans. Europeans were more likely to select 'other' \nand enter news organizations, presumably from their own countries. BBC and Time  only \nsaw a statistically significant increased trust from North Americans in 2018 (BBC: \u03c72 = 7.849, \np = 0.005, Time: \u03c72 = 9.086, p = 0.003). The New York Times , the Wall Street Journal , \nAssociated Press and Reuters  saw a statistically significant increas ed trust from North \nAmericans in both 2018 (NYT: \u03c72 = 13.670, p < 0.001, WSJ: \u03c72 = 12.558, p < 0.001, AP: \u03c72 = \n10.910, p = 0.001, Reuters: \u03c72 = 9.086, p = 0.003) and in 2024 (NYT: \u03c72 = 4.684, p = 0.030, \nWSJ: \u03c72 = 5.225, p = 0.022, AP: \u03c72 = 31.188, p < 0.00 1, Reuters: \u03c72 = 7.347, p = 0.007).  \n5.5.2 Local vs National/ International News Organizations  \nThis survey question asked the respondents which they find more trustworthy when given \nthe following options \u201cLocal news organizations\u201d, \u201cNational/international news \norganizations\u201d and \u201cEqual trust/distrust in local and (inter)national news organizations\u201d.  \n55 \n  \nGraph 11: Trust in local vs (inter)national news organizations by respondents  \nThe most apparent difference between 2018 and 2024 was the increase in respondents who \nselected equal trust or distrust in both local and national/international news organizations. \nIn 2024 , the two options were almost equal, while in 2018 , national and international news \nhad a slight edge over local news.  \n5.5.3 Social Media as a Source of News \nRespondents were asked to select how much they trust social media as a source of news \nusing the following options: \u201cIt is a reliable source of information\u201d, \u201cIt is a good starting \npoint but I do additional research elsewhere\u201d, \u201cIt is rarely reliable\u201d and \u201c It is completely \nunreliable\u201d. There was also an option to select \u201cother\u201d and specify their position.  26; 25.0%\n31; 29.8%47; 45.2%2018 Trust in local vs \n(inter)national\nLocal (Inter)national Equal34; 22.7%\n35; 23.3%81; 54.0%2024 Trust in local vs \n(inter)national\nLocal (Inter)national Equal\n56 \n  \nGraph 12: Trust in social media by respondents  \nBoth in 2018 and 2024 , the most common response was using social media as a starting \npoint for news, followed by social media being rarely reliable and completely unreliable. \nUsing social media as a starting point increased between 2018 and 2024 , while considering \nsocial media rarely or completely unreliable decreased. In 2018 , no respondent considered \nsocial media as a reliable source of news on its own, while in 2024 , roughly 5% did. This \nseems to indicate that the general trust in social media increased slightly between 2018 and \n2024. Two of the respondents who selected \u201cother\u201d wrote that the reliability of social media \nas a source of news depends on several factors, such as the groups you follow. One \nrespondent stated that they do not use social media.  \n5.5.4 Number of News Sources  \nThis question asked the respondents how many news sources they check before they make \nup their mind s on a certain event in the news. The available options were \u201c1\u201d, \u201c2 -3\u201d and \n\u201cmore than 3\u201d.  56; 53.8%\n30; 28.8%16; 15.4%2; 1.9%2018 Trust in social media\nReliable Starting point Rarely\nUnreliable Other7; 4.7%\n90; 60.0%36; 24.0%16; 10.7%1; 0.7%2024 Trust in social media\nReliable Starting point Rarely\nUnreliable Other\n57 \n  \nGraph 13: Number of news sources used by respondents  \nBoth in 2018 and 2024 , most respondents tend ed to check 2 -3 sources, followed by those \nwho checked  more than 3, while the lowest number of the respondents chec ked only 1 \nnews source. The percentage of those who check 2 -3 sources increased between 2018 and \n2024 , while the percentage s of those who check either less or more decreased. 2 -3 source \nrange appears to be the one most people fall into, and the percentage of those who do so \nis increasing.  \n5.5.5 Sources of Different Political Leaning  \nRespondents were asked how often they tend to check sources of different political leaning \nto see how a piece of news might be reported differently. The given options were \u201calways\u201d, \n\u201cusually\u201d, \u201csometimes\u201d, \u201crarely\u201d and \u201cnever\u201d.  9; 8.7%\n74; 71.2%21; 20.2%2018 Number of sources\n1 2 to 3 More than 36; 4.5%\n106; \n79.7%21; 15.8%2024 Number of sources\n1 2 to 3 More than 3\n58 \n  \nGraph 14: Tendency to check news sources of different political leaning by respondents  \nRespondents most commonly claimed that they sometimes check sources of different \npolitical leanings, followed by those who usually and rarely do so. Very few respondents \nclaimed that they either always or never check sources of different political leaning. The \npercentages of each choice are very similar between 2018 and 2024 indicating no shift in \nthe tendency of checking political bias.  \n5.5.6 Trust Based on Headlines  \nThis question asked how often respondents trust or distrust a news article based solely on \nthe headline alone without reading the rest of the article. The given options were \u201calways\u201d, \n\u201cusually\u201d, \u201csometimes\u201d, \u201crarely\u201d and \u201cnever\u201d.  6; 5.8%\n34; 32.7%\n37; 35.6%20; 19.2%7; 6.7%2018 Checking sources of \ndifferent political leaning\nAlways Usually Sometimes Rarely Never10; 6.7%\n45; 30.0%\n56; 37.3%28; 18.7%11; 7.3%2024 Checking sources of \ndifferent political leaning\nAlways Usually Sometimes Rarely Never\n59 \n  \nGraph 15: Trust or distrust in news based on headline alone by respondents  \nThe largest group in both 2018 and 2024 claimed that they sometimes make up their mind \nbased on the headline alone ; the percentage of this choice grew slightly between 2018 and \n2024. The second most popular option was rarely trusting or distrusting based on headline \nalone. The popularity of this option grew significantly between 2018 and 2024, almost \nreaching the perce ntage of those who selected \u201csometimes\u201d. The third most popular option \nin 2018 was \u201cusually\u201d, but the popularity of this option decreased in 2024, resulting in it \nbeing surpassed by those who chose \u201cnever\u201d. The smallest group of responses in both cases \nwas always trusting or distrusting news articles based on headline s alone. These results \nshow a trend towards lesser decision -making based on headlines alone.  \n5.5.7 Frequency of Encountering Fake News \nRespondents were asked how often they encounter something they consider to be \u201cfake \nnews\u201d. The given options were \u201calways\u201d, \u201cusually\u201d, \u201csometimes\u201d, \u201crarely\u201d and \u201cnever\u201d.  4; 3.8%\n19; 18.3%\n42; 40.4%26; 25.0%13; 12.5%2018 Trust based on headline\nAlways Usually Sometimes Rarely Never2; 1.3%\n17; 11.3%\n55; 36.7%51; 34.0%25; 16.7%2024 Trust based on headline\nAlways Usually Sometimes Rarely Never\n60 \n  \nGraph 16: Frequency of encountering fake news by respondents  \nBoth in 2018 and 2024 not a single respondent selected the option \u201cnever\u201d, indicating that \neveryone has to deal with fake news at least every once in a while. The most commonly \nselected option was \u201csometimes\u201d, but the frequency of this option decreased somewhat \nbetween 2018 and 2024, with the increase of those who usually encounter fake news. This \nindicates a small trend towa rds encountering fake news becoming more common.  \n5.5.8 Most Common Reason Behind Fake News \nThis survey question asked respondents what is , in their opinion , the most common reason \nbehind the fake news they encounter. The available answers were \u201cpolitical bias/agenda\u201d, \n\u201cPaid promotion of a product/service\u201d, \u201ccharacter assassination\u201d, \u201csatire/comedy\u201d, \u201chonest \nmistake or bad sources\u201d, \u201crushing the news about an event to be the first to post them\u201d, \u201cI \nnever encounter fake news\u201d and the ability to select \u201cother\u201d and write in an answer.  5; 4.8%\n33; 31.7%\n55; 52.9%11; 10.6%2018 Frequency of  fake \nnews\nAlways Usually Sometimes Rarely8; 5.3%\n58; 38.7%\n71; 47.3%13; 8.7%2024 Frequency of  fake \nnews\nAlways Usually Sometimes Rarely\n61 \n  \nGraph 17: The most common reason behind fake news encountered by respondents  \nAs expected, based on the previous question, no respondent selected the \u201cI never \nencounter fake news\u201d option in either 2018 or 2024. The most common response in both \nyears was political bias or agenda, but the popularity of this choice decreased from 66.3% \nto 54.7% between 2018 and 2024. The second most popular selection was \u201cpaid promotion \nof a product/service\u201d. This option was selected more often in 2024 (20.7%) compared to \n2018 (12.5%), gaining most of the percentage that political bias or agenda lost. T he third \nmost common response was inaccuracy due to rushing to be the first to post news, followed \nby the remaining three options at roughly the same rate. These four options saw no major \nchange in popularity between 2018 and 2024. The most common written responses under \nother were clickbait and monetary gain in both 2018 and 2024. In 2024 , one respondent \nmentioned AI.  \n5.5.9 Method of Identifying Fake News \nRespondents were asked to select how they personally primarily try to identify fake news. \nThe presented options were \u201cI don\u2019t bother with trying to identify fake news\u201d, \u201ccheck \ndifferent news sources\u201d, \u201cconduct my own research\u201d, \u201cwait for confirmation, corr ection , or 69; 66.3%13; 12.5%2; 1.9%3; 2.9%2; 1.9%9; 8.7%6; 5.8%2018 Fake news reason\nBias/agenda Paid promotion\nCharacter assassination Satire/comedy\nMistake/bad sources Rushing\nOther82; 54.7%\n31; 20.7%3; 2.0%3; 2.0%2; 1.3%13; 8.7%16; 10.7%2024 Fake news reason\nBias/agenda Paid promotion\nCharacter assassination Satire/comedy\nMistake/bad sources Rushing\nOther\n62 \n retraction from the same source later\u201d , and \u201cother\u201d where respondents were able to write \nin their own answer.  \n \nGraph 18: Respondents' methods of identifying fake news  \nThe most common response in both 2018 and 2024 was checking different news sources, \nfollowed by conducting own research. Apart from more respondents selecting to write their \nown responses , there are no significant differences between 2018 and 2024. In 2024 , \nseveral respondents wrote about their ability to identify fake news based on aspects like \nthe quality of writing, sensationalist language and headlines.  \n5.5.10  Actions of a News Publisher After Posting Fake News \nFor this question , we asked the respondents about the actions a news publisher should take \nif their news turn out to be fake for any reason. Respondents were able to select multiple \nanswers from the following: \u201cnothing\u201d, \u201cpost a correction/retraction\u201d, \u201cpost an apology\u201d, \n\u201cdelete the original news article/video when possible\u201d and \u201cother\u201d where the respondents \nwere able to write in their own answer.  8; 7.6%\n60; 57.1%31; 29.5%4; 3.8%2; 1.9%2018 Fake news \nidentification\nDon't bother Different news sources\nOwn research Wait for same source\nOther11; 7.3%\n81; 54.0%41; 27.3%8; 5.3%9; 6.0%2024 Fake news \nidentification\nDon't bother Different news sources\nOwn research Wait for same source\nOther\n63 \n  \nGraph 19: Respondents' desired actions of a publisher after posting fake news  \nMost respondents in both 2018 and 2024 thought that the news publisher should post a \ncorrection, while almost nobody thought that the publisher should do nothing. The \npercentage of the respondents who think that the publisher should delete the article or \nvideo grew significantly ; those who think an apology should be posted also saw an increase. \nSome of those who selected \u2018Other \u2019 claimed that news publishers do it on purpose , so any \naction would be pointless, while some others stated that there should be ful l transparency \nfrom the publisher on the steps taken to prevent this in the future. One respondent \nsuggested that news publishers should reimburse everyone affected by their fake news. \nThree responses suggested violent actions against the publisher.  \n5.5.11  Impact of COVID -19 Pandemic on Trust in News Publishers  \nIn 2024 , respondents were asked how the COVID -19 pandemic affected their trust in news \npublishers. The available answers were \u201csignificantly lowered trust\u201d, \u201cslightly lowered \ntrust\u201d, \u201cunchanged trust\u201d, \u201cslightly increased trust\u201d and \u201csignificantly increased trust\u201d . 2.7%4.9%62.0%58.0%90.7%\n1.0%7.7%46.2%51.0%87.5%\n0.0% 10.0% 20.0% 30.0% 40.0% 50.0% 60.0% 70.0% 80.0% 90.0% 100.0%NothingOtherDeletePost an apologyPost a correctionPublisher actions in % of responses\n2018 2024\n64 \n  \nGraph 20: Effect of COVID -19 pandemic of the respondents' trust in news organizations  \nMost respondents claim that their trust in news publishers was unchanged by the \npandemic. This was closely followed by those whose trust was slightly lowered and \nsignificantly lowered. A very small portion of the respondents reported either a slight or \nsignificant increase in trust.  \nWhile those over 30 were somewhat more likely to have unchanged trust in news publishers \nafter the pandemic, neither age (\u03c72 = 5.525, p = 0.063), gender (\u03c72 = 3.683, p = 0.159) or \nlocation (\u03c72 = 2.419, p = 0.298) showed a statistically significant differen ce. \n5.6  Identifying Fake News Based on Headline s Alone  \nThe final section of the 2018 survey provided respondents  with  eight headlines of news \narticles and they had to state whether they know the story is true, they think it is true, they \nthink it is false, they know it is false or they either cannot or will not decide.  42; 28.0%\n50; 33.3%55; 36.7%1; 0.7%2; 1.3%Effect of pandemic on trust\nSignificantly lowered trust Slightly lowered trust\nUnchanged trust Slightly increased trust\nSignificantly increased trust\n65 \n 5.6.1 First Headline  \nThe first title provided was \u201cIntroducing Wave, an iOS8 Exclusive Feature Allowing Your \nPhone to Synchronize with Microwave Frequencies to Recharge Your Battery Quickly\u201d . \nThis was not the actual title of a specific publish ed article , but it refers to a hoax from 2014 \nwhere users were sharing an infographic on social media that presented a new feature \nwhich allowed users to charge their phones in their microwave.  \n\u201cIn reality the messages are nothing more than a potentially damaging hoax, presumably \nwith the objective of duping more gullible readers into irreversibly damaging their phones, \nwhich is exactly what putting it in a microwave and trying to charge it will do.\u201d (Charles, \n2014)  \nThis headline is , therefore , fake news with an explicit malicious intent to make users destroy \ntheir phones. There have been other similar online campaigns to get people to destroy their \nbelongings or even seriously harm themselves. Those sharing the malicious information \ngain nothin g apart from the satisfaction they might derive from making others suffer. As in \nthe presented case, Apple product users were often the target, making hatred for the Apple \nbrand and its users a potential motivator.  \n66 \n  \nGraph 21: Respondents' opinion on the truth of the first headline  \nAround two in five users confidently and correctly identified the title as fake news with a \nthird correctly thinking it is. Only one respondent incorrectly stated that it was true with \nconfidence, with three respondents considering it. One fifth of those s urveyed decided not \nto respond. There could be several potential reasons for such a high rate of correct \nidentification of the validity of this headline. Some respondents might know about the hoax \nitself, while others might be aware of what happens upon mi crowaving a phone and some \nmight simply think it is too bizarre to be true.  \nA potential reason for why some thought it could have been true is that the headline does \nnot explicitly state the phone has to be put inside the microwave instead merely mentioning \nsynchronisation with microwave frequencies.  \nThe response to this headline is extremely one -sided, with little differences between \ndifferent demographics , so further analysis provides no significant data.  1; 1.0%3; 2.9%\n21; 20.2%\n36; 34.6%43; 41.3%Response to the first headline\nKnow it is true Think it is true Cannot/will not decide Think it is fake Know it is fake\n67 \n 5.6.2 Second Headline  \nThe second title provided was \u201cPervert who groped chest of 11 -year -old girl spared jail \nbecause his wife can't speak English\u201d  (White, 2014) . This is an article published by the \nUnited Kingdom -based Daily Mirror .  \nThe story is true , but it could be argued that the title is somewhat sensationalistic and could \npotentially be considered clickbait.  \n\u201cAn Islamic teacher who molested a young girl was spared jail today after claiming his six \nchildren were dependent on him because his wife spoke \u201cvery little English\u201d . . . Maknojioa, \nwho had denied the charges, will be supervised for two years and made th e subject of a \nSexual Offences Prevention Order for 10 years. He is banned from working with children for \nseven years.\u201d (White, 2014)  \nThis title was chosen because it evokes emotions and could be considered clickbait or \npolitical propaganda by some respondents.  \n \nGraph 22: Respondents' opinion on the truth of the second headline  6; 5.8%\n10; 9.6%\n20; 19.2%\n58; 55.8%10; 9.6%Response to the second headline\nKnow it is true Think it is true Cannot/will not decide Think it is fake Know it is fake\n68 \n One-tenth  of the respondents w ere incorrectly confident that the headline was fake news, \nwhile over half the respondents incorrectly thought it was fake. As with the previous \nquestion, one-fifth  chose not to decide. One-tenth  of those surveyed correctly thought that \nit was true, while only six respondents were correctly confident that it was. Such a result is \nto be expected from a headline that appears so much like clickbait or potential propaganda.  \n \nGraph 23: Comparison of responses to the second headline based on gender  \nMen were much more likely to be either indecisive or fully confident than women were, \nwith no female respondent selecting the \u201cknow it is fake\u201d option if combining the \u201cthink it \nis fake\u201d and \u201cknow it is fake\u201d ratios, a similar percentage of men and women d ecided \nincorrectly, but, due to so many males not deciding, more women were correct.  51\n46\n182\n4216\n10\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%MaleFemaleResponse to the second headline based on gender\nKnow it is true Think it is true Cannot/will not decide Think it is fake Know it is fake\n69 \n  \nGraph 24: Comparison of responses to the second headline based on region  \nEuropeans were more correct in their assumptions than North Americans were on average. \nA majority of North Americans thought the headline to be fake. A potential cause for this \nmight be the fact that this event happened in the United Kingdom  and, therefore , in Europe.  \nThere were no significant differences between different age groups or the three largest \neducation groups, but those with a high sc hool education level were more inclined to be \nindecisive.  \n \nGraph 25: Comparison of responses to the second headline based on the response to a previous question about headlines  15\n19\n515\n2927\n54\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%North AmericaEuropeResponse to the second headline based on region\nKnow it is true Think it is true Cannot/will not decide Think it is fake Know it is fake\n1311\n1252\n14564\n21027136\n1351\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%AlwaysUsuallySometimesRarelyNeverResponse to the second headline based on the question \nabout deciding on headline alone\nKnow it is true Think it is true Cannot/will not decide Think it is fake Know it is fake\n70 \n When comparing the response to the second headline with the response to the question \nabout how many times respondents make up their mind s based on the headline alone, we \ncan see a steady increase in incorrect responses as the trust in the headline alone increased, \nwith \u201csometimes\u201d being the only outlier. It does seem that trusting headlines has a negative \nimpact on the ability to identify fake news in this case. It is important to note that the \nsample for those who always trust news based o n headline alone is very small.  \n5.6.3 Third Headline  \nThe third headline used was \u201cDozens of Other Countries That Interfered In 2016 Election \nAnnoyed Russia Getting All the Credit\u201d (the Onion, 2018) . \nThis is a satirical article posted on the Onion website, a well -know n publisher of satirical \nmade -up news with the intent to entertain. The intent of using  this article was to include a \npurely satirical work and see how many respondents consider it factual.  \n \nGraph 26: Respondents' opinion on the truth of the third headline  \nNo respondent was convinced that this headline was true, but one-eighth  of the \nrespondents still thought it was true. More respondents were indecisive compared to the 0; 0.0%\n13; 12.5%\n30; 28.8%\n32; 30.8%29; 27.9%Response to the third headline\nKnow it is true Think it is true Cannot/will not decide Think it is fake Know it is fake\n71 \n previous two headlines. Almost three in five respondents were correct, with almost half of \nthem being confident in their answer s. \n \nGraph 27: Comparison of responses to the third headline based on region  \nEuropeans were more likely to incorrectly think the headline to be true than North \nAmericans, with North Americans being very decisive in their correct response. This could \nbe a potential result of the headline being about United States politics hosted on a United \nStates -based satirical website.  \n \nGraph 28: Comparison of responses to the third headline based on the response to a previous question about headlines  112\n921\n1217\n1910\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%North AmericaEuropeResponse to the third headline based on region\nKnow it is true Think it is true Cannot/will not decide Think it is fake Know it is fake\n445\n4996\n241475\n251552\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%AlwaysUsuallySometimesRarelyNeverResponse to the third headline based on the question \nabout deciding on headline alone\nKnow it is true Think it is true Cannot/will not decide Think it is fake Know it is fake\n72 \n In this case , the four respondents who trust news based on headlines alone were all correct, \nbut it is also important to note that none of those who never trust headlines got it wrong \neither, but some of them were indecisive. The only incorrect assumptions came from th e \nmiddle three groups, but it is important to note that these three groups have larger samples.  \nThere were no significant differences in identifying this headline when it comes to gender, \nage group , or education level, although those with a high school level were once again the \nmost indecisive of the major groups.  \n5.6.4 Fourth Headline  \nThe fourth headline was \u201cUS Congress Debating a New Law That Would Require a Gun \nLicense and Proper Training to Purchase Violent Video Games\u201d. This title was fabricated \nspecifically for this survey and relates to no real event and is thus completely fake.  \nThe intent of using this headline was to see the response to a title that those survey ed could \nnot have possibly seen before. There are several talking points regarding violent video  \ngames throughout the world, but the title was purposely made to sound somewhat \nunbelievable.  \n73 \n  \nGraph 29: Respondents' opinion on the truth of the fourth headline  \nOne person was confidently incorrect in stating that this headline was true, with 15 others \nincorrectly thinking that it was true. As with the first and second headlines, one-fifth  of \nthose surveyed were indecisive. The majority of the respondents were correct in marking \nthe headline as fake, with more than one-fourth  of all respondents being correctly confident \nin its falsity.  \nThere were no significant differences in identifying this headline between different \ndemographics or based on the response to the question about trusting headlines alone.  \n5.6.5 Fifth Headline  \nThe fifth headline used was \u201cMan Accused of Stealing Historic Gold Bar Killed His Crying \nInfant Son\u201d (Glowatz, 2018) . \nBoth of these events happened, but they happened 14 years appart, as the man killed his \nson in 1996, spent nine years in prison, and stole the gold bar in 2010 , receiving his \nconviction for the theft in 2018. This headline could be considered as clickbait to generate \nmore site traffic , but it is factual.  1; 1.0%\n15; 14.4%\n20; 19.2%\n40; 38.5%28; 26.9%Response to the fourth headline\nKnow it is true Think it is true Cannot/will not decide Think it is fake Know it is fake\n74 \n This headline was selected because, while being true, it is borderline clickbait and might be \nconsidered to o strange to be real, it has a lot of potential to make people undecisive. A \nmore truthful way to headline this story would have been to add any notion that the events \ndid not happen in a short timeframe.  \n \nGraph 30: Respondents' opinion on the truth of the fifth headline  \nTwo -fifths  of those surveyed were indecisive about this headline , meaning that the intent \nof the headline was met. Only five respondents were correctly confident in its validity, and \none-fifth  of the respondents correctly thought it was true. More than a quarter of those \nsurveyed thought it was fake, and five respondents were incorrectly sure of it . \nThere were no significant differences in identifying the validity of this headline when it \ncomes to different demographics. The only information worth noting is that all four \nrespondents who trust news based on the headline alone incorrectly identified the  headline \nas false.  5; 4.8%\n22; 21.2%\n43; 41.3%29; 27.9%5; 4.8%Response to the fifth headline\nKnow it is true Think it is true Cannot/will not decide Think it is fake Know it is fake\n75 \n 5.6.6 Sixth Headline  \nThe sixth headline was \u201cTrump Offering Free One -Way Tickets to Africa & Mexico for Those \nWho Want to Leave America\u201d  \n\u201cThis story had 801 million Facebook engagements, but is complete and utter twaddle.\u201d \n(Kanter, 2017)  Kanter claims that the fake news story was hosted on tmzhiphop.com , but \nthe post there can no longer be found. At the time of writing, a copy is still available on \nthugify.com.  \nThis story is thus fake. It was chosen due to the large number of engagements it received.  \n \nGraph 31: Respondents' opinion on the truth of the sixth headline  \nNo respondent was incorrectly certain that this headline was truthful, but almost a fifth of \nthose surveyed incorrectly thought it was true. Almost a fifth o f respondents were \nindecisive once again. A quarter of  the respondents were correctly sure that the headline \nrepresents  fake news, with more than a third correctly thinking it is.  0; 0.0%\n19; 18.3%\n19; 18.3%\n40; 38.5%26; 25.0%Response to the sixth headline\nKnow it is true Think it is true Cannot/will not decide Think it is fake Know it is fake\n76 \n  \nGraph 32: Comparison of responses to the sixth headline based on the completed level of education  \nWhile there is no large difference, an increase in correctly evaluating the headline can be \nseen with the increase in the completed level of education. This increase, however, is not \nsignificant enough to support our hypothesis. There were no other signifi cant differences \nbased on demographics or the question regarding headline trust for this headline.  \n5.6.7 Seventh Headline  \nThe seventh title was \u201cFBI Agent Suspected in Hillary Email Leaks Found Dead in Apparent \nMurder -Suicide\u201d.  \n\u201cThere was no truth to this story. The Denver Guardian is simply a fake news web site \nmasquerading as the online arm of a (non -existent) big city newspaper.\u201d (Mikkelson, 2016)  \nThe original article and the Denver Guardian website it was posted on no longer exist, but \nan article on the topic quoting the original still exists on worldtruth.tv.  \nThis headline was chosen for the same reasons as the sixth headline, with this headline \nmentioning Hillary Clinton while the previous one mentioned Donald Trump, to feature \nboth of the main candidates of the 2016 United States presidential election.  763\n557\n10811\n597\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%High schoolSome collegeBachelor's degreeResponse to the sixth headline based on level of completed \neducation\nKnow it is true Think it is true Cannot/will not decide Think it is fake Know it is fake\n77 \n  \nGraph 33: Respondents' opinion on the truth of the seventh headline  \nThis headline had a high rate of indecisiveness , with over two-fifths  of all respondents \nselecting the \u201ccannot/will not decide\u201d option. Five respondents were incorrectly convinced \nthat the headline was valid , followed by almost a fifth of those surveyed incorrectly thinking \nit was true. One quarter of respon dents  correctly though t the headline to be fake news , \nwith eight respondents being correctly sure of it. It is interesting to note how much more \nindecisive and incorrect respondents were about  this headline compared to the sixth one , \nwith both being very similar in nature , created and shared in a similar fashion.  5; 4.8%\n19; 18.3%\n44; 42.3%28; 26.9%8; 7.7%Response to the seventh headline\nKnow it is true Think it is true Cannot/will not decide Think it is fake Know it is fake\n78 \n  \nGraph 34: Comparison of responses to the seventh headline based on the completed level of education  \nAs with some previous headlines, those with a high school level of education were the most \nindecisive. Due to their decisiveness, those with a bachelor\u2019s degree had both the highest \npercentage of correct and incorrect responses compared to the other two gr oups. Those \nwith a bachelor\u2019s degree were about twice as likely to incorrectly consider the headline to \nbe true than the other two groups. This does not support our hypothesis that those with \nhigher education are  more capable of identifying fake news.  311\n138\n17137\n5710\n142\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%High schoolSome collegeBachelor's degreeResponse to the seventh headline based on level of \ncompleted education\nKnow it is true Think it is true Cannot/will not decide Think it is fake Know it is fake\n79 \n  \nGraph 35: Comparison of responses to the seventh headline based on the response to a previous question about \nheadlines  \nThose who never trust news articles based on the headline alone were the least likely to be \ncorrect in their assumption of the validity of the headline. This is potentially due to them \nnot being used to judging based on  the headline alone and relying on the body of each \narticle.  \n5.6.8 Eighth Headline  \nThe eighth and final headline was \u201cRomanian Court Rejects Man's Claim That He's Alive\u201d \n(Associated Press, 2018)  \nThis headline is truthful, as a wife got a death certificate for her husband after he did not \nreturn for several years from a foreign country to which he travelled for work. Upon \nreturning to his native country, his claim of being alive was rejected.  \nThis headline was used because it might be considered too strange to be true. It was also \nrecent at the time of the survey , so some respondents might have seen it and remembered \nit. 212\n25732\n621116\n281071\n242\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%AlwaysUsuallySometimesRarelyNeverResponse to the seventh headline based on the question \nabout deciding on headline alone\nKnow it is true Think it is true Cannot/will not decide Think it is fake Know it is fake\n80 \n  \nGraph 36: Respondents' opinion on the truth of the eighth headline  \n17 respondents were correctly certain of the validity of the headline, followed by 18 \nrespondents who correctly thought the headline was true. Almost one-fourth  of those \nsurveyed, however, incorrectly thought the headline to be fake news , with nine \nrespondents incorrectly convinced that it was false. One-third  of the responders were \nindecisive. This headline had the highest percentage of people selecting the \u201cknow it is true\u201d \noption, potentially a result of the article being recent and the responders encoun tering it \nthemselves.  17; 16.3%\n18; 17.3%\n35; 33.7%25; 24.0%9; 8.7%Response to the eighth headline\nKnow it is true Think it is true Cannot/will not decide Think it is fake Know it is fake\n81 \n  \nGraph 37: Comparison of responses to the eighth headline based on gender  \nFemale respondents were more likely to be correct and less likely to be incorrect than male \nrespondents, with a similar percentage of indecisive answers. Males, however, were more \nlikely to be certain that the headline is factual.  \n \nGraph 38: Comparison of responses to the eighth headline based on the completed level of education  \nAs with some previous headlines, those with a high school level of education were the most \nindecisive. Those with a bachelor\u2019s degree were more likely to be incorrect than the other \ntwo groups, once again not supporting our hypothesis, despite them also being the most \nlikely to be confidently correct.  143\n108\n278\n214\n72\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%MaleFemaleResponse to the eighth headline based on gender\nKnow it is true Think it is true Cannot/will not decide Think it is fake Know it is fake\n457\n374\n13106\n439\n332\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%High schoolSome collegeBachelor's degreeResponse to the eighth headline based on level of \ncompleted education\nKnow it is true Think it is true Cannot/will not decide Think it is fake Know it is fake\n82 \n 5.7  Hypotheses  \nHypothesis  1: Most people encounter fake news very often . \nThis hypothesis was supported , with no respondent stating that they never encounter it in \neither 2018 or 2024.  Roughly only one in ten respondents in 2018 and one in twelve in 2024 \nstated  that they encounter ed them rarely . Around half the respondents in both years stated \nthat they encounter ed them sometimes, with  almost a third  in 2018 and over a third in 2024  \nstating that they usually encounter ed them , and 5%  in both years  stating that they always \nencounter ed them.  The slight increase in the frequency of e ncountering fake news is likely \ndue to higher awareness of it because of  the events between the two years such as the \npandemic.  \nHypothesis 2: Most people get their news online . \nThis hypothesis was supported , as in 2018 , only around a third of respondents used \ntelevision, radio and print as their source of news, while the online options were all selected \nby half or more respondents.  The discrepancy between online and legacy media increased \nfurther in 2024, as radio and print decreased in popularity, while  the popularity of  news \nwebsites and social media grew.  The discrepancy between 2018 and 2024 is probably due \nto the constant shift from legacy media to social media. It is interesting to note that the \naverage age of the respond ents was noticeably higher in 2024, yet the shift away from \nlegacy media towards the internet was still present.  \nHypothesis 3: People mainly trust large news organisations . \nThis hypothesis was not  entirely  supported. While some large organisations were at the top \nof the list, some were trusted by less than one-fifth or even less than one-tenth  of the \nrespondents, with roughly a quarter stating that they trust ed no news organisations  in both \nyears . The trust in some large news organizations did grow between 2018 and 2024, but so \ndid the percentage of those who d id not trust any.  The growth in trust towards certain large \nnews organizations could be explained by the higher averag e age of respondents. The \n83 \n higher frequency of those who d id not trust any news organizations is possibly linked to \nlower trust in media as a whole due to the events between the two years and the perceived \nhigher frequency of fake news.  \nHypothesis 4: People trust national/international news more than local news . \nThis hypothesis was not supported , as almost half of respondents trust ed or distrust ed both \nforms equally and national/international news barely beat out local news by a small margin  \nin 2018 and was almost equal in 2024 . \nHypothesis 5: People do not trust social media as a source of news . \nThis hypothesis was somewhat supported , as almost half of respondents consider ed social \nmedia rarely reliable or completely unreliable  in 2018 , but this percentage fell somewhat in \n2024 , and more people started considering it reliable.  The increase in trust towards social \nmedia is interesting, especially due to the higher average age of the respondents, but it is \npotentially only due to the increase in social media use as a whole. More  than half of \nrespondents in both 2018 and 2024 consider ed social media only to be a starting point to \nfind news, which they then researched elsewhere.  \nHypothesis 6: People usually check only one source . \nThis hypothesis was not supported , as in 2018 only one in ten and in 2024 only one in twenty \nrespondents stated that they only check ed one source. Most claim ed that they check ed 2-\n3 sources , while 20% in 2018 and 16% in 2024 check ed more than 3. In both 2018 and 2024 \nthree quarters of respondents also claimed that they at least sometimes check ed sources \nof different political leaning to compare how a certain event was reported on.  The decrease \nin the number of the respondents who only check one source is mo st likely due to the \ndecrease in trust towards news in general.  \nHypothesis 7: People consider political agendas as the main reason for fake news . \n84 \n This hypothesis was supported , as in 2018  two, two -thirds  of respondents selected political \nbias and agenda as the main reason for fake news.  This ratio fell to slightly over half of \nrespondents in 2024 , but political bias and agenda still remained the most commonly \nchosen reason.  The decrease between 2018 and 2024 was mostly due to more respondents \nconsidering monetary gain as the main reason.  \nHypothesis 8: People mainly try to identify fake news by looking at other news \nwebsites . \nThis hypothesis was supported , with over half of respondents  in both 2018 and 2024  \nclaiming that they check ed other news websites to confirm the validity of news.  The \nhypothesis was also supported by three -quarters  of respondents in both years , claiming \nthey at least sometimes check ed sources of different political leaning.  \nHypothesis 9: North Americans are more capable of identifying fake news  \nThis hypothesis was not supported , as the ability to identify fake news varied vastly from \nheadline to headline.  \nHypothesis 10: Higher education provides better capabilities for identifying fake \nnews  \nThis hypothesis was not supported , as the ability to identify fake news varied vastly from \nheadline to headline, with those with higher education sometimes identifying fake news \nincorrectly at a higher rate.  \nHypothesis 11: Trust in news and news organizations has generally lowered due to \nthe COVID -19 pandemic  \nThis hypothesis was supported. 28% of respondents claim ed that their trust in news \norganizations was significantly lowered, with 33% stating  that their trust lowered slightly. \nSeveral large news organizations, namely BBC, CNN, Reuters, the Guardian, Associated \nPress and Al Jazeera, did , however , receive noticeably more trusted responses.  \n85 \n 6 Analysis of Two Fake News Articles  \nThe two articles analysed are the ones mentioned in the analysis of the sixth and seventh \nheadlines of the survey.  \n\u201cTrump Offering Free One -Way Tickets to Africa & Mexico for Those Who Wanna Leave \nAmerica\u201d hosted on Thugify.com  \n\u201cFBI Agent Suspected in Hillary Email Leaks Found Dead in Apparent Murder Suicide\u201d hosted \non WorldTruth. tv \nThe articles have been  be analysed based on words and sentence structures used, sources \nprovided and additional media, such as images, available with the article.  \nLooking at the Thugify.com website , one can immediately notice that the site is filled with \n\u201cemoji\u201d , giving it an unprofessional look. This is not the original website of this article. It \nwas originally posted on tmzhiphop.com, which claimed to be a satirical website but is no \nlonger accessible. Despite the site being satirical, the article was shared considerably  on \nsocial media as factual.  \nThe title on T hugify replaces the \u201cwant to\u201d of the original with \u201cwanna\u201d, making the title \nitself seem unprofessional and less believable , potentially making it less likely to be shared \nas fact.  \nThe first paragraph of the article states: \u201cThe extraordinary, and highly controversial, offer \nwas revealed by an aid at a press conference in New York this morning.\u201d Then continues \nwith \u201cA statement from Trump said:\u201d (Thugify, 2016)  Quoting a statement in first person \ncontradicting the previous sentence about the offer being revealed by an aid instead of \nDonald Trump himself. No recording or any other source of the statement is provided.  \nThe next paragraph starts with \u201cThe Donald , who \u2026\u201d addressing the president elect by his \nfirst name, which gives it an unprofessional feel.  \n86 \n The article finishes with \u201cWill anyone take up the President elect on his offer? We\u2019ll have to \nwait and see.\u201d (Thugify, 2016) , once again giving it an unprofessional appearance . \nMany sensationalistic words are used throughout the article. The article even starts by using \nthe word \u201csensationally\u201d , followed by \u201cextraordinary\u201d and \u201chighly controversial\u201d.  \nThe article features three pictures, the first being a picture of an angry Donald Trump \npointing up and the last being a picture of a wall, as the article mentions Donald Trump\u2019s \npromise to build a wall between the United States and Mexico.  \n \nFigure 15: Second picture used in the article  from Thugify  \nThe second picture features a falsified breaking news report, which once again uses the \nword \u201cwanna\u201d and generally looks falsified.  \nThe article reads more like satire than anything else, providing no sources and not hiding its \nunprofessionalism. Judging by the engagement the original article on this topic got, \nhowever, that might not stop everyone from believing this article.  \nThe second website, WorldTruth.Tv , appears more professional than Thugify.com and \nresembles general serious media sites. The article starts with what is presumably a stock \nphoto of firefighters fighting a fire. The usage of stock photos is common in online news \narticles, but they are generally credited, which is not the case in the analysed article.  \n\n87 \n  \nFigure 16: The picture used in the article from WorldTruth  \nThe article features background information on those involved, time of the 9 -1-1 call and \neven supposed quotes from a named Police Chief, a neighbour and an FBI official.  \nThe article then mentions that some media outlets are spreading conspiracy theories \nregarding the even t. This might be an attempt to distance this article from those less trusted \noutlets.  \nThe article mentions that the FBI Director refused to comment, instead asking for privacy. \nConcluding the article is a message saying: \u201cThis is a developing story.\u201d (WorldTruth.Tv, \n2018) . It then provides  denverguardian.com, the origin of this fake news, as the only source. \nAs a lot of time has passed since the supposed event, this should not still be marked as a \ndeveloping story.  \nThis article puts a lot of effort into looking professional , apart from not listing the date of \npubli cation .  \n\n88 \n It is possible that the author had no intent to spread fake news and merely fell for the \noriginal article.  \nThe second article appeared far more professional and did not use as many sensationalistic \nwords as the first one, making it far more believable and giving it a larger potential to \nspread.  The main paragraph , lowering its credibility , is the one about conspiracy theories \namong some media outlets, as it adds nothing to the main story at ha nd. \nBoth articles do not explicitly feature an author or a publication  date, which are important \nfactors for identifying valid news.   \n89 \n 7 Identifying Fake News  \nIFLA or the International federation of Library Associations published this infographic to \ninform readers on the eight main ways to identify fake news. The infographic is also \navailable translated into over forty languages. This infographic is a good start ing point for \nthe topic of identifying fake news.  \n \nFigure 17: How to spot fake news (IFLA, 2019)  \n\n90 \n 7.1 Consider the Source  \nThe first logical step is to analyse the website on which the article is posted. The first \nquestion is whether  it is even a news site or just a blog post. A good example of a site that \ndoes not look trustworthy to begin with is the \u201cemoji\u201d filled website , on which the article \nabout Donald Trump paying for flights was posted. Using an online search engine to search \nthe name of the website is also a good option to start with, as results stating the validity, \npolitical leaning and other information about the s ite can quickly be found. Similar to fact \nchecking sites that will be discussed in the eight h step, there are also sites that provide \ninformation about  the trustworthiness, political bias and whether a particular  site is a \nsatir ical one . Examples of such websites are mediabiasfactcheck.com  and realorsatire.com, \nbut it is important to note that such sites can also have a bias themselves.  \n7.2  Read Beyond  \nReading beyond essentially means that one should not make up one\u2019s  mind based on the \nheadline or thumbnail alone, as discussed with the survey question regarding the topic. \nClickbait is a serious issue and blatantly clickbait titles should be ignored or their articles \nread in full.  \n7.3  Check the Author  \nIf an author is not listed, that already takes credibility away from the post. In cases where \nthe author is listed, we can use a search engine to check if the author is a real person \u2013 as it \ncould be a made -up name \u2013 and their past work. One important factor to check is also if the \nsite allows user sub mitted articles.  \n7.4  Supporting Sources  \nAs mentioned in previous sections, sources are an important signifier of the validity of news. \nThe article on the death of a n FBI agent analysed in the previous section is a good example \nof this. While it uses quotes from people throughout the article, the only real source \nprovided is a single website that does not exist anymore, while the other article on Donald \n91 \n Trump paying for flights cites no sources at all. Sources could also be featured in the form \nof recordings or images, especially if the news publisher conducted interviews and research \non their own.  \n7.5  Check the Date \nIt is always important to check the date when the article was published to see if it is still \nrelevant to the current events. Some news sites, such as The Guardian , even feature \nwarnings about articles being old.  News sites that do not feature the date for when the \narticle was published are generally less trustworthy.  \n7.6  Is It a Joke?  \nThis refers  to the first and third points , as checking on those two would reveal if the site s \nwere satir ical. \n7.7  Check Your Biases  \nIt is normal for anyone to be biased to some extent and to prefer news that support their \nbias. Still, it  is important to take a step back and check sources of different political leanings \nand biases to see the difference in reporting.  \n7.8  Ask the Experts  \nAs the infographic is created by a library federation, they recommend asking a librarian to \nhelp you determine the validity of news. Another, more accessible option, is fact-checking  \nsites such as politifact.com, snopes.com and factcheck.org. Once again, it is important to \nnote that these sites can potentially have their own bias, so the recommended course of \naction is to check several.   \n92 \n 8 Fake News as a Topic in English Teaching  \nTo equip future generations with what they need to identify fake news, fake news could be \nused as a topic in English teaching, especially when teaching English as a foreign language, \nas students from countries where English is not the national language are  likely to \nencounter fake news in English. English as a foreign language allows the use of various \ndifferent topics and fake news could be used as one of them. It is, however, important for \nthe teacher to stay as non -biased as possible and avoid pushing an y agenda on the students.  \nSome schools already teach students about fake news in some form or another, oftentimes  \nunder the umbrella of media or news literacy. California is one of the areas that adopted a \nlaw about teaching media literacy in schools and helping  them identify fake and real news. \n\u201cThe new law requires California\u2019s Department of Education to provide materials related to \nmedia literacy on its website. The materials are designed to inform teachers and provide \ntools for them to teach the subject.\u201d (Voice of America, 2018)  \nAs a part of my teaching practice as a student teacher, I used fake news as a topic for two \nhours in a class of 17\u201318-year -old English as a foreign language (EFL) students. We mainly \ntalked about fake news with the purpose of character assassination, where a certain person \nis targeted in an effort to make them lose their position of power, platform or source of \nincome. The students responded very well to this topic . They were  eager to participate in \nthe discussion, with some students personally approaching me to compliment the lesson \nafterward , which I fully credit to the topic of the lesson and not me being a good teacher.  \nAs was discussed, on the internet one of the main factors for the spread of fake news is \nhumans, social network users, who share the fake news articles they find interesting  \nwithout any malicious intent, not knowing that the articles they shared were fake. \nEquipping younger generations with the ability to spot these articles might lower the \nnumber of  shared articles in the future, thus lowering the spread of fake news in general.   \n93 \n 9 Conclusion  \nThis thesis addressed  the issue of fake news and the perception people have about it. First , \nvarious definitions of fake news were analysed and compared to establish what should \ngenerally be considered fake news. Forms of fake news and related content , such as satire , \nwere described. Fake news was defined as news articles and media, sometimes based on \nfact with added context or manipulation, other times completely fabricated, with a specific \nintent to push a political bias, narrative or agenda, or to generate profit. Some of the history \nof fake new s and propaganda is also presented leading up to t he introduction of the \ninternet. Some emphasis is put on the COVID -19 pandemic and its effect on the perception \nof fake news. The rise of AI and its role in manufacturing fake news is also discussed.  \nSome existing research papers regarding fake news were analysed to provide additional \ndata on the topic and to be used as a comparison to  the survey s used for this thesis . The \nsurvey s were  conducted  online  in 2018 and 2024 , with 104 and 150 respondents , \nrespectively . Our first hypothesis , \u201cmost people encounter fake news very often ,\u201d was \nsupported , as the majority of respondents claimed common encounters. This result aligns \nwith the analysed Pew Research Center findings. The second hypothesis , \u201cmost people get \ntheir news online ,\u201d was supported , as traditional media was used the least , with most \nrespondents using at least one online form of obtaining news. The third hypothesis , \u201cpeople \nmainly trust large news organisations ,\u201d was not  entirely  supported , as large news \norganisations were seen a t different levels of trust, with a large number  of people not \ntrusting any organisations at all. The fourth hypothesis , \u201cpeople trust national/international \nnews more than local news ,\u201d was not supported, as the respondents trusted both or neither \nat a similar rate. The fifth  hypothesis , \u201cpeople do not trust social media as a source of news ,\u201d \nwas somewhat supported, as most respondents rarely or never trust ed social media, with \nmany only using it as a starting point.  The distrust of social media as a news s ource did \ndecrease somewhat between 2018 and 2024.  The sixth  hypothesis , \u201cpeople usually check \nonly one source ,\u201d was not supported , as the vast majority of respondents claimed that they \n94 \n check ed more than one source. The  seventh  hypothesis , \u201cpeople consider political agendas \nas the main reason for fake news ,\u201d was supported , with two-thirds  of respondents  in 2018 \nand slightly over half of respondents in 2024  selecting this reason. The eighth  hypothesis , \n\u201cpeople mainly try to identify fake news by looking at other news websites ,\u201d was supported , \nas over half of respondents claimed to check other news websites. The ninth hypothesis , \n\u201cNorth Americans are more capable of identifying fake news ,\u201d was not supported, as there \nwere no constant differences between the two main regions. The  tenth  hypothesis , \u201chigher \neducation provides better capabilities of identifying fake news ,\u201d was not supported, as \nthose with a higher education did not perform better than those with a lower education, \nsometimes performing e ven worse.  The final , eleventh  hypothesis \u201c trust in news and news \norganizations has generally lowered due to the COVID -19 pandemic ,\u201d was somewhat \nsupported , with over a quarter of respondents claiming significantly lowered trust and a \nthird of respondents claiming slightly lowered trust.  \nThe main limitation of the study was the small sample size, as the survey was carried out \nonline based on the goodwill of those taking it. Small differences in demographics were also \na problem, as the majority of those taking the survey were from Europe an d North America , \nwith little to no information from other regions ; most of those taking the survey in 2018 \nwere male and most fell into a single age group . The survey in 2024 did end up having more \ndiverse demographics , apart from the geographical location . \nThis research can serve as a basis for research on a larger scale. The survey could also be \ntranslated into different languages to gather responses from different regions . \nThis thesis  also presents an analysis  two very different fake news articles to see their \nstructure and words used along with the media and sources provided. One of the articles \nwas structured in a way t hat appear ed much more believable , while the other felt like \nobvious fake news.  \nOur research suggests that among the guidelines for iden tifying fake news , the main \nemphasis lies on checking other sources and analysing the source of the article itself.  \n95 \n A suggestion was made on how fake news could be used as a topic in teaching English as a \nforeign language to equip students with better capabilit ies of identifying fake news.  \nFake news is certainly a topic worth y of further investigation , especially with the rapid \nchanges in the political climate and the evolution of technology. A  possible  research topic \nwould be to give people complete news articles and ask them  to research  whether a \nparticular news  article is real or fake.  However, such a study would take a lot of time and \nwould be hard to carry out on a large scale .   \n96 \n Works Cited  \n4chan. (2024). Advertise . Retrieved August 28, 2024, from 4chan: \nhttps://www.4chan.org/advertise  \nAdams, B. (2022). Eli Lilly Issues Rare Apology as Fake Twitter Blue Account Proclaims Free \nInsulin for All.  Retrieved August 21, 2024, from Fierce Pharma: \nhttps://www.fiercepharma.com/marketing/eli -lilly-hit-new -twitter -blue -fake -\naccount -forced -apologize -over -free-insulin -tweet  \nAllcott, H., Gentzkow, M., & Yu, C. (2018, October). Trends in the Diffusion of \nMisinformation.  Retrieved from Stanford: https://web.stanford.edu/~gentzkow/  \nresearch/fake -news -trends.pdf  \nAlmenar, E., Aran -Ramspott, S., Suau, J., & Masip, P. (2021). Gender Differences in Tackling \nFake News: Different Degrees of Concern,. Media and Communication, 9 (1) , pp. \n229-238. Retrieved from https://www.cogitatiopress.com/mediaandcommunica  \ntion/article/view/3523/1982  \nAlvarez, B. (2017, January 11). Public Libraries in the Age of Fake News . Retrieved June 18, \n2019, from Public Libraries Online: http://publiclibrariesonline.org/2017/01/  \nfeature -public -libraries -in-the-age-of-fake -news/  \nAndreau, G. (2021). Defining Fake News. KRITERION \u2013 Journal of Philosophy, 35 (3) , pp. 197 -\n215. Retrieved from https://doi.org/10.1515/krt -2021 -0019  \nAssociated Press. (2018, march 16). Romanian Court Rejects Man\u2019s Claim That He\u2019s Alive.  \nRetrieved June 20, 2019, from The New York Times : https://www.nytimes.com/  \n2018/03/16/world/europe/romania -dead -man.html  \nBalakrishnan, V., Ng, W., Soo, M., Han, G., & Lee, C. (2022). Infodemic and Fake News \u2013 A \nComprehensive Overview of Its Global Magnitude During the COVID -19 Pandemic in \n2021: A Scoping Review. International Journal of Disaster Risk Reduction, 78 . \n97 \n Retrieved from https://www.sciencedirect.com/science/article/pii/S221242092200  \n3636  \nBaptista, J., & Gradim, A. (2022). A Working Definition of Fake News. Encyclopedia, 2 , pp. \n632-645. Retrieved from https://doi.org/10.3390/encyclopedia2010043  \nBlakemore, E. (2018, April 20). How Photos Became a Weapon in Stalin\u2019s Great Purge . \nRetrieved June 7, 2019, from History : https://www.history.com/news/josef -stalin -\ngreat -purge -photo -retouching  \nBrugman, B., Burgers, C., & Konijn, E. (2022). Satirical News From Left to Right: Discursive \nIntegration in Written Online Satire. Journalism, 23 (8) , pp. 1626 -1644. Retrieved \nfrom https://doi.org/10.1177/1464884920979090  \nBurkhardt, J. (2017, November/December). Chapter 1: History of Fake News. Library \nTechnology Reports Vol. 53 Issue 8 , pp. 5 -9. Retrieved from https://journals.ala.org/  \nindex.php/ltr/article/view/6497  \nCanva. (2024). Canva Free Online AI Image Generator . Retrieved September 8, 2024, from \nCanva: https://www.canva.com/ai -image -generator/  \nCharles, C. (2014, September 19). IOS8 Wave Allows Users to Charge iPhone in Microwave?  \nRetrieved June 17, 2019, from that'snonsense.com: https://www.thatsnonsense  \n.com/ios8 -wave -allows -users -to-charge -iphone -in-microwave/  \nCharlton, E. (2019, March 6). Fake News: What It Is, and How to Spot It . Retrieved June 10, \n2019, from World Economic Forum : https://www.weforum.org/agenda/2019/03/  \nfake -news -what -it-is-and-how -to-spot -it/ \nClark, M., & Peters, J. (2022). Twitter Announces \u2018Blue for Business\u2019 to Help Identify Brands \nand Their Employees.  Retrieved September 1, 2024, from The Verge : \nhttps://www.theverge.com/2022/12/19/23517733/twitter -blue -for-business -\nbrands -affiliation -gray -checkmark -badge  \n98 \n David, E. (2024). Openai\u2019s Voice Cloning AI Model Only Needs a 15 -Second Sample to Work . \nRetrieved September 3, 2024, from The Verge : https://www.theverge.com/2024/  \n3/29/24115701/openai -voice -generation -ai-model  \nDerakhshan, H., & Wardle, C. (2017, October 31). One Year On, We\u2019re Still Not Recognizing \nthe Complexity of Information Disorder Online . Retrieved May 28, 2018, from First \nDraft: https://firstdraftnews.org/coe_infodisorder/  \nDuarte, F. (2024, April 24). Reddit User Age, Gender, & Demographics (2024).  Retrieved \nAugust 11, 2024, from Exploding Topics: https://explodingtopics.com/blog/reddit -\nusers  \nEnnis, G. (2018, April 12). What Do Imposter Accounts Mean for Businesses?  Retrieved \nAugust 27, 2024, from NSDESIGN: https://www.nsdesign.co.uk/what -do-imposter -\naccounts -mean -for-businesses/  \nEuropean Comission. (2019, June 17). Tackling Online Disinformation . Retrieved July 2, \n2019, from Digital Single Market - European Commission: https://ec.europa.eu/  \ndigital -single -market/en/tackling -online -disinformation  \nGelfert, A. (2018). Fake News: A Definition. Informal Logic Vol. 38, No.1 , 84-117.  \nGlowatz, E. (2018, April 2). Man Accused of Stealing Historic Gold Bar Killed His Crying Infant \nSon.  Retrieved June 17, 2019, from Newsweek : https://www.newsweek.com/man -\naccused -stealing -historic -gold -bar-killed -infant -son-869001  \nGuess, A., Nagler, J., & Tucker, J. (2019, January 9). Less Than You Think: Prevalence and \nPredictors of Fake News Dissemination on Facebook. Science Advances, 5 (1) . \nRetrieved from ScienceAdvances: https://advances.sciencemag.org/content/5/1/  \neaau4586  \nHaire, B. (2017, January 11). \u201cFake News\u201d Has Long History. Southeast Farm Press, 44 (2) , 4-\n5. \n99 \n Historic Newspapers. (2020). A History of TIME Magazine . Retrieved October 3, 2024, from \nHistoric Newspapers: https://www.historic -newspapers.com/blog/time -magazine -\nhistory/  \nIFLA. (2019, April 17). How To Spot Fake News . Retrieved July 11, 2019, from IFLA: \nhttps://www.ifla.org/publications/node/11174  \nKanter, J. (2017, February 7). Only 4% of People Could Spot the Fake News Stories in These \nHeadlines \u2014 Can You?  Retrieved June 21, 2019, from Business insider : \nhttps://www.businessinsider.com/spot -the-fake -news -story -2017 -2 \nLebied, M. (2018, August 8). Misleading Statistics Examples \u2013 Discover The Potential For \nMisuse of Statistics & Data In The Digital Age . Retrieved June 27, 2019, from \ndatapine: https://www.datapine.com/blog/misleading -statistics -and-data/  \nLing 300. (2008). Tutorial: Pearson's Chi -square Test for Independence . Retrieved November \n28, 2024, from University of Pennsylvania: https://www.ling.upenn.edu/~clight/  \nchisquared.htm  \nLoos, E., & Nijenhuis, J. (2020). Consuming Fake News: A Matter of Age? The Perception of \nPolitical Fake News Stories in Facebook Ads. Lecture Notes in Computer Science, \n12209 , pp. 69 -88. Retrieved from https://doi.org/10.1007/978 -3-030-50232 -4_6 \nMerriam -Webster. (no date). propaganda . Retrieved June 17, 2019, from Merriam -\nWebster: https://www.merriam -webster.com/dictionary/propaganda  \nMikkelson, D. (2016). FBI Agent Suspected in Hillary Email Leaks Found Dead in Apparent \nMurder -Suicide.  Retrieved June 16, 2019, from Snopes: https://www.snopes.com/  \nfact-check/fbi -agent -murder -suicide/  \nMitchell, A., Gottfried, J., Walker, M., Fedeli, S., & Stocking, G. (2019, June 5). Many \nAmericans Say Made -Up News Is a Critical Problem That Needs To Be Fixed.  \nRetrieved July 2, 2019, from Pew Research Center: https://www.journalism.org/  \n100 \n 2019/06/05/many -americans -say-made -up-news -is-a-critical -problem -that-needs -\nto-be-fixed/  \nNocera, J. (2017). RIP, Time Inc. It Was Fun While It Lasted.  Retrieved October 3, 2024, from \nBloomberg : https://www.bloomberg.com/view/articles/2017 -11-27/rip -time -\nmagazine -meredith -will-make -you-fade -away  \nNsoesie, E., Cesare, N., M\u00fcller, M., & Ozonoff, A. (2022). COVID -19 Misinformation Spread \nin Eight Countries: Exponential Growth Modeling Study. J Med Internet Res, 22 (12) . \nRetrieved from https://www.jmir.org/2020/12/e24425/  \nOBERLO. (2024). Facebook Age Demographics . Retrieved August 28, 2024, from OBERLO: \nhttps://www.oberlo.com/statistics/facebook -age-demographics  \nQuattrociocchi, W., Scala, A., & Sunstein, C. (2016, June 13). Echo Chambers on Facebook.  \nRetrieved from SSRN: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=  \n2795110  \nRocha, Y., de Moura, G., Desid\u00e9rio, G., de Oliveira, C., Louren\u00e7o, F., & de Figueiredo Nicolete, \nL. (2023). The Impact of Fake News on Social Media and Its Influence on Health \nDuring the COVID -19 Pandemic: A Systematic Review. Journal of Public Health, 31 , \npp. 1007 -1016. Retrieved from https://doi.org/10.1007/s10389 -021-01658 -z \nRodr\u00edguez -Ferr\u00e1ndiz, R. (2023). An Overview of the Fake News Phenomenon: From Untruth -\nDriven to Post -Truth -Driven Approaches. Media and Communication, 11 (2) , pp. 15 -\n29. Retrieved from https://doi.org/10.17645/mac.v11i2.6315  \nSample, I. (2020). What Are Deepfakes \u2013 And How Can You Spot Them?  Retrieved from The \nGuardian: https://www.theguardian.com/technology/2020/jan/13/what -are-\ndeepfakes -and-how -can-you-spot -them  \nSardarizadeh, S. (2022). Ukraine war: False TikTok videos draw millions of views.  Retrieved \nSeptember 13, 2024, from BBC: https://www.bbc.com/news/60867414  \n101 \n Schulze, E. (2019, June 14). Facebook, Google and Twitter Need to Do More to Tackle Fake \nNews, EU Says.  Retrieved July 1, 2019, from CNBC : https://www.cnbc.com/  \n2019/06/14/facebook -google -twitter -need -to-do-more -to-tackle -fake -news -eu-\nsays.html  \nShao, C., Ciampaglia, G., Varol, O., Yang, K. -C., Flammini, A., & Menczer, F. (2018, November \n20). The Spread of Low -Credibility Content by Social Bots. Nature Communications, \n9. Retrieved from https://www.nature.com/articles/s41467 -018-06930 -7 \nStatista Research Department. (2016, February 25). Distribution of Reddit Users in the \nUnited States as of February 2016, by Gender . Retrieved July 5, 2019, from Statista: \nhttps://www.statista.com/statistics/517155/reddit -user -distribution -usa-gender/  \nStroud, F. (no date). Fake News.  Retrieved July 4, 2019, from webopedia: \nhttps://www.webopedia.com/TERM/F/fake -news.html  \nSwift, J. (1710s). The Art of Political Lying.  Retrieved from Bartleby.com: \nhttps://www.bartleby.com/209/633.html  \nTavistock Books. (2014, April 1). The Six Hoaxes of Edgar Allan Poe.  Retrieved June 7, 2019, \nfrom http://blog.tavbooks.com/?p=909  \nthe Onion. (2018, March 9). Dozens Of Other Countries That Interfered In 2016 Election \nAnnoyed Russia Getting All The Credit . Retrieved July 4, 2019, from \nhttps://politics.theonion.com/dozens -of-other -countries -that-interfered -in-2016 -\nelect -1823645160  \nThugify. (2016, November 12). Trump Offering Free One -Way Tickets to Africa & Mexico for \nThose Who Wanna Leave America.  Retrieved June 12 2019, from Thugify: \nhttp://thugify.com/trump -offering -free-flights -to-mexico -and-africa/  \n102 \n van der Linden , S., Panagopoulos, C., & Roozenbeek, J. (2020). You Are Fake News: Political \nBias in Perceptions of Fake News. Media, Culture & Society, 42 (3) , pp. 460 -470. \nRetrieved from https://doi.org/10.1177/0163443720906992  \nVincent, J. (2019, June 10). AI Deepfakes Are Now as Simple as Typing Whatever You Want \nYour Subject to Say . Retrieved August 23, 2024, from The Verge: \nhttps://www.theverge.com/2019/6/10/18659432/deepfake -ai-fakes -tech -edit-\nvideo -by-typing -new -words  \nVoice of America. (2018, September 25). More US Schools Teaching Skills to Recognize False \nNews . Retrieved July 6, 2019, from VoA learning English: https://learningenglish.  \nvoanews.com/a/more -us-schools -teaching -skills -to-recognize -false -\nnews/4586357.html  \nVosoughi, S., Roy, D., & Aral, S. (2018, March 9). The Spread of True and False News Online. \nScience, 359 (6380) , 1146 -1151. Retrieved from https://science.sciencemag.org/  \ncontent/359/6380/1146  \nWardle, C. (2017, February 16). Fake news. It\u2019s complicated.  Retrieved June 19, 2019, from \nFirst Draft : https://firstdraftnews.org/fake -news -complicated/  \nWebwise. (2018, July 2). Explained: What is Fake News?  Retrieved June 14, 2019, from \nWebwise: https://www.webwise.ie/teachers/what -is-fake -news/  \nWhite, S. (2014, March 19). Pervert Who Groped Chest of 11 -Year -Old Girl Spared Jail \nBecause His Wife Can\u2019t Speak English.  Retrieved July 1, 2024, from Mirror : \nhttps://www.mirror.co.uk/news/uk -news/pervert -suleman -maknojioa -who -\ngroped -3257864  \nWismans, A., van der Zwan, P., Wennberg, K., Franken, I., Mukerjee, J., Baptist, R., . . . Thurik, \nR. (2022). Face Mask Use During the COVID -19 Pandemic: How Risk Perception, \nExperience With COVID -19, and Attitude Towards Government Interact With \n103 \n Country -Wide Policy Stringency. BMC Public Health 22, 1622 . Retrieved from \nhttps://doi.org/10.1186/s12889 -022-13632 -9 \nWorldTruth.Tv. (2018, December 6). FBI Agent Suspected In Hillary Email Leaks Found Dead \nIn Apparent Murder Suicide.  Retrieved July 2, 2019, from WorldTruth.Tv: \nhttps://worldtruth.tv/fbi -agent -suspected -in-hillary -email -leaks -found -dead -in-\napparent -murder -suicide/  \nX Help Center. (2023). How to Get the Blue Checkmark on X.  Retrieved August 27, 2024, \nfrom X: https://help.x.com/en/managing -your -account/about -x-verified -accounts  \nX Help Center. (2024). About Community Notes on X. Retrieved August 27, 2024, from \nhttps://help.x.com/en/using -x/community -notes  \nYour Dictionary. (2018, October 22). What Is Fake News?  Retrieved June 14, 2019, from \nhttps://examples.yourdictionary.com/reference/examples/what -is-fake -news.html  \nZoschak, V. (2014, February 21). A Brief History of Propaganda . Retrieved June 18, 2019, \nfrom ILAB: https://ilab.org/articles/brief -history -propaganda  \n \n  \n104 \n Appendix  1 \u2013 Survey  \nThe first four questions focus on demographics for data analysis. Feel free to select \"I'd \nrather not answer.\" on any of them.  \n1. What is your gender?  \n\u2022 Male  \n\u2022 Female  \n\u2022 Other  \n\u2022 I\u2019d rather not answer.  \n \n2. What is your age?  \n\u2022 Under 18  \n\u2022 18-24 \n\u2022 25-30 \n\u2022 31-40 \n\u2022 41-50 \n\u2022 Over 50  \n \n3. Where do you currently live?  \n\u2022 Asia \n\u2022 Middle East, North Africa, and Greater Arabia  \n\u2022 Europe  \n\u2022 North America  \n\u2022 Central America and the Caribbean  \n\u2022 South America  \n\u2022 Sub-Saharan Africa  \n105 \n \u2022 Australia and Oceania  \n\u2022 I\u2019d rather not answer  \n \n4. What is the highest degree or level of school you have completed?  \n\u2022 Less than high school  \n\u2022 High school or equivalent  \n\u2022 Some college, no degree  \n\u2022 Associate degree (e.g. AA, AS)  \n\u2022 Bachelor\u2019s degree (e.g. BA, BS)  \n\u2022 Master\u2019s degree (e.g. MA, MS, MEd)  \n\u2022 Professional degree (e.g. MD, DDS, DVM)  \n\u2022 Doctorate (e.g. PhD, EdD)  \n\u2022 I\u2019d rather not answer  \n \nThe following questions determine, where people get their news and what sources they \ntrust.  \n5. Where do you usually get your news (select all that apply)?  \n\u2022 News websites (e.g. CNN, BBC, Fox)  \n\u2022 Social media (e.g. Facebook, twitter)  \n\u2022 TV \n\u2022 Radio  \n\u2022 Newspapers, magazines  \n\u2022 Forums, boards, blogs, Reddit  \n\u2022 YouTube and other video sharing sites  \n\u2022 Other (please specify)  \n \n106 \n 6. Which news organizations do you trust (select all that apply)?  \n\u2022 CNN  \n\u2022 BBC \n\u2022 Fox News  \n\u2022 The New York Times  \n\u2022 The Wall Street Journal  \n\u2022 Daily mail  \n\u2022 The Guardian  \n\u2022 Associated Press  \n\u2022 Reuters  \n\u2022 Al Jazeera  \n\u2022 Time  \n\u2022 I don\u2019t trust any news organizations  \n\u2022 Other (please specify)  \n \n7. Which do you find more trustworthy?  \n\u2022 Local news organizations  \n\u2022 National/international news organizations  \n\u2022 Equal trust/distrust in local and (inter)national news organizations  \n \n8. How much do you trust social media as a source of news  \n\u2022 It is a reliable source of information  \n\u2022 It is a good starting point but I do additional research elsewhere  \n\u2022 It is rarely reliable  \n\u2022 It is completely unreliable  \n\u2022 Other (please specify)  \n107 \n  \n9. How many news sources do you check before you make up your mind on a certain \nevent in the news?  \n\u2022 1 \n\u2022 2-3 \n\u2022 More than 3  \n \n10. How often do you check news sources of different political leaning to see if and how \nthey reported the same piece of news differently?  \n\u2022 Always  \n\u2022 Usually  \n\u2022 Sometimes  \n\u2022 Rarely  \n\u2022 Never  \n \n11. How often do you decide to trust or distrust a news article based solely on the \nheadline without reading the rest of the article?  \n\u2022 Always  \n\u2022 Usually  \n\u2022 Sometimes  \n\u2022 Rarely  \n\u2022 Never  \n \n \n \n108 \n 12. How often do you encounter something you would consider to be \u201cfake news\u201d?  \n\u2022 Always  \n\u2022 Usually  \n\u2022 Sometimes  \n\u2022 Rarely  \n\u2022 Never  \n \n13. What is in your opinion the most common reason behind the \u201cfake news\u201d you \nencounter?  \n\u2022 Political bias/agenda  \n\u2022 Paid promotion of a product/service  \n\u2022 Character assassination  \n\u2022 Satire/comedy  \n\u2022 Honest mistake or bad sources  \n\u2022 Rushing the news about an event to be the first one to post them  \n\u2022 I never encounter fake news  \n\u2022 Other (please specify)  \n \n14. How do you personally primarily try to identify \u201cfake news\u201d?  \n\u2022 I don\u2019t bother with trying to identify fake news  \n\u2022 Check different news sources  \n\u2022 Conduct my own research  \n\u2022 Wait for confirmation, correction or retraction from the same source later  \n\u2022 Other (please specify)  \n \n109 \n 15. What should a news publisher do, if their news turns out to be fake for any reason \n(select all that apply)?  \n\u2022 Nothing  \n\u2022 Posta correction/retraction  \n\u2022 Post an apology  \n\u2022 Delete the original article/video when possible  \n\u2022 Other (please specify)  \n2018 survey  only : \nRead the news headlines and select whether you think the news are fake or true.  \n16. Introducing Wave, an iOS8 Exclusive Feature Allowing Your Phone to Synchronize \nwith Microwave Frequencies to Recharge Your Battery Quickly  \n\u2022 I know it\u2019s true  \n\u2022 I think it\u2019s true  \n\u2022 I can\u2019t/won\u2019t decide  \n\u2022 I think it\u2019s fake  \n\u2022 I know it\u2019s fake  \n \n17. Pervert Who Groped Chest of 11 -Year -Old Girl Spared Jail Because His Wife Can\u2019t \nSpeak English  \n\u2022 I know it\u2019s true  \n\u2022 I think it\u2019s true  \n\u2022 I can\u2019t/won\u2019t decide  \n\u2022 I think it\u2019s fake  \n\u2022 I know it\u2019s fake  \n \n110 \n  \n18. Dozens of Other Countries That Interfered In 2016 Election Annoyed Russia Getting \nAll the Credit  \n\u2022 I know it\u2019s true  \n\u2022 I think it\u2019s true  \n\u2022 I can\u2019t/won\u2019t decide  \n\u2022 I think it\u2019s fake  \n\u2022 I know it\u2019s fake  \n \n19. US Congress Debating a New Law That Would Require a Gun License and Proper \nTraining to Purchase Violent Video Games  \n\u2022 I know it\u2019s true  \n\u2022 I think it\u2019s true  \n\u2022 I can\u2019t/won\u2019t decide  \n\u2022 I think it\u2019s fake  \n\u2022 I know it\u2019s fake  \n \n20. Man Accused of Stealing Historic Gold Bar Killed His Crying Infant Son  \n\u2022 I know it\u2019s true  \n\u2022 I think it\u2019s true  \n\u2022 I can\u2019t/won\u2019t decide  \n\u2022 I think it\u2019s fake  \n\u2022 I know it\u2019s fake  \n \n \n111 \n  \n21. Trump Offering Free One -Way Tickets to Africa & Mexico for Those Who Want to \nLeave America  \n\u2022 I know it\u2019s true  \n\u2022 I think it\u2019s true  \n\u2022 I can\u2019t/won\u2019t decide  \n\u2022 I think it\u2019s fake  \n\u2022 I know it\u2019s fake  \n \n22. FBI Agent Suspected in Hillary Email Leaks Found Dead in Apparent Murder -Suicide  \n\u2022 I know it\u2019s true  \n\u2022 I think it\u2019s true  \n\u2022 I can\u2019t/won\u2019t decide  \n\u2022 I think it\u2019s fake  \n\u2022 I know it\u2019s fake  \n \n23. Romanian Court Rejects Man's Claim That He's Alive  \n\u2022 I know it\u2019s true  \n\u2022 I think it\u2019s true  \n\u2022 I can\u2019t/won\u2019t decide  \n\u2022 I think it\u2019s fake  \n\u2022 I know it\u2019s fake  \n \n \n \n112 \n 2024 survey  only : \n \n24. How has the COVID -19 pandemic affected your trust in news publishers?  \n\u2022 Significantly lowered trust  \n\u2022 Slightly lowered trust  \n\u2022 Unchanged trust  \n\u2022 Slightly increased trust  \n\u2022 Significantly increased trust  \n  \n113 \n Appendix  2 \u2013 Analysed Articles  \nFull pictures of both articles are provided in case the articles get taken down.  \n \n\n114 \n  \nSource: (Thugify, 2016)  \n\n115 \n  \n\n116 \n  \nSource: (WorldTruth.Tv, 2018)  \n", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "The emergence and public perception of misinformation: a study on fake news", "author": ["D Leskovar"], "pub_year": "2024", "venue": "NA", "abstract": "Magistrsko delo obravnava temo la\u017enih novic. La\u017ene novice so izraz, ki ga dandanes  sli\u0161imo povsod. Zamisel o oblikovanju la\u017enih informacij sama po sebi ni nova, podrobno"}, "filled": false, "gsrank": 491, "pub_url": "https://core.ac.uk/download/pdf/643772396.pdf", "author_id": [""], "url_scholarbib": "/scholar?hl=en&q=info:j_t9jjGWW-4J:scholar.google.com/&output=cite&scirp=490&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D490%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=j_t9jjGWW-4J&ei=YLWsaK7sAqzWieoPic2ZoAU&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:j_t9jjGWW-4J:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://core.ac.uk/download/pdf/643772396.pdf"}}, {"title": "Characterizing the social media news sphere through user co-sharing practices", "year": "2020", "pdf_data": "Proceedings of the Fourteenth International AAAI Conference on Web and Social Media (ICWSM 2020)\nCharacterizing the Social Media News Sphere through User Co-Sharing Practices\nMattia Samory\u2217\nComputational Social Science\nGESIS, Cologne, Germany\nmattia.samory@gesis.orgVartan Kesiz Abnousi\nDepartment of Computer Science\nVirginia Tech, V A, USA\nvkesizab@vt.eduTanushree Mitra\nDepartment of Computer Science\nVirginia Tech, V A, USA\ntmitra@vt.edu\nAbstract\nWe describe the landscape of news sources which share so-\ncial media audience. We focus on 639 news sources, both\ncredible and questionable, and characterize them accordingto the audience that shares their articles on Twitter. Based on\nuser co-sharing practices, what communities of news sources\nemerge? We \ufb01nd four groups: one is home to mainstream,\nhigh-circulation sources from all sides of the political spec-\ntrum; one to satirical, left-leaning sources; one to bipartisanconspiratorial, pseudo-scienti\ufb01c sources; and one to right-\nleaning, deliberate misinformation sources. Next, we measure\nwhich assessments of credibility, impartiality, and journalisticintegrity correspond to social media readers\u2019 choices of news\nsources, and uncover the multifaceted structure of the social\nnews sphere. We show how news articles shared on Twitterdiffer across the four groups along linguistic and psycholin-\nguistics measures. Further, we \ufb01nd that with a high degree\nof accuracy (\u02dc80%), we can classify in what news commu-nity an article belongs to. Our data-driven categorization of\nnews sources will help to navigate the complex landscape of\nonline news and has implications for social media platformmaintainers to reliably triage questionable outlets.\nIntroduction\nTwo-thirds of American adults read news on social media,\neven though a majority of them expect the reported infor-mation to be mostly inaccurate (Matsa and Shearer 2018).It is a pressing concern to inform users about the quality ofthe news outlets in their social media feeds. In response, so-cial media platforms, advocacy groups, and the social com-\nputing research community developed multiple assessmentsof the quality of news sources\u2014for example, political slant(Elejalde, Ferres, and Herder 2017) or credibility (Soni et al.2014). These assessments of news outlets allow researchersto study the social news sphere and to characterize its so-cial media audience. However, on the one hand, there is\nno single reliable assessment of the quality of a news out-let (Zhang et al. 2018). On the other, we know little about\n\u2217A portion of this work was conducted while the author was at\nVirginia Tech\nCopyright c/circlecopyrt2020, Association for the Advancement of Arti\ufb01cial\nIntelligence (www.aaai.org). All rights reserved.which characteristics of the news outlets drive the actual au-\ndience engagement on social media. What do users shareand reply to most? This poses challenges for identifyingwhich of the many assessments to prioritize when evaluat-ing news. In particular, the assumption that users are con-suming an exclusive diet of partisan-aligned news appearsincreasingly less relevant in the social news sphere, whichcalls for a more nuanced characterization (Guess et al. 2018;Starbird 2017).\nThis paper provides just such a characterization. We fo-\ncus on 639 news sources, both credible and questionable,through a quantitative analysis of over 31 million Tweets and1 million news articles. We adopt multiple measures of newssource quality, including external assessments of sources\u2019factuality, impartiality, and journalistic integrity. Then, wecompare and contrast the external assessments of the newssources, the characteristics of their articles, and the audiencethat shares them on Twitter. This brings novel insight intowhich external assessments of news sources correspond tocommon audience, as well as into what types of news arti-cles do different audience pools engage with. The paper isstructured around three research questions:\nRQ1: What communities of news sources emerge,\nwhen considering the sharing practices of their socialmedia audience?\nFirst, we connect sources to re\ufb02ect how many users share\nlinks to both\u2014a costly signal that sources cannot easily fal-sify (Donath 2011). We \ufb01nd four communities of connectedsources: highly circulated news sources spanning the entirepolitical spectrum; engaging misinformation, such as click-bait and satirical news sources; factoid misinformation, suchas conspiratorial and junk science sources; and misinforma-tion with an ulterior motive, comprised of far-right propa-ganda and fake news sources.\nRQ2: How well external assessments of news sources\ndistinguish the audience-based news communities?\nWe \ufb01nd that the social news sphere does not simply follow\npartisan polarization. We show that it is necessary to com-bine multiple assessments of political slant and journalisticnorms to explain similar audiences on Twitter.\n602\nFigure 1: Cluster composition in terms of expert assessments of credibility. We map the primary category of opensources.co\nfor each source to the corresponding cluster. Cluster 1 is the destination for all of the credible sources, as well as for politicalmisinformation sources; cluster 2 is home to satirical and clickbait sources; cluster 3 hosts most conspiratorial, unreliable, and\njunk science sources; cluster 4 is largely comprised of fake news and biased sources.\nFinally we look for differences in the news articles that\nthe communities of news sources shared on Twitter.\nRQ3: What kind of language and engagement attributes\ncharacterize communities of news sources?\nWe analyze how the articles differ in content, style, senti-\nment, psycholinguistic categories, as well as in how muchengagement they received on Twitter.\nTo summarize, our data-driven study highlights a discon-\nnect between how experts categorize news sources, and howtheir audience selects them on Twitter. We shed light onwhich expert assessments do help in differentiating betweenaudience-based clusters of news sources\u2014although in pre-\nviously unidenti\ufb01ed combinations. Our analysis of the con-tent of the articles pertaining to each cluster can guide mediascholars in assessing what kind of news sources bring to-gether or divide social news media readerships. Our resultsalso yield important practical implications. This work canhelp social media platforms to reliably triage new informa-tion outlets, thus alleviating the labor-intensive labeling ofan ever growing information space. While creating informa-\ntion outlets is cheap, controlling its audience similarity spaceis costly; hence a reliable, hard-to-fake signal. For example,a new conspiratorial outlet will have to wield signi\ufb01cant ef-fort to position itself close to credible information sources inthe shared social media audience space.\nRelated Work\nHere we outline the scholarly work that informed our study.First we present a line of research studying the social newssphere from the point of view of an ecosystem of news pro-ducers and their audience. Then, we discuss the large bodyof work aimed at characterizing misinformation and devel-oping indicators for identifying it.\nEcosystem of news and audiences on social media\nMost related to this work is research that identi\ufb01es groups ofnews sources by the social media audience they have in com-mon (Mukerjee, Gonz \u00b4alez-Bail \u00b4on, and Maj \u00b4o-V\u00b4azquez 2018;Webster and Ksiazek 2012). These studies contribute funda-\nmental methods to assess the levels of fragmentation of thesocial news sphere. However, results from this line of workare ambiguous, thus calling for further study.\nOne study hypothesized that audience behavior is the in-\nclination of users to engage with like-minded media, result-ing in a highly fragmented news media sphere (Webster andKsiazek 2012). Indeed, most social media users are often notcareful about shaping a balanced information diet for them-selves, and consume content on a limited number of topics(Kulshrestha et al. 2015). Furthermore, certain topics, suchas conspiratorial (Bessi et al. 2015) and political (Barber \u00b4a\net al. 2015) content, seem especially polarizing, and resultin a fragmentation of the social media audience. In partic-ular, several works depict social media audience as dividedinto conservative and liberal stances (Conover et al. 2011;Garimella et al. 2018; Bakshy, Messing, and Adamic 2015).\nIn contrast, another line of research \ufb01nds little evi-\ndence of ideological segmentation in media use (Mukerjee,Gonz \u00b4alez-Bail \u00b4on, and Maj \u00b4o-V\u00b4azquez 2018; Webster and\nKsiazek 2012). For example, Mukerjee, Gonz \u00b4alez-Bail \u00b4on,\nand Maj \u00b4o-V\u00b4azquez \ufb01nd no evidence of selective exposure\nor self-selection of the audience, and surface a tightly inter-connected core of news sources\u2014fundamentally formed bylegacy brands. Opposing the view of a politically polarizedaudience, recent studies suggest that the dualism betweenconservative and liberal stances is increasingly less relevantin the social news sphere (Starbird 2017; Guess et al. 2018;Druckman, Levendusky, and McLain 2018). In particular,these studies show that groups of news producers employmechanisms that are yet to be studied deeply to in\ufb02uencetheir readers (Horne and Adali 2018; Starbird et al. 2018).\nCollectively, this body of literature highlight the impor-\ntance of studying the social news sphere as an intercon-nected ecosystem of news sources, and not as a collection ofindependent actors. We build upon their intuition, and takethe analyses a step further, by looking at how audience frag-mentation corresponds to characteristics of news sources aswell as of their content.\n603\nDifferently from related work, we connect how Twitter\nusers share news articles, how the emerging news commu-\nnities from the user co-shares compare across the assess-\nments of the articles\u2019 sources, and what are the distinguish-able characteristics in the content of those articles. Simi-larly to (Horne and Adali 2018), we use multiple assess-ments of news sources to study their relationship. However,we use common Twitter audience as a measure of sourcesimilarity, which sources cannot directly control. This workalso differs from (Mukerjee, Gonz \u00b4alez-Bail \u00b4on, and Maj \u00b4o-\nV\u00b4azquez 2018), which introduces a network backbone ex-\ntraction method to remove spurious audience overlap\u2014butwhich, potentially, also penalizes sources with small orniche audience overall. To account for this, we employ in-stead an information-theoretic measure for computing audi-ence overlap (Martin 2017).\nMisinformation in online social media\nThe prevalence of online misinformation recently broughtthe social news sphere at the epicenter of the social com-puting research community as much as of the public dis-course. On one end of the misinformation spectrum thereare sites explicitly designed to deceive people, to publishprovably false claims, and to propagate them through so-cial media platforms in an attempt to increase readershipand pro\ufb01t (Marwick and Lewis 2017). On the other end aresatirical news sites which produce misleading content forentertainment. Regardless of the sources\u2019 intent and of howwe label them (satire, clickbait, or the more contested term\u201cfake news\u201d), misleading information has a negative effecton citizen\u2019s news consumption and on their ability to makefully informed decisions (Thorson 2016). Misinformation,once assimilated, is hard to eradicate from the convictionsof its audience (Lewandowsky et al. 2012). Hence, a largebody of scholarly work has focused on characterizing onlinemisinformation to prevent its spread. Notable among themare Horne and Adali\u2019s characterization of differences in thenews titles of fabricated and real stories (Horne and Adali2017), Golbeck et al.\u2019s attempt to automatically determinewhether a newswire article is satirical or not (Golbeck et al.2018), and Ferrara et al.\u2019s study of social bots spreading ru-mors on Twitter (Ferrara et al. 2014).\nDespite the scholarly efforts, identifying misinformation\nis still challenging. In fact, even humans perform poorly inidentifying non-obvious misinformation (Kumar, West, andLeskovec 2016). De\ufb01ning what constitutes high-quality in-formation requires careful considerations about content at-tributes and adherence to journalistic norms (Hayes, Singer,and Ceppos 2007; Diakopoulos 2015). Thus, recent researchfocuses on characterizing different types of misinformationalso, on summarizing reliable indicators, rather than auto-matically identifying information (Zhang et al. 2018).\nMotivated by the need for reliable indicators of misinfor-\nmation, in this work we characterize the landscape of newsproducers on Twitter. Instead of focusing on the prevalenceof one type of misinformation among news sources or theirsocial media audience, we study the relationship betweennews sources and their audience on Twitter through multipledimensions of news source quality.Data\nWe \ufb01rst compiled a list of news sources of interest. Then, wecollected tweets containing links to articles published by thesources. We complemented this with the full-text of the arti-cles linked from Twitter, and with multifaceted assessmentsof the news sources.\nList of news sources: We started by curating a list of news\nsources and combined a wide selection of both credible andquestionable sources. We relied on OpenSources\n1, a profes-\nsionally curated list of online sources available for free forpublic use. News sources in this resource range from cred-ible to misleading and outright fake websites. OpenSourcesheavily focuses on misinformation, with little emphasis oncredible content. Therefore, we complemented this list byincluding additional mainstream news websites. We \ufb01rst re-ferred to various online resources, including surveys by PewResearch\n2and NPR3. Then, a journalism and communica-\ntions expert re\ufb01ned the list by referring to the most circu-lated\n4and the most trusted news sources (Matsa and Shearer\n2018). In total, the list contains 639 distinct web domains.\nTweets containing links to sources: Next, we collected\nall tweets linking to news articles by the sources in our list.Speci\ufb01cally, we accessed data through the Twitter streamingAPI, continuously from September 2016 to February 2017,and \ufb01ltering those containing links to the news sources of in-terest. We collected a total of 31,567,501 tweets. We furthercollected all tweets that replied to them.\n5We use this mea-\nsure as a signal for audience attention to the original tweet.6\nNews articles linked from Twitter: Furthermore, we en-\nhanced our dataset by collecting the news articles that the\n1opensources.co\n2https://www.journalism.org/2014/10/21/political-\npolarization-media-habits/\n3https://www.npr.org/sections/alltechconsidered/2016/10/28/\n499495517\n4https://www.cision.com/us/2017/09/top-10-u-s-daily-\nnewspapers-2/\n5The dataset shows skewed distributions typical of social me-\ndia: although it includes over 2 million unique users, 60% of the\ntweets are authored by the top 1% most proli\ufb01c users.\n6We relied on the list of over 300 news bots from (Lokot and\nDiakopoulos 2016) to check for the presence of inorganic Twitteractivity. We found that bots in the list author less than 0.1% of our\ndataset. Out of the bots individually contributing over 1k tweets, all\nbut two exclusively share credible news by a single source (theseare bots by major outlets like New York Times and Washington\nPost, or sports bots). The remaining two bots are 365Arizona, shar-\ning overwhelmingly biased tweets from counterjihad.com and cen-terforsecuritypolicy.org, and dubvNOW, a newsletter born out of\nthe University of West Virginia sharing half credible (usatoday),\nhalf conspiracy (infowars) news. Given the minor impact of bots,we do not exclude them for the sake of completeness. Even if the\nlist above might have missed some non-organic accounts, we stress\nthat even the most proli\ufb01c bots would skew the number of co-\noccurring users between two sources at most by 1, as we disregard\ncontribution volume and focus only on user co-sharing practices.\n604\nwidespread satirical/clickbait conspiratorial right-wing/fake\nwashingtonexaminer.com usuncut.com activistpost.com angrypatriotmovement.com\nthegatewaypundit.com countercurrentnews.com 21stcenturywire.com usasupreme.com\nwnd.com occupydemocrats.com thedailysheeple.com truthandaction.org\nNewsmax.com attn.com russia-insider.com prntly.com\nheatst.com rawstory.com blacklistednews.com subjectpolitics.com\ndailycaller.com bipartisanreport.com washingtonsblog.com usanews\ufb02ash.com\ndailywire.com dailykos.com veteranstoday.com christiantimesnewspaper.com\namericanthinker.com addictinginfo.org investmentwatchblog.com ilovemyfreedom.org\nconservativereview.com politicususa.com ronpaulinstitute.org departed.co\nwsj.com alternet.org wearechange.org supremepatriot.com\nTable 1: Top 10 central news sources in each cluster\u2014i.e., sources that are the most cosine-similar to each KMeans centroid.\ntweets link to. We used the library newspaper to scrape\nthe text and publication date of the articles from the web. Wediscarded articles that are non-English or too short\n7. Since\nmultiple links within each web domain may refer to the samearticle, we identify duplicates by stemming and matching\ntheir plain text. We keep only the oldest instance of dupli-cate articles by publication date. The \ufb01nal dataset contains1,212,304 articles.\nExternal assessments of the news sources: We com-\npleted our dataset by gathering assessments of the newssources along multiple dimensions of factuality, journalis-tic integrity, and impartiality. Our choice of dimensions wasdictated by decades of research from journalism and com-munications scholars(Sundar 1999).\nFactuality : OpenSources, a professionally-curated list of\nnews sources, provides annotations for each news sourcebased on a taxonomy of twelve assessments, such as fakeand junk science. We use the primary annotation of each\nsource as a judgment of its factuality.\nJournalistic Integrity : Journalists at NewsGuard\n8label\nnews sources according to multiple criteria, includingwhether sources regularly correct errors or disclose own-ership and \ufb01nancing. Newsguard attributes a weight toeach assessment and combines them into an overall scorefrom 0 to 100. An increasingly important assessment ofthe journalistic stance of online sources is whether theyidentify as alternative or mainstream outlets. To this end,we label outlets as \u201calternative\u201d or \u201cmainstream\u2019 accord-ing to the corresponding lists of sources\nfrom Wikipedia.\nImpartiality : Finally, we also include assessments of po-\nlitical impartiality by Media Bias/Fact Check\n9and All-\nSides10. Both services divide new sources into \ufb01ve cat-\negories: biased left, center-left, center, center-right andright. After obtaining the assessments from their website,we performed semi-automatic matching of the news do-main names to the domains from OpenSources. We used\n7less than 140 characters since we \ufb01nd that those mostly consist\nof broken links or 404 pages for URLs that no longer exists.\n8newsguardtech.com\n9mediabiasfactcheck.com\n10allsides.coma fuzzy string match to \ufb01nd the best candidate in Open-\nSources, and manually validated the match.\nOut of 639 new sources, we include annotations for 595\nsources from OpenSources, 124 from Newsguard, 101 fromMediaBias, 42 from Allsides, and 26 from Wikipedia.\nRQ1: Communities of news sources based on\ncommon audience\nBy aggregating the news sharing practices of Twitter users,\nwhat communities of news sources emerge?\nMethod\nWe start by building a news source similarity space, basedon shared Twitter audience. Then, we characterize groups ofsources within this space.\nMeasuring audience similarity through co-shares: We\nbase our audience similarity measure on user co-shares: twosources are more similar when more users share links to bothof them. Intuitively, user-based similarity hypothesizes thatusers choose news sources according to their interests, suchas the topics covered in the articles or their partisan af\ufb01lia-tion; if many users choose two sources, those sources likelycater to a common interest. One issue with using the rawnumber of co-sharing users as a similarity measure is that itis bound by the overall number of users sharing a source.High-circulation sources are more likely to be shared in-dependently of the users\u2019 choices, and therefore may co-occur with other sources by chance. The converse holds forsmaller less popular news outlets. Therefore, we measure in-stead whether a surprisingly high number of users share two\nsources, given the number of users who share each sourceindependently from the other, using positive pointwise mu-\ntual information (PPMI) (Martin 2017).\nFinding groups of news sources sharing Twitter audi-\nence: We adopt an unsupervised machine learning ap-\nproach to cluster news sources according to user co-sharing practices. We use the popular K-Means method with\nkmeans++ initialization. One crucial step in K-means is totune the number of clusters and evaluate their validity. We\ufb01nd the best number of clusters by looking for an elbow\n605\npoint in explained variance, which offers a natural trade-\noff between accuracy and number of clusters. Thus, we as-\nsume four as the optimal number of news source clusters.\nWe check that the \ufb01nal clusters remain stable with respect todifferent random seeds.\nCharacterizing groups of news sources through exter-\nnal assessments of journalistic norms: Next, we look\nat the composition of each cluster in terms of external as-sessments of credibility, impartiality, and journalistic prac-tices. We aggregate the assessment for the sources withineach cluster. Since assessments lack complete coverage\u2014e.g., some sources have assessments for credibility but notfor transparency\u2014we look at each separately. We do notrun statistical tests on the differences between clusters, since\nthose would neglect the partial overlap of sources with dif-ferent assessments.\nRQ1: Results\nWe \ufb01nd four clusters of news sources based on Twitter co-sharing practices. Table 1 displays the top 10 news sourcesclosest to the cluster centroids. Next, we study the clustersby looking at the external assessments of factuality, impar-tiality, and journalistic integrity of their members. We con-clude our results by showing which of the external assess-ment best conforms to the Twitter audience-based clusters.\nCredibility: We study the credibility of the sources in\neach cluster. Figure 1 summarizes the \ufb01ndings. We \ufb01nd thatall clusters contain a variety of OpenSources assessments\u2014e.g., all clusters include conspiratorial sources to some ex-tent. However, their composition yields a clearly de\ufb01ned pic-ture. Cluster 1 is the largest of the four. It contains all of thecredible sources in our dataset, as well many misinformationsources of widespread appeal, such as political (Breitbart)and satirical (liberalbias.com) sources. High circulationsources like nytimes.com and wsj.com belong to the clus-ter. Cluster 2 is home to a signi\ufb01cant fraction of satirical andclickbait sources such as theonion.com, as well as left-wingactivist groups like dailykos.com. Cluster 3 hosts most ofthe conspiratorial, unreliable, and junk science sources, e.g.,corbettreport.com and prisonplanet.com. The fourth and lastcluster contains misinformation with an ulterior motive: it islargely comprised of fake news and biased sources such asabcnews.com.co\n11, abcnewsgo.co, nbc.com.co12. Although\ncluster 1 contains relatively widespread news sources, ninehateful news sources also belong to this cluster. Upon further\ninspection, we \ufb01nd that users piggyback on the popularity ofmainstream articles to promote hate by sharing links to bothin the same tweet; this explains the co-presence of hate andwidespread news in the co-sharing practices.\nImpartiality: Figure 2 shows the composition of the clus-\nters in terms of partisan bias. We present results based onassessments from MediaBias; results based on Allsides are\n11https://en.wikipedia.org/wiki/ABCnews.com.co\n12https://mediabiasfactcheck.com/nbc-com-co/\nFigure 2: Cluster composition in terms of external assess-ments of impartiality, according to MediaBias. We omitsources for which MediaBias does not provide assessments.All political sides are represented in cluster 1\u2014which isin line with their widespread appeal according to credibil-ity assessments; all sources in cluster 2, containing satireand clickbait, are left-wing; cluster 3, home to conspirato-rial sources, shows a mix of left and right-wing sources; allsources with an assessment in cluster 4, hosting predomi-nantly fake news, are right-wing.\nqualitatively consistent, therefore we omit them. Cluster 1\nincludes sources from all political sides. This is in line with\nthe \ufb01nding that emerged from the previous credibility as-sessments, that cluster 1 is home to high circulation credibleand misinformation news sources. In contrast, cluster 2, con-taining satire and clickbait, includes exclusively left-wingsources. Research from cognitive psychology suggests thatleft-wing political inclinations are more eager to respond topleasing content (Dodd et al. 2012), which may explain thecorrelation between satirical content and left-wing politicalcomposition of the cluster. Cluster 3, home to most conspir-atorial sources, shows a mix of left and right-wing sources,such as the left-leaning activistpost.com and the right-wingreturnofkings.com. Conspiracy theorizing is in fact a biparti-san issue (Oliver and Wood 2014). Finally, cluster 4, hostingpredominantly fake news, includes exclusively right-wingsources. Grinberg et al. also found a cluster of fake newssources sharing overlapping audiences on the extreme rightof the political spectrum (Grinberg et al. 2019).\nJournalistic integrity: Finally, we examine the journalis-\ntic integrity of the sources as assessed by Newsguard. We\ufb01rst look at the overall source quality score (0\u2013100, a higherscore means higher quality). A website needs a score of atleast 60 for it to receive an overall positive assessment. Theresults corroborate our previous \ufb01ndings. Cluster 1, whichcontains widespread news sources, receives the best scores:it has the highest mean (40.12) and median (32.5) score, andmany of its sources receive perfect scores. Cluster 2, followsa pattern similar to cluster 1. On the other hand, clusters3 (conspiratorial) and 4 (fake) contain mostly low-qualityjournalism. Barely any source in cluster 3 exceeds a score of20 (with the notable exception of counterpunch.org, a non-\n606\npro\ufb01t ultra-liberal magazine). Almost all sources in cluster\n4 are below the cutoff score of 60 for reputable sources.\nNext, we look at the break-down of the journalistic in-\ntegrity score into its different facets. Sources in clusters 1and 2 score systematically the highest on all criteria. In par-ticular, clusters 1 and 2 are notably higher than clusters 3and 4 in regularly correcting errors, presenting informationresponsibly, and avoiding publishing false content.\nTo summarize We discover four clusters of news sources,\naccording to common audience on Twitter. They show dis-tinct compositions in terms of credibility, impartiality, andjournalistic integrity. Therefore, we will interchangeably re-fer to the clusters with the following shorthand:\ncluster 1: widespread news sources\ncluster 2: satirical/clickbait news sources\ncluster 3: conspiratorial news sources\ncluster 4: right-wing/fake news sources\nWe stress that the characterization in this section applies to\nthe clusters collectively, and individual sources may showdifferent characteristics.\nRQ2: News source assessments distinguishing\nthe social news audience sphere on Twitter\nWe identi\ufb01ed communities of news sources that differ along\nmultiple assessments of credibility, integrity, and impartial-ity. However, which assessments correspond to differentchoices of news sources by the audience? We answer thisquestion by ranking the assessments according to how wellthey distinguish the communities of shared audience. To cor-roborate this, we show that the top-ranking assessments alsocorrelate highly with overall audience variance.\nWhich assessments best explain communities of shared\naudience? We measure how well grouping sources by\neach assessment conforms to the communities of shared au-dience. We label sources with the cluster they belong to,as well as with each assessment\u2014e.g., the left- and right-wing bias labels given by MediaBias. Then, we computehow much the assessment and cluster labels overlap, accord-ing to measures of cluster quality. We repeat this process forall assessments, and we rank them accordingly.\nWe \ufb01nd that the simple assessment of alternative vs. main-\nstream media sources shows the highest homogeneity, com-pleteness, and v-measure, whereas the assessments by Open-Sources show the highest Rand Index and Mutual Informa-tion. This discrepancy may be because few news sourceshave labels for the alternative vs. mainstream assessment,and therefore are penalized in the latter chance-adjustedmeasures. Furthermore, we ask if all the labels in Open-Sources\u2019 assessments\u201412 of them\u2014are actually informa-tive. We repeat the experiment assessing each label in aone-versus-all fashion, and we \ufb01nd that only few of themcorrespond to the audience-based clusters: bias, clickbait,conspiracy, fake, and satire rank higher than all other as-sessments in the study. This suggests that the opensourcesassessments may be redundant or, more concerningly, that\nonly few of them correspond to audience co-sharing prac-tices. Surprisingly, in all cases, the often-adopted assess-ments of partisan bias by both MediaBias and Allsides per-form poorly.\nWhich assessments correlate with audience variance?\nTo corroborate these \ufb01ndings, we investigate the correla-tion between source assessments and audience similarity.We use principal component analysis on the PPMI-weightedaudience similarity matrix, and identify the direction alongwhich audience similarity changes the most. We focus onthe \ufb01rst principal component, which explains the most vari-ance ( 18%, more than double the variance explained by the\nsecond component). The component correlates highly andpositively with assessments of factuality, such as conspiracyand junk science, and only mildly and negatively with po-litical bias. This con\ufb01rms that audience similarity does notsimply follow a polarized, partisan structure.\nFinally, to interpret along which direction does audience\nvary, we inspect the sources that load highly on the princi-pal component. On the positive end we \ufb01nd conspiratorial\nsources that propose a revolution in public institutions. For\nexample, the anarcho-capitalist conspiratorial website cor-bettreport.com includes stories about deep state and global-ist control, and loads among the most highly positive. Onthe negative end, instead, we \ufb01nd sources adopting the polit-ical status quo. For example, the website madpatriots.com,which leverages established conservative narratives to craftsensationalist headlines, such as \u201cviolent immigrants\u201d and\u201cred scare,\u201d loads the most negative. This echoes Starbird\u2019sobservation that political leanings of alternative news sites\nfeature an anti-/pro-globalist orientation, rather than conser-vative/liberal.\nRQ3: Identifying the source community of\nnews articles on Twitter\nThe previous sections identi\ufb01ed clusters of news sources\nshared by distinct audiences, and characterized the journalis-tic qualities of the sources within the clusters. Yet, if we seean article shared on Twitter, can we automatically identifythe cluster it belongs to? We provide a \ufb01ne-grained contentanalysis of the clusters articles. Then, we show that a classi-\ufb01er identi\ufb01es the correct cluster with over 80% accuracy.\nMethod\nIn particular, we look at the content, style, sentiment, andpsycholinguistic categories used in the articles, as well asthe overall engagement that the articles produce on Twitter.\nContent: First, we look at what kind of content distin-\nguishes the clusters. We rely on newspaper python moduleto extract the plaintext of the articles and to discard oneswhose primary language is not English. Then, we prepro-cess the plaintext using standard procedures of convertingto lowercase, removing punctuation and accents, stripingwhitespaces, and removing stopwords and links. Next, we\n607\nWe use Sparse Additive Generative models\u2014 SAGE (Eisen-\nstein, Ahmed, and Xing 2011)\u2014to \ufb01nd words that are spe-\nci\ufb01c to each cluster. SAGE uses a regularized log-odds ratio\nmeasure to contrast word distributions between one corpusof interest against a baseline corpus. We contrast articleswithin each cluster against all the remaining. For example,we compare the distribution of words in widespread newssources vs. the distribution of words in all other clusters. Theoutput will give the distinguishing words of the widespreadnews sources, relative to other clusters.\nStyle: Content analysis tells us what sources in each clus-\nter are writing about. However, how are they writing aboutit? We next turn to writing style. We use two feature sets: cueverbs, which re\ufb02ect the sources\u2019 choices in terms of journal-istic reporting style , and stylometric features re\ufb02ecting the\nsophistication of the writing style.\nCue verbs correspond to factuality judgments of journal-\nistic reports (Sauri 2008). Sources may choose certain typesof cues to nudge their audience into believing a reportedclaim. For example, \u201cWSJ learns\nthat...\u201d asserts more cer-\ntainty than \u201cWSJ suspects that.... \u201d F ollowing (Soni et al.\n2014), we use \ufb01ve groups of cue verbs common in Twitter:\u201cReport\u201d (e.g., say, report), \u201cKnowledge\u201d (e.g. learn, admit),\u201cBelief\u201d (e.g. think, predict), \u201cDoubt\u201d (e.g. doubt, wonder),and \u201cPerception\u201d (e.g., sense, hear).\nWe also look at stylometric features that capture the arti-\ncles\u2019 style beyond choices for reporting. On the one hand,sources may embrace more formal linguistic registers toappear more credible. On the other hand, they may makecontent more accessible by simplifying their language. Weassess the readability of the text using the SMOG index.SMOG estimates the grade level needed for understanding\nthe article, and correlates well with human judgments of textclarity\u2014high SMOG implies more complex language.\nAdditionally, we look into markers of complexity: the\noverall number of words, number of informative words,type-token ratio, and long and complex words, all give us anindication of the writer\u2019s effort. Finally, we include functionword types, such as conjunctions and prepositions. Functionwords do not carry meaning per se . However, they re\ufb02ect\nhow people are communicating (Tausczik and Pennebaker2010).\nSentiment and psycholinguistic categories: Sentiment\nand psycholinguistic categories depict the emotional and\npsychological states evoked in the articles. We use twoscores from V ADER (Hutto and Gilbert 2014): compound\nscore measures sentiment polarity, while neutrality captures\nthe proportion of words that are not emotionally charged.Intuitively, sensationalist journalism uses more emotionally\ncharged and polarized language (Zhang et al. 2018).\nAdditionally, LIWC (Tausczik and Pennebaker 2010) of-\nfers validated categories of words for assessing such psy-cholinguistic dimensions. We use the following categories.At the most super\ufb01cial level, content word categories explic-itly reveal the articles\u2019 focus\u2014we include the topical cate-gory personal concerns . The categories pronouns and verb\ntense also expose the attentional focus of the writers, al-\nthough more subtly. For example, pronouns like \u201cwe\u201d and\u201cour\u201d often indicate references to group identity. Similarly,\nwriters may use verb tense to signal endorsement: for exam-ple, past tense increases psychological distance with the re-ported facts, compared to present tense. We include the cate-gories social and emotional because they may signal less in-\nformative, more sensationalist content. We also include mo-\ntion,cognitive processes , and sense words, since they are\nknown signals of truthfulness. Finally, the spoken category\nexposes departure from journalistic style, capturing, for in-stance, the occurrence of swearwords and non\ufb02uencies. Toaccount for the high variability in article length, for everyarticle we compute the fraction of words in each category.\nTwitter engagement: Finally, we gauge the feedback that\narticles receive on Twitter. We compute the number of fol-lowers and friends (followees) of the Twitter users to assesstheir authority or hub status. For individual tweets, we reportthe number of favorites, replies, and retweets, which signalthe reach of and engagement with the tweet.\nMeasuring differences across clusters We use a machine\nlearning pipeline to assess differences in articles\u2019 languageand engagement between clusters. We train a multinomiallogistic regression model (henceforth MNLogit), penalizedfor dealing with sparsity and multicollinearity. We compareeach cluster to the baseline \u201cwidespread news sources.\u201d Wechoose \u201cwidespread news sources\u201d as a reference class be-cause, according to our previous \ufb01ndings, we expect thecontent to be more varied and mainstream, and becauseit contains the largest number of articles. Since MNlogitis a one-vs-all model, we validate the results with a post-hoc statistical signi\ufb01cance test of the observed differencesbetween clusters. We perform a series of non-parametricKolmogorov\u2013Smirnov (KS) tests for all six possible pair-wise combinations of the clusters. We chose the KS test overthe parametric t-student to account for the fact that none ofour features are normally distributed. For all features we as-sessed normality using Anderson\u2013Darling tests.\nRQ3 Results: characterizing of news communities\u2019\nlanguage\nWe \ufb01nd that clusters signi\ufb01cantly differ in the articles\u2019 lan-\nguage and in the engagement they produce.\nContent: Table 2 reports the words that best distinguish\narticles from each cluster. The \u201cwidespread news\u201d clus-ter, home to a wide range of sources, intuitively does notshow a predominant topic: we \ufb01nd references to currentevents, like the \ufb01re which burnt the Grenfell tower and themarches against the Malaysian Prime Minister Najib. Theother three clusters, instead, manifest more focused top-ics. Cluster \u201csatirical/clickbait\u201d features several polarizingterms (like lgbtq, @realdonaldtrump, impeach, protectors),as well as words often used to poke fun of stereotypical con-spiracy theorists (like pyramids, extraterrestrial), and fooditems often featured in prodigious diets meant to generateclick revenue (like coconut, turmeric).\n13Cluster \u201cconspir-\n13the top terms like \u201cmashshare\u201d and \u201c\ufb02ipboard\u201d are artifacts of\nthe automated text extraction, and correspond to the text found in\n608\natorial\u201d aptly uses the conspiratorial lingo, such as refer-\nences to the New World Order, Zionists, Illuminati, and the\nRothschild\u2014all being frequent actors in conspiratorial nar-\nratives of world domination. Cluster \u201cright-wing/fake\u201d usesterms of the hyper-conservative propaganda, such as refer-ences to the alleged lack of virility of the left (like effemi-nization, soy bois, and cucks) and political opponents (likeantifa, leftwing, Ocasio-Cortez). In summary, we \ufb01nd thatthe content of the articles conforms with the external assess-ment of the sources within the news communities.\nStyle: Beyond content, news sources may make subtle\nstylistic choices to convey their message. Recall that all re-sults are relative to the reference class, \u201cwidespread news.\u201dAll three remaining clusters use signi\ufb01cantly fewer report\ncue words. Report cue words, such as sources report that...\norWhistleblower tells Congress.. , are a landmark of formal\njournalistic style. While \u201csatirical/clickbait news\u201d expressmore doubt in its reports than \u201cwidespread news,\u201d \u201cconspira-torial news\u201d and \u201cright-wing/fake\u201d doubt less. In fact, \u201ccon-spiratorial news\u201d use more cue verbs expressing knowledge.Knowledge cues, such as Scientists \ufb01nd that change driven\nlargely by increased carbon dioxide.. , are part of the class\nof factive predicates, which imply the truth of the claim thatfollows (Sauri 2008).\nWe then turn to stylometric features to probe the sophis-\ntication of writing style. \u201cSatirical/clickbait news\u201d use moredif\ufb01cult to read language (possibly mimicking a pompouswriting style), whereas \u201cright-wing/fake news\u201d use simplerlanguage, according to SMOG. This is re\ufb02ected in the othermarkers of complexity: \u201cright-wing/fake news\u201d use fewerlong words, and a less varied vocabulary according to thetype-token ratio. Yet, \u201cright-wing/fake news\u201d also use morepolysyllabic words (complex words in the table) and moreword types overall. This may signal a somewhat diluted lan-guage: more verbose, but not as informative. We \ufb01nd thatthis is indeed the case. All three clusters use a larger pro-portion of function words than \u201cwidespread news,\u201d whichmeans that the informative words are relatively fewer. Ina nutshell, \u201cwidespread news\u201d adhere to the expectationsof standard journalistic style, whereas \u201csatirical/clickbaitnews,\u201d \u201cconspiratorial news,\u201d and \u201cright-wing/fake news\u201duse less formal, less informative, more accessible language.In particular, \u201cconspiratorial news\u201d and \u201cright-wing/fakenews\u201d express more certainty in their reporting, possibly toappear more credible.\nSentiment and psycholinguistic categories: Abstracting\nfrom the level of content, we next analyze the sentiment andpsycholinguistic categories expressed in the articles. Figure3 reports the differences between clusters. First, we sum-marize the characteristics that cluster \u201csatirical/clickbait,\u201d\u201cconspiratorial,\u201d and \u201cright-wing/fake\u201d have in common,and compare them to cluster \u201cwidespread news sources\u201d. Asexpected, clusters \u201csatirical/clickbait,\u201d \u201cconspiratorial,\u201d and\u201cright-wing/fake\u201d use more emotionally charged and polar-\nized language (i.e. a higher score for neutral score). In par-\nsocial buttons typical of clickbait sites. For integrity, we did not\neditorialize them from the table.ticular, we \ufb01nd that the clusters disproportionately express\nanger death-related words. In addition, they use more in-\nformal (spoken word categories like swear) language, with\nmore references to collective identity (we pronoun) than\u201cwidespread news sources\u201d. These psycholinguistic cate-gories signal attempts of engaging rather than objective con-tent (Hartung et al. 2016). Moreover, \u201csatirical/clickbait,\u201d\u201cconspiratorial,\u201d and \u201cright-wing/fake\u201d express more cer-tainty (cognitive mechanism categories like certain and in-sight) through visual language references (see); visual lan-guage makes concepts more concrete, thus more memo-rable and accessible. Although journalistic news reportingstyle implies a temporal focus on the present or the nearpast, we \ufb01nd that \u201csatirical/clickbait,\u201d \u201cconspiratorial,\u201d and\u201cright-wing/fake\u201d focus more on the future. This is in linewith the stylistic analyses in the previous section, suggestingthat \u201cwidespread news sources\u201d is the one that is most con-forming to a formal reporting practices. Yet, psycholinguis-tic categories allow us to characterize clusters with higherprecision. For example, \u201csatirical/clickbait\u201d uses more so-cial words and references biological processes like sexu-ality and health. Cluster \u201cconspiratorial\u201d shows the mostworry with the self (I pronoun) and the least focus on thepresent. Cluster \u201cright-wing/fake\u201d references conservativevalues like family, home, and work, and expresses the mostnegative sentiment (anger and anxiety).\nTwitter engagement The previous sections show that the\nclusters departing from journalistic practices may seek theaudience\u2019s attention through multiple content, stylistic, andframing devices. Are they successful? Twitter users shar-ing links from the \u201cwidespread\u201d cluster follow less, and arefollowed more than Twitter handles that share links from\u201csatirical/clickbait,\u201d \u201cconspiratorial,\u201d and \u201cright-wing/fake.\u201dIn other words, users tweeting about mainstream content\ntend to be information authorities, rather than hubs. Links\nfrom sources in \u201cwidespread\u201d also engage more users, bothin terms of replies and retweets, than links in the other clus-ters. Intuitively, the only exception is that the audience es-pecially favors links from the \u201csatirical/clickbait.\u201d It appearsthat fringe content does not, after all, elicit as many reactionsas mainstream articles. All reported effects are statisticallysigni\ufb01cant. We skip showing result table due to space limits.\nAutomatically identifying an article\u2019s cluster We con-\nclude by demonstrating that the differences between clus-ters are not just qualitative, but that we can automaticallyinfer the cluster that produced an article with high preci-\nsion. We train an XGBoost model using the style, sentiment,and psycholinguistic features presented above; we also in-\nclude the top 50k terms in the articles weighted by TF-IDFas topical features. Since classes are heavily imbalanced, weuse SMOTE to oversample the minority classes in the train-ing set\u2014test sets contain only actual and not synthetic data.We assess model performance in a 10-fold strati\ufb01ed shuf\ufb02e-split scheme, and tune hyperparameters using a randomized,cross-validated search approach.\nWe \ufb01nd that we can classify all clusters with a weighted\naccuracy of 82% (weighted F1 score). We can classify\u201cwidespread news sources\u201d almost perfectly (90%). Al-\n609\nwidespread news satirical/clickbait news conspiratorial news right-wing/fake news\ntriforium, gassama, otw,\nluzhniki, agung, moorland,sidebars, shortcode, win-drush, southgate, grenfell,yas, nbsp, \ufb01tr, istockphoto,liege, playback, sidebar,abbey, prix, najib, redis-tributed, getty, rebounds,derby, greets, afp, heathrow,caption, rewrittenmashshare, \ufb02ipboard, stum-\nbleupon, digg, attn, anonhq,truea, screengrab, loading,republish, spoilers, impeach,featured, protectors, dapl,screenshot, queer, realdon-aldtrump, pinterest, eichen-wald, lgbt, extraterrestrial,lgbtq, attribution, android,tumblr, turmeric, pyramids,coconut, deliciousaltnews, in5d, naturalnews,\ndmca, coward, analyses, glp,adsense, quoting, eyeo, abu-sive, nwo, ammol, button,sheeple, pravda, vibration,\nrothschilds, anonymous,neocon, neocons, violation,aipac, illuminati, disclaimer,click, zionist, cabal, user,oligarchycommments, eagler, over-\nsign, duely, effeminization,photopin, adblock, ocasio,digestible, corruptly, un-\nmasking, gleaned, minted,\nquaking, antifa, hawkins,disable, turley, shoebat,czars, im, glazov, overturns,cuck, chapters, leftwing,cortez, slinging, bois,pitchforks\nTable 2: Most distinguishing words for each cluster, extracted using SAGE. Intuitively, no predominant topic distinguishes\u201cwidespread,\u201d since it is the most diverse in terms of sources. The other clusters show distinctive topics, in line with thecharacterization of the sources composing them. \u201csatirical/clickbait,\u201d include satirical and clickbait sources, sports polarizing(e.g. impeach, protectors) and stereotypical conspiratorial words (e.g. pyramids, extraterrestrial). Cluster \u201cconspiratorial newssources,\u201d include conspiracy and junks science sources, adopts a conspiratorial lingo (e.g. illuminati, sheeple). Cluster \u201cright-wing/fake,\u201d include fake news and right-wing sources, uses hyper-conservative propaganda terms (e.g. pitchforks, [soy] bois).\nthough recall is relatively lower for the other clusters (84%).\nWe reach high precision for all other clusters. On the onehand we see that several articles are false positives for\u201cwidespread\u201d, which demonstrates that alternative contentmay be subtly similar to mainstream. On the other hand,achieving high precision has practical implications in au-tomating the triaging of fringe content, because it allows todistinguish between satirical, conspiracy, and fake content.\nDiscussion\nImplications for characterizing the news sphere\nbased on social media audience co-shares\nRecent research calls for a nuanced picture of the social\nnews sphere that goes beyond established partisan polariza-tion (Starbird 2017; Guess et al. 2018; Druckman, Leven-dusky, and McLain 2018). Our results add to this line ofwork by detailing the social news sphere as a multifacetedenvironment, and quantify the need for multiple assessmentsto understand audience choices, instead of relying on stan-dalone external assessments.\nWe \ufb01nd that several dimensions of credibility and journal-\nistic stance intertwine with political partisanship to shape thesocial media readership. For instance, a cluster containingentertaining misinformation like satire is also home to far-left activism. A second cluster which is deeply embedded\nin established political narratives publishes extreme right-wing propaganda and fake news. The correlation betweenpoliticized audiences and misinformation outlets mirrors ex-isting research, that associates individuals on the politicalleft-wing to engagement with positive content, whereas theright-wing to threats (Dodd et al. 2012). Yet, misinformationon Twitter does not only target politicized audiences, or even\naudiences that are distinct from those of credible sources.For example, a cluster focusing on conspiracism and \u201cal-ternative truths\u201d comprises of sources spanning the politi-\ncal spectrum. Similarly, a cluster of widespread news con-tains all of the credible sources under study, but also manypolitical misinformation sources, and even hateful content.In other words, quality and questionable information live in\nclose quarters.\nThe present work sheds light on the relationship be-\ntween users and news sources. Speci\ufb01cally, we \ufb01nd that cer-tain types of misinformation are better than others at ex-\nplaining this relationship. Distinct Twitter audiences seemto follow factoid misinformation (conspiracy and junk sci-ence), misinformation with an ulterior motive (fake, bias),and misinformation with a frame of engagement (clickbait,satire). In particular, the drivers for extreme left- and right-wing activism appears different. The left appears associatedwith satirical, socially versed and engaging misinformation.Right-wing activism instead appears associated with emo-tionally negative, fake stories of threats by the political ad-versary. Our data-driven categorization of the four clustersof news communities is corroborated by the composition ofthe clusters, the external assessments of the sources withinthem, and the analyses of the news articles they publish. We\ufb01nd that other assessments of news sources, e.g., labels ofhate, (un)reliable, rumor, state and political, do not help an-alyze the social news sphere\u2014be it because they are not asfrequent in the data, not reliable as indicator, or not as in-formative of sharing practices. These \ufb01ndings advance ourunderstanding of what assessments of news sources are rel-evant to navigate the way social media audience shares them.\nOur results end on a positive note. We \ufb01nd that fringe\ncontent does not receive as many reactions as mainstreamcontent. This suggests that, although Twitter users may beexposed to both quality and questionable information, theydo not grant as much attention to the latter. However, a more\nconclusive analysis of the effect of fringe content should also\ntake into account the overall reach, strength, polarity, andquality of those reactions.\nIt is far from our intentions to put a moral or severity\njudgment on different types of misinformation. Censoringright-wing fake news or conspiracy theories as more morallywrong or dangerous than clickbait frauds and hyper-liberalhacktivism might do more damage than good. For example,\n610\nsentiment clickbait conspir. fake\nsentimentintercept -2.1\u22c6 -1.36\u22c6 -2.92\u22c6\ncompound -0.04\u22c6 0.00 -0.31\u22c6\nneu -0.33\u22c6 -0.38\u22c6 -0.39\u22c6\npsycholinguistic clickbait conspir. fake\npronounsintercept -2.17\u22c6 -1.58\u22c6 -3.01\u22c6\nwe 0.01 * 0.02\u22c6 0.02\u22c6\nthey 0.01\u22c6 -0.01\u22c6 0.01\u22c6\nyou 0.00 -0.04\u22c6 -0.04\u22c6\ni -0.01\u22c6 0.02\u22c6 -0.08\u22c6\nshehe 0.03\u22c6 -0.18\u22c6 -0.02\u22c6\nipron 0.03\u22c6 -0.00 0.08\u22c6\nsocial\nwordsfriends 0.01\u22c6 -0.10\u22c6 -0.06\u22c6\nfamily 0.05\u22c6 -0.10\u22c6 0.04\u22c6\nemotion\nwordsanger 0.10\u22c6 0.17\u22c6 0.24\u22c6\nsad 0.02\u22c6 -0.01 * -0.01 *\nanx 0.04\u22c6 0.00 0.05\u22c6\nposemo 0.06\u22c6 0.31\u22c6 -0.09\u22c6\ncognitive\nmech.discrep -0.04\u22c6 -0.06\u22c6 -0.03\u22c6\ncause 0.13\u22c6 0.25\u22c6 -0.02\u22c6\ntentat 0.09\u22c6 0.02\u22c6 0.10\u22c6\nexcl 0.01 + 0.02\u22c6 -0.01 *\ncertain 0.23\u22c6 0.23\u22c6 0.21\u22c6\ninsight 0.19\u22c6 0.32\u22c6 0.11\u22c6\nperceptionhear -0.14\u22c6 -0.41\u22c6 -0.19\u22c6\nsee 0.12\u22c6 0.07\u22c6 0.09\u22c6\nfeel -0.01\u22c6 -0.10\u22c6 -0.11\u22c6\nbilogical\nprocessessexual 0.03\u22c6 -0.14\u22c6 -0.03\u22c6\nbody 0.11\u22c6 0.07\u22c6 0.04\u22c6\nhealth 0.05\u22c6 0.10\u22c6 -0.03\u22c6\ningest 0.11\u22c6 0.05\u22c6 -0.01 *\ntemporal\nfocusfuture 0.04\u22c6 0.03\u22c6 0.03\u22c6\npresent 0.03\u22c6 -0.11\u22c6 0.18\u22c6\npast -0.06\u22c6 -0.30\u22c6 0.03\u22c6\nrelativitymotion -0.01\u22c6 -0.03\u22c6 0.04\u22c6\nspace -0.01 * 0.03\u22c6 -0.11\u22c6\ntime 0.02\u22c6 -0.15\u22c6 -0.17\u22c6\npersonal\nconcernsachiev -0.03\u22c6 -0.24\u22c6 -0.10\u22c6\nwork -0.06\u22c6 -0.09\u22c6 0.03\u22c6\nleisure -0.14\u22c6 -0.15\u22c6 -0.24\u22c6\ndeath 0.01\u22c6 0.09\u22c6 0.01 *\nhome 0.02\u22c6 -0.05\u22c6 0.02\u22c6\nrelig -0.07\u22c6 0.02\u22c6 0.16\u22c6\nmoney -0.15\u22c6 0.00 + -0.31\u22c6\nspoken\ncategories\ufb01ller 0.08\u22c6 0.02\u22c6 0.07\u22c6\nnon\ufb02u -0.03\u22c6 -0.00 + 0.00\nswear 0.16\u22c6 0.14\u22c6 0.08\u22c6\nassent -0.00 -0.01\u22c6 0.05\u22c6\nFigure 3: Sentiment and psycholinguistic differences be-\ntween clusters. Cluster 1 being the baseline comparison clus-ter is not shown. All numbers should be interpreted in refer-ence to cluster 1. Clusters 2, 3, and 4 express more emotional(less neu) and informal (spoken words) language than cluster1. Cluster 2, satirical and clickbait, uses more social wordsand references sexuality and health. Cluster 3, conspirato-rial, shows the most self-concerns (I pronoun) and the leastfocus on the present. Cluster 4, right-wing fake news, refer-\nences values like family, home, and work, and expresses the\nmost negative sentiment (anger and anxiety). \u22c6\u21d2p<. 001,\n\u2217\u21d2p<. 01,+\u21d2p<. 05attempts to directly confront extreme political or conspira-\ntorial views may prove counterproductive (Bail et al. 2018;\nPeter and Koch 2016; Lewandowsky et al. 2012). Instead, it\nis crucial to understand the types of misinformation that theaudience engages with on social media. The present work isa step in this direction. This will, in turn, inform media lit-eracy campaigns to educate the audience about journalisticnorms and practices.\nSocial media audience as a costly signal of news\nsource similarity\nOne merit of using audience as a similarity measure be-\ntween news sources is that the sources cannot easily manip-ulate their position in this space. For example, a junk sciencesource to position itself close to credible information sourceswould need to control a large fraction of shares of its owncontent, and to also consistently share markers of crediblecontent. These two procedures would be costly in terms ofeffort for the junk science source, and counterproductive interms of audience targeting. In other words, audience simi-larity is a signal that is hard to falsify (Donath 2011). A wordof caution: hard does not mean impossible. However costlya signal, individuals with enough resources and motivationcan, for example, acquire fake audience or fake engagement.\nImplications for gatekeeping the social news sphere\nOnline platforms are increasingly embracing source-levelassessments for nudging their users towards high-qualitynews while preserving access to a pluralistic social newssphere. For example, Facebook offers additional informa-tion about sources appearing in the users\u2019 feed\n14. YouTube\nlabels videos that come from state-funded media outlets15.\nSimilarly, Microsoft integrates NewsGuard in their mobilebrowser\n16. Yet, it is easy to quickly create, rebrand, and shut\ndown online news sources: all it takes is to edit a web page.Whereas the position of a news source in the audience simi-larity space is a costly signal, appearing as a news source inthe \ufb01rst place is a ludicrously cheap one. Efforts like Open-Sources continuously examine news sources that soon dis-appear. Indeed, several in our dataset were shut down withinmonths from our data collection. Assessing news sources is\ntime-consuming and requires the labor of experts, whose ef-\nforts must be directed towards critical cases. Our data-drivenapproach can help social media platforms triage new infor-mation outlets in two possible ways. First, our clusteringapproach discussed in RQ1 can identify sources that sharethe same audience with a news outlet for which external as-sessments is already available. Then, it can propagate thoseassessments to the news outlet, thus alleviating the labor-intensive annotation of the ever growing space of informa-tion sources. Second, the classi\ufb01er discussed in RQ3 can\nclassify the articles by the news outlet with high precisionas either widespread content, or one of the different types of\n14https://newsroom.fb.com/news/2018/04/inside-feed-article-\ncontext\n15https://money.cnn.com/2018/02/02/media/youtube-state-\nfunded-media-label/\n16https://twitter.com/MWautier/status/1081346843487854593\n611\nmisinformation: satirical/clickbait, conspiratorial/junk sci-\nence, and right-wing/fake news. Experts may use these in-\ndications to triage questionable content.\nImplications for developing novel assessments for\nthe social news sphere\nWhereas automation can help scale the process of assess-\ning news sources, the matter of communicating those as-sessments to the users requires careful consideration. In ourcomparison of expert- versus audience-based clustering, wehighlighted the necessity of using multiple existing assess-ments to characterize news sources accurately. However,such redundant assessments complicate the clear communi-cation of a news source\u2019s quality to their users. We believethis challenge is best addressed through research. Commu-nication and social computing scholars have access to exclu-sive domain knowledge that is essential to synthesize novelassessments, so as to better describe current user practicesin their choice of news sources. To this end, our approachallows researchers to combine multiple assessments of newssources, and to interpret them in the light of how users inter-act with them.\nLimitations\nIt is important for us to highlight some limitations in thiswork, which the readers should take into consideration wheninterpreting the results. One such limitation is arguably ourchoice of data. We focus on an expansive list of 639 English-speaking news sources. However, this list is likely not rep-resentative of the landscape of news outlets on Twitter. Toaddress this shortcoming, one would arguably require thecomplete data from the platform for an extended period oftime\u2014in fact, simply accessing a subsample of the Twitterstream would result in neglecting smaller sources. Yet, eventhen one would be omitting large players of the wider infor-mation ecosystem that includes television and talk radio. Forpractical reasons, we focused on sources for which reliableexternal assessments were available.\nA second major limitation is that we rely on expert as-\nsessments by third-party initiatives. As a byproduct, not allsources have the same assessment coverage. For instance,one source might be assessed from MediaBias but not fromAllsides, and vice-a-versa. In our analyses, we address thisissue by looking at different providers of assessments sepa-rately. A different approach might involve in-house humanannotators to harmonize assessments for all sources. How-ever, training annotators for journalistic norms is still a sub-ject of research (Zhang et al. 2018). Furthermore, we relyon source-level assessments of news, although different arti-cles by one same source might have different qualities. Oneexemple is RT, which shares a combination of high-qualitynews reports and state propaganda (Starbird et al. 2018).This choice is in line with our goal of studying the audienceof news sources, as we focus on the aggregate characteristicsof the news media sphere. Yet, practitioners should use cau-\ntion when applying source annotations to individual articles.\nFinally, a crucial limitation is that our results are largely\ncorrelational in nature. In particular, we \ufb01nd strong corre-lations between external assessments of news sources, and\ntheir social media audience. Yet, qualitative research in on-line journalism and media literacy would be essential to un-derstand whether those characteristics of the news sourcesare driving the users\u2019 choice of sharing them.\nConclusion\nIn this research, we characterized the landscape of newssources based on their audience on Twitter. We showed thatnews sources aggregate in communities of shared audience,that uphold distinct factuality standards, political partisan-ship, and journalistic norms.\nIn particular, we uncovered four data-driven communi-\nties of news sources: highly circulated news spanning theentire political spectrum; engaging misinformation, such asclickbait and satire; factoid misinformation, such as conspir-atorial and junk science sources; and misinformation withan ulterior motive, such as far-right propaganda and fabri-cated news sources. We found distinguishing stylistic and\ntopical markers that match with the characteristics of thenews sources composing the clusters. For example, whereassources in the widespread news cluster use more formal lan-guage, the conspiratorial cluster adopts an overcon\ufb01dent re-\nporting style. The difference between the clusters is measur-able: classi\ufb01ers can automatically distinguish between newsarticles coming from different clusters with high precision.Thus, the Twitter audience delineates different segments ofthe news media landscape, differing in both the characteris-tics of the news sources and of the news content that Twit-ter users engage with. Yet, our \ufb01ndings challenge the com-mon understanding of the news media landscape, exhibitingcomplex interrelation between popularity, partisan lines, andjournalistic quality\u2014with deep implications for gatekeepingand triaging misinformation in social media.\nAcknowledgments\nWe would like to thank the Social Computing lab at VirginiaTech for comments on the early versions of the paper and\nthe anonymous reviewers for their feedback. This work ispartially supported by NSF grant IIS-1755547.\nReferences\nBail, C. A.; Argyle, L. P.; Brown, T. W.; Bumpus, J. P.; Chen, H.;\nHunzaker, M. B. F.; Lee, J.; Mann, M.; Merhout, F.; and V olfovsky,A. 2018. Exposure to opposing views on social media can increase\npolitical polarization. PNAS 115(37):9216\u20139221.\nBakshy, E.; Messing, S.; and Adamic, L. A. 2015. Exposure\nto ideologically diverse news and opinion on Facebook. Science\n348(6239).\nBarber \u00b4a, P.; Jost, J. T.; Nagler, J.; Tucker, J. A.; and Bonneau, R.\n2015. Tweeting From Left to Right: Is Online Political Commu-\nnication More Than an Echo Chamber? Psychological Science\n26(10).\nBessi, A.; Coletto, M.; Davidescu, G. A.; Scala, A.; Caldarelli, G.;\nand Quattrociocchi, W. 2015. Science vs Conspiracy: Collective\nNarratives in the Age of Misinformation. PLOS ONE 10(2).\nConover, M. D.; Ratkiewicz, J.; Francisco, M.; Gonc \u00b8alves, B.;\nFlammini, A.; and Menczer, F. 2011. Political polarization on\ntwitter. In ICWSM .\n612\nDiakopoulos, N. 2015. Picking the NYT Picks: Editorial Criteria\nand Automation in the Curation of Online News Comments. #ISOJ\n5(1).\nDodd, M. D.; Balzer, A.; Jacobs, C. M.; Gruszczynski, M. W.;\nSmith, K. B.; and Hibbing, J. R. 2012. The political left rollswith the good and the political right confronts the bad: Connecting\nphysiology and cognition to preferences. Philos. Trans. Royal Soc.\nB367(1589).\nDonath, J. 2011. Signals , cues and meaning.\nDruckman, J. N.; Levendusky, M. S.; and McLain, A. 2018. No\nNeed to Watch: How the Effects of Partisan Media Can Spread via\nInterpersonal Discussions. Am. J. Pol Sci 62(1).\nEisenstein, J.; Ahmed, A.; and Xing, E. P. 2011. Sparse additive\ngenerative models of text. In ICML .\nElejalde, E.; Ferres, L.; and Herder, E. 2017. The Nature of Real\nand Perceived Bias in Chilean Media. HT.\nFerrara, E.; Varol, O.; Davis, C.; Menczer, F.; and Flammini, A.\n2014. The Rise of Social Bots. arXiv preprint arXiv:1407.5225\n(grant 220020274):1\u201311.\nGarimella, K.; Morales, G. D. F.; Gionis, A.; and Mathioudakis,\nM. 2018. Political Discourse on Social Media: Echo Chambers,\nGatekeepers, and the Price of Bipartisanship. WWW .\nGolbeck, J.; Everett, J. B.; Falak, W.; Gieringer, C.; Graney, J.;\nHoffman, K. M.; Huth, L.; Ma, Z.; Jha, M.; Khan, M.; Kori, V .;Mauriello, M.; Lewis, E.; Mirano, G.; Mohn IV , W. T.; Mussenden,\nS.; Nelson, T. M.; Mcwillie, S.; Pant, A.; Shetye, P.; Shrestha,\nR.; Steinheimer, A.; Auxier, B.; Subramanian, A.; Visnansky, G.;Bhanushali, K. H.; Bonk, C.; Bouzaghrane, M. A.; Buntain, C.;\nChanduka, R.; and Cheakalos, P. 2018. Fake News vs Satire. In\nProceedings of the 10th ACM Conference on Web Science - WebSci\n\u201918, 17\u201321. New York, New York, USA: ACM Press.\nGrinberg, N.; Joseph, K.; Friedland, L.; Swire-Thompson, B.; and\nLazer, D. 2019. Fake news on twitter during the 2016 U.S. Presi-\ndential election. Science 378.\nGuess, A.; Nyhan, B.; Lyons, B.; and Rei\ufb02er, J. 2018. Avoiding\nthe Echo Chamber about Echo Chambers.\nHartung, F.; Burke, M.; Hagoort, P.; and Willems, R. M. 2016.\nTaking perspective: Personal pronouns affect experiential aspectsof literary reading. PLoS ONE 11(5).\nHayes, A. S.; Singer, J. B.; and Ceppos, J. 2007. Shifting Roles,\nEnduring Values: The Credible Journalist in a Digital Age. Journal\nof Mass Media Ethics 22(4):262\u2013279.\nHorne, B. D., and Adali, S. 2017. This Just In: Fake News Packs a\nLot in Title, Uses Simpler, Repetitive Content in Text Body, More\nSimilar to Satire than Real News.\nHorne, B. D., and Adali, S. 2018. An Exploration of Verbatim\nContent Republishing by News Producers. NECO .\nHutto, C. J., and Gilbert, E. 2014. Vader: A parsimonious rule-\nbased model for sentiment analysis of social media text. In ICWSM .\nKulshrestha, J.; Zafar, M. B.; Noboa, L. E.; Gummadi, K. P.; and\nGhosh, S. 2015. Characterizing Information Diets of Social Media\nUsers. In ICWSM .\nKumar, S.; West, R.; and Leskovec, J. 2016. Disinformation on the\nWeb: Impact, Characteristics, and Detection of Wikipedia Hoaxes.\nWorld Wide Web Conference Committee 591\u2013602.\nLewandowsky, S.; Ecker, U. K. H.; Seifert, C. M.; Schwarz, N.;\nand Cook, J. 2012. Misinformation and Its Correction: ContinuedIn\ufb02uence and Successful Debiasing. Psychological Science in the\nPublic Interest 13(3):106\u2013131.Lokot, T., and Diakopoulos, N. 2016. News Bots: Automating\nnews and information dissemination on Twitter. Digital Journalism\n4(6):682\u2013699.\nMartin, T. 2017. community2vec: Vector representations of online\ncommunities encode semantic relationships. In NLP CSS Work-\nshop .\nMarwick, A., and Lewis, R. 2017. Media Manipulation and Disin-\nformation Online. Data & Society Research Institute 1\u2013104.\nMatsa, K. E., and Shearer, E. 2018. News use across social media\nplatforms 2018. In Pew Research Center .\nMukerjee, S.; Gonz \u00b4\nalez-Bail \u00b4on, S.; and Maj \u00b4o-V\u00b4azquez, S. 2018.\nNetworks of Audience Overlap in the Consumption of Digital\nNews. Journal of Communication 68(1):26\u201350.\nOliver, J. E., and Wood, T. J. 2014. Conspiracy theories and the\nparanoid style(s) of mass opinion. Am J Pol Sci 58(4).\nPeter, C., and Koch, T. 2016. When Debunking Scienti\ufb01c Myths\nFails (and When It Does Not). Science Communication 38(1):3\u201325.\nSauri, R. 2008. A factuality pro\ufb01ler for eventualities in text.Soni, S.; Mitra, T.; Gilbert, E.; and Eisenstein, J. 2014. Modeling\nFactuality Judgments in Social Media Text. In ACL.\nStarbird, K.; Arif, A.; Wilson, T.; Koevering, K. V .; Ye\ufb01mova, K.;\nand Scarnecchia, D. 2018. Ecosystem or echo-system? exploringcontent sharing across alternative media domains. In ICWSM .\nStarbird, K. 2017. Examining the Alternative Media Ecosystem\nthrough the Production of Alternative Narratives of Mass Shooting\nEvents on Twitter. In ICWSM .\nSundar, S. S. 1999. Exploring receivers\u2019 criteria for perception of\nprint and online news. JMCQ 76(2).\nTausczik, Y . R., and Pennebaker, J. W. 2010. The Psychological\nMeaning of Words: LIWC and Computerized Text Analysis Meth-ods. J. Lang. Soc. Psychol. 29(1).\nThorson, E. 2016. Belief Echoes: The Persistent Effects of Cor-\nrected Misinformation. Political Communication 33(3):460\u2013480.\nWebster, J. G., and Ksiazek, T. B. 2012. The Dynamics of Audi-\nence Fragmentation: Public Attention in an Age of Digital Media.\nJournal of Communication 62(1):39\u201356.\nZhang, A. X.; Robbins, M.; Bice, E.; Hawke, S.; Karger, D.; Mina,\nA. X.; Ranganathan, A.; Metz, S. E.; Appling, S.; Sehat, C. M.;Gilmore, N.; Adams, N. B.; Vincent, E.; and Lee, J. 2018. A\nStructured Response to Misinformation: De\ufb01ning and Annotating\nCredibility Indicators in News Articles. In WWW .\n613", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Characterizing the social media news sphere through user co-sharing practices", "author": ["M Samory", "VK Abnousi", "T Mitra"], "pub_year": "2020", "venue": "\u2026 AAAI Conference on Web and Social \u2026", "abstract": "We describe the landscape of news sources which share social media audience. We focus  on 639 news sources, both credible and questionable, and characterize them according to"}, "filled": false, "gsrank": 492, "pub_url": "https://ojs.aaai.org/index.php/ICWSM/article/view/7327", "author_id": ["m5hUWjsAAAAJ", "_rx-yNgAAAAJ", "5q_BkVAAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:yW4gzrvmVf8J:scholar.google.com/&output=cite&scirp=491&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D490%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=yW4gzrvmVf8J&ei=YLWsaK7sAqzWieoPic2ZoAU&json=", "num_citations": 18, "citedby_url": "/scholar?cites=18398865546983534281&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:yW4gzrvmVf8J:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://ojs.aaai.org/index.php/ICWSM/article/download/7327/7181/"}}, {"title": "Detecting stance in media on global warming", "year": "2020", "pdf_data": "Detecting Stance in Media on Global Warming\nYiwei Luo1Dallas Card2Dan Jurafsky1;2\nStanford University\n1Department of Linguistics2Department of Computer Science\nfyiweil, dcard, jurafsky g@stanford.edu\nAbstract\nCiting opinions is a powerful yet understud-\nied strategy in argumentation. For example,\nan environmental activist might say, \u201cLead-\ning scientists agree that global warming is a\nserious concern,\u201d framing a clause which af-\n\ufb01rms their own stance ( that global warming\nis serious ) as an opinion endorsed ( [scientists]\nagree ) by a reputable source ( leading ). In con-\ntrast, a global warming denier might frame the\nsame clause as the opinion of an untrustworthy\nsource with a predicate connoting doubt : \u201cMis-\ntaken scientists claim [...].\u201d Our work studies\nopinion-framing in the global warming (GW)\ndebate,1an increasingly partisan issue that has\nreceived little attention in NLP. We introduce\nGlobal Warming Stance Dataset (GWSD) , a\ndataset of stance-labeled GW sentences, and\ntrain a BERT classi\ufb01er to study novel aspects\nof argumentation in how different sides of a de-\nbate represent their own and each other\u2019s opin-\nions. From 56K news articles, we \ufb01nd that sim-\nilar linguistic devices for self-af\ufb01rming and\nopponent-doubting discourse are used across\nGW-accepting and skeptic media, though GW-\nskeptical media shows more opponent-doubt.\nWe also \ufb01nd that authors often characterize\nsources as hypocritical, by ascribing opinions\nexpressing the author\u2019s own view to source en-\ntities known to publicly endorse the opposing\nview. We release our stance dataset, model,\nand lexicons of framing devices for future\nwork on opinion-framing and the automatic de-\ntection of GW stance.\n1 Introduction\nAscribing opinions to other people is a power-\nful yet understudied strategy in argumentation.\n1Throughout, we use the term debate to refer to the ex-\nistence of contrasting opinions about GW expressed in the\nmedia; it is important to emphasize that there is virtually\n100% consensus among scientists regarding the reality of an-\nthropogenic global warming (Powell, 2017).\nSOURCEPREDICATEOPINIONFew  researchersClimate expertsMost  Americansbelieveclaimtend to  agreehumans have negligible impact on the climateman-made greenhouse gases are responsible for global warmingthe report exaggerates climate change risks Figure 1. Examples of SOURCE ,PREDICATE , and\nOPINION components, and within components, exam-\nples of af\ufb01rming and doubting framing devices.\nFor example, an environmental activist might say,\n\u201cLeading scientists agree that global warming is\nserious,\u201d whereas a global warming denier could\nsay, \u201c Mistaken scientists claim that global warm-\ning is serious.\u201d In both these examples, the em-\nbedded clause ( that global warming is serious ) is\npresented as an opinion belonging to a source entity\n(scientists ). However, differences in the choice of\npredicate ( agree vs.claim ) and in how the source\nis described lead to very different interpretations.\nWe henceforth refer to the use of such [ENTITY]\n[EXPRESS] [STATEMENT] sentences as opinion-\nframing , and to the respective components as the\nSOURCE ,PREDICATE , and OPINION (see Fig. 1).\nDespite its pervasiveness in argumentative dis-\ncourse, opinion-framing is understudied as a per-\nsuasive strategy. This paper studies opinion-\nframing in the media coverage of global warming\n(GW), an increasingly partisan issue in the United\nStates (Pew Research Center, 2020) that has re-\nceived little attention in NLP despite its real world\nurgency. We focus on acts of opinion-framing rep-\nresenting self-af\ufb01rming andopponent-doubting dis-\ncourses, i.e., discourse af\ufb01rming one\u2019s own OPIN -\nIONS (embedded clauses ascribed to a SOURCE , as\ndepicted in Fig. 1) and discourse casting doubt on\n3296\nFindings of the Association for Computational Linguistics: EMNLP 2020 , pages 3296-3315\nNovember 16 - 20, 2020. \u00a92020 Association for Computational LinguisticsarXiv:2010.15149v2  [cs.CL]  16 Jan 2021\nthe other side\u2019s. Studying such discourses requires\na way to identify the stance of a given OPINION\nwith respect to GW, but this is a challenging task.\nTo this end, we introduce GWSD: G lobal\nWarming Stance Dataset, a dataset for detecting\nand analyzing GW stance in text. We collect hu-\nman judgments of GW stance for 2K sentences\nwith Amazon Mechanical Turk (AMT)2and use\nour dataset to train a BERT-based classi\ufb01er that\nachieves 75% accuracy (competitive with human\nperformance) for GW stance detection. Extending\nprior work in NLP and linguistics, we develop lex-\nicons of af\ufb01rming and doubting framing devices\nwith respect to the PREDICATE that embeds the\nOPINION (e.g., know vs.claim ) and the SOURCE to\nwhich the opinion is ascribed (e.g., a peer-reviewed\nstudy vs.a misleading paper ) (see Fig. 1).\nWe then apply our model and lexicons to study\ntwo questions about opinion-framing in argumen-\ntation: Q1: Do different sides of a debate (in\nthis case, GW-accepting and GW-skeptical media)\nshow symmetry in their use of self-af\ufb01rming and\nopponent-doubting discourse? We might expect\nsome similarities (e.g., the use of agree to frame\nOPINIONS expressing one\u2019s own side\u2019s stance, or\nthe use of claim to cast doubt on OPINIONS from\nthe opposing side), but given inherent asymme-\ntries in the nature of the GW debate, it is not clear\nwhether such strategies will be found across sides\nto equal extents.\nSecond, since opinion-framing is a way of\nputting words into someone\u2019s mouth, we also ask\nQ2: In cases where OPINIONS are ascribed to a\nnamed entity with a known (public) stance, does the\nstance of the OPINION match the expected stance\nof the named entity?\nApplying our model to a set of 500K OPINIONS\n(Opfull) extracted from 56K GW articles, we \ufb01nd\nthat GW-skeptical media engages in comparatively\nmore opponent-doubt, though both sides of the de-\nbate show more self-af\ufb01rmation overall, and use\nsimilar sets of framing devices for each respective\ndiscourse type. We also \ufb01nd that opinion-framing\ndoes indeed ascribe OPINIONS differing from the\novert views of entities to those entities nonethe-\nless, as part of a rhetorical strategy of ascribing\nhypocrisy: authors portray their own OPINION as\n2We also experimented with tweets from GW-\nactivists/skeptics and headlines from extreme conservative/lib-\neral outlets as potential sources of softly stance-labeled\nsentences, but found that classi\ufb01ers trained on these data\nperform poorly on news discourse.being held (in private) by \ufb01gures who endorse the\nopposite OPINION (in public).\nOur contributions are the following:\n1.GWSD , a dataset of 2K sentences from GW\nnews with annotations for stance.\n2.A weighted extension of BERT competitive\nwith human performance for classifying the\nstance of a sentence with respect to GW.\n3.Lexicons of af\ufb01rming and doubting PREDI -\nCATES (e.g., know, claim ) and SOURCE modi-\n\ufb01ers ( e.g., peer-reviewed, misleading ).\n4.Analyses on a set of 500K opinions from GW\nnews to illustrate the utility of our dataset and\nlexicons for studying opinion-framing.\nWe release our dataset, model, and lexicons as part\nof this paper.3\n2 Related work\nOur work is related to social psychology research\non persuasion (Cialdini, 1993; Orji et al., 2015)\nand recent NLP research on argumentation, such as\npredicting argument convincingness (Habernal and\nGurevych, 2016; Simpson and Gurevych, 2018)\nand studying discourse-level and non-linguistic fea-\ntures predictive of persuasion (Yang and Kraut,\n2017; Zhang et al., 2016). The latter\u2019s work on\nself- vs. opponent-coverage is particularly relevant\nto the GW debate and we apply a similar catego-\nrization to the stance of ascribed opinions.\nAlso relevant is the literature on factuality and\nspeaker commitment (de Marneffe et al., 2011;\nSoni et al., 2014; Werner et al., 2015; Rudinger\net al., 2018; Jiang and de Marneffe, 2019), and\nrelatedly, work studying how words can express\nsubjectivity or bias (Riloff and Wiebe, 2003; Re-\ncasens et al., 2013; Pryzant et al., 2020). Our cur-\nrent paper builds upon previous work by examining\nsuch triggers as opinion-framing devices in an argu-\nmentation context, where biases related to people\u2019s\nprior beliefs may interact with the lexical effects of\nthese words.\nOpinion-framing can be thought of as a special\ncase of the broader phenomenon of framing as dis-\ncussed in the communications and political science\nliteratures (Entman, 2006; Lakoff and Ferguson,\n2006; Chong and Druckman, 2007), as well as in\nNLP (Tsur et al., 2015; Field et al., 2018; Roy and\n3https://github.com/yiweiluo/GWStance\n3297\nGoldwasser, 2020). Both phenomena serve to em-\nphasize particular aspects of an issue, and are often\nused with the intent to in\ufb02uence perception of that\nissue. Our attention to the component of SOURCE\nin instances of opinion-framing is also informed by\ncommunications research on the messenger effect\n(that people\u2019s perceptions of a message may de-\npend heavily on the message source) (Bolsen et al.,\n2019; Myrick and Evans Comfort, 2020; Fielding\net al., 2020; Esposo et al., 2013). Furthermore, our\ninterest in predicates of opinion attribution is in-\nspired by communications studies examining how\nthe choice of predicate ( sayvs.assert ) can encode\njournalist stance (Caldas-Coulthard, 2002) and bias\naudience perception of the quoted entity (Gidengil\nand Everitt, 2003). Finally, our dataset contribution\nbuilds on Mohammad et al. (2016), who created\nthe \ufb01rst climate change stance task and dataset.\n3 GWSD: A dataset for GW stance\nTo enable our study of opinion-framing, and to fa-\ncilitate further work on stance, we create a new\npublicly-available dataset of OPINION spans ex-\ntracted from GW news articles (described in \u00a73.1)\nthat we have annotated with stance judgements us-\ning AMT ( \u00a73.2). To investigate potential annotator\nbiases, we study the impact of annotator character-\nistics on their perception of stance (with approval\nfrom our Institutional Review Board) ( \u00a73.3), and\ncombine ratings so as to infer a distribution over\nstance labels for each span while accounting for\nbias ( \u00a73.4), which we release along with the raw\nannotations.\n3.1 Extracting sentences for the dataset\nOur base dataset consists of OPINION spans ex-\ntracted from 56K GW news articles, published\nfrom Jan. 1, 2000 to April 12, 2020 by 63 U.S.\nnews sources. We collected these articles using\nthe MediaCloud API4and SerpAPI.5The key-\nwords we used for API requests were: fclimate\nchange, global warming, fossil fuels, carbon diox-\nide, methane, co2g. We note that some of the arti-\ncles in our dataset come from newswires (N=1.3K),\nbut as we show later, including wire articles does\nnot affect our studies\u2019 conclusions. Moreover, since\nit is ultimately up to media outlets to decide which\nwire articles to publish, we believe that instances of\n4https://cyber.harvard.edu/research/m\nediacloud\n5https://serpapi.com/search-apiLeft-leaning outlets Right-leaning outlets\nNYT 6K Breitbart 2.7K\nMoth. Jones 3.2K Fox 2.6K\nWaPo 2K Forbes 2K\nCS Monitor 1.9K Wash. Times 1.4K\nThe Nation 1.4K Daily Caller 1.2K\nV ox 1.4K Newsmax 1.2K\nDem. Now 1K Wash. Exam. 1K\nTotal 20K Total 36K\nTable 1. Number of unique articles from the top 7 left-\nleaning and right-leaning media outlets in our dataset\n(LL and RL), by volume of articles contributed. We\ncategorize political leaning using the Media Bias/Fact\nCheck project.\nopinion-framing from wire articles are still re\ufb02ec-\ntive of what an outlet endorses (despite not originat-\ning from the outlet). We also include op-ed articles\nin our dataset, as their exclusion is made challeng-\ning by idiosyncrasy in their coding across outlets.\nFuture work might exclude op-ed articles for model\ntraining and analysis. Please refer to Appendix A\nfor details on our \ufb01ltering and de-duplication steps.\nTab. 1 and Fig. 2 summarize the distribution of\narticles by source.\nTo identify the rhetorical components of relevant\nsentences, we make use of syntactic dependency\nparsing to extract embedded OPINION spans (e.g.,\nScientists believe that [ climate change requires\nimmediate action ]) from a given article, as well as\nspans for SOURCE (who or what the OPINION is\nascribed to) and PREDICATE (the verb that syntacti-\ncally embeds the OPINION ). Note that we exclude\nOPINIONS under the scope of negation or modals.\nOur pipeline consists of \ufb01rst passing each article\nthrough the spaCy pre-processing pipeline with a\nneural coreference resolution add-on,6then extract-\ning and annotating instances of SOURCE ,PREDI -\nCATE and OPINION using a rule-based algorithm\n(please refer to Appendix B). To validate our al-\ngorithm, we manually annotated 25 articles and\ncompared results. We found that a dependency\nparsing-based approach has a high recall, identify-\ning all clausal complements including some false\npositives such as indirect questions and subjunc-\ntive clauses. We therefore used several lexical re-\nsources to \ufb01lter the extracted clauses to indicative\n6https://github.com/huggingface/neura\nlcoref , which implements the model from Clark and Man-\nning (2015).\n3298\n2007 2011 2015 2019\nLeft-leaning media0100020003000400050006000Counta)\nVox\nNew York Times\nMother Jones\nWashington Post\nThe Nation\nGuardian (US)\nChristian Science Monitor\nDemocracy Now\nBuzzfeed\nother\n2007 2011 2015 2019\nRight-leaning mediab)\nNewsmax\nFox\nRedstate\nWashington Examiner\nWashington Times\nBreitbart\nDaily Caller\nAmerican Thinker\nForbes\notherFigure 2. Number of GW articles in our dataset from 2007 to 2020 in a)Left-, b)Right-leaning media.\nstatements.\nFinally, since many of the OPINIONS that we\nextracted are not explicitly on the topic of GW, we\nonly keep the OPINION spans that contain a stem\nfrom a set of 73 manually curated keywords (e.g.,\nclimat, environ, temperatur ).\n3.2 Crowd-sourcing labels for the dataset\nWe used AMT to label a subset of 2,050 OPINION\nspans containing high-precision keywords (see Ap-\npendix C). The set of 2,050 spans was constructed\niteratively by randomly sampling, then manually\n\ufb01ltering spans containing potentially upsetting ma-\nterial (e.g., mocking Greta Thunberg\u2019s disability) or\nthat were off-topic (e.g., used \u201cclimate\u201d in the sense\nof a workplace environment). For each OPINION ,\nwe collected judgements as to whether it expresses\nthe target opinion: \u201cClimate change/global warm-\ning is a serious concern,\u201d with the potential labels\nbeing \u201cagree,\u201d \u201cneutral,\u201d or \u201cdisagree.\u201d\nFollowing 4 pilot studies, we decided to collect\n8 judgements per item (to enable robust analysis\nof demographic variation in annotator judgements),\nfor a total of 16,400 annotations, paying the Cali-\nfornia minimum wage of $12USD per hour. Using\ntypical exclusion criteria, we recruited a set of 398\nquali\ufb01ed annotators over 5 rounds and had them\nrate 30-50 items. We also asked for basic demo-\ngraphic information and their personal opinions on\na series of questions related to GW (see Appendix\nD for details and an example).\nAlthough stance datasets are typically created\nwith the notion of a \u201ctrue\u201d label for each item, we\nnote that there is some degree of inherent ambiguity\nin this task due to the complex nature of the GW\ndebate as well as the items\u2019 being taken out of\ncontext. The average inter-annotator agreement\n(IAA) measured as Krippendorff\u2019s alpha rangedfrom 0.54 to 0.64 over the 5 rounds of annotation,\nthough the vast majority of disagreements were\nbetween adjacent labels. Some items with high\ndisagreement are shown in Tab. 2, showing the\npossibility of genuine ambiguity in GW stance.\n3.3 Demographic effects on annotation\nGiven that GW has become a polarized issue in the\nUS, we test whether we observe any bias related\nto party af\ufb01liation in stance annotation. Past work\nhas called attention to the importance of consid-\nering demographic biases in annotation (Cowan\nand Khatchadourian, 2003; Sap et al., 2019). In-\ntuitively, we might expect that those skeptical of\nGW would be more likely to perceive a sentence as\nexaggerating its threat, and therefore more likely to\nclassify the sentence as one that suggests that GW\nis a serious concern (even though they themselves\nmay disagree).\nIn order to test for the presence of demographic\nbias, we make use of Bayesian hierarchical ordinal\nregression models to estimate the effect of various\nannotator characteristics, such as party af\ufb01liation\n(Gelman and Hill, 2007), which we \ufb01t using Stan\n(Carpenter et al., 2017). Because we have 8 anno-\ntations per item and 30-50 annotations from each\nannotator, we model variation in both items and\nworker biases, with the latter drawn from a hierar-\nchial prior incorporating annotator characteristics\n(please see Appendix E for details).\nAs expected, we do \ufb01nd clear evidence of a slight\nbias along party lines. For a typical OPINION , (self-\nidenti\ufb01ed) Republicans are approximately 1.05 ( \u0006\n0.016 s.d.) times more likely to label an item as\n\u201cagree\u201d compared to non-Republicans, and simi-\nlarly less likely to respond with \u201cdisagree.\u201d We see\nthe opposite trend for Democrats, though the effect\nof the latter is mitigated by the inclusion of addi-\n3299\n1.Global warming is inevitably going to be, at best, managed. 2.Global warning will be over-\nridden by this effect, giving humankind and the Earth 30 years to sort out our pollution.\n3.The global warming debate is over. 5.Global warming would open stretches of the Arctic\nOcean to shipping and drilling.\nTable 2. Examples of items eliciting the highest disagreement among annotators (measured as entropy over labels).\nEach of these items was annotated with all 3 labels \u2013 \u201cagree,\u201d \u201cneutral,\u201d and \u201cdisagree.\u201d The stance of these items\nseems to depend not only on the linguistic content present but also on who the speaker might be, or what the\nstatement is said in response to, making them dif\ufb01cult to label.\ntional covariates. More surprisingly, we also \ufb01nd\na slight gender bias, with those who self-identify\nas female being 1.04 times more likely to respond\nwith \u201cagree\u201d (\u00060.011 s.d.). This effect is robust to\nthe inclusion of other variables, but should be inter-\npreted with caution, as women were somewhat un-\nderrepresented in our study (see Tab. 7 in Appendix\nE for full modeling results). Regardless, this rein-\nforces the importance of taking potential annotator\nbiases into account (Cowan and Khatchadourian,\n2003; Sap et al., 2019) and is suggestive for further\nresearch.\n3.4 Aggregating annotations\nBecause some workers are more reliable than oth-\ners, we again make use of Bayesian modeling to\naggregate the annotations for each item. Drawing\ninspiration from MACE (Hovy et al., 2013), we\n\ufb01t a model which includes a distribution over la-\nbels associated with each item (i.e., agree, neutral,\ndisagree), corresponding biases for each annota-\ntor, and a parameter indicating the degree to which\nthey are in\ufb02uenced by their own biases. Whereas\nMACE assumes that annotators sometimes choose\nlabels at random on individual instances, but oth-\nerwise identify the true label, we assume that an-\nnotators are always somewhat in\ufb02uenced by their\nbiases, but to differing degrees. This model allows\nus to simultaneously infer a distribution over labels\nfor each instance (i.e., the probability of each label\nbeing chosen by a typical worker), as well as bias\nand vigilance terms for each annotator. (Please see\nAppendix F for full model details). Based on this\nmodel, we assign the highest probability label to\neach OPINION , as summarized in Table 3.\n4 A model for GW stance classi\ufb01cation\nIn order to classify stance in Opfull, the full dataset\nof 500K OPINIONS , we train a model using the set\nof 2K annotated examples. The goal of this task\nis to predict the stance of a sentence Stoward theLabel Count\nneutral 873\nagree 777\ndisagree 400\nTable 3. Distribution of labels in GWSD , as aggregated\nby our model when the label with highest inferred prob-\nability is selected.\ntarget opinion T(\u201cClimate change/global warming\nis a serious concern\u201d). To evaluate performance,\nwe \ufb01rst select a random test set of 200 annotated\ninstances (strati\ufb01ed by label and political leaning\nof the source media outlet) and use 5-fold cross\nvalidation to train on the remaining 1850 examples.\nHere, we report on variations on a BERT classi-\n\ufb01er (Devlin et al., 2019), as well as a linear baseline,\nin order to provide a sense of relative performance\nin comparison to past work. To ensure compari-\nson against a strong baseline, we perform a grid\nsearch over hyperparameters for both approaches,\nand choose the best model from each according to\nvalidation accuracy, evaluating only the best model\nof each type on the held-out test set.\nFor our neural model, we use the general-\npurpose BERT basearchitecture, trained by mini-\nmizing cross-entropy loss. We use the Transform-\ners library7as the basis for the models that we\ndevelop and compare. As potential augmentations,\nwe experiment with a) \ufb01ne-tuning the base model\nas a language model to unlabeled data; b) includ-\ning the text of the target opinion as an input to\nthe model; and c) using label weights as opposed\nto simply using the most probable label. For the\nweighted version, we include a copy of each train-\ning instance with each label, along with an instance\nweight corresponding to the label probability esti-\nmated by our label aggregation model above. (Full\ndetails of hyperparameter tuning in Appendix H).\n7https://huggingface.co/transformers/\n3300\nThe test-set performances of best models we ob-\ntain are shown in Table 4, along with majority class\nand human performance (see Appendix F). The\nbest performing BERT model used weighted data\nand incorporated the target opinion as an input, but\nwas not \ufb01ne-tuned as a language model. The ac-\ncuracy of this model is competitive with human\nperformance (estimated using leave-one-out sub-\nsets of 10% of annotators), and mis-classi\ufb01cations\nof \u201cagree\u201d as \u201cdisagree\u201d or vice versa occurred in\nless than 9% of test examples.\nFurther inspection of the validation results re-\nveals that training on the weighted data offers a\nstatistically signi\ufb01cant improvement on validation\naccuracy, but the expected performance is statisti-\ncally indistinguishable with respect to \ufb01ne-tuning\nand/or incorporating the target opinion as an input.\nThe best linear model was a simple l2-weighted\nlogistic regression classi\ufb01er using unigrams and\nbigrams (details in Appendix H).\nacc FA FN FDFavg\nMajority class 0.43 0.0 0.52 0.0 0.17\nLinear 0.62 0.55 0.66 0.56 0.60\nBERT 0.75 0.68 0.76 0.75 0.73\nHuman 0.71\nTable 4. Test-set performance, reported as accuracy,\nand macro-F1 score for each label (agrees, neutral, dis-\nagrees) and on average, of the best model of each type,\ntrained using hyperparameters values corresponding to\nthe model with the best cross-fold validation perfor-\nmance, with the overall best performing model shown\nin bold. See Appendix F for further details on how hu-\nman performance was estimated.\n5 Analyses\nIn this section, we \ufb01rst describe the lexicons of\nframing devices we use for our analyses ( \u00a75.1). We\nthen present analyses that address our two research\nquestions.\nIn\u00a75.2, we \ufb01nd that qualitatively -speaking, both\nsides leverage similar linguistic framing devices\nfor self-af\ufb01rmation and opponent-doubt, but quan-\ntitatively -speaking, GW-skeptical media engages\nin more opponent-doubt. In \u00a75.3, we \ufb01nd that both\nsides use opinion-framing to ascribe OPINIONS ex-\npressing their own stance to SOURCES known to\npublicly endorse the opposing view, thereby depict-\ning such SOURCES as hypocritical.5.1 Linguistic framing devices\nSince GW opinion is closely connected to one\u2019s at-\ntitude toward scienti\ufb01c evidence, we focus on fram-\ning devices with epistemic and evidential connota-\ntions in creating lexicons of af\ufb01rming and doubting\nframing devices. We draw from work on factuality,\ncommitment, and persuasion, as well as our own\nlexical semantic analysis, to create seed word sets;\nthese seed sets are then augmented using WordNet\nto become our \ufb01nal lexicons.\nAf\ufb01rming devices We include factive and semi-\nfactive predicates ( point out, understand (N=20)),\nstudied extensively in de Marneffe et al. (2011),\nSaur\u00b4\u0131 and Pustejovsky (2012), Rudinger et al.\n(2018), Jiang and de Marneffe (2019), Ross and\nPavlick (2019), among others. We add verbs with\nconnotations of factivity and/or high subject com-\nmitment ( con\ufb01rm, attest, certify, validate (N=7)).\nWe also add high commitment adjectives ( proven,\nsettled (N=4)) and adjectives of \u201chyping\u201d from\nLerchenmueller et al. (2019) ( breakthrough, expert\n(N=38)). To complement these adjectives that af-\n\ufb01rm the quality of evidence, we add modi\ufb01ers that\naf\ufb01rm the quantity of evidence and index consensus\n(many, numerous, dozens of (N=11)).\nDoubting devices We include words from se-\nmantic \ufb01elds largely antonymous to those repre-\nsented in the af\ufb01rming seed words: neg-factive\nverbs (Saur \u00b4\u0131 and Pustejovsky, 2009) such as claim,\npretend (N=5), low commitment verbs ( doubt, dis-\npute (N=3)), low commitment adjectives ( dubi-\nous, so-called (N=7)), adjectives of undermining\n(\ufb02awed, debunked (N=47)) and adjectives indexing\nlack of consensus ( few, contentious (N=6)). We\nadditionally include verbs with argumentative con-\nnotations ( argue, insist (N=11)), as these can rein-\nforce frames of debate and controversy.\nWe hope that our full lexicons (see Appendix I)\nwill be useful for future work that looks at opinion-\nframing, especially in the context of other scienti\ufb01c\ndebates (e.g., the COVID-19 pandemic).\n5.2 Study 1 results\nWe apply our stance classi\ufb01cation model to Opfull8\nto get a stance label for all embedded OPINIONS .\nWe restrict our analysis to OPINIONS receiving a\n8Because OPINION spans from certain media outlets are\nover-represented in Opfull, we repeat all analyses in Studies\n1 and 2 while excluding data points from the top 5 LL and\nRL outlets (10 total) and obtain largely similar results (see\nAppendix L) to those presented in the main paper.\n3301\nnon-neutral label, as we can better guarantee hav-\ning few mis-classi\ufb01cations of GW-agree (the sen-\ntence agrees with the target that GW is a serious\nconcern) as GW-disagree (the sentence disagrees\nwith the target that GW is a serious concern), and\nvice versa. We use political leaning as catego-\nrized by the Media Bias/Fact Check project9as a\nproxy for stance toward GW, with left-leaning and\nright-leaning outlets (LL and RL) corresponding\nto GW-accepting and GW-skeptical media, respec-\ntively. To \ufb01nd instances of self-af\ufb01rmation in GW-\naccepting media, we retrieve GW-agree OPINIONS\noccurring with a PREDICATE orSOURCE modi\ufb01er\nfrom the group of af\ufb01rming devices (e.g., show,\npeer-reviewed ); to \ufb01nd instances of opponent-\ndoubt, we retrieve GW-disagree OPINIONS occur-\nring with PREDICATES orSOURCE modi\ufb01ers from\nthe set of doubting devices (e.g., claim, mislead-\ning). This is repeated for GW-skeptical media, with\nOPINION stances swapped.\nThe resulting distribution over coverage types is\nshown in Fig. 3, indicating that the two sides are\nnotsymmetric in terms of their quantities of each\ncoverage type: though both sides engage in more\nself-af\ufb01rmation overall, GW-skeptical media (i.e.,\nRL) shows a greater amount of opponent-doubt.\nThis pattern corroborates prior work documenting\nthe use of doubt by opponents of GW to dilute the\nscienti\ufb01c consensus (Oreskes and Conway, 2011).\n0.0 0.1 0.2 0.3 0.4 0.5LL\nRLProportions of coverage types across media\nSelf-Affirming\nOpponent-Doubt\nFigure 3. Proportions (among non-neutral OPINIONS )\nof self-af\ufb01rming vs. opponent-doubting coverage in LL\nand RL, showing that LL primarily exhibits discourse\nwhere a GW-agree OPINION occurs with an af\ufb01rming\ndevice, whereas RL exhibits more balanced amounts\nof self-af\ufb01rmation and opponent-doubt. Most of the re-\nmaining OPINIONS are framed by words beyond those\nin our lexicons.\nTurning to qualitative aspects of self-af\ufb01rming\nand opponent-doubting discourse, we \ufb01nd that the\ntwo sides show symmetry in the framing devices\nused: devices that LL tends to use to frame GW-\nagree OPINIONS (e.g., understand, recall, discover ;\n9https://mediabiasfactcheck.com/important, peer review ) tend to be used by RL\nfor GW-disagree OPINIONS , and devices that RL\nuses to frame GW-agree OPINIONS (e.g., pretend,\nclaim; inaccurate, alleged ) tend to be used in LL\nfor GW-disagree OPINIONS (see Fig. 4). We mea-\nsure the tendency for a framing device to occur\nwith a given OPINION stance as a log-odds-ratio be-\ntween the number of times it frames OPINIONS of\neach stance, excluding words that occur under 20\ntimes (see Appendix J for details). Broken down by\nthe individual framing device (Figs. 5-6), we also\nsee that, with some exceptions, the use of framing\ndevices across LL and RL displays some symme-\ntry. Notably, there seems to be a lack of af\ufb01rming\nmodi\ufb01ers framing GW-agree OPINIONS in RL, sug-\ngesting that RL uses different modi\ufb01ers to qualify\nSOURCES as convincing.10\nAffirming Doubting2.0\n1.5\n1.0\n0.5\n0.00.51.0Log odds of ascribing a GW-agree opinion\nA)   Verbal Predicates\nAffirming Doubting4\n2\n024\nB)   Source Modifiers\nMedia slant\nLL\nRL\nFigure 4. Distribution of the (log) odds of ascribing\na GW-agree OPINION in LL and RL for af\ufb01rming and\ndoubting a)PREDICATES ;b)SOURCE modi\ufb01ers, show-\ning that LL tends to ascribe GW-agree OPINIONS using\naf\ufb01rming devices over doubting devices, whereas RL\ntends to ascribe GW-agree OPINIONS using doubting\nover af\ufb01rming devices. Each point represents one fram-\ning device, and the size corresponds to its frequency in\nOpfull.\n10As a robustness check, we repeat the same log-odds com-\nputation for the subset of data that excludes articles from\nnewswires and \ufb01nd that the results are highly correlated with\nthe full dataset (Pearson\u2019s r= 0.90, p < 0:0001 for verbs,\nPearson\u2019s r= 0.82, p <0:0001 for modi\ufb01ers).\n3302\n2\n 1\n 0 1understand**\nrecall*\nconcede\ndiscover\nshow**\nindicate\nknow*\ndemonstrate\nconclude*\nlearn\nconfirm\nacknowledge\nreveal\nrealize\naffirm\nsuspect\nagree\nsee\npointout\ndeclare\nadmit\nargue*\nnotice\nimagine\nlie\nassert\nremember\nverify\nmaintain\nassume*\nproclaim\ninsist*\nallege\nclaim**\nboast\npretend*Log odds of ascribing a GW-agree opinionMedia slant\nLL\nRLFigure 5. Log odds of ascribing a GW-agree OPINION\nforaf\ufb01rming anddoubting predicates present in LL\nand RL, showing an overall symmetry in the devices\nLL and RL use for self-af\ufb01rmation and opponent-doubt.\nA double asterisk (**) indicates a signi\ufb01cant bias for\nGW-agree OPINIONS in both LL and RL; (*) indicates\nsigni\ufb01cance in one side. Signi\ufb01cance ( p<0:05) is de-\ntermined via a chi-squared test and applying Benjamini-\nHochberg correction with a false discovery rate of 0.1.\nWord order is given in descending value of log odds, as\nmeasured in LL.\n5.3 Study 2 results\nHow faithfully does the media ascribe OPINIONS\ntoSOURCES ? We use Wikipedia lists11for GW-\nactivist and GW-skeptic entities (Greta Thunberg,\nThe Sierra Club; William Happer, The Heartland\nInstitute) to label the stance of SOURCES that are\nnamed entities, after using fuzzy matching to re-\nsolve SOURCES to a canonical form. We de\ufb01ne an\nOPINION asfaithfully ascribed if the stance of the\n11Activist lists: https://en.wikipedia.org/w\niki/Category:Climate activists ,https://\nen.wikipedia.org/wiki/Category:Climate c\nhange environmentalists . Unfortunately, the lists\nwe used for climate change skeptics and deniers have since\nbeen deleted by Wikipedia. We manually removed entries\nthat are neither people nor organizations, e.g., \u201cEnvironmental\nActivism of Al Gore.\u201d\n4\n 2\n 0 2 4truth\nlandmark\nnumerous\nkey\nrecent*\nexpert*\nimportant\npeerreview\ndozen\nmultiple\ntop\nfact\nevidence*\nnobel*\nprize*\nrenowned\nsignificant\nevangelical*\nmajor\nstrong\nevery\nleading\nhundred\nthousand\nhoax\nbreakthrough\nfamous\nmany\nbad\nfew*\nfraud\nassumption\ndebated\ndubious\nfaulty\nflaw\nwrong\nalarmist\ndistinguished\ncontroversial\nnarrative*\nalleged*\nfake\nmisleading*\nproblematic*\nfalse*\ninaccurate*Log odds of ascribing a GW-agree opinionMedia slant\nLL\nRLFigure 6. Log odds of ascribing a GW-agree OPINION\nfor the af\ufb01rming anddoubting modi\ufb01ers present in LL\nand RL. An asterisk (*) indicates a signi\ufb01cant bias for\nGW-agree OPINIONS in either LL or RL. Signi\ufb01cance\n(p<0:05) is determined via a chi-squared test and ap-\nplying Benjamini-Hochberg correction with a false dis-\ncovery rate of 0.1. Word order is given in descending\nvalue of log odds, as measured in LL.\nOPINION matches the stance of the SOURCE , e.g.,\na GW-agree OPINION is ascribed to a GW-activist.\nSurprisingly, among the 4.3K OPINIONS as-\ncribed to a named entity from the Wikipedia lists,\nwe \ufb01nd that 37% and 38% are unfaithfully ascribed\nin LL and RL, respectively, suggesting that both\nsides frequently attribute OPINIONS to entities that\ndiffer from the well-established public positions of\nthose entities. (See Appendix Tab. 10 for examples\nof unfaithfully ascribed OPINIONS .)\nWhen we examine the unfaithful instances from\nLL more closely, we notice that the most frequent\nSOURCES have ties to the fossil fuel industry (e.g.,\nExxon knew that the result of burning fossil fuels\nwould create a climate crisis ), emphasizing the\nnarrative of hypocritical oil companies that have\nlong known about the harmful effects of green-\nhouse gases. In RL, by contrast, the unfaithful\n3303\nLeft-leaning media Right-leaning media\nunderstand, concede , realize, recall ,\nrecall , demonstrate, learn, see\nknow, acknowledge, admit, concede\nagree reveal\nTable 5. P REDICATES biased toward hypocritical opin-\nion attribution, i.e., attributing an own-side OPINION\nto an opposing-side SOURCE , in LL and RL. Bolded\nPREDICATES are used for hypocritical attribution in\nboth LL and RL.\ninstances quote from a wide-range of activists and\nscienti\ufb01c bodies, but similarly emphasize these en-\ntities\u2019 hypocrisy: Gore admits that carbon dioxide\nis only responsible for about 40 % of the warming;\nNASA concedes that its temperature data are less\nthan reliable ).\nFinally, we ask whether certain PREDICATES are\nfavored for ascribing OPINIONS unfaithfully. We\nmight expect verbs like admit andacknowledge ,\nwhich have connotations of reluctance, to be used\nfor this purpose, and for verbs like declare andin-\nsistto be disfavored\u2014it would be counter-intuitive\nfor a reader of The New York Times to see the sen-\ntence, Exxon insists that fossil fuels cause global\nwarming , for example.\nTo answer this question empirically, we mea-\nsure each PREDICATE \u2019Stendency to ascribe an\nOPINION to a SOURCE with an activist vs. skeptic\nstance, similar to how we measured PREDICATES \u2019\ntendency to embed an OPINION with a given stance.\nWe retrieve in Tab. 5 the PREDICATES that are\nbiased under this measure toward ascribing GW-\nagree OPINIONS to GW-skeptic SOURCES , and vice\nversa.\nInterestingly, in addition to verbs we expected\n(acknowledge, admit, concede ), we also \ufb01nd verbs\nlikeunderstand, agree, realize, know . One ten-\ndency among these verbs seems to be that they\ndenote non-spoken acts of belief. Intuitively, it\nwould be incompatible with real world events to de-\nscribe Exxon as vocally denouncing fossil fuels or\nAl Gore as vocally criticizing climate science, but it\nispossible to describe such entities as silently hold-\ning contradictory beliefs (and in doing so, highlight\ntheir hypocrisy). However, we also see exceptions\n(demonstrate in LL, reveal in RL), suggesting that\nmore complex interactions are involved.6 Discussion and future work\nIn this work, we introduced GWSD , a novel dataset\nof 2K sentences from news media for studying GW\nstance. Using our dataset, we trained a weighted\nBERT model competitive with human performance\nto predict the stance of 500K opinions in news ar-\nticles. Our initial analyses showed that both sides\nof the GW debate make use of framing devices\nin largely symmetric ways, though GW-skeptic\nmedia exhibits more opponent-doubt, in line with\nprior work on the propagation of GW skepticism\n(Oreskes and Conway, 2011). We also found that\nboth sides exhibit considerable amounts of unfaith-\nful opinion attribution, in particular to portray \ufb01g-\nures as hypocritical. Future work could take a\nmore \ufb01ne-grained approach to our analyses, such\nas disaggregating op-ed articles from non-op-eds or\nadopting labels for outlet stance beyond the binary\n\u201cright-\u201d vs. \u201cleft-leaning.\u201d We also categorized\nnamed entities as either activists or skeptics, which\nobscures distinctions between, e.g., corporations\nwith economic incentives for GW skepticism vs.\nindividuals that may be ideologically motivated.\nOur methodology may also be useful for work\nin argument mining: the main object of our in-\nquiry\u2014ascribed OPINIONS and the linguistic de-\nvices of SOURCE and PREDICATE used as syntactic\nmarkers of the attributive act\u2014represents a novel\ndimension along which to analyze how premises\nare used to support claims (Stab and Gurevych,\n2017).\nOur work also highlights challenges inherent to\nstudying stance: we found that many items can\nbe ambiguous at the sentence-level, without a sin-\ngle \u201ctrue\u201d stance, and that demographic attributes\nlike party af\ufb01liation and gender can affect how\npeople respond. At the same time, we showed\nhow Bayesian modeling can be used to account\nfor this variation. Such \ufb01ndings reinforce the idea\nthat NLP should be conscious of who the training\ndata comes from, and how a model might be bi-\nased as a result. We hope that future research can\nbene\ufb01t from and extend the current work to study\nargumentation inclusive of the many subjective and\ndemographically-diverse attitudes in our society.\nAcknowledgements\nWe thank the reviewers and the Stanford NLP\nGroup for helpful feedback, and Adina Abeles\nfor feedback on the demographics portion of the\nMTurk task.\n3304\nReferences\nDee Alexander, WJ Kunz, and Fred Walter House-\nholder. 1964. Some classes of verbs in English , vol-\nume 1. Linguistics Research Project, Indiana Uni-\nversity.\nToby Bolsen, Risa Palm, and Justin T Kingsland. 2019.\nThe impact of message source on the effectiveness\nof communications about climate change. Science\nCommunication , 41(4):464\u2013487.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nInProceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing , pages\n632\u2013642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nLoraine I Bridgeman and Fred Walter Householder.\n1965. More classes of verbs in English . Indiana\nUniversity Linguistics Club.\nCarmen Rosa Caldas-Coulthard. 2002. On reporting\nreporting: The representation of speech in factual\nand factional narratives. In Advances in written text\nanalysis , pages 309\u2013322. Routledge.\nBob Carpenter, Andrew Gelman, Matthew D. Hoff-\nman, Daniel Lee, Ben Goodrich, Michael Betan-\ncourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and\nAllen Riddell. 2017. Stan: A probabilistic program-\nming language. Journal of Statistical Software , 76.\nIgnacio Cases, Clemens Rosenbaum, Matthew Riemer,\nAtticus Geiger, Tim Klinger, Alex Tamkin, Olivia\nLi, Sandhini Agarwal, Joshua D. Greene, Dan Juraf-\nsky, Christopher Potts, and Lauri Karttunen. 2019.\nRecursive routing networks: Learning to compose\nmodules for language understanding. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 3631\u20133648, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nDennis Chong and James N Druckman. 2007. Framing\ntheory. Annu. Rev. Polit. Sci. , 10:103\u2013126.\nRobert B Cialdini. 1993. In\ufb02uence: The psychology of\npersuasion . Harper Collins.\nKevin Clark and Christopher D. Manning. 2015.\nEntity-centric coreference resolution with model\nstacking. In Proceedings of the 53rd Annual Meet-\ning of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers) ,\npages 1405\u20131415, Beijing, China. Association for\nComputational Linguistics.\nGloria Cowan and D \u00b4esir\u00b4e Khatchadourian. 2003. Em-\npathy, ways of knowing, and interdependence as\nmediators of gender differences in attitudes towardhate speech and freedom of speech. Psychology of\nWomen Quarterly , 27.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171\u20134186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJesse Dodge, Suchin Gururangan, Dallas Card, Roy\nSchwartz, and Noah A. Smith. 2019. Show your\nwork: Improved reporting of experimental results.\nInProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 2185\u2013\n2194, Hong Kong, China. Association for Computa-\ntional Linguistics.\nRobert M. Entman. 2006. Framing: Toward Clari\ufb01ca-\ntion of a Fractured Paradigm. Journal of Communi-\ncation , 43(4):51\u201358.\nSarah R Esposo, Matthew J Hornsey, and Jennifer R\nSpoor. 2013. Shooting the messenger: Outsiders\ncritical of your group are rejected regardless of argu-\nment quality. British Journal of Social Psychology ,\n52(2):386\u2013395.\nAnjalie Field, Doron Kliger, Shuly Wintner, Jennifer\nPan, Dan Jurafsky, and Yulia Tsvetkov. 2018. Fram-\ning and agenda-setting in Russian news: A compu-\ntational analysis of intricate political strategies. In\nProceedings of EMNLP .\nKelly S Fielding, Matthew J Hornsey, Ha Anh Thai,\nand Li Li Toh. 2020. Using ingroup messengers\nand ingroup values to promote climate change pol-\nicy.Climatic Change , 158(2):181\u2013199.\nAndrew Gelman and Jennifer Hill. 2007. Data Anal-\nysis Using Regression and Multilevel/Hierarchical\nModels . Cambridge University Press, New York.\nElisabeth Gidengil and Joanna Everitt. 2003. Talk-\ning tough: Gender and reported speech in cam-\npaign news coverage. Political communication ,\n20(3):209\u2013232.\nIvan Habernal and Iryna Gurevych. 2016. Which ar-\ngument is more convincing? Analyzing and pre-\ndicting convincingness of web arguments using bidi-\nrectional LSTM. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1589\u2013\n1599, Berlin, Germany. Association for Computa-\ntional Linguistics.\nDirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,\nand Eduard Hovy. 2013. Learning whom to trust\nwith MACE. In Proceedings of the 2013 Conference\nof the North American Chapter of the Association\n3305\nfor Computational Linguistics: Human Language\nTechnologies , pages 1120\u20131130, Atlanta, Georgia.\nAssociation for Computational Linguistics.\nNanjiang Jiang and Marie-Catherine de Marneffe.\n2019. Do you know that Florence is packed with vis-\nitors? Evaluating state-of-the-art models of speaker\ncommitment. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics , pages 4208\u20134213, Florence, Italy. Associa-\ntion for Computational Linguistics.\nGeorge Lakoff and Sam Ferguson. 2006. The framing\nof immigration. Retrieved from The Rockridge Insti-\ntute. URL: https://escholarship.org/uc/it\nem/0j89f85g .\nMarc J Lerchenmueller, Olav Sorenson, and Anupam B\nJena. 2019. Gender differences in how scientists\npresent the importance of their research: Observa-\ntional study. BMJ , 367.\nMarie-Catherine de Marneffe, Christopher D Manning,\nand Christopher Potts. 2011. Veridicality and utter-\nance understanding. In 2011 IEEE Fifth Interna-\ntional Conference on Semantic Computing .\nSaif Mohammad, Svetlana Kiritchenko, Parinaz Sob-\nhani, Xiaodan Zhu, and Colin Cherry. 2016.\nSemEval-2016 task 6: Detecting stance in tweets.\nInProceedings of the 10th International Workshop\non Semantic Evaluation (SemEval-2016) , pages 31\u2013\n41, San Diego, California. Association for Computa-\ntional Linguistics.\nJessica Gall Myrick and Suzannah Evans Comfort.\n2020. The pope may not be enough: How emo-\ntions, populist beliefs, and perceptions of an elite\nmessenger interact to in\ufb02uence responses to climate\nchange messaging. Mass Communication and Soci-\nety, 23(1):1\u201321.\nNaomi Oreskes and Erik M Conway. 2011. Merchants\nof doubt: How a handful of scientists obscured the\ntruth on issues from tobacco smoke to global warm-\ning. New York: Bloomsbury.\nRita Orji, Regan L Mandryk, and Julita Vassileva. 2015.\nGender, age, and responsiveness to Cialdini\u2019s persua-\nsion strategies. In International Conference on Per-\nsuasive Technology , pages 147\u2013159. Springer.\nAlexander Michael Petersen, Emmanuel M Vincent,\nand Anthony LeRoy Westerling. 2019. Discrepancy\nin scienti\ufb01c authority and media visibility of climate\nchange scientists and contrarians. Nature communi-\ncations , 10(1):1\u201314.\nPew Research Center. 2020. As economic concerns re-\ncede, environmental protection rises on the public\u2019s\npolicy agenda.\nJames Powell. 2017. Scientists reach 100% consensus\non anthropogenic global warming. Bulletin of Sci-\nence, Technology & Society , 37(4):183\u2013184.Reid Pryzant, Richard Diehl Martinez, Nathan Dass,\nSadao Kurohashi, Dan Jurafsky, and Diyi Yang.\n2020. Automatically neutralizing subjective bias in\ntext. In Proceedings of the AAAI Conference on Ar-\nti\ufb01cial Intelligence , volume 34, pages 480\u2013489.\nMarta Recasens, Cristian Danescu-Niculescu-Mizil,\nand Dan Jurafsky. 2013. Linguistic models for an-\nalyzing and detecting biased language. In Proceed-\nings of the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers) , pages 1650\u20131659, So\ufb01a, Bulgaria. Associa-\ntion for Computational Linguistics.\nEllen Riloff and Janyce Wiebe. 2003. Learning extrac-\ntion patterns for subjective expressions. In Proceed-\nings of the 2003 Conference on Empirical Methods\nin Natural Language Processing , pages 105\u2013112.\nAlexis Ross and Ellie Pavlick. 2019. How well do NLI\nmodels capture verb veridicality? In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP) , pages 2230\u20132240, Hong Kong,\nChina. Association for Computational Linguistics.\nJoel Ross, Andrew Zaldivar, Lilly Irani, and Bill Tom-\nlinson. 2010. Who are the Turkers? Worker demo-\ngraphics in Amazon Mechanical Turk. In Proceed-\nings of CHI .\nShamik Roy and Dan Goldwasser. 2020. Weakly\nsupervised learning of nuanced frames for analyz-\ning polarization in news media. arXiv preprint\narXiv:2009.09609 .\nRachel Rudinger, Aaron Steven White, and Benjamin\nVan Durme. 2018. Neural models of factuality. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long Papers) , pages 731\u2013744, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nMaarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,\nand Noah A. Smith. 2019. The risk of racial bias\nin hate speech detection. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics , pages 1668\u20131678, Florence,\nItaly. Association for Computational Linguistics.\nRoser Saur \u00b4\u0131 and James Pustejovsky. 2009. FactBank:\nA corpus annotated with event factuality. Language\nresources and evaluation , 43(3):227.\nRoser Saur \u00b4\u0131 and James Pustejovsky. 2012. Are you\nsure that this happened? Assessing the factuality de-\ngree of events in text. Computational Linguistics ,\n38(2):261\u2013299.\nEdwin Simpson and Iryna Gurevych. 2018. Finding\nconvincing arguments using scalable Bayesian pref-\nerence learning. Transactions of the Association for\nComputational Linguistics , 6:357\u2013371.\n3306\nSandeep Soni, Tanushree Mitra, Eric Gilbert, and Jacob\nEisenstein. 2014. Modeling factuality judgments in\nsocial media text. In Proceedings of the 52nd An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers) , pages 415\u2013\n420, Baltimore, Maryland. Association for Compu-\ntational Linguistics.\nChristian Stab and Iryna Gurevych. 2017. Parsing ar-\ngumentation structures in persuasive essays. Com-\nputational Linguistics , 43(3):619\u2013659.\nOren Tsur, Dan Calacci, and David Lazer. 2015. A\nframe of mind: Using statistical models for detection\nof framing and agenda setting campaigns. In Pro-\nceedings of the 53rd Annual Meeting of the Associa-\ntion for Computational Linguistics and the 7th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , pages 1629\u20131638,\nBeijing, China. Association for Computational Lin-\nguistics.\nGregory Werner, Vinodkumar Prabhakaran, Mona\nDiab, and Owen Rambow. 2015. Committed belief\ntagging on the Factbank and LU corpora: A compar-\native study. In Proceedings of the Second Workshop\non Extra-Propositional Aspects of Meaning in Com-\nputational Semantics (ExProM 2015) , pages 32\u201340,\nDenver, Colorado. Association for Computational\nLinguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers) , pages 1112\u20131122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nDiyi Yang and Robert E. Kraut. 2017. Persuading team-\nmates to give: Systematic versus heuristic cues for\nsoliciting loans. Proc. ACM Hum.-Comput. Inter-\nact., 1.\nJustine Zhang, Ravi Kumar, Sujith Ravi, and Cristian\nDanescu-Niculescu-Mizil. 2016. Conversational\n\ufb02ow in Oxford-style debates. In Proceedings of the\n2016 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies , pages 136\u2013141, San\nDiego, California. Association for Computational\nLinguistics.\n3307\nAppendices\nA Data collection details\nURL \ufb01lters We \ufb01ltered out articles that may be\nirrelevant on the basis of containing one of the\nfollowing URL tags:\n/automobiles/, /autoreviews/, /autoshow/, /busi-\nness/, /campaign -stops/, /crosswords/, /booming/,\n/giving/, /gmcvb/, /jobs/, /lens/, /letters/, /newyork-\ntoday/, /nutrition/, /sept-11-reckoning/, /smallbusi-\nness/, /sunday-review/, /garden/, /arts/, /theater/,\n/sports/, /dining/, /books/, /weekinreview/, /your-\nmoney/, /movies/, /fashion/, /technology/, /pa-\ngeoneplus/, /travel/, /nytnow/, /public-editor/, /edu-\ncation/, /learning/, /podcasts/, /style/, /t-magazine/,\n/reader-center/, /awardsseason/, /brie\ufb01ng/, /deal-\nbook/, /es/, /greathomesanddestinations/, /interac-\ntive/, /media/, /mutfund/, /obituaries/, /personal-\ntech/, /realestate/, /smarter-living/, /todayspaper/,\n/your-money/, /yourtaxes/, /slideshow/, /interac-\ntive/, /tag/, /author/, /clips/, /podcasts/, /subject/,\n/authors/, /category/, /person/, /category/, /shows/,\n/video/, /topic/, /topics/, /de/, /tags/, /slideshow/,\n/interactive/, /transcripts/, /headlines/\nArticle deduplication details We deduplicated\narticles by normalized URLs. In addition, we no-\nticed that the same article corresponded in some\ncases to multiple different normalized URLs in our\ndataset, due to hyperlinking from different sections\nof a news site (e.g., blog section, RSS feed, front\npage). We de-duplicated these articles by compar-\ning the article titles using a criterion adapted from\nPetersen et al. (2019): for two titles Tj;Tkwith\nDamerau-Levenshtein edit distance of Djk, if\nDjk\u00140:2\u0001Min(jTjj;jTkj);\nthen we consider the two titles, and hence the cor-\nresponding URLs, to index the same article.\nB S OURCE , PREDICATE , OPINION\nextraction algorithm\n1.Find complement clause(s) in the dependency\nparse of a sentence, i.e., sub-tree(s) whose\nroot has the dependency label \u201cccomp\u201d (=\nOPINION );\n2.Get head(s) of the complement clause(s),\nwhich correspond to the main verb that syn-\ntactically embeds the comp. clause (= PRED -\nICATE ); get children of the PREDICATE withthe dep. label \u201cprt\u201d (particle) in cases of multi-\ntoken verbs, e.g. point out ;\n3.To \ufb01nd the SOURCE , \ufb01rst check if the PREDI -\nCATE token is a participle (e.g., \u201ca researcher,\nwarning that [...]\u201d\u2014if yes, then \ufb01nd the head\nnoun, otherwise, look within all children of\nPREDICATE and \ufb01nd the syntactic subject (to-\nken with the \u201cnsubj*\u201d dependency label). In\nsome cases, the head noun/syntactic subject\nmay have the dependency label \u201crelcl\u201d, indi-\ncating that it\u2019s inside a relative clause (e.g.,\n\u201c[...], who warns that\u201d)\u2014in this case, the true\nSOURCE is the antecedent of the relative pro-\nnoun, which we fetch by getting the head of\nthe relative pronoun;\n4.Get additional modi\ufb01ers of SOURCE ,PREDI -\nCATE and OPINION by recursively retrieving\ntheir children.\nC Lexical \ufb01lters\nWe use the following lexical resources to \ufb01lter ex-\ntracted complement clauses to true indirect state-\nments on the topic of GW.\nThe Indiana Lists Our algorithm returns sub-\njunctive clausal complements, e.g., \u201cPoliticians re-\nquire that [oil companies pay a carbon tax]\u201d, which\nare nearly identical to embedded opinions, e.g.,\n\u201cPoliticians claim that [oil companies pay a car-\nbon tax]\u201d. The Indiana Lists (Alexander et al.,\n1964; Bridgeman and Householder, 1965) catego-\nrize predicates according to whether they syntac-\ntically embed a subjunctive or indicative comple-\nment clause. We keep extracted ( SOURCE ,PRED -\nICATE ,OPINION ) tuples only if the PREDICATE\nlemma is one of 418 indicative-clause-embedding\nverbs in these lists. This \ufb01lter also effectively ex-\ncludes extracted instances such as \u201cWe watch [oil\ncompanies pay a carbon tax]\u201d.\nImplicatives In addition to separating ( S,P,O )\ntuples with overt negations ( The researchers did\nnotsay [that the effects of global warming are\nclear] ,Nostudies \ufb01nd that [...] ), we also need\nto separate tuples that are implicitly negated ( The\nstudies fail to \ufb01nd that [...], Researchers refuse to\nsay [...] in order to accurately study how opinions\nare attributed. Since the dependency parser only\nrecognizes explicit cases of negation, we use a list\nof 92 implicative constructions from Cases et al.\n(2019) to exclude tuples where the PREDICATE is in\nthe scope of such an implicitly negating expression.\n3308\nIndirect questions We exclude complement\nclauses that represent indirect questions ( Scientists\naskwhat the future of nuclear looks like ) by ex-\ncluding tuples that have a question word from the\nsetfwho, what, when, where, how, whether, which g\nas the complementizer.\nTopic keywords climat, climact, global, warm,\ncarbon, fossil, oil, energi, environ, co2, green, ice,\nglacier, glacial, melt, sea, temperatur, heat, hot,\nmethan, greenhous, arctic, antarct, celsiu, fahren-\nheit, ecosystem, pole, environ, coal, natur, human,\neconomi, electr, futur, health, scienc, econom, air,\npollut, \ufb01re, wild\ufb01r, ipcc, epa, market, scientist,\nearth, planet, wind, solar, record, fuel, ocean, nu-\nclear, scientif, pipelin, emit, emiss, concensu, re-\nnew, accord, forest, pruitt, drought, hurrican, at-\nmospher, activist, coast, agricultur, water, plant,\nweather, polar\nD AMT task details\nTo choose the subset of items for annotations from\nour full set of extracted OPINION spans, we \ufb01lter\nto items that contain a smaller set of keywords\n(\u201cclimate\u201d, \u201cwarming\u201d, \u201ccarbon\u201d, \u201cco2\u201d, or \u201cfossil\nfuels\u201d) and make a manual selection for each round\nof annotation such that the \ufb01nal sample is roughly\nbalanced across different outlets.\nWe settle on a task design as follows: annota-\ntors are told that we are collecting their judgments\nof GW stance for a series of sentences; we then\nshow an instructions page and guide them through\n6 practice trials. They then annotate the main trial\nitems for agreeing ,disagreeing , or being neutral\nwith respect to the target opinion, \u201cClimate change/-\nglobal warming is a serious concern.\u201d Additional\nhelp text for each label is adapted from the set-\nup described in Mohammad et al. (2016). The\nmain trial items consist of 5 screen sentences and\n30-50 sentences that have been transformed from\nthe extracted OPINION using basic operations such\nas cleaning whitespace, capitalizing the \ufb01rst word,\nadding clause-\ufb01nal punctuation, matching for tense,\nand substituting abbreviations of named entities\nwith the non-abbreviated form.\nWe divide the annotation into 5 rounds and re-\ncruit 8 annotators to annotate each item. Other\nthan one worker who did the task on 3 different\nrounds, all other annotations come from unique\nannotators. We also restrict to annotators whose\nIP address is in the US, who have a minimum HIT\napproval rating of 98%, and at least 1,000 HITs\napproved. We collect annotator age, gender, level\nof education, political af\ufb01liation, state of residence,\nas well as measures of their own stance towards\nGW borrowed from the American Public Opinion\non Global Warming project.12There is some demo-\ngraphic imbalance in our total sample of annotators\n(see Tab. 6 in E) but the distribution is similar to the\nestimated demographics of the AMT population lo-\ncated in the US as a whole (Ross et al., 2010). The\nprice per item was set to ensure that workers were\npaid the California minimum wage of $12 USD per\nhour.\nE Demographic and linguistic effects on\nannotations\nThe marginal statistics for annotator demographics\nare given in Table 6, and show a relatively repre-\nsentative sample in terms of age, gender, education,\nand political af\ufb01liation, though women are are dis-\ntinctly under-represented.13\nIn order to measure the bias associated with var-\nious characteristics of annotator demographics, we\nmake use of the hierarchical ordinal logistic model\ngiven below. In this model, Yijis the response of\nannotatorjto instancei(taking a value inf1;2;3g,\ncorresponding to \u201cdisagree\u201d, \u201cneutral\u201d, \u201cagree\u201d).\nIn addition, qiis the unnormalized stance associ-\nated with instance i(on a spectrum from \u201cdisagree\u201d\nto \u201cagree\u201d),wjis the bias associated with worker j,\n12https://pprggw.wordpress.com/\n13For political af\ufb01liation by age and gender in the US, see\nhttp://pewrsr.ch/2FVWtww\n3309\nAnswer % of annotators\nAge over 34 48.3 %\nFemale 37.3 %\nMale 62.5 %\nCollege degree or higher 66.5%\nDemocrat 46.0 %\nRepublican 21.2 %\nIndependent 28.8 %\nOther political af\ufb01liation 4.0 %\nTable 6. Demographic information on the 400 Mechan-\nical Turk annotators who participated in our study.\nXjis a vector of covariates associated with worker\nj,\u001b2\nqand\u001b2\nware learned variance parameters, and\nc1andc2are learned thresholds. We model the\nprobability of each response according to:\np(Yij=k) =8\n>>>><\n>>>>:1\u0000g(\u0011ij\u0000c1)ifk= 1\ng(\u0011ij\u0000c1)\n\u0000g(\u0011ij\u0000c2)if1<k<K\ng(\u0011ij\u0000c2) ifk=K\n(1)\nwhere\n\u0011ij=qi+wj (2)\nqi\u0018N(0;\u001b2\nq) (3)\nwj\u0018N(\fTXj;\u001b2\nw) (4)\nTo complete the model, we place weakly in-\nformative half-normal priors on \u001b2\nqand\u001b2\nw, and\nweakly informative normal priors on \f.\nUsing the above speci\ufb01cation, we \ufb01t a series of\nmodels in which Xjrepresents, in turn, each of\nthe covariates individually, followed by a series\nof combined models. We \ufb01t these models in Stan\nusing 5 chains with 2000 samples, the \ufb01rst half\nthrown away as burn in.\nTable 7 shows the estimated effects from each\nmodel on the propensity to respond with \u201cagree\u201d\nrelative to \u201cneutral\u201d. Those with 95% credible in-\ntervals which exclude 1.0 are marked in bold. The\nresults on the propensity to respond with \u201cneutral\u201d\nrelative \u201cdisagree\u201d are not shown, but are broadly\nsimilar.\nIn addition, we test whether the political lean-\ning of the source outlet has any effect on the an-\nnotations received by the items drawn from the\nsource. We \ufb01nd, unsurprisingly, that items from\nleft-leaning media (LL) are signi\ufb01cantly more\nlikely to receive ratings of \u201cagree\u201d, but we do not\ufb01nd a signi\ufb01cant difference in level of annotator\nagreement for items drawn from LL vs. RL. Fi-\nnally, we \ufb01nd that item length (no. of words) is\nslightly correlated with IAA (measured as entropy\nover labels; Spearman\u2019s \u001a= 0.06, p = 0.016).\nF Estimating Stance Distributions\nTo aggregate all ratings and obtain estimates of\nthe stance distribution for each instance, we use a\nvariant of the above model which allows inferring\na distribution for each instance and each worker,\nalong with a parameter representing the degree to\nwhich an annotator is failing to pay attention to\nthe instance being annotated. Although the \u201cdis-\nagree\u201d, \u201cneutral\u201d, and \u201cagree\u201d categories can be\ntreated as ordered (as above), here we treat them\nas unordered nominal categories, so as to allow for\nthe possibility, for example, that an instance evokes\nboth \u201cagree\u201d and \u201cdisagree\u201d, but not \u201cneutral\u201d (i.e.\nit is ambiguous, but clearly not neutral).\nLetYijbe the response from worker jto item\ni, letqikbe the degree to which label kapplies\nto instancei, and letwjkbe the bias of worker j\ntowards label k. Finally, let vjbe the vigilance\nof workerj(i.e. the degree to which they pay\nattention to the prompt). We assume the following\nmodel\nYij\u0018Multinomial (Softmax k(\u0011ij)) (5)\n\u0011ijk=vj\u0001qik+ (1\u0000vj)\u0001wjk (6)\nqik\u0018N(\u0016k;\u001b2\nq) (7)\nwjk\u0018N(0;\u001b2\nw) (8)\nand \ufb01t it in Stan, placing weakly informative priors\non\u001b2\nq,\u001b2\nw, and a uniform prior on vj2[0;1]. In\norder to help stabilize the model we set the mean pa-\nrameter of the prior on qikto be\u0016k= logpk, where\npkis the overall proportion of the corresponding\nresponse in the data.\nIn order to estimate human performance for the\npurpose of comparison, we \ufb01t this model multiple\ntimes, but each time leave out a random 10% of\nthe annotators. As can be seen in Figure 8, there\nis great variation in the degree to which annotators\nagree with the label inferred from the remaining\n90% of annotators. To characterize the distribution\nof work accuracies, we \ufb01t a mixture of two normal\ndistributions, and report the mean of the distribu-\ntion corresponding to the high-accuracy annotators\nin the main paper (0.71).\n3310\nCovariate M1 M2 M3 M4 M5 M6 M7 M8\nAge over 34 0.98 0.98\nFemale 1.04 1.04 1.04\nCollege degree or higher 1.0 1.0\nDemocrat 0.96 0.97 0.98 0.98\nRepublican 1.06 1.05 1.04 1.05\nTable 7. Effects of annotator demographics on the propensity to respond with \u201cagree\u201d rather than \u201cneutral\u201d. Coef-\n\ufb01cients in bold have 90% credible intervals which exclude 1.0.\n0.0 0.2 0.4 0.6 0.8 1.0\nMACE competence0.00.20.40.60.81.0Worker vigilence (vj)\nFigure 7. Showing the correlation between worker\ncompetence (estimated using MACE) and worker vigi-\nlance (estimated using our model) for the 400 annota-\ntors who participated in our data collection.\nMACE / Ours disagree neutral agree\ndisagree 386 6 0\nneutral 12 852 19\nagree 2 15 785\nTable 8. Confusion matrix of (dis)agreements between\nMACE and our model\n0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nWorker accuracy0102030405060overall mean = 0.66\noverall median = 0.68\ncomponent 1 mean = 0.71\ncomponent 2 mean = 0.43\nFigure 8. To estimate human performance, we re\ufb01t the\nlabel aggregation model multiple times, each time leav-\ning out 10% of annotators, and then comparing their an-\nnotations against the inferred label for the correspond-\ning items. This plot shows that human performance ap-\npears to be a mixture of two distributions representing\nlow and high agreement annotators.G Additional BERT experiments\nLeading up to our hyperparameter search and base-\nline comparison, we experiment with a variety of\ntraining set-ups. Due to class imbalance in our\ntraining data (see Table 3), we try downsampling\nthe majority classes as well as upsampling the mi-\nnority class by adding back translations thereof.\nHowever, we did not obtain performance gains\nfrom either strategy in preliminary experiments.\nWe further experiment with additional features in\nthe form of the pre- and postceding nsentences\n(n= [1;2]) surrounding a training example, but\ndid not obtain performance gains. We are also\nlimited in the kinds of additional features (e.g.,\nthe political leaning of the outlet that a sentence\ncomes from, the source entity that a sentence is\nattributed to) that we can use, since our goal is\nto analyze how the stance of the embedded state-\nment is correlated with precisely these variables.\nWe also try \ufb01ne-tuning BERT \ufb01rst on a language\nmodeling task (using our raw news data) and a\nnatural language inference task (using the SNLI\n(Bowman et al., 2015) and MNLI (Williams et al.,\n2018) datasets), respectively, prior to \ufb01ne-tuning\nfor sequence classi\ufb01cation, but obtain no perfor-\nmance gains. Finally, we experiment with using\ntweets from known GW activists/skeptics as well\nas GW article headlines taken from extreme liber-\nal/conservative news sources as additional training\ndata, inferring labels based on the stance of the\nTwitter user or news source. However, we \ufb01nd that\nadding these examples yields a lower performance\ncompared to using only the human-annotated data.\nThis is not too surprising, given that embedded\nstatements in the news tend to be longer and more\ncomplex than tweets/headlines.\nH Hyperparameter Tuning\nAll experiments took less than 7 days on one GPU\n(16 cores, 2.6GhZ, 128GB mem). For the BERT-\nbased models (110M parameters), we use a max-\n3311\nHyperparameter \u0001accuracy p-value\nLabel weights 0.020 <0:001\nLM \ufb01ne-tuning 0.004 0.11\nTarget opinion -0.002 0.48\nLR 2e-5 vs 1e-5 0.009 0.03\nLR 4e-5 vs 1e-5 0.002 0.35\nTable 9. Estimated effects of various hyperparame-\nter choices on the average validation performance of\nthe BERT base model.p-values are obtained using a\nWilcoxon signed-rank test on the paired results from\ngrid search.\nimum sequence length of 256, a batch size of 16,\nand train for 7 epochs, saving a checkpoint after\neach epoch. In addition, we perform a grid search\nover the following hyperparmaters:\n\u2022 Label weights: [True, False]\n\u2022 Language model \ufb01ne-tuning: [True, False]\n\u2022 Target opinion as second input: [True, False]\n\u2022 Learning rate: [1e-5, 2e-5, 4e-5]\nWe train models for each combination of settings\nusing \ufb01ve random seeds, and ultimately choose the\nhyperparmeter con\ufb01guration (including number of\ntraining epochs and random seed) that has the best\nvalidation performance, averaged over \ufb01ve folds,\nfor a total of 600 con\ufb01guration tested (including\nseeds and folds). We then retrain a model using\nthose hyperparameter values on all non-test data.\nBecause we are using grid search, we can con-\nveniently compare the effects of various hyperpa-\nrameter choices. The overall average validation\nperformance was 0.71, with a standard deviation\nof 0.04, and a 95% interval of [0.64, 0.77]. Table\n9 shows the average increase in accuracy associ-\nated with each hyperparameter choice, along with\nap-value computed using a Wilcoxon signed-rank\ntest. As can be seen, using label weights leads to\na signi\ufb01cant increase in accuracy, as does using a\nlearning rate of 2e-5 in comparison to 1e-5.\nFor the linear models (91504 params), we con-\nsider both logistic regression and SVM models,\nagain using grid search and choosing the best-\nperforming model on average validation perfor-\nmance, as described above. For the SVM, we\nsearch over all combinations of the following hy-\nperparameters:\n\u2022 Label weights: [True, False]\n0 100 200 300 400 500 600\nHyperparameter assignments0.500.550.600.650.700.75Expected validation accuracyBERT-based\nLinearFigure 9. Expected validation performance of both\ntypes of models using validation accuracy scores from\nthe hyperparameter grid search.\n\u2022 n-gram order: [1, 2]\n\u2022 kernel [rbf, linear, polynomial]\n\u2022 gamma [scale, auto]\n\u2022 Stopword removal: [True, False]\n\u2022 Convert digits: [True, False]\n\u2022 Regularization strength f0.01, ..., 1000g\nFor the logistic regression model, we search over\nall combinations of the following hyperparameters:\n\u2022 Label weights: [True, False]\n\u2022 n-gram order: [1, 2]\n\u2022 Stopword removal: [True, False]\n\u2022 Convert digits: [True, False]\n\u2022 Regularization type: [ l1,l2]\n\u2022 Regularization strength f0.01, ..., 1000g\nThe mean validation accuracy among a total of\n640 linear models tested is 0.56, with a standard de-\nviation of 0.06, and a (0.41-0.62) 95% con\ufb01dence\ninterval. The linear model which performed best\non validation data was a logistic regression bigram\nmodel using label weights, trained with l2regular-\nization, no stopword removal, no digit conversion,\nand regularization strength of 1.0.\nFigure 9 compares these results directly, showing\nthat the expected validation performance (Dodge\net al., 2019) of the BERT-based models is uni-\nformly better than that of the linear models, at least\nin terms of number of hyperparameter assignments.\n3312\nI Framing devices\nAf\ufb01rming devices\n\u2022Factive and semi-factive verbs: uncover, re-\nalize, know, understand, learn, concede, re-\nmember, recall, discover, show, reveal, see,\nforget, \ufb01nd, point out, indicate, acknowledge,\nadmit, realize, notice\n\u2022High-commitment verbs: certify, verify, cor-\nroborate, af\ufb01rm, con\ufb01rm, agree, conclude\n\u2022High commitment adjectives: proven, set-\ntled, conclusive, de\ufb01nitive\n\u2022Hyping adjectives: famed, unequivocal, skil-\nful, notable, strong, famous, Nobel, skillful,\nNobelist, Nobel Laureate, Nobel prize winner,\nNobel prize winning, prize winning, award\nwinning, distinguished, well-grounded, es-\nteemed, pro\ufb01cient, key, evidence, noted, top,\npreeminent, breakthrough, signi\ufb01cant, intel-\nligent, of import, celebrated, novel, recent,\nmajor, landmark, important, distinguished,\nrenowned, peer-reviewed, expert, leading\n\u2022Consensus of evidence adjectives: thou-\nsand, 1000, hundred, 100, unanimous, diverse,\nsubstantial, many, multiple, dozen, numerous\nDoubting devices\n\u2022Neg-factive verbs: pretend, lie, claim, allege,\nassume\n\u2022Low commitment verbs: doubt, dispute, de-\nbate\n\u2022Argumentative verbs: boast, declare, argue,\nmaintain, contend, insist, proclaim, assert,\nbrag, tout, convince\n\u2022Low commitment modi\ufb01ers: narrative,\nevangelical, hoax, dubious, alleged, in ques-\ntion, so-called\n\u2022Undermining adjectives: discredited, de-\nbunked, distorted, misleading, inaccurate, cor-\nrupted, sketchy, faulty, erroneous, de\ufb01cient,\nwrong, \ufb02awed, imprecise, incomplete, insuf-\n\ufb01cient, invalid, unreliable, adulterated, false,\nmistaken, cherry-picked, defective, presump-\ntive, non-peer-reviewed, exaggerated, over-\ndone, overstated, delusive, awry, fake, bad,misguided, substandard, \ufb01ctive, \ufb01ctitious, un-\ncomplete, blemished, uncompleted, shoddy,\ndubitable, lacking, moot, untrue, problematic,\nfaux, incorrect, inferior\n\u2022Lack of consensus adjectives: controversial,\ncontentious, debated, few, debatable, con-\ntested\nJ Quantifying bias toward framing\nOPINIONS with a GW-agree stance\nWe measure, Bf;L, the tendency for a framing de-\nvice, f, within media with leaning L, to frame a\nGW-agree OPINION as:\nBf;L= log\u0012af\nA\u0000af\u0013\n\u0000log\u0012df\nD\u0000df\u0013\n;\nwhereafis the number of times foccurs with\na GW-agree OPINION ,Ais the total number of\nGW-agree OPINIONS ,dfis the number of times f\noccurs with a GW-disagree OPINION , andDis the\ntotal number of GW-disagree OPINIONS , all within\nL.\nK Named entity fuzzy matching\nWe use FuzzyWuzzy ( https://github.com/sea\ntgeek/fuzzywuzzy ) to retrieve fuzzy matches of\nnamed entity SOURCES , setting the limit of matches\nto N = 100. We then manually \ufb01lter out incorrect\nmatches.\n3313\nTillerson acknowledged thatclimate change has \u2018real\u2019 and \u2018serious\u2019 risks but has pre-\nviously downplayed climate change effects.\nExxon knows thatfossil fuels caused global warming in the 1970s.\nExxon knew that the result of burning fossil fuels would create a climate crisis.\nGore admits that carbon dioxide is only responsible for about 40 percent of the warming.\nEven the IPCC acknowledges thattheir previous estimates of \u201c climate sensitivity \u201d to green-\nhouse gases their reported in 2007 were signi\ufb01cantly ex-\naggerated.\nNASA concedes thatits temperature data are less than reliable.\nTable 10. Examples of unfaithfulness in opinion attribution. Top: Examples of LL attributing\nGW-agree OPINIONS toGW-skeptic SOURCES .Bottom : Examples of RL attributing GW-disagree OPINIONS\ntoGW-activist SOURCES . The IPCC refers to the U.N.\u2019s Intergovernmental Panel on Climate Change.\nL Results on non-top-5 LL and RL\nmedia\nAffirming Doubting2\n1\n012345Log odds of ascribing a GW-agree opinion\nA)   Verbal Predicates\nAffirming Doubting4\n2\n024B)   Source Modifiers\nMedia slant\nLL\nRL\nFigure 10. Log odds of ascribing a GW-agree OPINION\nin LL and RL for af\ufb01rming and doubting PREDICATES\n(left panel) and SOURCE modi\ufb01ers (right panel).\n3314\n2\n 0 2 4concede*\nunderstand**\ndemonstrate*\ndiscover\nknow*\nagree**\nshow**\nconclude\nacknowledge\nindicate\nsee\ndeclare\nconfirm\nlearn\nrealize\nassume*\npointout\nimagine\naffirm\nrecall*\nreveal\nargue*\nsuspect\nadmit\nremember\nassert\nmaintain\ninsist\nclaim**\nnotice\nallege\nproclaim\nboast\npretend**Log odds of ascribing a GW-agree opinionMedia slant\nLL\nRLFigure 11. Log odds of ascribing a GW-agree OPIN -\nION in RL and LL for different PREDICATES , exclud-\ning the top 5 outlets by number of articles in each. A\ndouble asterisk (**) indicates a signi\ufb01cant bias for GW-\nagree OPINIONS in both LL and RL; (*) indicates sig-\nni\ufb01cance in one side. Signi\ufb01cance ( p <0:05) is deter-\nmined via a chi-squared test and applying Benjamini-\nHochberg correction with a false discovery rate of 0.1.\nWord order is given in descending value of log odds, as\nmeasured in LL.\n4\n 2\n 0 2 4landmark\nkey\nhundred\nnumerous\nhoax\nrenowned\nsignificant\nfact\npeerreview\nrecent*\nexpert*\nimportant\nmultiple\ntop\nevidence*\ndozen\nmajor\nfew*\nmany\nthousand\nbad\nstrong\ntruth\nflaw\nfaulty\nfalse\nassumption\ndebated\ndubious\nevangelical*\nnarrative\nalarmist\nprize\nnobel\nevery\nleading\ndistinguished\nalleged*\nfake\nproblematic*\ncontroversial\ninaccurate*Log odds of ascribing a GW-agree opinionMedia slant\nLL\nRLFigure 12. Log odds of ascribing a GW-agree OPIN -\nIONin RL and LL for different SOURCE modi\ufb01ers, ex-\ncluding the top 5 outlets by number of articles in each.\nA single asterisk (*) indicates a signi\ufb01cant bias for\nGW-agree OPINIONS in either LL or RL. Signi\ufb01cance\n(p<0:05) is determined via a chi-squared test and ap-\nplying Benjamini-Hochberg correction with a false dis-\ncovery rate of 0.1. Word order is given in descending\nvalue of log odds, as measured in LL.\n3315", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Detecting stance in media on global warming", "author": ["Y Luo", "D Card", "D Jurafsky"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2010.15149", "abstract": "Citing opinions is a powerful yet understudied strategy in argumentation. For example, an  environmental activist might say, \"Leading scientists agree that global warming is a serious"}, "filled": false, "gsrank": 493, "pub_url": "https://arxiv.org/abs/2010.15149", "author_id": ["RNFw8T0AAAAJ", "qH-rJV8AAAAJ", "uZg9l58AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:eVO7sU_SofsJ:scholar.google.com/&output=cite&scirp=492&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D490%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=eVO7sU_SofsJ&ei=YLWsaK7sAqzWieoPic2ZoAU&json=", "num_citations": 97, "citedby_url": "/scholar?cites=18132004814496420729&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:eVO7sU_SofsJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2010.15149"}}, {"title": "A multiple change-point detection framework on linguistic characteristics of real versus fake news articles", "year": "2023", "pdf_data": "1\nVol.:(0123456789) Scientific Reports  |         (2023) 13:6086  | https://doi.org/10.1038/s41598-023-32952-3\nwww.nature.com/scientificreportsA Multiple change\u2011point \ndetection framework on linguistic \ncharacteristics of real versus fake \nnews articles\nNikolas Petrou 1*, Chrysovalantis Christodoulou 1, Andreas  Anastasiou 2, George Pallis 1 & \nMarios D. Dikaiakos 1\nExtracting information from textual data of news articles has been proven to be significant in \ndeveloping efficient fake news detection systems. Pointedly, to fight disinformation, researchers \nconcentrated on extracting information which focuses on exploiting linguistic characteristics that \nare common in fake news and can aid in detecting false content automatically. Even though these \napproaches were proven to have high performance, the research community proved that both the \nlanguage as well as the word use in literature are evolving. Therefore, the objective of this paper is to \nexplore the linguistic characteristics of fake news and real ones over time. To achieve this, we establish \na large dataset containing linguistic characteristics of various articles over the years. In addition, we \nintroduce a novel framework where the articles are classified in specified topics based on their content \nand the most informative linguistic features are extracted using dimensionality reduction methods. \nEventually, the framework detects the changes of the extracted linguistic features on real and fake \nnews articles over the time incorporating a novel change\u2011point detection method. By employing our \nframework for the established dataset, we noticed that the linguistic characteristics which concern the \narticle\u2019s title seem to be significantly important in capturing important movements in the similarity \nlevel of \u201cFake\u201d and \u201cReal\u201d articles.\nFake news are defined as stories that \u201cdescribe events in the real world, typically by mimicking the conventions \nof traditional media reportage, yet known by their creators to be significantly false, and transmitted with the \ncombined goals of being widely re-transmitted and of deceiving at least some of its audience\u201d1\u20133. The term \u201cfake \nnews\u201d is used in the literature to describe misinformation and disinformation that take place on the Internet and \nin online social media platforms like Facebook, Twitter, and Reddit, with \u201cmisinformation\u201d referring to the spread \nof falsehood regardless of intent, and \u201cdisinformation\u201d characterizing the \u201cdeliberate falsehood spread to deceive \nand cause harm\u201d4. Several studies have demonstrated that state and non-state actors increasingly weaponize social \nmedia to spread fake news in the context of information-warfare operations of unprecedented scale and velocity, \nwith a negative impact on societies and the democratic  process5\u20137. The explosive growth and the impact of fake \nnews on real-world events, such as the 2016 US presidential election and the COVID-19 pandemic, have raised \nthe need for techniques and tools to identify fake news and mitigate their spread and penetration at scale and \nspeed commensurate with their online spread.\nTo combat fake news, many research  efforts8 are pursuing: (i) application of knowledge-based perspectives to \nidentify falsehoods contained in online content; (ii) the detection of linguistic traits, writing style, format, and \nsentiment that are typical of fake content, and (iii) the detection of sources that disseminate fake news, such as \nweb-sites and social media accounts. The identification of falsehoods is often supported by active citizens who \nidentify suspicious posts, gather evidence to highlight false claims therein, and report them to fact-checking \nwebsites. Efforts to identify fake news in an automated manner analyze large datasets of both genuine and fake \nnews articles to extract linguistic characteristics, select features that are useful for training fake-news detec -\ntion models, train classification models (logistic regression, random forest, neural networks, etc.) with selected \nfeatures, and apply models that provide the best  results9. Careful feature engineering is required to select and \nextract appropriate textual or latent features in order to build and train effective and efficient models: elimina -\ntion of ineffective features leads to lighter classification models that require less computational resources for OPEN\n1Computer Science Department, University of Cyprus, Nicosia, Cyprus. 2Department of Mathematics and Statistics, \nUniversity of Cyprus, Nicosia, Cyprus. *email: nick.petrou.lim@gmail.com\n2\nVol:.(1234567890) Scientific Reports  |         (2023) 13:6086  | https://doi.org/10.1038/s41598-023-32952-3\nwww.nature.com/scientificreports/training and deployment, as well as more accurate results. Trained models are then deployed on social media \nplatforms or on end-user devices (browsers, applications) and used to classify articles as fake or not, and flag or \nfilter-out falsehood. The predictive power of models can be improved further by taking into account additional \nfeatures, which represent the profile and \u201creputation\u201d of news-media sources and distributors, such as websites \nand social media  accounts10,11.\nIn summary, a key challenge in the automated detection of fake news is the identification of linguistic features \nthat help differentiate between fake and authentic news articles. Prior work has shown that the text of fake news \nis characterized by informal, sensational, affective language since their ultimate goal is to attract attention for \nshort-term financial or political gains rather than to build a long-term relationship with  readers12,13. Also, that \nsignificant deviations exist between linguistic profiles of authentic versus fake news  articles11,14.\nHowever, since human languages and the use of words evolve steadily with  time15, it is expected that lin-\nguistic features of fake news will also change with time. Our goal is to explore this hypothesis and investigate \npossible differences in the temporal evolution of linguistic features of fake versus authentic articles. To this end, \nwe develop a framework that (i)\u00a0extracts the most informative linguistic features of news articles; (ii)\u00a0classifies \narticles to various categories based on their content; (iii)\u00a0applies a novel change-point detection method to detect \nthe temporal evolution of linguistic features extracted, and (iv)\u00a0compares differences between the language \nevolution of real and fake news. Change-point detection is an active area in statistical research, which features \nalgorithms that segment data into smaller, homogeneous parts using flexible statistical models that can adapt to \nnon-stationary environments. Due to the natural heterogeneity of data in many real-life problems, novel change-\npoint detection methodologies have been applied in a wide range of application areas, from credit  scoring16 and \ncyber  security17 to  finance18.\nIn this paper we investigate the following research questions: (i) Which linguistic characteristics mainly \nappear informative in identifying a fake news article? (ii) Taking into account that language is evolving, how \ndo these characteristics change over time? (iii) Can we detect the occurrence of such changes? To address these \nresearch questions:\n\u2022 We introduce DECLARE, a novel framework that retrieves articles from trusted and suspicious domains \n(where the majority of them are in UK and US) and conducts an extensive linguistic analysis to investigate \n(i)\u00a0which are the most informative linguistic features for detecting fake news articles, and (ii)\u00a0how the lin -\nguistic features of fake news articles change over the years.\n\u2022 We develop and release a new publicly available large dataset from news articles, which have been published \nbetween 2009 and 2019 (more details can be found in the \"Data availability\" section). This dataset consists \nof 534 linguistic features of time-stamped fake and real news-articles. The feature selection process uses a \nlasso regression model and is described in more detail in the \" Framework \"  section.\nThe rest of this article is organized as follows: The \"Related work \" section reviews previous works on exploit-\ning linguistic characteristics that are common in fake news, the \" Framework \" section describes the proposed \nframework that has been used for this analysis. The results are presented and discussed in the \"Results\" and \n\"Conclusion \" sections.\nRelated work\nIn this section, we summarize works focusing on \u201cstyle-based fake news\u201d , namely news articles with quantifiable \nfeatures that represent linguistic traits, writing style, format and sentiment, and which can help distinguish fake \nnews content from authentic news. A comprehensive survey of such approaches is given  in8. These methods \nfocus on extracting features that capture language  use19, sentiment and writing  style20 or combinations  thereof11. \nLinguistic features are classified into three categories: (1) Stylistic features, which represent the syntax and textual \nstyle of articles\u2019 headlines and content, with textual style reflected in features like the frequency of stop-words, \npunctuation, quotes, negations and words that appear in all capital letters, and syntax style reflected in the fre -\nquency of Part-of-Speech (POS) tags in the text, and the use of proper nouns and capital words in the  titles14. \n(2) Complexity features, which aim at capturing the overall intricacy of articles and headlines. These features \ncan be computed based on several word-level metrics that include readability indexes and vocabulary richness, \nand which achieve higher classification accuracy in contrast to other feature  combinations21,22. (3) Psychological \nfeatures, which are based on frequencies of words found in dictionaries associated to sentiment and psychologi-\ncal processes. Such words are found in dictionaries like LIWC, which are curated by human domain experts. \nThe sentiment score is computed via the AFINN sentiment  lexicon23, a list of English terms manually rated for \n valence11. The premise behind using psychological features is that fake content has been shown to be consider -\nably more negative than authentic  news14,22.\nOther promising approaches include multi-class logistic regression for conducting stance  classification24, \ngraph-kernel based hybrid support vector machine classification of high-order propagation patterns in addition \nto semantic features such as topics and  sentiments25, the combination of time-series-based features extracted from \nthe evolution of news along with characteristics of the user accounts involved in the spreading of  news26, and \ndeep learning (DL)27. Finally, one of the most important recent advances in NLP , the DL-based BERT language \nrepresentation  model28, has also been used for fake news  detection3.\nTo sum up, although there have been several approaches demonstrating the importance of writing-style \nfeatures for the detection of fake news, there are hardly any studies exploring how the linguistic characteristics \nchange over the years as well as to apply statistical change-point detection on fake news trends. Such a study \nwill contribute towards understanding the fake news articles as well as improving the classification models for \nfake news detection.\n3\nVol.:(0123456789) Scientific Reports  |         (2023) 13:6086  | https://doi.org/10.1038/s41598-023-32952-3\nwww.nature.com/scientificreports/DECLARE framework\nThe proposed framework is partitioned into four different modules, as shown in Fig.\u00a0 1. Initially, the articles are \ncollected based on given URL domains (trusted/untrusted) and the feature extraction is accomplished. Next, \nthe optional step of Topic Classification is performed in order to select articles of desired topics. Subsequently, \nDimensionality Reduction is carried out for the extracted features of the selected articles, and finally multivariate, \noffline change-point detection is conducted on the data subset. For the rest of this section, the problem overview \nis formulated and the different segments of the work are examined in detail.\nProblem overview\nWe have a set of K\u2208N articles, which we denote by A:={a1,a2,...,aK} . Each article ai , i=1, 2, ...,K is a \nvector of dimensionality d+1 , with d\u2208N . The first d  elements of ai are the relevant values for each one of the d \nlinguistic features employed. In addition, each news article ai , i=1, 2, ...,K is categorized as \u201cReal\u201d or \u201cFake\u201d . \nHence, the last element of each vector ai is a value in the set {0, 1} and denotes the respective article\u2019s class label, \nwhere the values of 0 and 1 correspond to the classes \u201cReal\u201d and \u201cFake\u201d , respectively.\nThe problem that we deal with in this work is to study the attributes which most accurately characterize \npolitical articles, and to identify how they evolve over time. To do so, a set B  which corresponds to those informa-\ntive linguistic features is required. Therefore, after deciding these features, each article ai , i=1, 2, ...,K will \nbecome a vector of a reduced dimensionality |B|\u2264d +1 . Next, the articles are stamped by the time component \nt\u2208{ 1, ..., T} , and thus multivariate data sequences will be constructed by utilizing the article set A; for more \ninformation regarding the construction of data sequences, refer to the \" Change-point detection \" section. Then, \na multivariate change-point detection technique is employed to detect abrupt changes in the linguistic behavior \nof articles categorized as \u201cFake\u201d compared to the behavior of those categorized as \u201cReal\u201d . Distinctively, the pro-\ncess shall reveal the estimated number of change-points, denoted by \u02c6N , while the estimated locations, sorted \nin an increasing order, are denoted by \u02c6r1,\u02c6r2,...,\u02c6r\u02c6N . In addition, for every estimated change-point location \u02c6ri , \ni=1, 2, ...,\u02c6N , post-processing will be performed in order to determine the linguistic feature set Bi , with Bi\u2286B , \nthat contains the linguistic features in which the relevant change-point, \u02c6ri , was discernible the most.\nData collection and feature extraction\nRegarding the Data Collection process, a list of collected articles was constructed by crawling the  WebArchive29 \nfor news articles published from 2009 through 2019. The articles were divided into untrusted and trusted ones, \nbased on multiple sources. Initially, we used two pairs of domain credibility lists, which were originally formed \nFigure\u00a01.  DECLARE: overview scheme of framework.\n4\nVol:.(1234567890) Scientific Reports  |         (2023) 13:6086  | https://doi.org/10.1038/s41598-023-32952-3\nwww.nature.com/scientificreports/ in30. The first, contains domain names that usually publish Fake News and are highly scrutinized by fact-checking \norganizations, including Snopes, PolitiFact, and others. The latter includes high reputation domains, which have \nrarely or never been criticized by fact-checking sites. The lists contain both popular and uncommon domains with \nEnglish content from newspapers as well from blogs. Since the use of only one source could have brought bias \nto the analysis, labels provided by the independent online fact-checking outlet  MediaBiasFactCheck31 (MBFC) \nwere also employed to verify the validity of these lists. MBFC specifies how often a domain publishes factual \nnews by employing seven labels ranging from VERY LOW to VERY HIGH. We adopted the approach of Chen \nand  Freire32, and we define \u201cuntrusted\u201d domains as those with labels VERY LOW , LOW , MIXED, and \u201ctrusted\u201d \ndomains as those with VERY HIGH, HIGH, and MOSTLY FACTUAL. Thus, utilizing the knowledge of these \nlists and MBFC, the possibility of false positives was limited by discarding domains with discrepancies on their \nlabels and domains which were not classified by MBFC. The procedure has resulted to 2152 untrusted and 256 \ntrusted domains.\nEventually, the crawling procedure is the most vital and resource-demanding part of the data collection \nprocess. This component is responsible for crawling the domains inside the WebArchive efficiently. WebArchive \nhas saved more than 583 billion web pages over time and provides the WayBackMachine service, which allows \nthe traversal of a website through time. To crawhigh-reputatione, we used  Scrapy33, a Python library, which is \nconsidered one of the fastest web-crawling tools, especially for complex crawling and scrapping applications.\nThe next step of the data collection process is to extract the actual article from the website. However, extract-\ning information from various articles with different formats which are coming from different domains is a \nchallenging procedure. To address this issue, the  Newspaper3K34 library of Python was utilized. Specifically, \nNewspaper3K helps in building an article\u2019s object including fields like the article\u2019s title, body, and publish date in \na way that does not depend on the orientation of the website. After the execution of the data collection pipeline, \nwe perform data wrangling to remove duplicate or unreadable articles, as well as to fix formatting errors and \nremove any HTML code embedded in the text.\nFinally, in order to analyse how text-linguistic characteristics differ between real and fake sources, we extract \na plethora of different linguistic features to gather all the available knowledge from the text. To this end, we rely \non a previous  study11, which identified 534 features that capture Stylistic, Complexity and Psychological aspects \nof articles. Table\u00a0 1 unveils a list summarizaing a subset of the employed features. This processing results in a \nlarge dataset, named LinCFNA, which consists of a total 320,960 time-stamped articles, both fake and real, each \ncharacterized by 534 different linguistic features.\nTopic classification\nThe initial corpus did not contain any labels regarding the topics of articles. To be able to select articles for fur -\nther analysis that belong to particular topics (e.g., politics) we perform an automated topic classification to map \nall corpus articles to predefined topics of interest. To this end, we adopt Zero-Shot text  classification35, a highly \naccurate method that allows classification to classes not used or seen during the model\u2019s  training35,36. Zero-shot \nclassifiers leverage pre-trained language models, and therefore it can be thought of as an instance of transfer \nlearning, which generally refers to using a model trained for one task in a different application than what it was \noriginally trained  for37. This is particularly useful for situations where there are little to no available labelled \ndata. The original proposed textual entailment Zero-Shot method uses Natural Language Inference (NLI) based \nmodels as ready-made Zero-Shot text classifiers. In particular, the method converts the original text data into \nentailment data where, it considers a text sequence under classification as the NLI premise and constructs a \nhypothesis from each candidate label. For instance, in order to evaluate whether a sequence\u2019s topic corresponds \nto the user-defined \u201cpolitics\u201d class, the model can extract the corresponding label probabilities. Regarding the \ntask formulation and training procedure of the Zero-Shot textual models, as well examples of created hypotheses \nfor modeling different aspects, details can be found  in35. This approach was proven to be remarkably effective in \nmany text classification tasks, particularly when used with large pre-trained models like BART 38 or  RoBERTa39. \nIn this work, for the deployment of the NLI model, we employ the publicly available Hugging Face Transformers \n library40 along with the pre-trained BART-large model, developed by Meta (formerly Facebook)41. The specific \nmodel, which works without requiring any data other than the provided text, is fine-tuned for the Multi Genre \nNatural Language Inference corpus (MNLI) dataset, that contains ten different text  categories42. The Hugging \nFace\u2019s model pipeline employs both the pre-trained model as well as the pre-processing procedures that were \nperformed during the fitting stage. Therefore, little to none text preprocessing is required during inference time. \nIn addition, lemmatization or stemming are not required either, due to the fact these models are mostly trained \non raw text, using WordPiece  tokenizers28. To that extent, only a minimal text cleaning was performed prior \nto using the aforementioned pipeline by removing HTML code, fixing unicode errors and finally using special \ntokens for any embedded URLs or phone numbers.\nIn addition, LinCFNA contains observations of articles which could be considered to belong to more than \none category. Therefore, we follow a multi-label approach. Specifically, unlike normal classification tasks where \nclass labels are mutually exclusive, multi-label classification involves predicting multiple mutually non-exclusive \nlabels, where the labels are considered independent. The probabilities are normalized for each candidate label by \napplying a softmax of the \u201cY es\u201d versus the \u201cNo\u201d scores. That way, for a given article, the model produces a vector \nof independent probability values for each candidate label. An article is said to belong to a predefined candidate \nlabel as long as the respective probability is larger than 0.5.\nIn this work, our analysis focused on political news articles, since we would like to study fake news around \npolitical articles. Thus, each article ai , i=1, 2, ...,K is classified as part of the \u201cPolitics\u201d class and kept for fur -\nther analysis as long as the probability of ai being a political article is greater or equal than Q , with Q=0.75 . \nWe used Q=0.75 instead of the model\u2019s default threshold 0.5 for two reasons. Firstly, since the accuracy of the \n5\nVol.:(0123456789) Scientific Reports  |         (2023) 13:6086  | https://doi.org/10.1038/s41598-023-32952-3\nwww.nature.com/scientificreports/Zero-Shot approach could not be directly observed, and therefore bias or ambiguous labels could have been \ncreated during the analysis phase due to its non-supervised nature, we used a larger threshold than the default \none in order to discard the articles with the most controversial labels. Secondly, by using a very large value of Q , \nfor example, Q=0.9 , led to significantly fewer articles classified as political, which then created problems in the \nchange-point detection part that requires the existence of observations for every time point within the period \ntested. Therefore, Q  was chosen to keep in mind that a plethora of observations were selected, while the most \nambiguous observations were not kept for further analysis, in order to produce more representative and robust \nresults. Out of the original 320,960 observations of LinCFNA, the aforementioned procedure led to a set A  of \n87,\u00a0066 articles about politics. Even though the non-political or ambiguously selected articles were limited by \nadjusting the value of Q , we further examined the validity of the selected articles for a random sample of them. \nSpecifically, since the ground-truth labels of the \u201cPolitics\u201d class were not present, to examine the validity of the \npolitical labels assigned, we sampled and checked the correctness of 100 out of the total assigned political articles. \nBased on our evaluation, around 90% of the articles were correctly assigned the \u201cPolitics\u201d class label. Based on \nthe set A , we construct the matrix X\u2208R|A|\u00d7535 which comprises the linguistic representation of the selected \npolitical articles along with their validity labels.\nDimensionality reduction\nThe aim of dimensionality reduction is to reduce the size of the X\u2208R|A|\u00d7535 matrix by pruning some of the \ninitial 534 features, and end-up with a matrix XR\u2208R|A|\u00d7 (d+1) of a lower dimensionality, which contains only \nd\u2208Z+ informative and supportive features that reveal fruitful information about the credibility of topic spe-\ncific articles. A matrix with a lower dimensionality will improve substantially the effectiveness of the upcom-\ning change-point detection analysis. As Fig.\u00a0 1 illustrates, feature reduction procedure consists of two different \nphases. The first applies filtering whereas the second applies embedded feature selection and component based \nreduction techniques.\nDuring the first phase, we remove 29 features with very low sample variance, namely attributes that have the \nsame value in nearly all observations, which lead to a new reduced feature matrix XF\u2208R|A|\u00d7505 . The removed \nattributes of LinCFNA along with their sample variance are included in Table\u00a0S1 in the Supplementary Tables \nSection of the online supplement. During the second phase, instead of just using a filtering evaluation function \nthat relies solely on properties of the features (e.g. the variance threshold or Pearson\u2019s correlation coefficient), \nwe apply two individual non-filtering methods. In the first one, with X[j] denoting the jth column of the matrix \nX, the articles\u2019 labels in X[506] were also employed in a supervised embedded method for a further selection of \nimportant features, while the second approach relied on a component based reduction technique.\nFor the first approach, we employed a classification model that utilizes a feature selection mechanism during \nthe algorithm\u2019s modelling execution. Such mechanisms generally aim at the inclusion of predictors that strongly \naid the generalization of unseen data, by preventing the learning algorithm from overfitting the training dataset. \nEmbedded methods perform feature selection in the process of training and are usually specific to given learning \n machines43. Typical embedded methods include various types of tree based algorithms, like CART, C4.5, and \nRandom Forest  trees44, but also other algorithms such as logistic regression  variants45. Among the embedded \nmethods, there are regularized models which perform feature weighting based on objective functions that mini-\nmize fitting errors while forcing the predictor coefficients to be exactly zero. These methods, which are based \non the penalization of the L1 norm (Lasso), induce penalties to features that do not contribute to the model and \nthey usually work with linear classifiers, such as Support Vector Machines or Logistic  Regression46.\nEven though many embedded feature selection options are available, for this specific work, we adopt a logistic \nregression model penalized using the L1 norm, to obtain a robust classifier with sparsity in the  coefficients47. \nAdditionally, regarding the strength of regularization, we compute the regularization path at a grid of values in \na stratified k -fold Cross Validation (CV) manner for the regularization parameter /afii9838 , which controls the overall \nstrength of the lasso penalty. The choice of the number of folds, k, is usually 5 or 10, but there is no formal  rule48. \nIn our work, different values for the number of folds have been employed since there is a trade-off on the choice \nof k; larger values lead to much higher computational complexity and lower prediction errors, while smaller \nvalues lead to more computationally efficient results with possibly higher prediction errors. Therefore, we took \nk\u2208{5, 10, 25 } , and since the results were extremely similar in terms of the selected features, we carried out the \nanalysis with k=10 . With respect to /afii9838 , since larger values of this hyperparameter produce solutions with more \nsparsity in the coefficients, for assessing the different amount of features, the classifier was evaluated on the left-\nout folds with different values of /afii9838 . The procedure exploited the F1 score evaluation metric, since it combines \nboth the precision and recall of a classifier into a single metric by taking their harmonic  mean49. We selected a \nmodel size with a penalty large enough, which though still allows to accurately classify observations, in respect \nto the F1 score metric. The obtained F1 scores for the different regularization parameter values, /afii9838 , can be found \nin Figure\u00a0S1 in the Supplementary Figures Section of the online supplement. Based on the aforementioned figure \nand the slope of the obtained curve, the regularization parameter value to be used was chosen as the point on \nthe x-axis where the absolute value of the slope in the curve is at its minimum; not long after this point, we see \nthat there is a steep drop on the F1-scores. Finally, this approach led to a new reduced feature matrix XLasso that \nconsists of 34 unique features, which are summarized in Table\u00a0 1.\nWe selected the aforementioned lasso approach because it can exploit the sparsity in the input matrix XF , \nand since, for high-dimensional data, it is extremely fast over other feature selection methods. Therefore, it was \ncomputationally convenient to employ the lasso approach with different settings in order to explore how differ -\nent model sizes and informative features could perform in predicting the labels of the articles. Apart from the \nlasso method, which is easily interpretable, we also utilize a component-based approach. The intuition behind \n6\nVol:.(1234567890) Scientific Reports  |         (2023) 13:6086  | https://doi.org/10.1038/s41598-023-32952-3\nwww.nature.com/scientificreports/the use of another dimensionality reduction method was to study the robustness of the framework and compare \nthe different estimated change-point locations that would have been produced by the two methods.\nAs a component-based approach, we employed Principal Component Analysis (PCA), mainly because it was \nperceived that there are variables which are strongly correlated; therefore, feature reduction has been essential, \nwithout losing critical information. PCA is an unsupervised dimensionality reduction technique, which ignores \nthe class labels of the articles. Instead, PCA focuses on capturing the direction of maximum variation in the \ndata set, by defining an orthogonal projection of the data onto a lower dimensional linear space, known as the \nprincipal subspace, such that the variance of the projected data is  maximized50. Regarding the selected number \nof components, given that the upcoming multivariate Change-point Detection module first performs a data \naggregation operation that accumulates the data points to k  day periods, considerably reducing the number of \nobservations to 255, the number of components needed to be an order of magnitude less than the length of the \ndata sequence; otherwise, we would potentially run into high-dimensionality and large computational complex-\nity issues. We worked on taking the first 10,\u00a020, and 30 principal components with the results being extremely \nsimilar due to the robustness of the proposed change-point detection algorithm; for more information on this, \nsee Section\u00a0 Results  of51. Therefore, in the article, we present the obtained results for the most parsimonious case, \nmeaning when the first 10 principal components were selected. Ultimately, this approach led to a new reduced \nfeature matrix XPCA that consists of 10 features, which are linear combinations of the original ones. The results \nacquired when we used the first 20 or 30 principal components are given in the Supplementary Methods Section \nof the online supplement.\nChange\u2011point detection\nBased on whether we have full knowledge of the data to be analysed, change-point detection is split into two \nmain categories; offline detection, where the data are already obtained, and online detection, in which the \nobservations arrive sequentially at present. With respect to the dimensionality of the data, change-point detec-\ntion can be further separated into algorithms that act only on univariate data and to those that are suitable for \nchange-point detection in multivariate, possibly high-dimensional data sequences. In this section, the focus is \non offline change-point detection and with p\u2208N denoting the dimensionality of the given data, the model in \nfull generality is given by \nwhere, at each time point t , Xt\u2208Rp\u00d71 are the observed data and ft\u2208Rp\u00d71 is the underlying, unknown, p -dimen-\nsional deterministic signal which undergoes structural changes at certain, unknown points. The matrix /Sigma1\u2208Rp\u00d7p \nis diagonal, while the noise terms \u03b5t\u2208Rp\u00d71 are random vectors with mean the zero vector and covariance the \nidentity matrix. In the current manuscript, we are looking for changes in the vector of first order derivatives, or, \nin other words, changes in the slope of any of the univariate component data sequences. In Fig.\u00a0 2, we graphically \nprovide an example of a three-dimensional data sequence of length T=400 . The diagonal elements of the matrix \n/Sigma1 are all equal to 7, while \u03b5t,i follow the standard Gaussian distribution for i=1, 2, 3  . There are three change-\npoints in the slope at locations r1=106, r2=200 and r3=248 . To be more precise, the first two component \ndata sequences have two change-points each; for X1,t at locations t=106 and t=248 , while for X2,t at locations \nt=200 and t=248 . There are no change-points in X3,t.\nA recently developed change-point detection method, called Multivariate Isolate-Detect (MID), is applied \nto the given multivariate data sequence Xt ; the component data sequences are related to the extracted features; \nmore details are given in Steps 1\u20133 below. MID has been introduced  in51 and is a generic technique for the detec-\ntion of the number and the location of multiple structural changes in the behaviour of given multivariate, possibly \nhigh-dimensional data. It provides maximal detection power by testing for change-points into intervals that \ncontain at most one change-point; this specific isolation technique was first introduced in the Isolate-Detect (ID) \nmethodology  of52. The main idea is that for the observed data sequences xt,jt=1,..., T,j=1,...,p, and with (1) Xt=ft+\ufffd\u03b5t,t=1,..., T,X1,t X2,t X3,t\n0 100 200 300 400\nTime point sObserved dataExample of a three\u2212dimensional data sequence with three change\u2212point s\nFigure\u00a02.  An example of a three dimensional data sequence with piecewise-linear structure, that undergoes \nthree changes in its first derivative at locations r1=106, r2=200 and r3=248.\n7\nVol.:(0123456789) Scientific Reports  |         (2023) 13:6086  | https://doi.org/10.1038/s41598-023-32952-3\nwww.nature.com/scientificreports//afii9838T a positive constant, MID first creates two ordered sets of K=\u2308T//afii9838T\u2309 right- and left-expanding intervals. For \ni=1,...,K , the ith right expanding interval is Ri=[1, min {i/afii9838T,T}] while the ith left-expanding interval is \nLi=[max{1,T\u2212i/afii9838T+1},T]. We collect these intervals in the ordered set SRL={R1,L1,R2,L2,...,RK,LK} . \nThe algorithm first acts on the interval R1=[ 1,/afii9838T] by calculating, for every univariate component data sequence, \nan appropriate contrast function value for the Q\u2208Z+ possible change-point candidates in this interval (details \non suitable contrast functions to be used are given in Section\" Data collection and feature extraction \"of52). This \nprocess will return Q  vectors yj,j=1,...,Q of length p  each; for example, the elements of y1\u2208Rp will be the \ncontrast function values related to the first change-point candidate in R1 , for each of the p  component data \nsequences, the elements of y2\u2208Rp will be the relevant values for the second candidate in R1 , and so on. The next \nstep is to apply to each yj the mean-dominant norm L:Rp\u2192R with L(yj)=1\u221ap/radicalBig/summationtextp\ni=1y2\nj,i where yj,i\u22650,\u2200i,j . \nApplying L(\u00b7) to each yj , will return a vector v of length Q . We identify \u02dcbR1 := argmaxj/braceleftbig\nvj/bracerightbig\n . If v\u02dcbR1 exceeds a certain \nthreshold, which has been explicitly derived  in51, then \u02dcbR1 is taken as a change-point. If not, then the process tests \nthe next interval in SRL . Upon detection, the algorithm makes a new start from the end-points of the expanding \ninterval where the detection occurred.Table 1.  Informative linguistic features extracted from lasso logistic-regression.Feature Definition\nComplexity features\n\u00a0Automated readability index 0.39/parenleftBig\nTotal# of Words\nTotal # of Sentences/parenrightBig\n+11.8/parenleftBig\nTotal #of syllables\nTotal # of words/parenrightBig\n\u221215.59\n\u00a0Coleman-Liau readability index 5.88/parenleftBig\nTotal # of Letters\nTotal #of Words/parenrightBig\n\u221229.6\u2217/parenleftBig\nTotal #of Sentences\nTotal #of Words/parenrightBig\n\u221215.8\n\u00a0Sichel\u2019s Vocabulary Richness Total # of Happaxdilsegomena /Total #of Words\nStylistic features\n\u00a0Part of speech Tag: JJR Adjective, comparative (e.g. bigger)\n\u00a0Part of speech Tag: NNP Proper noun, singular (e.g. Harrison)\n\u00a0Structural Feature:\navg_number_of_stopwords_per_sentenceAverage number of stop-words per sentence\n\u00a0Structural feature:\ntitle_ratio_uppercaseRatio of uppercase in title\n\u00a0Structural feature:\ntitle_avg_number_of_all_caps_per_sentenceAverage number of all caps per sentence in title\n\u00a0Structural feature:\ntitle_total_number_of_sentencesTotal number of sentences in title\n\u00a0Structural feature:\ntitle_total_number_of_charactersTotal number of characters in title\n\u00a0Structural feature:\ntitle_total_number_of_begin_upperTotal number of words which begin with upper in title\nPsychological features\n\u00a0AFINN sentiment  score23A number in [\u22125, 5] , indicating the negative or positive sentiment\n\u00a0Laver Garry\u2019s dictionary: CULTURE_POPULAR Related with popular culture (e.g. media)\n\u00a0Loughran-McDonald\u2019s56 WEAK_MODAL Classifies the words into sentiment categories\n\u00a0Laver Garry\u2019s dictionary: ECONOMY Related with economy (accounting, earn, loan)\n\u00a0Laver Garry\u2019s dictionary: INSTITUTIONS_ NEUTRAL Related with neutral institutions (chair, scheme, voting)\n\u00a0RID secondary feeling: SOCIAL_ BEHAVIOR Related with social behavior (ask, tell, call)\n\u00a0RID secondary feeling: TEMPORAL_ REPERE Related with temporal references (e.g. when, now, then)\n\u00a0LIWC:  AUXVERB57Linguistic dimensions-auxiliary verbs (e.g. is, was, be, have)\n\u00a0LIWC:  PPRON57Linguistic dimensions-personal pronouns (e.g. i, you, my, me)\n\u00a0LIWC:  THEY57Linguistic dimensions-3rd pers plural (e.g. they, their, they\u2019 d)\n\u00a0LIWC:  WE57Linguistic dimensions-1st pers plural (e.g. we, us, our)\n\u00a0LIWC:  IPRON57Linguistic dimensions-impersonal pronouns (e.g. it, it\u2019s, those)\n\u00a0LIWC:  NEGATE57Linguistic dimensions-negate (e.g. no, not, never)\n\u00a0LIWC:  WORK57Personal concerns-work (e.g. job, majors)\n\u00a0LIWC:  RELIG57Personal concerns-religion (e.g. altar, church)\n\u00a0LIWC:  DEATH57Personal concerns-death (e.g. bury, coffin, kill)\n\u00a0LIWC:  CERTAIN57Psychological processes-certainty (e.g. always, never)\n\u00a0LIWC:  NEGEMO57Psychological processes-negative emotion (e.g. hurt, ugly, nasty)\n\u00a0LIWC:  TIME57Time orientations-time (e.g. end, until, season)\n\u00a0LIWC:  RELATIV57Time orientations-relativity (e.g. area, bend, exit)\n\u00a0LIWC:  HEAR57Perceptual processes-hearing (e.g. listen, hearing)\n\u00a0LIWC:  ACHIEV57Drives and needs-achievement (e.g. win, success, better)\n8\nVol:.(1234567890) Scientific Reports  |         (2023) 13:6086  | https://doi.org/10.1038/s41598-023-32952-3\nwww.nature.com/scientificreports/Under Gaussianity of the noise terms \u03b5t , MID has been proven to be consistent in accurately estimating the \ntrue number and locations of the change-points; for further details, please see Section \"Related work \"  of51. How -\never, in practice, both ID and MID (which is employed in the current multivariate framework) have been shown \nto be robust and to exhibit very strong performance in scenarios where we could have either auto-correlated or \nheavy-tailed noise; for more details on such robustness results, please see Sections \"Results\" and \"Conclusion \"  \n of52.\nFor the purposes of this paper, the matrices XLasso and XPCA , which are derived from the dimensionality \nreduction stage explained in the previous section, will be used in creating {Xt}t=1,2, ...,T of Eq.\u00a0( 1), to which the \nalgorithm MID will be applied. The three main steps that have been followed are given below.\nStep 1  From the given matrix ( XLasso or XPCA , depending on the dimensionality reduction technique \nemployed), two smaller matrices are first created splitting the data into those obtained from articles categorised \nas \u201cFake\u201d and those obtained from articles categorised as \u201cReal\u201d . From now on, the notation employed for the \naforementioned two matrices is YF and YR , respectively.\nStep 2  To have more information, and therefore higher detection power, we worked on biweekly data. Aggrega-\ntion over time is essential in order to avoid direct utilization of raw, misleading observations. In addition, it was \nimportant to achieve aggregation of the observations in a way that both fake and real data would appear within \nevery period. Regarding the selected aggregation interval, by experimenting we confirmed that aggregating into \ntwo-week periods (specifically 14-day periods) allowed us to have sufficient information for both fake and real \nnews within every time period used for the change-point detection part of our proposed framework. Therefore, \nthe values for the various features in the matrices YF and YR have been aggregated over a period of 14 days. For \nthe aggregation step, we employed the sample average of the relevant values for each feature. To be more precise, \nwith p\u2208N being the number of features used, since there are 255 periods of 14 days in our data, after this second \nstep, two matrices, Y\u2217\nR and Y\u2217\nF of dimensionality 255\u00d7p are created, which contain aggregated information for \narticles classified as \u201cReal\u201d or \u201cFake\u201d , respectively.\nStep 3  The matrix Ydi\ufb00=Y\u2217\nR\u2212Y\u2217\nF\u2208R255\u00d7p is created.\nAfter the above three steps have been completed, we will be under the scenario of looking for abrupt changes \nin the trend of Xt as in (1 ), where specifically now Xt=[Ydi\ufb00][t],t=1, 2, ..., 255  , where [Ydi\ufb00][t]\u2208Rp\u00d71 is \nthe tth row of the matrix Ydi\ufb00 . We work on the differences between the values of the characteristics for \u201cFake\u201d \nand \u201cReal\u201d articles in order to capture significant deviations in the comparative behaviour between articles from \nthe two aforementioned categories. This will provide an indication of attempts from unreliable news agencies to \nwrite articles in such a way that resembles those published from trusted news sites.\nEthical approval\nThe authors declare that no human participants were involved in the study or data acquisition.\nResults\nInformative Linguistic characteristics of political articles\nApplying the lasso approach in the DECLARE framework as described above, we extract the most informative \nlinguistic features which aid in identifying whether a political article is fake or not. Table\u00a0 1 unveils the list of the \n34 important features which were most informative in linearly separating the articles to fake and real. The results \nsupport that most of the linearly informative characteristics are related to psychology. That indicates the contrast \nbetween fake and real political articles, in psychological factors that demonstrate the intention of the writers to \nconvince readers that the content is realistic by using certain mechanisms (e.g. by emotionally influencing readers \nor by overdramatizating certain events through Persuasive Language)53,54. The extracted feaure AFINN sentiment \n score23 was indeed expected to be informative, since sentiment analysis on fake news articles had revealed that \nfake news tends to contain increased negative emotional  language14,22. Moreover, the many selected features from \nthe widely used Linguistic Inquiry and Word Count (LIWC) dictionary, demonstrate that fake and real news \ndifferentiate in terms of choosing the words that reveal psychometrics characteristics.\nFurthermore, the results reveal that most of the significant stylistic attributes are structural features and are \nlinked to the title of an article. Interestingly the knowledge which can be extracted from uppercase text is certainly \nfruitful as it was found that three of the important features are based on uppercase letters. This observation is \nlinked with previous studies around fake news, as fake news were found to be more dramatic with copious use \nof uppercase letters to make it a click-bait for the  readers55. Finally, regarding characteristics which expound \nthe complexity and writing style of articles, we spotted that a group of word-level readability and vocabulary \nrichness measures are helpful in distinguishing fake and non-fake political articles, which concur with identical \nfindings  in21,22.\nEvolution of linguistic characteristics\nBy taking into account the established results of the lasso-based model, we showed that certain characteristics \ndiffer between the fake and non-fake political articles. However, since the time-component is involved and due \nto the fact that both language and writing are continuously evolving, these differences between the informative \ncharacteristics are expected to be changing as well. Applying the MID change-point detection algorithm with \nits default parameter values, the following results were obtained:\nFor the lasso-based dimensionality reduction method, four change-points were detected. Those correspond \nto the following 14-day periods; in parentheses we provide the features in which these changes were apparent \nthe most:\n9\nVol.:(0123456789) Scientific Reports  |         (2023) 13:6086  | https://doi.org/10.1038/s41598-023-32952-3\nwww.nature.com/scientificreports/\u2022 First change-point Period: 2011-03-02 until 2011-03-15 (\u201cTitle_total_number_of_sentences\u201d , \u201cTitle_ratio_\nuppercase\u201d).\n\u2022 Second change-point  Period: 2011-10-26 until 2011-11-08 (\u201cLIWC_Negate\u201d).\n\u2022 Third change-point: Period 2014-02-12 until 2014-02-25 (\u201cTitle_total_number_of_sentences\u201d).\n\u2022 Fourth change-point Period: 2016-09-07 until 2016-09-20 (\u201cTitle_total_number_of_sentences\u201d , \u201cTitle_ratio_\nuppercase\u201d , \u201cTitle_avg_number_of_all_caps_per_sentence\u201d , \u201cRID_Secondary_Social_Behavior\u201d , \u201cRID_Sec-\nondary_Temporal_Repere\u201d , \u201cLM_Weak_Modal\u201d , \u201cSichel\u2019s Vocabulary Richness\u201d).\nWe observe that two features, \u201cTitle_total_number_of_sentences\u201d and \u201cTitle_ratio_uppercase\u201d , seem to be more \nimportant than the rest in capturing important movements in the similarity level of \u201cFake\u201d and \u201cReal\u201d articles. \nFigure\u00a0 3 presents the results for the aforementioned two features.\nIn an attempt to provide a possible explanation of the estimated change-point locations, looking at the move-\nments of the estimated signals in Fig.\u00a0 3 between consecutive segments, we observe that there is a significant \ndrop in the value of their slopes after the first and the last change-point. Changes in the slope of such nature \nindicate that after the aforementioned two change-points, the distinction between fake and real articles became \nless apparent, meaning that there had possibly been an intentional, successful attempt from news agencies that \nproduce fake articles to resemble more the writing style of trusted and long-established agencies.\nContinuing now with the PCA-based dimensionality reduction method with 10 Principal components, the 14 \nday change-point periods were detected. Specifically, four change-point locations were detected for the periods \nof 2011-01-19 until 2011-02-01, 2012-06-06 until 2012-06-19, 2012-10-10 until 2012-10-23 and finally for the \nperiod of 2016-06-15 until 2016-06-28. Lastly, it should be highlighted that since PCA performs dimensionality \nreduction and not feature selection, the resultant components cannot be directly interpreted like the lasso-based \napproach.\nWe conclude that abrupt changes in the trend of the signal are present. Four change-points have been detected \nwhen either the Lasso or the PCA based dimensionality reduction method has been employed; the estimated \nchange-point locations for the two dimensionality reduction methods are similar. This indicates the robustness of \nthe proposed framework in this paper, as described in the \"Dimensionality reduction\" and \"Change-point detec-\ntion\" sections. It is crucial to mention that even though the detected locations are close to each other for the two \nmethods, there are some discrepancies which occurred, mainly due to the fact that the two reduction methods \nobviously produce different feature sets. Furthermore, since the Lasso based reduction method provides some \ninterpretation regarding the characteristics which the changes were perceptible the most, it was perceived that \nmost of the detected periods had abrupt changes in at least one stylistic feature which was related to the title of \nthe article. Finally, we observed that our dataset included numerous articles related to major political events that \ntook place near the dates of the detected change-points (e.g. phenomena regarding the beginning of the Syrian \nCivil War which took place in March 2011, as well as political scandals which occurred in early October 2016, \nthat played an important role in the 58th United States presidential election). These findings merit additional \nanalysis and discussion in potential future works with the utilization of the DECLARE framework.\nThe robustness of our results has also been investigated under the scenario where the data are slightly altered. \nTo be more precise, 5% of the observations in every time period have been randomly taken out before we carried Title_ratio_uppercase Title_total_number_of_sentences\n0 100 200\u22120.050.000.050.10\n\u22120.40.00.40.8\nTime (14\u2212day  periods after 29\u221204\u22122009)Results fo r two significant features\nFigure\u00a03.  The data sequences for \u201cTitle_total_number_of_sentences\u201d (bottom row) and \u201cTitle_ratio_\nuppercase\u201d (top row), the estimated change-point locations (red vertical lines), and the fitted continuous and \npiecewise-linear signals (presented with blue colored lines) obtained by the MID change-point detection \nalgorithm.\n10\nVol:.(1234567890) Scientific Reports  |         (2023) 13:6086  | https://doi.org/10.1038/s41598-023-32952-3\nwww.nature.com/scientificreports/out change-point detection; this data removal process has been repeated a total of 100 times, meaning that 100 \ndifferent data sets were created, which contained approximately 95% of the values of the initial data set. The \nchange-point detection results related to these 100 shrunken data sets are in fact very good. More specifically, \n84% of the times we get either the same number of estimated change-points as in the original data set or we are \nat the smallest possible distance (\u00b11)  , meaning that we estimate either 3 or 5 change-points. Furthermore, the \nestimated change-point locations are in a close neighbourhood of the locations obtained when the original data \nset was employed in our analysis.\nConclusion\nThis work was initially focused on the establishment of LinCFNA, a large open dataset which consists of 534 \nlinguistic features of time-stamped fake and real news-articles. Due to the massive amount of available articles \nin LinCFNA, it allows for the re-use and discovery by the research community, in order to further build upon \nand advance the knowledge and tools around disinformation and fake-news detection.\nFurthermore, the paper proposes a framework which includes the data collection, topic classification, dimen-\nsionality reduction and change point detection procedures. The proposed framework permits the usage of spe-\ncific parameters for selecting desired articles and features of a certain timeline and topic for further analysis. \nUltimately, experiments and results have been carried out by employing our methods on political news articles. \nIt has been demonstrated that most of the linearly informative characteristics of political articles are related to \npsychology, while interestingly, most of the significant stylistic attributes are structural features and are linked \nto the title of the articles. Moreover, the results of the application of change-point detection methods not only \nsupport the fact that linguistic characteristics of the political articles are evolving, but have also indicated that \nin several time-points, even the most informative features of the articles undergo significant changes, making \nthe fake and non-fake articles harder to distinguish. Lastly, we observed that significant changes in the linguistic \nbehaviour of political articles occurred on dates where major real-life political phenomena took place. In potential \nfuture work using the DECLARE framework, these findings deserve more examination and discussion.\nIt should be highlighted that the crawling process for extracting information from old websites or domains \nthat changed ownership is challenging. This is due to the fact that WayBackMachine removes information of \nwebsites when a well-reasoned request has been made by the content creators or when a new ownership for \nthe page has commenced. Another important challenge is the modern and advanced generative based models \nwhich can be used for article  writing58. Even though that family of models can generate false content, it may \nbe difficult to identify the validity of the content based on the writing style and linguistic features, since those \nmodels try to mimic human-like writing behaviour. This matter deserves more investigation, and in particular, \nwith the application of the proposed framework, the evolution of linguistic features that can be extracted from \nthe content of generation models could be studied.\nOverall, the analytical and collective methods developed in this work have proved to be sensitive and benefi-\ncial for the analysis of fake news and articles in general. Through this study, we would like to encourage research-\ners to consider the usage of certain linguistic characteristics, as well as the linguistic alternations of fake news due \nto the temporal component, in order to further improve their methodologies and models in engaging fake news.\nData availibility\nThe LinCFNA dataset along with the original text of the articles collected and the political articles\u2019 features that \nlasso had produced, have all been made publicly available at the following repository: https://  github.  com/  nikop  \netr/ LinCF  NA.\nReceived: 16 November 2022; Accepted: 5 April 2023\nReferences\n 1. Molina, M. D., Sundar, S. S., Le, T. & Lee, D. \u201cFake news\u201d is not simply false information: A concept explication and taxonomy of \nonline content. Am. Behav. Sci. 65, 180\u2013212. https://  doi. org/ 10. 1177/  00027  64219 878224  (2021).\n 2. Rini, R. Fake news and partisan epistemology. Kennedy Inst. Ethics J.  27, E-43 (2017).\n 3. Szczepanski, M., Pawlicki, M., Kozik, R. & Choras, M. New explainability method for bert-based model in fake news detection. \nSci. Rep. 11, 23705 (2021).\n 4. Notions of disinformation and related concepts (erga report). https://  erga-  online.  eu/ wp- conte  nt/ uploa  ds/ 2021/  03/ ERGA-  SG2-  \nReport- 2020-  Notio  ns- of- disin  forma  tion-  and-  relat  ed- conce  pts- final.  pdf (2021).\n 5. Vosoughi, S., Roy, D. & Aral, S. The spread of true and false news online. Science  359, 1146\u20131151. https://  doi. org/ 10. 1126/  scien  \nce. aap95  59 (2018).\n 6. Aral, S. The Hype Machine. How Social Media Disrupts Our Elections, Our Economy, and Our Health\u2013and How We Must Adapt  \n(Currency, 2020).\n 7. Clack, T. & Johnson, R. (eds) in The World Information War. Western Resilience, Campaigning, And Cognitive Effects (Routledge, \n2021).\n 8. Zhou, X. & Zafarani, R. A survey of fake news: Fundamental theories, detection methods, and opportunities. ACM Computing \nSurveys (CSUR) 53, 1\u201340 (2020).\n 9. Guo, Z., Schlichtkrull, M. & Vlachos, A. A survey on automated fact-checking. Trans. Ass. Comput. Linguist. 10, 178\u2013206 (2022).\n 10. Hounsel, A. et\u00a0al.  Identifying disinformation websites using infrastructure features. In Ensafi, R. & Klein, H. (eds.) 10th USENIX \nWorkshop on Free and Open Communications on the Internet, FOCI 2020, August 11, 2020 (USENIX Association, 2020).\n 11. Paschalides, D. et al. Check-it: A plugin for detecting fake news on the web. Online Soc. Netw. Media 25, 100\u2013156 (2021).\n 12. Allcott, H. & Gentzkow, M. Social media and fake news in the 2016 election. J. Econ. Perspect.  31, 211\u201336 (2017).\n 13. Bakir, V . & McStay, A. Fake news and the economy of emotions: Problems, causes, solutions. Digit. J.  6, 154\u2013175 (2018).\n 14. Horne, B. D. & Adali, S. This just in: Fake news packs a lot in title, uses simpler, repetitive content in text body, more similar to \nsatire than real news. In Proceedings of the 11th International AAAI Conference on web and social media  (2017).\n11\nVol.:(0123456789) Scientific Reports  |         (2023) 13:6086  | https://doi.org/10.1038/s41598-023-32952-3\nwww.nature.com/scientificreports/ 15. Scheffer, M., van de Leemput, I., Weinans, E. & Bollen, J. The rise and fall of rationality in language. In\u00a0 Proceedings of the National \nAcademy Science  Vol. 118, e2107848118 (2021).\n 16. Bolton, R. J. & Hand, D. J. Statistical fraud detection: A review. Stat. Sci. 17, 235\u2013255 (2002).\n 17. Siris, V . A. & Papagalou, F. Application of anomaly detection algorithms for detecting syn flooding attacks. In Proceedings of the \nIEEE Global Telecommunications Conference, 2004. GLOBECOM\u201904., vol. 4, 2050\u20132054 (IEEE, 2004).\n 18. Lavielle, M. & Teyssiere, G. Adaptive detection of multiple change-points in asset price volatility. In Long Memory in Economics , \n129\u2013156 (Springer, 2007).\n 19. Volkova, S., Shaffer, K., Jang, J.\u00a0Y . & Hodas, N. Separating facts from fiction: Linguistic models to classify suspicious and trusted \nnews posts on twitter. In Proceedings of the 55th annual meeting of the association for computational linguistics (volume 2: Short \npapers) , 647\u2013653 (2017).\n 20. Przybyla, P . Capturing the style of fake news. In\u00a0 Proceedings of AAAI Conference on Artificial Intelligence Vol. 34, 490\u2013497 (2020).\n 21. P\u00e9rez-Rosas, V ., Kleinberg, B., Lefevre, A. & Mihalcea, R. Automatic detection of fake news. In Proceedings of the 27th International \nConference on Computational Linguistics , 3391\u20133401 (Association for Computational Linguistics, 2018).\n 22. Zollo, F. et al. Debunking in a world of tribes. PLoS ONE 12(7), e0181821 (2017).\n 23. Nielsen, F. A new anew: Evaluation of a word list for sentiment analysis in microblogs. In Proceedings of the ESWC2011 Workshop \non \u2019Making Sense of Microposts\u2019: Big things come in small packages, no. 718 in CEUR Workshop Proceedings, 93\u201398 (CEUR-WS, \n2011).\n 24. Ferreira, W . & Vlachos, A. Emergent: A novel data-set for stance classification. In Proceedings of the 2016 Conference of the North \nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, 1163\u20131168 (Association for \nComputational Linguistics, 2016).\n 25. Wu, K., Y ang, S. & Zhu, K. Q. False rumors detection on sina weibo by propagation structures. In Proceedings of the 2015 IEEE \n31st International Conference on data engineering, 651\u2013662 (IEEE, 2015).\n 26. Previti, M., Rodriguez-Fernandez, V ., Camacho, D., Carchiolo, V . & Malgeri, M. Fake news detection using time series and user \nfeatures classification. In Proceedings of the International Conference on the Applications of Evolutionary Computation , 339\u2013353 \n(Springer, 2020).\n 27. Li, J. & Lei, M. A brief survey for fake news detection via deep learning models. Proc. Comput. Sci. 214, 1339\u20131344. https://  doi. \norg/ 10. 1016/j. procs.  2022. 11.  314 (2022).\n 28. Devlin, J., Chang, M., Lee, K. & Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human \nLanguage Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2\u20137, 2019, Volume 1 (Long and Short Papers)  (eds Burstein, \nJ. et al.) 4171\u20134186 (Association for Computational Linguistics, 2019).\n 29. Internet-Archive. WebArchive: The Wayback Machine. https://  web.  archi  ve. org/ (2014).\n 30. Hagen, S. OpenSources: Curated lists of online sources. https://  github.  com/  BigMc  Large  Huge/  opens  ources  (2017).\n 31. Van\u00a0Zandt, D. Media bias/fact check news: An American fact-checking website). https://  media  biasf  actch  eck. com/  (2020).\n 32. Chen, Z. & Freire, J. Proactive discovery of fake news domains from real-time social media feeds. Companion Proc. Web Conf.  \n2020, 584\u2013592 (2020).\n 33. Scrapy. Scrapy: A fast high-level web crawling & scraping framework. https://  github.  com/  scrapy/  scrapy (2020).\n 34. codelucas. Newspaper3k: Article scraping & curation. https://  github. com/  codel  ucas/  newsp  aper (2020).\n 35. Yin, W ., Hay, J. & Roth, D. Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach. In Proceedings \nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural \nLanguage Processing (EMNLP-IJCNLP), 3914\u20133923 (2019).\n 36. Y e, Z. et\u00a0al. Zero-shot text classification via reinforced self-training. In Proceedings of the 58th Annual Meeting of the Association \nfor Computational Linguistics , 3014\u20133024 (2020).\n 37. Weiss, K., Khoshgoftaar, T. M. & Wang, D. A survey of transfer learning. J. Big data  3, 1\u201340 (2016).\n 38. Patadia, D., Kejriwal, S., Mehta, P . & Joshi, A. R. Zero-shot approach for news and scholarly article classification. In Proceedings \nof the 2021 International Conference on Advances in Computing, Communication, and Control (ICAC3), 1\u20135 (IEEE, 2021).\n 39. Koutsomitropoulos, D. Validating ontology-based annotations of biomedical resources using zero-shot learning. In Proceedings \nof the 12th International Conference on Computational Systems-Biology and Bioinformatics, 37\u201343 (2021).\n 40. Wolf, T. et\u00a0al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods \nin natural language processing: system demonstrations , 38\u201345 (2020).\n 41. Lewis, M. et\u00a0al.  Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehen-\nsion. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , 7871\u20137880 (2020).\n 42. Williams, A., Nangia, N. & Bowman, S. A broad-coverage challenge corpus for sentence understanding through inference. In Pro -\nceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language \nTechnologies, Volume 1 (Long Papers), 1112\u20131122 (2018).\n 43. Lal, T. N., Chapelle, O., Weston, J. & Elisseeff, A. Embedded methods. in Feature Extraction  137\u2013165 (Springer, 2006).\n 44. Sandri, M. & Zuccolotto, P . Variable selection using random forests. In Data Analysis, Classification and the Forward Search, \n263\u2013270 (Springer, 2006).\n 45. Cawley, G., Talbot, N. & Girolami, M. Sparse multinomial logistic regression via bayesian l1 regularisation. Adv. Neural Inf. Process. \nSyst. 19 (2006).\n 46. Ma, S. & Huang, J. Penalized feature selection and classification in bioinformatics. Brief. Bioinform.  9, 392\u2013403 (2008).\n 47. Shi, J., Yin, W ., Osher, S. & Sajda, P . A fast hybrid algorithm for large-scale l1-regularized logistic regression. J. Mach. Learn. Res.  \n11, 713\u2013741 (2010).\n 48. Kuhn, M. & Johnson, K. Applied Predictive Modelling  (Springer, 2018).\n 49. Sokolova, M., Japkowicz, N. & Szpakowicz, S. Beyond accuracy, f-score and roc: a family of discriminant measures for performance \nevaluation. In Proceedings of the Australasian Joint Conference on Artificial Intelligence, 1015\u20131021 (Springer, 2006).\n 50. Bishop, C. M. & Nasrabadi, N. M. Pattern Recognition and Machine Learning  Vol. 4 (Springer, 2006).\n 51. Anastasiou, A. & Papanastasiou, A. Generalized multiple change-point detection in the structure of multivariate, possibly high-\ndimensional, data sequences. Stat. Comput.  33, 94 (2023).\n 52. Anastasiou, A. & Fryzlewicz, P . Detecting multiple generalized change-points by isolating single ones. Metrika  85, 141\u2013174 (2022).\n 53. Ghanem, B., Rosso, P . & Rangel, F. An emotional analysis of false information in social media and news articles. ACM Trans. \nInternet Technol.  20, 1\u201318 (2020).\n 54. Shu, K., Sliva, A., Wang, S., Tang, J. & Liu, H. Fake news detection on social media: A data mining perspective. ACM SIGKDD \nExplor. Newsl. 19, 22\u201336 (2017).\n 55. Reddy, H., Raj, N., Gala, M. & Basava, A. Text-mining-based fake news detection using ensemble methods. Int. J. Autom. Comput.  \n17, 210\u2013221 (2020).\n 56. Loughran, T. & McDonald, B. The use of word lists in textual analysis. J. Behav. Financ.  16, 1\u201311 (2015).\n 57. Pennebaker, J. W ., Boyd, R., Jordan, K. & Blackburn, K. The development and psychometric properties of liwc2015 (LIWC. Net, \nAustin, TX, 2015).\n 58. Thorp, H. H. Chatgpt is fun, but not an author. Science  379, 313\u2013313 (2023).\n12\nVol:.(1234567890) Scientific Reports  |         (2023) 13:6086  | https://doi.org/10.1038/s41598-023-32952-3\nwww.nature.com/scientificreports/Acknowledgements\nThis work is partially supported by the EU Commission through RAIS Marie Sk\u0142odowska-Curie ITN, under \ngrant agreement No 813162. We thank the reviewers for their time spent on reviewing our manuscript, careful \nreading and insightful comments and suggestions that lead to improve the quality of this manuscript.\nAuthor contributions\nN.P ., C.C., A.A., G.P ., M.D. designed the study and wrote the manuscript. N.P . and C.C. developed the framework. \nN.P ., C.C. and A.A. conducted the experiments. All authors reviewed the manuscript.\nCompeting interests  \nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at https:// doi. org/ \n10. 1038/ s41598- 023- 32952-3.\nCorrespondence and requests for materials should be addressed to N.P .\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article\u2019s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat  iveco  mmons. org/ licen  ses/ by/4. 0/.\n\u00a9 The Author(s) 2023", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "A multiple change-point detection framework on linguistic characteristics of real versus fake news articles", "author": ["N Petrou", "C Christodoulou", "A Anastasiou", "G Pallis"], "pub_year": "2023", "venue": "Scientific Reports", "abstract": "Extracting information from textual data of news articles has been proven to be significant in  developing efficient fake news detection systems. Pointedly, to fight disinformation,"}, "filled": false, "gsrank": 495, "pub_url": "https://www.nature.com/articles/s41598-023-32952-3", "author_id": ["DAzZnaUAAAAJ", "", "s-X_UIQAAAAJ", "kNkLOHcAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:VSuySm_7HWIJ:scholar.google.com/&output=cite&scirp=494&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D490%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=VSuySm_7HWIJ&ei=YLWsaK7sAqzWieoPic2ZoAU&json=", "num_citations": 10, "citedby_url": "/scholar?cites=7070083445454678869&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:VSuySm_7HWIJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://www.nature.com/articles/s41598-023-32952-3.pdf"}}, {"title": "EU climate change news index: Forecasting EU ETS prices with online news", "year": "2023", "pdf_data": "EU Climate Change News Index: Forecasting\nEU ETS prices with online news\n\u00b4Aron D\u00b4 enes Hartvig, \u00b4Aron Pap, P\u00b4 eter P\u00b4 alos\nTackling Climate Change with Machine Learning workshop at ICLR 2024\n1 Introduction\nCarbon pricing is an economically efficient instrument in the policy toolkit to signal emis-\nsions\u2019 actual cost. Emissions trading systems are market-based mechanisms where a cap is\nset on certain sectors\u2019 emissions and the entities covered are allowed to trade emissions al-\nlowances. The world\u2019s first international carbon market, the EU Emissions Trading System\n(ETS), was implemented in 2005. ETS prices stayed low for a long time but finally started\nto rise in 2017. The ETS allowance prices skyrocketed in 2021, reaching almost \u20ac100 in\nFebruary 2022. Nevertheless, prices dropped below \u20ac60 in early-March after the disruptions\ncaused by the Russian invasion of Ukraine.\nOur contribution to the literature on carbon pricing is twofold. First, we propose a new EU\nClimate Change News Index (ECCNI) that tracks the ongoing discussion in the EU about\nclimate change using global media sources. Although some articles have already incorpo-\nrated news information into the forecasting of ETS prices (Ye and Xue, 2021; Zhang and\nXia, 2022), no meaningful feature has been created that captures the media\u2019s climate policy-\nrelated discussions. We apply term frequency\u2013inverse document frequency (TF-IDF) feature\nextraction to the GDELT1news database to measure the frequency of climate change-related\nkeywords in the news associated with the EU. The TF-IDF features help to quantify the\nintensity of the discussion about climate change in the EU and to incorporate policy context\nin the analysis of carbon prices.\nSecond, we apply the news index to predict the next day\u2019s ETS allowance price returns. We\ntest the forecast accuracy of the ECCNI against a set of control variables taken from the\nliterature. Our results suggest that the occurrence of climate change-related keywords in\nthe most reliable news sites improves the forecasts of the ETS prices.\nSeveral quantitative methods have already been developed in academic literature to fore-\ncast carbon prices. Zhao et al. Zhao et al. (2018) categorizes these papers into two groups:\n(1) forecasting based on time-series data using carbon price only; (2) extending with eco-\nnomic and energy data too. The carbon price-only methods mostly include ARIMA models;\nhowever, they can only capture linear relationships (Zhu and Chevallier, 2017). Therefore,\nmore advanced frameworks have been applied to carbon prices, like different varieties of\ngeneralized autoregressive conditional heteroscedasticity (GARCH) models (Arouri et al.,\n2012; Benschopa and L\u00b4 opez Cabreraa, 2014; Byun and Cho, 2013) and vector autoregressive\n(VAR) models (Arouri et al., 2012).\nNevertheless, carbon price-only methods do not incorporate all available information in the\nmarket. Various articles that aim to forecast carbon prices use economic and energy-related\nvariables proxying the demand for CO 2allowances (Arouri et al., 2012; Gu \u00f0brandsd\u00b4 ottir\nand Haraldsson, 2011; Zhao et al., 2018). Recently, alternative predictors, e.g., news data\nthrough natural language processing (NLP), have also been used to forecast ETS prices (Ye\nand Xue, 2021; Zhang and Xia, 2022). However, they only included the headlines and titles\nof online news from limited sources and consequently could only examine carbon prices with\na weekly frequency. To shed more light on the impact of news on daily ETS prices, we create\nfeatures by applying the TF-IDF method to the GDELT news dataset. TF-IDF has been\n1The Global Database of Events, Language, and Tone (GDELT) Project is a real-time\nnetwork diagram and database of global human society for open research that monitors the\nworld\u2019s broadcast, print, and web news in over 100 languages. For more information, see:\nhttps://www.gdeltproject.org/, accessed: 2022-10-21.\n1\nwidely used to improve the forecast accuracy of stock prices (Coyne et al., 2017; Lubis et al.,\n2021; Mittermayer, 2004; Nikfarjam et al., 2010).\n2 Data\nThe main contribution of this study is the conversion of online news articles to meaningful\nvariables that enhances our understanding of ETS. Therefore, we use GDELT, a free open\nplatform covering global news from numerous countries in over 100 languages with daily\nfrequency. The database includes, along with others, the actors, locations, organizations,\nthemes, and sources of the news items (Leetaru and Schrodt, 2013). GDELT has been used\nin various articles that apply NLP to extract alternative information from the news (Alamro\net al., 2019; Galla and Burke, 2018; Guidolin and Pedio, 2021).\nWe take the daily futures closing prices of the European Union Allowance (EUA) ( \u20ac/ton)\nas the dependent variable since that is the underlying carbon price of ETS. Besides news\ndata, we include the most fundamental drivers of ETS prices (Ye and Xue, 2021) in our\nanalysis to serve as control variables.\nThe data was collected from January 2, 2018 until November 30, 2021, with 1011 daily\nobservations in total. The availability of control variables gave the starting date, and the\nend was determined to avoid the possible distorting effect of the Russian-Ukrainian conflict.\nHowever, the latest ECCNI values are available on the EU ETS news tracker dashboard.\n3 Methodology\n3.1 Article collection\nOur ECCNI relies on the GDELT database that gathers a wide range of online news with\ndaily frequency. Thus, to focus our analysis, we restricted the dataset to the articles where\nthe actor is European Union orEUand extracted their URL-s. We chose to filter on the\nactor to focus on issues and policies that are dealt with by the EU. Moreover, carbon prices\nare also affected by global trends; consequently, filtering based on geography would not be\nadequate.\nMoreover, we removed the articles from the database that were coming from unreliable\nsources. For this purpose, we used one of the most cited media bias resources, Media Bias\nFact Check (MBFC) (MBFC, 2022). We removed the articles from the data that appeared\non \u2018questionable\u2019 websites according to the \u2018Factual/Sourcing\u2019 category of MBFC2.\nAfter the filtering, the overall number of news sites was reduced from 9,497 to 719, from\nwhich our web scraper collected 27,777 articles.\n3.2 Feature generation workflow\nWe performed basic string pre-processing steps on the raw texts using the Natural Language\nToolkit (NLTK) package (Bird et al., 2009). This package was also used to lemmatize words\nwith WordNetLemmatizer, a more advanced solution than standard stemming because of\nthe addition of morphological analysis. Since our keyword collection contains several multi-\nword elements, bigrams and trigrams were also formed with the lemmatizer to create the\nTerm Frequency-Inverse Document Frequency (TF-IDF) matrix, which is one of the most\ncommonly used methods for NLP. The TF-IDF method is an adequate tool to incorporate\nalternative information to forecast financial time series (Coyne et al., 2017; Lubis et al., 2021;\nMittermayer, 2004; Nikfarjam et al., 2010). It is generally accepted that the normalized form\nof TF-IDF is more effective than Bag-of-words methods in terms of ignoring common words,\nand it is also able to highlight rare terms.\n2We are grateful to Courtney Pitcher who fetched the data from MBFC and published an\norganized dataset on her blog (Pitcher, 2019).\n2\nThe rows of our calculated matrix represent the individual articles, and its columns are the\nelements of the partially external, partially custom-defined keyword list. We gathered our\nkeywords around five main groups: fossil fuels, renewable energy carriers, energy policy,\nemissions and gas as an independent topic. We used keyword suggestions from Google\nTrends and our intuition to expand the mentioned groups. The complete list of keywords\nis shown in Table A.1. We calculated the score for each keyword so it can also be used for\nfurther detailed analysis. Still, due to the high variance of the occurrences and the strong\ncorrelation between the keyword groups, we created the EU Climate Change News Index as\nthe aggregated TF-IDF score of the groups.\n4 Results\nIn this section, we present the ECCNI, the index constructed from the TF-IDF features that\nare derived using our methodology described in Section 3. First, we assess the evolution\nof the index qualitatively by walking through the most important events related to climate\nchange in the EU since 2020. We then use OLS and ElasticNet models to test the forecasting\nability of the index.\n4.1 EU Climate Change News Index\nSince policy uncertainty is substantial around the ETS system, measuring the intensity of\ndebate around it is crucial. One of the key drivers of the ETS prices is the EU\u2019s ever-\nincreasing emissions reduction targets, which set a cap on the number of ETS allowances.\nNevertheless, various other policy measures also impact carbon prices as sectoral policies,\nlike green energy mandates.\nFigure 1: EU Climate Change News Index between January 2, 2018 and November 30, 2021\n2019 2020 2021Jan\nJan\nJanFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0TF-IDF score\nIPCC 1.5C \n special report\nCOP24 and \n EU Renewable Energy Directive\nEU HeatwaveCOP25 and \n EU climate neutrality objective\nCOVID-19 \n first wave in EU\nEU Green RecoveryCOVID-19 \n second wave in EU\nEU 55% \n reduction target\nRenovation WaveFit for 55\nCOP26\nRaw scores\n30-day moving average\nWe present the evaluation of the ECCNI between January 2, 2018, and November 30, 2021,\nin Figure 1. The index is highly volatile, but several cycles are outlined in the 30-days moving\naverage. In the followings, we concentrate on the events influencing the index starting from\n2020. In January and February 2020, the index reached a relatively high and stable level\ndue to the recent presentation of the EU Green Deal. Then, in March 2020, the index\nstarted to decrease steadily as the COVID-19 pandemic overtook the public discourse in the\nEU. However, the concept of \u2019green recovery\u2019 soon emerged, and climate change keywords\nagain started trending. In October 2020, COVID-19 cases soared again, pushing down the\nindex. The European Council endorsed a binding EU target of a net domestic reduction\nof greenhouse gas emissions by at least 55% by 2030 (compared to 1990 levels), leading to\n3\na local peak at the end of the year. The proceeding period was less volatile; however, in\nJune 2021, the index jumped to a higher level as the European Council endorsed the new\nRenovation Wave strategy, and in July, the \u2019Fit for 55\u2019 package was presented. Finally, the\nclimate change news index peaked in November 2021 as the 26th Conference of the Parties\n(COP26) was held between October 31 and November 13 in Scotland. The most recent\nTF-IDF scores of the EU Climate Change News Index and the keyword groups is available\non the EU ETS news tracker dashboard.\n4.2 Forecasting performance\nWe argue that the EU Climate Change News Index can track the ongoing discussion about\nthe ETS. Since ETS prices are strongly dependent on the policy environment and the mea-\nsures introduced, the index could potentially help to better predict the evolution of carbon\nprices in the EU. Therefore, in the followings, we test the forecasting performance of the\nindex. In our analysis, we compare three models to measure the forecasting performance\nof the ECCNI. The first ( TF-IDF ) model includes the lags of the ETS price returns ( rt)\nand the ECCNI ( zt) as predictors. While the second model, called Control , serves as a\nbenchmark model which considers the lags of the ETS price returns and the fundamental\ndriving factors of carbon prices based on academic literature (lags of gas, electricity, coal,\noil and stock price returns represented by vector xtfor period t). The final, Full model\nincludes all predictors: the lags of the ETS price returns, the control variables\u2019 price returns\nand the ECCNI.\nOnly the lagged values of the predictors are included in the models to produce forecasts\nthat rely entirely on historical information. We run the models with k= 1,2,3,4,5 lags\nfor robustness purposes but only report the results from the best-performing models. Table\nA.2 summarises the out-of-sample 1-day ahead forecast results ( MAE andRMSE ) of the\nTF-IDF ,Control andFull models for carbon price return with different test windows. We\nused the last n\u2208 {50,75,100}days of the sample for the out-of-sample testing to examine\nthe performance of the models on the most recent data.\nBased on the results, the Full model consistently outperforms the others regardless of the\ntest window, the evaluation metric and the estimation method, while the TF-IDF model\nproduces the largest errors. These outcomes are in line with the literature exploring the\neffectiveness of additional textual information in carbon price prediction (Ye and Xue, 2021;\nZhang and Xia, 2022). News information alone cannot outperform the control variables, but\nextending these fundamental driving factors with the ECCNI provides additional predictive\npower to ETS price forecasting. The ECCNI captures policy uncertainty and is able to track\nthe discussion about climate change in the EU.\n5 Conclusions\nIn this paper we first aggregated textual information from online news articles representing\na novel data source for carbon price prediction. We produced TF-IDF features tracking the\nrelative occurrences of climate change-related keywords in online news related to the EU.\nThen, we derived the EU Climate Change News Index as the aggregated TF-IDF score of\nthe keywords. The index accurately reflects the ongoing discussion about climate change in\nthe EU. It outlines the most influential events in the topic like the annual United Nations\nClimate Change Conferences or the endorsement of the EU\u2019s emissions reduction target of\nat least 55% by 2030 below 1990 levels. Finally, we showed that the index brings valuable\nadditional information and predictive power to ETS price forecasting compared to a control\nmodel where the traditional predictors of carbon prices are included.\nThe increasing ambition of the EU climate targets brings significant uncertainty to carbon\nprices. ETS market participants are ever more exposed to the rapidly changing carbon\nprices; hence, news articles about EU climate issues are highly relevant to their market\nexpectations. Therefore, the proposed ECCNI could also help to manage volatility in the\nEU ETS. By integrating the index into forecasting models, companies can predict ETS\nprices more accurately and lower their associated risks.\n4\nReferences\nRawan Alamro, Andrew McCarren, and Amal Al-Rasheed. Predicting Saudi stock market\nindex by incorporating GDELT using multivariate time series modelling. International\nconference on computing , pages 317\u2013328, 2019. doi: 10.1007/978-3-030-36365-9 26.\nMohamed El H\u00b4 edi Arouri, Fredj Jawadi, and Duc Khuong Nguyen. Nonlinearities in carbon\nspot-futures price relationships during Phase II of the EU ETS. Economic Modelling , 29\n(3):884\u2013892, 2012. doi: 10.1016/j.econmod.2011.11.003.\nThijs Benschopa and Brenda L\u00b4 opez Cabreraa. Volatility modelling of CO2 emission al-\nlowance spot prices with regime-switching GARCH models. SFB 649 Discussion Paper\nNo. 2014-050, Humboldt University of Berlin, Collaborative Research Center 649 - Eco-\nnomic Risk, Berlin, 2014.\nSteven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python:\nanalyzing text with the natural language toolkit . O\u2019Reilly Media, Inc., 2009. doi: 10.1007/\ns10579-010-9124-x.\nSuk Joon Byun and Hangjun Cho. Forecasting carbon futures volatility using GARCH\nmodels with energy volatilities. Energy Economics , 40:207\u2013221, 2013. doi: 10.1016/j.\neneco.2013.06.017.\nScott Coyne, Praveen Madiraju, and Joseph Coelho. Forecasting stock prices using so-\ncial media analysis. 2017 IEEE 15th Intl Conf on Dependable, Autonomic and Se-\ncure Computing, 15th Intl Conf on Pervasive Intelligence and Computing, 3rd Intl\nConf on Big Data Intelligence and Computing and Cyber Science and Technology\nCongress (DASC/PiCom/DataCom/CyberSciTech) , pages 1031\u20131038, 2017. doi: 10.\n1109/DASC-PICom-DataCom-CyberSciTec.2017.169.\nDivyanshi Galla and James Burke. Predicting social unrest using GDELT. International\nconference on machine learning and data mining in pattern recognition , pages 103\u2013116,\n2018. doi: 10.1007/978-3-319-96133-0 8.\nHei\u00f0a Nj\u00b4 ola Gu \u00f0brandsd\u00b4 ottir and Haraldur \u00b4Oskar Haraldsson. Predicting the price of EU\nETS carbon credits. Systems Engineering Procedia , 1:481\u2013489, 2011. doi: 10.1016/j.sepro.\n2011.08.070.\nMassimo Guidolin and Manuela Pedio. Media attention vs. sentiment as drivers of con-\nditional volatility predictions: An application to brexit. Finance Research Letters , 42:\n101943, 2021. doi: 10.1016/j.frl.2021.101943.\nKalev Leetaru and Philip A Schrodt. GDELT: Global data on events, location, and tone,\n1979\u20132012. ISA annual convention , 2(4):1\u201349, 2013.\nArif Ridho Lubis, Mahyuddin KM Nasution, O Salim Sitompul, and E Muisa Zamzami.\nThe effect of the TF-IDF algorithm in times series in forecasting word on social media.\nIndonesian Journal of Electrical Engineering and Computer Science , 22(2):976, 2021. doi:\n10.11591/ijeecs.v22.i2.pp976-984.\nMBFC. Media bias fact check, 2022. https://mediabiasfactcheck.com/, Accessed: 2022-10-\n01.\nMarc-andre Mittermayer. Forecasting intraday stock price trends with text mining tech-\nniques. Proceedings of the 37th Annual Hawaii International Conference on System Sci-\nences, 2004. , pages 64\u201373, 2004. doi: 10.1109/HICSS.2004.1265201.\nAzadeh Nikfarjam, Ehsan Emadzadeh, and Saravanan Muthaiyah. Text mining approaches\nfor stock market prediction. 2010 The 2nd international conference on computer and\nautomation engineering (ICCAE) , 4:256\u2013260, 2010. doi: 10.1109/ICCAE.2010.5451705.\nM Hashem Pesaran and Allan Timmermann. Predictability of stock returns: Robustness\nand economic significance. The Journal of Finance , 50(4):1201\u20131228, 1995. doi: 10.1111/\nj.1540-6261.1995.tb04055.x.\n5\nCourtney Pitcher. My pitcher overfloweth, 2019. https://igniparoustempest.github.io/mediabiasfactcheck-\nbias/, Accessed: 2022-10-12.\nJing Ye and Minggao Xue. Influences of sentiment from news articles on EU carbon prices.\nEnergy Economics , 101:105393, 2021. doi: 10.1016/j.eneco.2021.105393.\nFang Zhang and Yan Xia. Carbon price prediction models based on online news information\nanalytics. Finance Research Letters , 46:102809, 2022. doi: 10.1016/j.frl.2022.102809.\nXin Zhao, Meng Han, Lili Ding, and Wanglin Kang. Usefulness of economic and energy\ndata at different frequencies for carbon price forecasting in the EU ETS. Applied Energy ,\n216:132\u2013141, 2018. doi: 10.1016/j.apenergy.2018.02.003.\nBangzhu Zhu and Julien Chevallier. Carbon price forecasting with a hybrid ARIMA and\nleast squares support vector machines methodology. Pricing and forecasting carbon mar-\nkets, pages 87\u2013107, 2017. doi: 10.1007/978-3-319-57618-3 6.\n6\n6 Appendix\nTable A.1: TF-IDF keyword list\nGroup Keywords\nEmissions carbon dioxide, CO 2, green deal, greenhouse gas, ghg\nFossil fuels coal, oil, crude, gasoline, diesel, petrol, fuel\nGas gas\nPolicy climate, sustainability, sustainable, environment, ets\nRenewables renewable, electricity, solar power, solar panel, solar energy, wind power,\nwind turbine, wind energy, nuclear power, nuclear plant, nuclear energy,\nclean energy, green energy\nTable A.2: Forecast performance comparison of different models (10\u22123)\nTest window Measure Model TF-IDF Control Full\n50MAE OLS 20.22891 18.98009 18.91033\nElasticNet 20.50418 20.25187 20.14700\nRMSE OLS 0.76571 0.69012 0.68978\nElasticNet 0.78675 0.74385 0.74026\n75MAE OLS 18.12085 17.27791 17.18600\nElasticNet 18.27116 18.00239 17.88948\nRMSE OLS 0.64362 0.60436 0.60363\nElasticNet 0.65887 0.62647 0.62402\n100MAE OLS 17.39619 16.67049 16.57782\nElasticNet 17.48796 17.34786 17.29831\nRMSE OLS 0.57801 0.54112 0.54108\nElasticNet 0.58773 0.55749 0.55667\nTable A.3: Clark and West test results for the Full and Baseline models\nTest window Model Baseline Full CW\nLags Lags p-value\n50OLS 1 1 0.088\nElasticNet 3 3 0.046\n75OLS 1 1 0.050\nElasticNet 3 3 0.141\n100OLS 1 1 0.055\nElasticNet 3 3 0.398\n7\nTable A.4: Forecast performance comparison of Baseline and Full models with rolling win-\ndow estimation method\nRolling Test Baseline Full CW\nwindow window Model MAE*RMSE*Lags MAE*RMSE*Lags p-value\n5050OLS 22.131 0.8474 1 22.057 0.8390 1 0.599\nElasticNet 21.229 0.8332 4 21.249 0.8099 4 0.024\n75OLS 20.115 0.7437 1 20.159 0.7519 1 0.900\nElasticNet 19.024 0.6960 4 18.984 0.6802 4 0.003\n100OLS 19.966 0.710 1 19.992 0.7228 1 0.970\nElasticNet 18.066 0.6155 4 18.036 0.6036 4 0.006\n50050OLS 19.113 0.6986 1 18.750 0.6916 1 0.108\nElasticNet 20.311 0.7409 3 19.916 0.7404 3 0.058\n75OLS 17.320 0.6049 1 16.991 0.6002 1 0.067\nElasticNet 18.030 0.6254 317.948 0.6308 3 0.547\n100OLS 16.729 0.5403 1 16.429 0.5375 1 0.068\nElasticNet 17.337 0.5591 317.285 0.5647 3 0.826\n* 10\u22123\nA Trading strategy backtesting\nIn this section we investigate whether the predictive ability of our index could be utilized\nin trading strategies. We use a simple approach in which we buy ETS when the forecasted\nreturn is positive, while we invest in cash when it is negative, following (Pesaran and Tim-\nmerman, 1995) (Pesaran and Timmermann, 1995). We use the forecasts of the models\npresented in Table A.2. We compare the results of this active strategy during our out-of-\nsample prediction windows with: (1) cash (where we assume 10 % annual interest rate);\n(2) passively investing into ETS. The results are shown in Table B.5. This initial exercise\ncould be made more realistic with incorporating trading costs (which could be substantial\nsince we are rebalancing the portfolio daily) and by using a more appropriate benchmark in\nterms of risk and return characteristics of the proposed active ETS trading strategy.\nTable B.5: Trading strategy comparison results\nTest window Model Total return (%)Active return\nvs cash (%p)Active return\nvs ETS (%p)\n50OLS 28.24 26.86 5.48\nElasticNet 28.18 26.80 5.42\n75OLS 37.56 35.48 9.20\nElasticNet 37.92 35.84 9.56\n100OLS 51.96 49.19 13.61\nElasticNet 41.32 38.54 2.96\n8", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "EU climate change news index: Forecasting EU ETS prices with online news", "author": ["\u00c1D Hartvig", "\u00c1 Pap", "P P\u00e1los"], "pub_year": "2023", "venue": "Finance Research \u2026", "abstract": "Our contribution to the literature on carbon pricing is twofold. First, we propose a new EU  Climate Change News Index (ECCNI) that tracks the ongoing discussion in the EU about"}, "filled": false, "gsrank": 496, "pub_url": "https://s3.us-east-1.amazonaws.com/climate-change-ai/papers/iclr2024/31/paper.pdf", "author_id": ["mmfnzigAAAAJ", "6cdQgE8AAAAJ", ""], "url_scholarbib": "/scholar?hl=en&q=info:7nERI1paOhgJ:scholar.google.com/&output=cite&scirp=495&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D490%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=7nERI1paOhgJ&ei=YLWsaK7sAqzWieoPic2ZoAU&json=", "num_citations": 24, "citedby_url": "/scholar?cites=1745807148741390830&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:7nERI1paOhgJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://s3.us-east-1.amazonaws.com/climate-change-ai/papers/iclr2024/31/paper.pdf"}}, {"title": "Meta-path-based fake news detection leveraging multi-level social context information", "year": "2022", "pdf_data": "Meta-Path-based Fake News Detection Leveraging Multi-level\nSocial Context Information\nJian Cui\ncj19960819@kaist.ac.kr\nKorea Advanced Institute of Science and Technology\nDaejeon, South KoreaKwanwoo Kim\nkw2128@kaist.ac.kr\nKorea Advanced Institute of Science and Technology\nDaejeon, South Korea\nSeung Ho Na\nharry.na@kaist.ac.kr\nKorea Advanced Institute of Science and Technology\nDaejeon, South KoreaSeungwon Shin\nclaude@kaist.ac.kr\nKorea Advanced Institute of Science and Technology\nDaejeon, South Korea\nABSTRACT\nFake news, false or misleading information presented as news, has\na significant impact on many aspects of society, such as in politics\nor healthcare domains. Due to the deceiving nature of fake news,\napplying Natural Language Processing (NLP) techniques to the\nnews content alone is insufficient. Therefore, more information is\nrequired to improve fake news detection, such as the multi-level\nsocial context (news publishers and engaged users in social me-\ndia) information and the temporal information of user engagement.\nThe proper usage of this information, however, introduces three\nchronic difficulties: 1) multi-level social context information is hard\nto be used without information loss, 2) temporal information is\nhard to be used along with multi-level social context information,\n3) news representation with multi-level social context and temporal\ninformation is hard to be learned in an end-to-end manner. To over-\ncome all three difficulties, we propose a novel fake news detection\nframework, Hetero-SCAN . We use Meta-Path to extract meaningful\nmulti-level social context information without loss. Meta-Path, a\ncomposite relation connecting two node types, is proposed to cap-\nture the semantics in the heterogeneous graph. We then propose\nMeta-Path instance encoding and aggregation methods to capture\nthe temporal information of user engagement and learn news rep-\nresentation end-to-end. According to our experiment, Hetero-SCAN\nyields significant performance improvement over state-of-the-art\nfake news detection methods.\nCCS CONCEPTS\n\u2022Computing methodologies \u2192Artificial intelligence ;\u2022In-\nformation systems \u2192Social networks .\nKEYWORDS\nFake News Detection; Graph Representation Learning\n1 INTRODUCTION\nThe wide dissemination of fake news has become a major social\nproblem in the world. The most recent and infamous distribution\nof fake news was in the 2020 United States presidential election\nfraud [ 9] and COVID-19 rumors [ 1]. Both industry and government\nAnonymous Submission to The Web Conference, 2022\n2021. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnnare making efforts to prevent the spread of fake news [ 10]. Nev-\nertheless, fake news verification still relies on human experts and\ntheir manual efforts in analyzing the news contents with additional\nevidence. Therefore, there should be an automatic and efficient way\nto identify the veracity of the news.\nThe most typical way to detect fake news is applying Natural\nLanguage Processing (NLP) techniques on the news content [ 15,\n18]. Considering that even people struggle in identifying the news\nauthenticity by the news content alone, these NLP solutions are\nineffective. Thus, more information is required to improve fake\nnews detection.\nThe first important information is the users in social media. So-\ncial media is one of the most influential mediums to propagate\ninformation, and it has become a common practice for people to\nshare their thoughts in social media. Even though regular users\nuse social media as a communication tool, some users, known as\ninstigators, intentionally spread fake news. Instigators usually have\na highly partisan-biased personal description and a lot of followers\nand followings, which is significantly different from the profiles\nof regular users (See in Figure 1). Therefore, analyzing the users\nengaged in the news can provide additional evidence for identi-\nfying news authenticity. The publisher information can also play\nan important role because certain partisan-biased publishers are\nmore likely to publish fake news [ 3,5,6]. Information on users and\npublishers can be viewed as multi-level social context information,\nand they provide additional clues for fake news detection.\nIn addition to multi-level social context information, temporal\ninformation of user engagement (temporal information for short) is\nanother instrumental information in fake news detection. Fake and\nreal news show different propagation properties in social media:\nFake news is periodically mentioned by people and usually lasts\nlonger, but real news receives attention only at the beginning of\nthe news publication [ 27]. In this context, the temporal information\nshould be included in the news representation along with multi-\nlevel social context information.\nUsing multi-level social context and temporal information, how-\never, leads to three chronic difficulties. Firstly, due to the hetero-\ngeneity of multi-level social context information, it is hard to use\nthis information without loss. Secondly, temporal information is\nhard to be used along with multi-level social context information.\nThe graph is a typical way to present social context and its con-\nnectivity to the news, but the graph itself has complications in\n1arXiv:2109.08022v2  [cs.SI]  17 Nov 2021\nAnonymous Submission to The Web Conference, 2022 Jian Cui, Kwanwoo Kim, Seung Ho Na, and Seungwon Shin\nUserUser B\u201cFootball lover.\u201dFollowing: 159 Follower: 52User A\u201cTrump Supporter, Hillary belongs to the jail\u201dFollowing: 69.9K Follower: 70.9K\nFactual Reporting: VERY LOWCredibility Rating: LOW \u201cpublish\u201dNews title: New Jersey police chief says black people have no value and should all be executedNews Content:  The former police chief of a small New Jersey town said African-Americanshad \u201cno value\u201d  \u2026Publisher\u201ctweet\u201dNews\nA\nB\nFigure 1: Example of fake news distribution and dissemina-\ntion. Publishers publish the news, and users tweet the news.\nSome publishers are regarded as low credibility sources ac-\ncording to the famous fact-checking website, MBFC. User A\nis an example of an instigator in Twitter, and User B is an\nexample of a regular user.\npresenting temporal information. The last difficulty is to learn the\nnews representation end-to-end. Multi-level social context and tem-\nporal information are two different kinds of information, which\nincreases the difficulty of adopting end-to-end learning while uti-\nlizing both information. To promise a high-performing fake news\ndetection, it is necessary to adopt end-to-end learning. It enables us\nto eliminate the effect from the sub-tasks and optimize the training\nparameters with a single news detection objective.\nTo the best of our knowledge, existing approaches fail to ad-\ndress all three difficulties, so we propose a novel fake news detec-\ntion framework, Hetero-SCAN , to tackle above-listed difficulties. In\nHetero-SCAN , to preserve multi-level social context information,\nwe use the Meta-Path . Meta-Path is a composite relation connecting\ntwo node types, aiming to capture the semantics in the hetero-\ngeneous graph. We define two Meta-Paths containing different\naspects of news (users and publishers) to extract multi-level social\ncontext information without information loss. Moreover, Meta-Path\ninstance encoding and aggregation methods are proposed to cap-\nture the temporal information of user engagement and learn the\nnews representation end-to-end.\nTo show that our proposed method outperforms existing solu-\ntions, we test Hetero-SCAN with two real-world datasets [ 16,32],\nand the results show that Hetero-SCAN achieves significant im-\nprovement over previous approaches in terms of F1 score, accuracy,\nand AUC score. Our code with data is released on the GitHub1for\nreproducibility. Our major contributions are:\n(1)We pose three chronic difficulties in social context aware fake\nnews detection and address them by proposing a novel fake\nnews detection framework, Hetero-SCAN .\n(2)We conduct diverse experiments on the two real-world fake\nnews datasets, covering the broad definition of fake news\n(Section 3), and demonstrate that Hetero-SCAN shows better\nperformance than existing solutions.\n(3)We provide new insights into the differences in the behavior\nof engaged users between intentional and unintentional fake\nnews.\n1https://github.com/(anonymous)/hetero_scanTable 1: Comparison of Hetero-SCAN with exiting graph-\nbased fake news detection methods.\nMulti-level\nSocial ContextInformation\nPreservingTemporal\nInformationEnd-to\n-end\nCSI [39] \u2717 \u2713 \u2713 \u2713\nSAFER [13] \u2717 \u2717 \u2717 \u2713\nFANG [32] \u2713 \u2713 \u2713 \u2717\nAA-HGNN [37] \u2717 \u2713 \u2717 \u2713\nHetero-SCAN \u2713 \u2713 \u2713 \u2713\n2 RELATED WORK\n2.1 Fake News Detection\nFake news detection methods can be categorized into two types:\ncontent-based and graph-based approaches.\nThe content-based approach models the content of the news,\nsuch as headline or body text, to detect news authenticity. Some\nresearch on content-based approaches utilizes linguistic features\nsuch as stylometry, psycholinguistic properties, and rhetorical rela-\ntions [ 12,34,35,38]. Researchers also use Multi-modal approaches,\nthe combination of visual and linguistic features to verify the news\nauthenticity [20, 24, 36, 46, 48].\nThe graph-based approach, also known as the social context\naware approach, adds auxiliary information of the user or publisher\nto model the news. CSI [ 39] is a framework that aims to capture the\ninformation of users and their temporal engagements. CSI, how-\never, does not consider publishers, and the connection between\nusers and news was also ignored. Bi-GCN [ 11] and SAFER [ 13]\nuse Graph Convolution Network (GCN) [ 25] to obtain the news\nrepresentation with user information. However, they suffer from a\nsevere information loss since they present news and user informa-\ntion in a homogeneous graph. In other words, they fail to taking\nthe node and relation types into account. Most recently, FANG [ 32]\nis proposed to preserve information by dividing the fake news de-\ntection task into several sub-tasks, such as textual encoding and\nstance detection. Nonetheless, dividing into sub-tasks causes the\nerror propagation problem: If the sub-tasks have errors, the errors\ncan propagate up to the final news representation and thereby dete-\nriorate the detection performance. AA-HGNN [ 37] uses adversarial\nactive learning and extends Graph Attention Network (GAT) [ 45]\ninto the heterogeneous graph to learn the news representation with\nlimited training data. Information of users and their temporal en-\ngagement information, however, are not considered in AA-HGNN.\nTable 1 compares Hetero-SCAN and existing fake news detection\nmethods.\n2.2 Graph Neural Network\nGraph Neural Network, the extension of the deep learning method\ninto graphs, shows its effectiveness in graph-represented data. The\nfirst method proposed is Graph Convolutional Network (GCN) [ 25]\nwhich aggregates the features from the adjacent nodes in the graph.\nTo further improve it, some methods adopt the attention mechanism\nand random work with restart sampling strategy, namely Graph\nAttention Network (GAT) [45] and GraphSAGE [22].\n2\nMeta-Path-based Fake News Detection Leveraging Multi-level Social Context Information Anonymous Submission to The Web Conference, 2022\nAs these methods are designed for homogeneous graphs, they\nare not general enough to apply to the heterogeneous graph, so new\napproaches tailor to heterogeneous graphs are then proposed. To\nmodel the multi-relations in the graph, the Relation aware GCN (R-\nGCN) [ 40] is proposed first. HetGNN [ 51] uses a sampling strategy\nbased on random walk with restart and Bi-LSTM to aggregate the\nnode features in the heterogeneous graph. Later, the methods based\non Meta-Path and attention mechanism, such as HAN [ 23] and\nMAGNN [19], are proposed.\n3 PRELIMINARIES\nDefinition 3.1 ( Broad Definition of Fake News ).Fake news is\nfalse news.\nDefinition 3.2 ( Narrow Definition of Fake News ).Fake news\nis intentionally false news published by a news outlet.\nContrary to the amount of research done, the term fake news\nhas only just been defined by the recent work of Zhou, Xinyi and\nReza Zafarani [ 52]. They define fake news in two scopes, broad and\nnarrow. The broad definition emphasizes the authenticity of the\ninformation, and the narrow one emphasizes the intentions of the\nauthor. Most research on fake news detection has employed a broad\ndefinition of fake news. We experiment on the two dataset (with\nand without intention) following broad definition and analyze how\nintention affect the performance of the detection (in Section 5.3).\nDefinition 3.3 ( Heterogeneous Graph ).A heterogeneous graph\nis defined as a graph G=(V,E)associated with a node type\nmapping function \ud835\udf19:V\u2192A and an edge type mapping function\n\ud835\udf13:E\u2192R .AandRdenotes the predefined sets of node types and\nedge types, respectively, with |A|+|R| >2.\nDefinition 3.4 ( Meta-Path ).A Meta-Path \ud835\udc43is defined as a path\nin the form of \ud835\udc341\ud835\udc451\u2212\u2212\u2192\ud835\udc342\ud835\udc452\u2212\u2212\u2192...\ud835\udc45\ud835\udc59\u2212\u2212\u2192\ud835\udc34\ud835\udc59(abbreviated as \ud835\udc341\ud835\udc342...\ud835\udc34\ud835\udc59),\nwhich describes a composite relation \ud835\udc45=\ud835\udc451\u25e6\ud835\udc452\u25e6...\u25e6\ud835\udc45\ud835\udc5bbetween\nnode types\ud835\udc34\ud835\udc59and\ud835\udc34\ud835\udc59+1, where\u25e6denotes the composition operator\non relations.\nDefinition 3.5 ( Meta-Path Instance ).Given a Meta-Path \ud835\udc43of a\nheterogeneous graph, a Meta-Path instance \ud835\udc5dof\ud835\udc43is defined as a\nnode sequence in the graph following the schema defined by \ud835\udc43.\n4 METHODOLOGY\n4.1 Graph Construction & Feature Engineering\nTo integrate multi-level social context information, we build a het-\nerogeneous graph of news (Figure 2). The graph consists of three\ntypes of nodes (publisher, news, and users) and four types of edges\n(citation, publication, tweet, and following). Formally, the hetero-\ngeneous graph of news is noted as G(V,E), and the set of three\nnode types are symbolized as A={\ud835\udc34\ud835\udc5d,\ud835\udc34\ud835\udc5b,\ud835\udc34\ud835\udc62}.\nBefore utilizing this heterogeneous graph, it is necessary to con-\nstruct initial node features for three types of nodes in the graph. For\nnews nodes, Doc2Vec [ 28] is applied to the news article to construct\ntheir initial features. The user and publisher nodes, however, need\nadditional information to construct their respective initial features.\nUsers\u2019 profiles are used for user nodes since the importance of the\nuser profiles for detecting news authenticity has been proved by\nNode2VecDoc2VecNode2VecDoc2VecDoc2Vec\nSource HomepageSource GraphNews ContentUser ProfileUser Graph\nPublisher\nUserCitationPublishTweetFriendshipNews\nFigure 2: Heterogeneous Graph of News and Node Feature\nEngineering.\nShu, Kai et al. [ 43]. The distinct feature of each publisher is acquired\nfrom about-us pages on their official websites; If there is no about-\nus page on the publisher\u2019s official website, we use Wikipedia\u2019s\ndescription instead. Doc2Vec is applied again to leverage these text\ncontents. To also include the structural role they play in their respec-\ntive networks, we apply Node2Vec [ 21] to capture user connections\nand citations among publishers as features. By concatenating the\ntwo vectors obtained from Doc2Vec and Node2Vec, we construct\nthe initial features of user and publisher nodes. Figure 2 shows the\noverall node feature construction process.\n4.2 Meta-Path Instance Extraction\nAfter constructing initial node features, we then need to learn\nthe news representation containing multi-level social context and\ntemporal information. Multi-level social context information should\nbe used without loss, which is the first difficulty in social context\naware fake news detection. To address this difficulty, we use the\nconcept, Meta-Path (defined in Section 3). Meta-Paths can be used\nto extract meaningful social context with respect to publishers and\nusers. We define two Meta-Paths that reflect the method used for\nactual news verification. When people verify the news authenticity,\nthey need to cross-check both publisher and the news published by\nthis publisher. The same goes for users: User information, as well\nas the news tweet by the user, needs to be reviewed. From these\ntwo intuitions, a set of Meta-Path Pthat we find useful is defined\nas below:\nP\u2208{P\ud835\udc48,P\ud835\udc46} (1)\nwhereP\ud835\udc48:\ud835\udc41\ud835\udc52\ud835\udc64\ud835\udc60\u2192\ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f\u2192\ud835\udc41\ud835\udc52\ud835\udc64\ud835\udc60 andP\ud835\udc46:\ud835\udc41\ud835\udc52\ud835\udc64\ud835\udc60\u2192\ud835\udc43\ud835\udc62\ud835\udc4f\ud835\udc59\ud835\udc56\ud835\udc60\u210e\ud835\udc52\ud835\udc5f\n\u2192\ud835\udc41\ud835\udc52\ud835\udc64\ud835\udc60 .\nAfter defining a set of Meta-Path, we extract Meta-Path instances\n\ud835\udc5dfollowing each Meta-Path, P\ud835\udc46orP\ud835\udc48, for each target news node.\nTo efficiently extract Meta-Path instances, we first divide the whole\ngraph into two sub-graphs, which only contain the nodes types\nspecified in the Meta-Path, P\ud835\udc46orP\ud835\udc48. Then, in each sub-graph, the\nMeta-Path instances following each Meta-Path are extracted. The\ncorresponding collection of features are fed into Hetero-SCAN to\nget the final representation of the target news node. The sets of\ninstances following two Meta-Path P\ud835\udc46andP\ud835\udc48are denoted as P\ud835\udc46\nandP\ud835\udc48respectively. For instance, if we want to extract the Meta-\nPath instances of the target news node \ud835\udc65\ud835\udc41\n2in Figure 3, we first\n3\nAnonymous Submission to The Web Conference, 2022 Jian Cui, Kwanwoo Kim, Seung Ho Na, and Seungwon Shin\n\ud835\udc99\ud835\udfcf\ud835\udc77\ud835\udc99\ud835\udfd0\ud835\udc77\ud835\udc99\ud835\udfd1\ud835\udc77\ud835\udc99\ud835\udfcf\ud835\udc75\ud835\udc99\ud835\udfd0\ud835\udc75\ud835\udc99\ud835\udfd1\ud835\udc75\ud835\udc99\ud835\udfd2\ud835\udc75\ud835\udc99\ud835\udfcf\ud835\udc7c\ud835\udc99\ud835\udfd0\ud835\udc7c\ud835\udc99\ud835\udfd1\ud835\udc7c\ud835\udc99\ud835\udfd2\ud835\udc7c\ud835\udc99\ud835\udfd3\ud835\udc7c\n\ud835\udc99\ud835\udfcf\ud835\udc77\ud835\udc99\ud835\udfcf\ud835\udc75\ud835\udc99\ud835\udfd0\ud835\udc75\ud835\udc99\ud835\udfd1\ud835\udc75\n\ud835\udc43'\n\ud835\udc99\ud835\udfd0\ud835\udc75\ud835\udc99\ud835\udfcf\ud835\udc77\ud835\udc99\ud835\udfcf\ud835\udc75\ud835\udc5d(\ud835\udc99\ud835\udfd0\ud835\udc75\ud835\udc99\ud835\udfcf\ud835\udc77\ud835\udc99\ud835\udfd1\ud835\udc75\ud835\udc5d)\ud835\udc99\ud835\udfcf\ud835\udc75\ud835\udc99\ud835\udfd0\ud835\udc75\ud835\udc99\ud835\udfd2\ud835\udc75\ud835\udc99\ud835\udfd0\ud835\udc7c\ud835\udc99\ud835\udfd2\ud835\udc7c\n\ud835\udc43*\n\ud835\udc99\ud835\udfd0\ud835\udc75\ud835\udc99\ud835\udfd0\ud835\udc7c\ud835\udc99\ud835\udfcf\ud835\udc75\ud835\udc99\ud835\udfd0\ud835\udc75\ud835\udc99\ud835\udfd2\ud835\udc7c\ud835\udc99\ud835\udfd2\ud835\udc75\ud835\udc5d+\ud835\udc5d,\nFigure 3: Extracting Meta-Path instances of the target news\nnode\ud835\udc65\ud835\udc41\n2.\ndivide the whole graph into two sub-graphs. One is composed of\nnews and publisher nodes, and the other is made of news and user\nnodes. Then, the Meta-Path instances follow Meta-Path P\ud835\udc46orP\ud835\udc48\nare selected from each sub-graph, and the corresponding features\nof nodes along these Meta-Path instances will be prepared for our\nmodel. In particular, the Meta-Path instance \ud835\udc5d1is made of features\nof nodes following Meta-Path P\ud835\udc46:\ud835\udc41\ud835\udc52\ud835\udc64\ud835\udc60\u2192\ud835\udc43\ud835\udc62\ud835\udc4f\ud835\udc59\ud835\udc56\ud835\udc60\u210e\ud835\udc52\ud835\udc5f\u2192\ud835\udc41\ud835\udc52\ud835\udc64\ud835\udc60 ,\nwhich is\ud835\udc65\ud835\udc41\n1,\ud835\udc65\ud835\udc43\n1and\ud835\udc65\ud835\udc41\n2in the graph. In the same manner, \ud835\udc5d2,\ud835\udc5d3\nand\ud835\udc5d4are extracted. For the target node \ud835\udc63, we use P\ud835\udc46andP\ud835\udc48to\ndenote the set of Meta-Path instances follow each Meta-Path. In\nthis case, P\ud835\udc46={\ud835\udc5d1,\ud835\udc5d2}andP\ud835\udc48={\ud835\udc5d3,\ud835\udc5d4}are set of Meta-Path\ninstances of target node \ud835\udc65\ud835\udc41\n2.\nThere are usually a large number of users engaged per news in\nthe real world. To cope with this situation, we extract Meta-Path\ninstances from our heterogeneous graph of news with random\nsampling. Specifically, a certain number of Meta-Path instances are\nrandomly sampled for each news node according to a pre-defined\nMeta-Path. At last, in order to capture the temporal information, the\nmodel should be aware of the chronological information of Meta-\nPath instances. Thus, the Meta-Path instances from the Meta-Path\nP\ud835\udc48are sorted chronologically before being fed into the proposed\nmodel. In the following sections, we assume that the Meta-Path\ninstances fromP\ud835\udc48are sorted in chronological order.\n4.3 Model Architecture\nHetero-SCAN takes in vectors from the previous step as input and\nprocesses them through four steps as shown in Figure 4 to tackle\nthe yet addressed chronic difficulties.\n4.3.1 Node Feature Transformation .The initial node features\nhave different dimensions since different sources and techniques\nare used in the feature engineering process (Section 4.1). To make\nthem lie in the same latent space, we apply the type-specific linear\ntransform on the features of each type of node. Type-specific trans-\nformation refers to the linear projection of a vector into anotherdimension for each type of node in the graph. The transformed\nfeature for a node \ud835\udc63\u2208V\ud835\udc34of type\ud835\udc34\u2208A is:\nh\ud835\udc34\n\ud835\udc63=W\ud835\udc34\u00b7x\ud835\udc34\n\ud835\udc63 (2)\nwhere x\ud835\udc63\u2208R\ud835\udc51\ud835\udc34is the initial feature of node \ud835\udc63, and W\ud835\udc34\u2208R\ud835\udc51\u2032\u00d7\ud835\udc51\ud835\udc34\nis the learnable type-specific weight matrix for node type \ud835\udc34.\n4.3.2 Meta-Path Instance Encoding .The first step transformed\nall the features of the node into the same dimension. We then need\nto efficiently summarize the Meta-Path instances for the remaining\naggregation steps, which is important in capturing temporal infor-\nmation and learning the representation end-to-end. To efficiently\nencode node features, we adopted the method that shows excellent\nperformance in knowledge graph triple embedding [17, 44, 49].\nThe major advantage of using knowledge graph triple embedding\nis the structural similarity between knowledge graph triples and\nour Meta-Paths. In the knowledge graph, the knowledge graph\ntriple usually refers to the subject, predicate, and object (\ud835\udc60,\ud835\udc5d,\ud835\udc5c).\nThe Meta-Path we defined is similar to the knowledge graph triple\nin a sense that Meta-Path is the same format along with one more\nentity and relation. Formally,\nKnowledge graph triple: e\ud835\udc60e\ud835\udc5d\u2212\u2212\u2192e\ud835\udc5c\nMeta-Path: h\ud835\udc62\ud835\udc5f\u2212 \u2192h\ud835\udc64\ud835\udc5f\u22121\n\u2212\u2212\u2212\u2192 h\ud835\udc63(3)\nwhere\ud835\udc63is target node, \ud835\udc62and\ud835\udc64refer to the nodes along the Meta-\nPath. Considering the Meta-Path we defined in the Section 4.2,\n\ud835\udc63\u2208\ud835\udc34\ud835\udc5b,\ud835\udc62\u2208\ud835\udc34\ud835\udc5b, and\ud835\udc64\u2208{\ud835\udc34\ud835\udc5d,\ud835\udc34\ud835\udc62}. The\ud835\udc5fand\ud835\udc5f\u22121is the relation\nbetween\ud835\udc62,\ud835\udc64and\ud835\udc64,\ud835\udc63respectively. his the transformed embedding\nof the node as we stated in Section 4.3.1, and eis the embedding of\nthe knowledge graph triple.\nSeveral research on knowledge graph domain tackle the triple\nembedding problem [ 17,44,49]. We use TransE [ 49] as our main\nencoding method for the proposed model. TransE [ 49] represents\nrelations as translations, so the object vector e\ud835\udc5cin the triple is\nconsidered as a translation of subject vector e\ud835\udc60on predicate vector\ne\ud835\udc5d. Other than TransE, RotatE [ 44] and ConvE [ 17] knowledge\ngraph embedding methods are also examined in our work. Ablation\nstudy on different knowledge graph triple embedding methods and\ntheir descriptions are provided in the Appendix.\nIn knowledge graph, there are usually explicit features for pred-\nicated ( e\ud835\udc5din Equation 3), but in our case, there is no explicit fea-\ntures for the relations ( \ud835\udc5fin Equation 3), so we use learnable em-\nbedding vectors to present relations. Inverse relationships, such\nas\ud835\udc43\ud835\udc62\ud835\udc4f\ud835\udc59\ud835\udc56\ud835\udc60\u210e\ud835\udc52\ud835\udc5f\u2212\ud835\udc41\ud835\udc52\ud835\udc64\ud835\udc60 and\ud835\udc41\ud835\udc52\ud835\udc64\ud835\udc60\u2212\ud835\udc43\ud835\udc62\ud835\udc4f\ud835\udc59\ud835\udc56\ud835\udc60\u210e\ud835\udc52\ud835\udc5f , are represented by\ntaking the sign inverses. For instance, if we define \ud835\udc5fas the em-\nbedding of\ud835\udc43\ud835\udc62\ud835\udc4f\ud835\udc59\ud835\udc56\ud835\udc60\u210e\ud835\udc52\ud835\udc5f\u2212\ud835\udc41\ud835\udc52\ud835\udc64\ud835\udc60 relationship, the inverse relationship,\n\ud835\udc41\ud835\udc52\ud835\udc64\ud835\udc60\u2212\ud835\udc43\ud835\udc62\ud835\udc4f\ud835\udc59\ud835\udc56\ud835\udc60\u210e\ud835\udc52\ud835\udc5f is\ud835\udc5f\u22121=\u2212\ud835\udc5f. Our encoding function \ud835\udc53\ud835\udc52\ud835\udc5b\ud835\udc50is defined\nas:\nh\ud835\udc5d=\ud835\udc53\ud835\udc52\ud835\udc5b\ud835\udc50(\ud835\udc5d)=\ud835\udc53\ud835\udc52\ud835\udc5b\ud835\udc50(h\ud835\udc62,\ud835\udc5f,h\ud835\udc64,\ud835\udc5f\u22121) (4)\nThe existing knowledge graph triple embedding methods ex-\nplained above are designed for two nodes and the relation between\nthem. In our Meta-Path, we have a total of three nodes and two\nrelations in a Meta-Path instance. We deal with this by slightly\ntuning the formulation to fulfill our needs. The original formula-\ntion of knowledge graph triple embedding methods and ours are\nsummarized in Table 2. In this table, the \u02dchmeans the reshape of\n4\nMeta-Path-based Fake News Detection Leveraging Multi-level Social Context Information Anonymous Submission to The Web Conference, 2022\n\ud835\udc5d\u2208\ud835\udc43!!!!\n\ud835\udc5d\u2208\ud835\udc43\"!!!\ud835\udc93\ud835\udc7a\ud835\udc93\ud835\udc7a\"\ud835\udfcf!!!!!!TransE Encoder\ud835\udc93\ud835\udc7c\ud835\udc93\ud835\udc7c\"\ud835\udfcf\ud835\udc93\ud835\udc7c\ud835\udc93\ud835\udc7c\"\ud835\udfcf!!!!!!TransE Encoder!!!!!!Attention\ud835\udc89\ud835\udfceGRUGRU!!!Attention\ud835\udc93\ud835\udc7a\ud835\udc93\ud835\udc7a\"\ud835\udfcf\ud835\udc4a&!\ud835\udc4a&\"\ud835\udc4a&!\ud835\udc4a&#!!!!!!$$$$$$$$$$$$\u2211\ud835\udc89\ud835\udc97(a)Node Feature Transformation(b) Meta-Path Instance Encoding(c) Meta-Path Instance Aggregation(d)Semantic Aggregation\ud835\udc65$!\ud835\udc65$\"\ud835\udc65$!\ud835\udc65$#\nFigure 4: Architecture of Hetero-SCAN .\nTable 2: Formulation of Encoding Method.\nMethod Original In Our Paper\nTransE e\ud835\udc60+e\ud835\udc5d\ud835\udc40\ud835\udc38\ud835\udc34\ud835\udc41[(h\ud835\udc62+\ud835\udc5f+\ud835\udc5f\u22121),(h\ud835\udc64+\ud835\udc5f\u22121)]\nConvE[e\ud835\udc60\u2225e\ud835\udc5d]\u2217W[\u02dch\ud835\udc62\u2225\u02dc\ud835\udc5f\u2225\u02dch\ud835\udc64\u2225\u02dc\ud835\udc5f\u22121]\u2217W\nRotatE e\ud835\udc60\u2299e\ud835\udc5d\ud835\udc40\ud835\udc38\ud835\udc34\ud835\udc41[(h\ud835\udc62\u2299\ud835\udc5f\u2299\ud835\udc5f\u22121),(h\ud835\udc64\u2299\ud835\udc5f\u22121)]\nvector hin a 2D form, and the \u2299and\u2225represent the element-wise\nproduct and concatenation of vector, respectively.\n4.3.3 Meta-Path Instance Aggregation .The encoded vectors\nfrom two different Meta-Paths are aggregated by using different\nmethods.\nThe encoded vectors from Meta-Path P\ud835\udc46:\ud835\udc41\ud835\udc52\ud835\udc64\ud835\udc60\u2192\ud835\udc43\ud835\udc62\ud835\udc4f\ud835\udc59\ud835\udc56\ud835\udc60\u210e\ud835\udc52\ud835\udc5f\u2192\n\ud835\udc41\ud835\udc52\ud835\udc64\ud835\udc60 contain information of other news from the same publisher.\nAmong the news published by the publisher, not all news will con-\ntain valuable information for detection. Thus, the model should\n\u2019focus\u2019 on some of the news published by this publisher and in-\nclude this information in the aggregated representation. For each\nMeta-Path instance \ud835\udc5d\u2208P\ud835\udc46:\n\ud835\udc52\ud835\udc5d=\ud835\udc3f\ud835\udc52\ud835\udc4e\ud835\udc58\ud835\udc66\ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48(a\ud835\udc47\u00b7h\ud835\udc5d)\n\ud835\udefc\ud835\udc5d=\ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65(\ud835\udc52\ud835\udc5d)=\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc52\ud835\udc5d)\u00cd\n\ud835\udc5d\u2032\u2208P\ud835\udc46\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc52\ud835\udc5d\u2032)(5)\nwhere\ud835\udc52\ud835\udc5dis the attention value calculated by multiplying encoded\nMeta-Path instance h\ud835\udc5dwith attention vector a\u2208R2\ud835\udc51\u2032\n, and it is\nnormalized by a softmax function over all Meta-Path instances of\nthe target node \ud835\udc63, the result is denoted as \ud835\udefc\ud835\udc5dabove.\nTo alleviate the effect of the high variance of the data in a het-\nerogeneous graph, we adopt multi-head attention mechanism. \ud835\udc3e\nindependent attention mechanisms execute the transformation as\nshown in Equation 6, and their features are concatenated after they\npass the activation function \ud835\udf0e. The output feature representation\ncan be formulated as:\nhP\ud835\udc46\ud835\udc63=\ud835\udc3e\n\u2225\n\ud835\udc58=1\ud835\udf0e(\u2211\ufe01\n\ud835\udc5d\u2208P\ud835\udc46[\ud835\udefc\ud835\udc5d]\ud835\udc58\u00b7h\ud835\udc5d) (6)\nwhere[\ud835\udefc\ud835\udc5d]\ud835\udc58is the normalized attention value of Meta-Path in-\nstance\ud835\udc5dof target node \ud835\udc63at the\ud835\udc58-th attention head.Temporal information of user engagement is another critical fea-\nture to determine the veracity of the given news, and incorporating\nthis information is the second difficulty to resolve. To capture the\ntemporal information, we aggregate the Meta-Path instances follow\nP\ud835\udc48:\ud835\udc41\ud835\udc52\ud835\udc64\ud835\udc60\u2192\ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f\u2192\ud835\udc41\ud835\udc52\ud835\udc64\ud835\udc60 through Recurrent Neural Network\n(RNN). Since Meta-Path instances are already encoded in the previ-\nous step, we can directly feed them into the RNN. There are usually\na large number of users engaged per news, so we choose GRU [ 14]\nas our RNN unit to avoid the vanishing or exploding gradients\nproblem.\nhP\ud835\udc48\ud835\udc63=GRU(h\ud835\udc5d1,h\ud835\udc5d2,...,h\ud835\udc5d\ud835\udc5b),\ud835\udc5d\ud835\udc56\u2208PU (7)\nThe last hidden state of the GRU is used for the downstream task\nas it is the high-level representation that summarizes the temporal\ninformation of the user engagement.\n4.3.4 Semantic Aggregation .Two vectors, hP\ud835\udc46\ud835\udc63andhP\ud835\udc48\ud835\udc63, from\nprevious step represents two different aspects of the news. The\nfinal news representation is produced by fusing these two vectors,\nwhich enables us to learn the news representation end-to-end (the\nthird difficulty). As two Meta-Paths show two different aspects of a\ngiven news, the model should be able to weigh the importance of\nthe two aspects with different news. To this end, we adopt another\nattention mechanism. Before applying the attention mechanism,\nnon-linear transformations are applied to summarize hP\ud835\udc46\ud835\udc63andhP\ud835\udc48\ud835\udc63.\nThus for\ud835\udc43\u2208{P\ud835\udc46,P\ud835\udc48}:\n\ud835\udc60\ud835\udc43=1\n|V|\u2211\ufe01\n\ud835\udc63\u2208V\ud835\udc61\ud835\udc4e\ud835\udc5b\u210e(M\ud835\udc34\u00b7h\ud835\udc43\n\ud835\udc63+b\ud835\udc34) (8)\nHere, M\ud835\udc34\u2208R\ud835\udc51\ud835\udc5a\u00d7\ud835\udc51\u2032\nandb\u2208R\ud835\udc51\ud835\udc5ais a learnable weight matrix and\nbias vector.Vis the set of news nodes.\nThen we apply the attention mechanism to aggregate two vectors\nto obtain our final news representation h\ud835\udc63.\n\ud835\udc52\ud835\udc43=\ud835\udc61\ud835\udc4e\ud835\udc5b\u210e(\ud835\udc5e\ud835\udc47\u00b7\ud835\udc60\ud835\udc43)\n\ud835\udefd\ud835\udc43=\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc52\ud835\udc43)\u00cd\n\ud835\udc43\u2032\u2208P\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc52\ud835\udc43\u2032)\nh\ud835\udc63=\u2211\ufe01\n\ud835\udc43\u2208P\ud835\udefd\ud835\udc5d\u00b7h\ud835\udc43\n\ud835\udc63(9)\n5\nAnonymous Submission to The Web Conference, 2022 Jian Cui, Kwanwoo Kim, Seung Ho Na, and Seungwon Shin\nwhere\ud835\udc5e\u2208R\ud835\udc51\ud835\udc5ais the attention vector and \ud835\udefd\ud835\udc43is the normalized\nimportance of Meta-Path \ud835\udc43.\n4.4 Training\nThe final representation of the target news vector is passed to the\nclassification layer to get the classification result. During training,\nour predictions and labels are used to calculate the loss, and we\nupdate the learnable parameters of the model by using the back-\npropagation algorithm. The loss function used in Hetero-SCAN is\ncross-entropy loss, which is:\nL=\u2212\u2211\ufe01\n\ud835\udc66logP\ud835\udc53\ud835\udc4e\ud835\udc58\ud835\udc52+(1\u2212\ud835\udc66)logP\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc59 (10)\nThe overall all learning algorithm is summarized in Algorithm 1\n(Appendix).\n5 EXPERIMENTAL RESULT AND ANALYSIS\n5.1 Dataset and Settings\nTo test the effectiveness of our method, we conducted our experi-\nments with two real-world datasets: FANG [ 32] and FakeHealth [ 16].\nThe dataset FANG was composed in a study by Nguyen et al. [ 32]\nbased on the datasets collected by related work on rumor and news\nclassification [26, 29, 41]. The original news content was obtained\nthrough the provided news url, and for the 100 news urls that did\nnot have the news content available, resorted to manually search-\ning the news title for the content. From provided tweet ids, users\nand their profiles on Twitter could be found through the Twitter\nAPI [ 8]. The labels of the news in FANG are obtained from two\nwell-known fact-checking websites: Snopes [ 7] and PolitiFact [ 4].\nFakeHealth is another publicly available benchmark dataset for\nfake news detection, mainly focused on the healthcare domain. The\ndataset consists of two subsets, HealthStory and HealthRelease;\nHealthStory was used in our study due to the number of news\narticles in HealthRelease being too small. HealthStory is collected\nfrom the healthcare information review website HealthNewsRe-\nviews [ 2]. On this website, the professional reviewers gave scores\nof 1 to 5 for each news. Similar to the original study that published\nthe FakeHealth dataset, an article is considered as fake if the score\nis less than three and real otherwise. The detailed statistics of the\ndataset used in our experiment are listed in Table 4.\nIn each dataset, we used 70% of news articles as our training\nset, and the remaining 30% of news articles are further divided into\nequal sizes of validation and test set. For the hyper-parameters, the\ntransformed hidden dimension and the learning rate are set to 512\nand 0.0001, respectively. The early-stopping training strategy with\npatience 20 is adopted to avoid overfitting. Since fake news detec-\ntion is a binary classification problem, the real class was treated as\npositive and the fake class as negative.\n5.2 Evaluation of ML Algorithms on News\nEmbedding\nWe trained Hetero-SCAN by connecting the output representation to\na fully connected layer to classify the news. After training, we eval-\nuated our news representation with five classical machine learning\nbaselines, such as Naive Bayes, Logistic Regression, etc. The metrics\nused for comparison are precision, recall, accuracy, F1 score, and\nAUC score, and the evaluation results are summarized in Table 3.As shown in Table 3, the trained classification layer gives rel-\natively better results than other machine learning algorithms in\nterms of F1 score and accuracy because the classification layer is\noptimized by classification objective (cross-entropy loss). In terms\nof AUC score, SVM gives a better result, but in terms of standard de-\nviation, random forest generally gives more stable results. Based on\nthis, random forest is chosen as the classification algorithm for up-\ncoming evaluations. Regardless of downstream classification\nmethods, Hetero-SCAN surpass any existing fake news detec-\ntion methods (details in Section 5.4). In the dataset - HealthStory,\nHetero-SCAN does not give an ideal result. The explanation for the\nresult on the HealthStory dataset is discussed in the next section.\n5.3 Misinformation vs Disinformation\nWardle et al. [ 50] published a report about information disorder on\nthe Council of Europe in 2017. The report intends to examine infor-\nmation disorder and its related challenges. The authors argue that\na large portion of the word \u2019fake news\u2019 consists of three concepts:\nmisinformation, disinformation, and malinformation. They point\nout the importance of distinguishing the fake news in accordance\nwith creators\u2019 intention and provide the definition of three terms:\nDefinition 5.1 ( Disinformation ).Information that is false and\ndeliberately created to harm a person, social group, organization\nor country.\nDefinition 5.2 ( Misinformation ).Information that is false, but\nnot created with the intention of causing harm.\nDefinition 5.3 ( Malinformation ).Information that is based on\nreality, used to inflict harm on a person, organization or country.\nAccording to the definition of malinformation, it is the infor-\nmation based on reality, while the fake news we talk about in this\npaper is false information. Therefore, we mainly consider disinfor-\nmation and misinformation here, which are classified according\nto the news creator\u2019s intention. Considering the definition of fake\nnews given in Section 3, the narrow definition of fake news only\ncovers disinformation, but the broad definition of fake news covers\nboth disinformation and misinformation.\nThe dataset FANG is mainly composed of checked news from\nPolitiFact and Snopes, which are political-related fact-checking\nwebsites. Thus, the fake news in this dataset is either partisan-biased\nnews or some false information to demean certain politicians, which\nare considered as information intended to harm the specific person\nor the organizations. Hence, the fake news in this dataset can be\nconsidered as disinformation . The news in HealthStory is collected\nand fact-checked from Health News Review where evaluates and\nrates the completeness, accuracy, and balance of news stories that\ninclude claims about medical treatments, health care journalism, etc.\nMost of this information is not spread deliberately to harm anyone,\nso the fake news in the HealthStory dataset can be regarded as\nmisinformation .\nFigure 5 compares the number of engaged users along with the\ntime to see how people react to disinformation, misinformation,\nand real news. As shown in Figure 5, the disinformation (fake in the\nleft) has many periodic spikes, which means the users periodically\ntalk about disinformation. On the contrary, the misinformation\n(fake in the right) does not have any periodic spikes and converges\n6\nMeta-Path-based Fake News Detection Leveraging Multi-level Social Context Information Anonymous Submission to The Web Conference, 2022\nTable 3: Detection result of two real-word dataset: FANG and FakeHealth. Bold numbers denote the best value in average, and\nunderscored numbers denote the smallest variation ( \u00b1stands for 95% confidence interval).\nDataset Classification Method Precision Recall F1 Score Accuracy AUC Score\nFANGClassification Layer 0.845\u00b10.052 0.843\u00b10.054 0.843\u00b10.053 0.843\u00b10.054 0.839\u00b10.048\nNaive Bayes 0.839 \u00b10.053 0.837\u00b10.058 0.835\u00b10.057 0.837\u00b10.058 0.840\u00b10.041\nLogistic Regression 0.835 \u00b10.054 0.835\u00b10.054 0.835\u00b10.054 0.835\u00b10.054 0.907\u00b10.058\nSVM 0.832\u00b10.036 0.839\u00b10.053 0.840\u00b10.053 0.839\u00b10.053 0.910\u00b10.047\n\u2605Random Forest 0.832\u00b10.036 0.831\u00b10.037 0.831\u00b10.037 0.831\u00b10.037 0.900\u00b10.057\nAdaBoost 0.811\u00b10.070 0.807\u00b10.076 0.808\u00b10.075 0.807\u00b10.076 0.881\u00b10.056\nHealthStoryClassification Layer 0.529 \u00b10.093 0.717\u00b10.003 0.599\u00b10.008 0.717\u00b10.003 0.500\u00b10.003\nNaive Bayes 0.662 \u00b10.139 0.600\u00b10.244 0.573\u00b10.289 0.633\u00b10.131 0.508\u00b10.177\nLogistic Regression 0.660 \u00b10.065 0.595\u00b10.206 0.594\u00b10.185 0.584\u00b10.180 0.557\u00b10.076\nSVM 0.649\u00b10.094 0.620\u00b10.137 0.612\u00b10.089 0.623\u00b10.137 0.536\u00b10.108\nRandom Forest 0.674\u00b10.117 0.550\u00b10.272 0.526\u00b10.327 0.520\u00b10.269 0.513\u00b10.134\nAdaBoost 0.656\u00b10.129 0.539\u00b10.302 0.492\u00b10.303 0.540\u00b10.301 0.554\u00b10.076\nTable 4: Dataset Statistics.\nFANG HealthStory\n# Users 52,357 63,723 (sampled)\n# News 1,054 1,638\n# of Users per News 71.9 227.26\n# Fake News 448 460\n# Real News 606 1,178\n# Publishers 442 31\n0 50 100 150\nTime (hours)0.00.51.01.5# of engaged usersfake\nreal\n0 50 100 150\nTime (hours)0102030# of engaged usersfake\nreal\nFigure 5: Comparison of temporal behaviours on two\ndatasets. Both figures show the # of engagements (tweets)\nper news vs. time (hours) for FANG (left) and HealthStory\n(right).\nto zero not long after the news is published, which is similar to the\nreal news. As such, disinformation behaves significantly differently\nfrom real information, but misinformation behaves in a similar\nmanner to real news.\nTo see the impact of temporal information in Hetero-SCAN , we\nreplace the RNN in Hetero-SCAN with attention mechanism. In\nother words, we checked the detection performance difference\nbetween the Hetero-SCAN with and without temporal information.\nWe set the hyperparameters the same for both approaches for a\nfair comparison, with Random Forest chosen as the classification\nalgorithm. The evaluation result on the datasets can be found in\nTable 5.Table 5: Performance of the Hetero-SCAN with and without\ntemporal information.\nDataset Hetero-SCAN F1 Accuracy AUC\nFANGw/ temporal 0.831 0.831 0.900\nw/o temporal 0.759 0.760 0.823\nHealthStoryw/ temporal 0.526 0.520 0.513\nw/o temporal 0.614 0.595 0.636\nThe results show that the RNN based approach performs better\nthan the other one in FANG dataset, but for the HealthStory dataset,\nthe performance is better when the attention is applied. This means\nthe existence of temporal information is not helpful in detecting\nmisinformation. Furthermore, in FANG dataset, the validation loss\nofHetero-SCAN with RNN converges much faster than the one with\nattention mechanism; by contrast, the convergence speed of the\ntwo approaches is similar in the HealthStory dataset. (See Figure 6\nin Appendix)\nTo sum up, in a dataset has temporal behavior difference between\nreal and fake class (i.e., disinformation dataset), Hetero-SCAN with\nRNN not only improves the performance of the fake news detection\nbut also accelerates the learning speed.\n5.4 Comparison with Existing Methods\nTo show that Hetero-SCAN is superior to other fake news detection,\nwe compared Hetero-SCAN with other existing fake news detec-\ntion methods. The bench-marked detection methods can be cate-\ngorized into text-based approaches and graph-based approaches.\nFor text-based approach, we use three different document embed-\nding methods, TF-IDF, LIWC [ 33], and Doc2Vec [ 28], combined\nwith SVM as baselines; and several representative graph-based fake\nnews detection frameworks [ 13,32,37,39] are also compared in\nthis experiment.\nHetero-SCAN is also compared with some Graph Neural Network\n(GNN) methods to show that Hetero-SCAN is better than just simply\n7\nAnonymous Submission to The Web Conference, 2022 Jian Cui, Kwanwoo Kim, Seung Ho Na, and Seungwon Shin\nTable 6: Comparison with other methods. The AUC score of\nthe CSI is from FANG, the F1 score and AUC score are not\nreported in this paper.\nCategory Method F1 Accuracy AUC\nText-\nbasedTF.IDF + SVM 0.746 0.750 0.735\nLIWC + SVM 0.512 0.550 0.511\nDoc2Vec + SVM 0.561 0.560 0.554\nGraph-\nbasedCSI - - 0.741\nSAFER 0.678 0.680 0.669\nFANG 0.676 0.687 0.750\nAA-HGNN 0.726 0.662 0.654\nGNN-\nbaselinesGCN 0.645 0.650 0.633\nGAT 0.642 0.650 0.630\nGraphSAGE 0.779 0.780 0.773\nR-GCN 0.765 0.770 0.753\nHAN 0.662 0.660 0.658\nHetero-SCAN 0.831 0.831 0.900\napplying the GNN on the graph. The basic GNN methods [ 22,25,\n45], as well as the methods tailor to the heterogeneous graph, are\ncompared [ 23,40]. The brief descriptions of the aforementioned\nfake news detection methods and GNN baselines we compared with\nare listed in the Appendix.\nThe results of Table 6 indicates that Hetero-SCAN outperforms\nexisting text-based or graph-based fake news detection methods.\nThis is because these existing approaches cannot produce repre-\nsentation with rich social context and temporal information as\nHetero-SCAN do, i.e., they fail to tackle all three difficulties. CSI\nand SAFER, for example, did not use multi-level social context, and\nthey also incurred some information loss as they ignored the node\nand relation types. AA-HGNN, including SAFER, miss temporal\ninformation in the news representation. AA-HGNN also did not use\nusers as social context. FANG performs better than these methods\nsince it tries to preserve multi-level social context and temporal\ninformation. To preserve information, FANG divides the fake news\ndetection task into several sub-tasks, and each sub-task deals with\ncertain information. Dividing into several sub-tasks is ineffective\nbecause errors in sub-task will be propagated up to the final news\nrepresentation and thus harm the detection performance. As such,\nthe result emphasizes the importance of resolving the proposed\nthree difficulties in fake news detection.\nFor GNN baselines, the graph embedding methods made for\nhomogeneous graphs, such as GCN, GAT, and GraphSAGE, did not\ngive ideal results since node types and relations are ignored in these\ncases. R-GCN and HAN, which are designed for heterogeneous\ngraph, also has no significant improvement, which implies that\nHetero-SCAN is better than a simple application of these graph\nembedding methods on the heterogeneous graph of news. The fail\nof GNN baselines target on the heterogeneous graph can attribute\nto the missing temporal information of user engagement, which is\nthe second difficulty that needs to be resolved in the social context-\naware fake news detection.5.5 Limited training data\nNormally, the fake news dataset has limited training data due to the\nlarge-scale requirement of human labor, so the model should work\nwell in the circumstance of limited training samples. To show that\nHetero-SCAN outperforms existing methods given the circumstance\nof scarce training data, we gradually enlarge the training data,\nfrom 10% to 90%, and compare the fake news detection result with\nexisting methods. Table 7 shows the comparison result.\nTable 7: Comparison of AUC score against other fake news\ndetection methods by varying the size of the training data.\n10% 30% 50% 70% 90%\nCSI 0.636 0.671 0.670 0.689 0.691\nSAFER 0.546 0.689 0.666 0.692 0.669\nFANG 0.669 0.704 0.717 0.723 0.752\nAA-HGNN 0.573 0.598 0.656 0.657 0.642\nHetero-SCAN \ud835\udc64/\ud835\udc5c \ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52 0.594 0.707 0.776 0.749 0.751\nHetero-SCAN \ud835\udc64/\ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52 0.764 0.835 0.878 0.889 0.900\nThe AUC score of Hetero-SCAN achieves over 0.8 with only 30%\nof training data and even outperforms the rest of the methods with\n90% of the training data. AA-HGNN is designed to overcome the\nscarcity of training data issues in the fake news detection task, but\nHetero-SCAN is still better than AA-HGNN even when the size of\ntraining data is small.\n6 DISCUSSION\n6.1 Inductiveness of Hetero-SCAN\nA deep learning based approach dealing with graph-structured\ndata should have generality to produce practical predictions for\nunseen data. A method is an inductive approach if it can generate\nembeddings for the nodes that were not seen during training. In\ncontrast, it is called a transductive approach if the method cannot\ngenerate embeddings for the nodes appearing in the testing phase\nfor the first time. For example, GCN is inductive, whereas Node2Vec\nis transductive.\nIn graph-based fake news detection, unseen nodes can appear in\nthe testing phase. It might be newly published news, new publishers,\nor new users. Some approaches using matrix decomposition [ 39,42]\nare not able to generate embedding for newly published news with\nsocial context information. In Hetero-SCAN , however, the learnable\nparameters in our model are used after Meta-Path extraction with\nrandom sampling, and they are shared by all nodes. Therefore, our\nmethod is highly inductive, that is, Hetero-SCAN can generate news\nembeddings that are not seen during the training.\n6.2 Limitation and Future Work\nAs expected, a single news article is engaged with by a large number\nof users. Using every single user\u2019s information as a feature is there-\nfore impractical, and we eventually used simple random sampling\nto select a certain number of users. Therefore, an improved method\nof screening important users is necessary for fake news detection to\novercome the limitation. In addition, to apply the proposed method,\nwe must first identify the relevant tweets for particular news. Since\nthis paper focuses primarily on identifying the news in the context\n8\nMeta-Path-based Fake News Detection Leveraging Multi-level Social Context Information Anonymous Submission to The Web Conference, 2022\nin which news and related tweets are given, finding relevant tweets\nfor particular news is left as future work.\n7 CONCLUSIONS\nFake news is a critical social problem threatening many aspects of\nthe lives of the general public. We pose three difficulties in social\ncontext aware fake news detection and address them by propos-\ning a novel fake news detection framework Hetero-SCAN . Our\nmodel overcomes the shortcomings of the previous graph-based\napproaches and exhibits state-of-the-art performance. We also pro-\nvide insight about misinformation and disinformation by clarifying\ntheir different propagation properties. Hetero-SCAN can be of aid\nin future studies not only residing to fake news detection but also\nvarious events concerning disinformation.\nACKNOWLEDGMENTS\nREFERENCES\n[1]2020. Coronavirus: The viral rumours that were completely wrong. https:\n//www.bbc.com/news/blogs-trending-53640964.\n[2] 2020. Health News Review. https://www.healthnewsreview.org/.\n[3]2020. Left bias Publishers checked by MBFC. https://mediabiasfactcheck.com/\nleft/.\n[4] 2020. PolitiFact. https://www.politifact.com/.\n[5]2020. Questionable Publishers checked by MBFC. https://mediabiasfactcheck.\ncom/fake-news/.\n[6]2020. Right bias Publishers checked by MBFC. https://mediabiasfactcheck.com/\nright/.\n[7] 2020. Snopes. https://www.snopes.com/.\n[8] 2020. Twitter API. https://developer.twitter.com/en/docs/twitter-api.\n[9]2020. US election 2020: Fact-checking Trump team\u2019s main fraud claims. https:\n//www.bbc.com/news/election-us-2020-55016029.\n[10] 2021. Facebook Media: Working to Stop Misinformation and False News.\nhttps://www.facebook.com/formedia/blog/working-to-stop-misinformation-\nand-false-news.\n[11] Tian Bian, Xi Xiao, Tingyang Xu, Peilin Zhao, Wenbing Huang, Yu Rong, and\nJunzhou Huang. 2020. Rumor detection on social media with bi-directional\ngraph convolutional networks. In Proceedings of the AAAI Conference on Artificial\nIntelligence , Vol. 34. 549\u2013556.\n[12] Carlos Castillo, Marcelo Mendoza, and Barbara Poblete. 2011. Information credi-\nbility on twitter. In Proceedings of the 20th international conference on World wide\nweb. 675\u2013684.\n[13] Shantanu Chandra, Pushkar Mishra, Helen Yannakoudakis, and Ekaterina\nShutova. 2020. Graph-based Modeling of Online Communities for Fake News\nDetection. arXiv preprint arXiv:2008.06274 (2020).\n[14] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014.\nEmpirical evaluation of gated recurrent neural networks on sequence modeling.\narXiv preprint arXiv:1412.3555 (2014).\n[15] Niall J Conroy, Victoria L Rubin, and Yimin Chen. 2015. Automatic deception\ndetection: methods for finding fake news. In Proceedings of the 78th ASIS&T An-\nnual Meeting: Information Science with Impact: Research in and for the Community .\n1\u20134.\n[16] Enyan Dai, Yiwei Sun, and Suhang Wang. 2020. Ginger cannot cure cancer:\nBattling fake health news with a comprehensive data repository. In Proceedings\nof the International AAAI Conference on Web and Social Media , Vol. 14. 853\u2013862.\n[17] Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018.\nConvolutional 2d knowledge graph embeddings. In Proceedings of the AAAI\nConference on Artificial Intelligence , Vol. 32.\n[18] Song Feng, Ritwik Banerjee, and Yejin Choi. 2012. Syntactic stylometry for\ndeception detection. In Proceedings of the 50th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Papers) . 171\u2013175.\n[19] Xinyu Fu, Jiani Zhang, Ziqiao Meng, and Irwin King. 2020. MAGNN: metap-\nath aggregated graph neural network for heterogeneous graph embedding. In\nProceedings of The Web Conference 2020 . 2331\u20132341.\n[20] Anastasia Giachanou, Guobiao Zhang, and Paolo Rosso. 2020. Multimodal Fake\nNews Detection with Textual, Visual and Semantic Information. In International\nConference on Text, Speech, and Dialogue . Springer, 30\u201338.\n[21] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for\nnetworks. In Proceedings of the 22nd ACM SIGKDD international conference on\nKnowledge discovery and data mining . 855\u2013864.[22] William L. Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive Representation\nLearning on Large Graphs. In NIPS .\n[23] Yi Han, Shanika Karunasekera, and Christopher Leckie. 2020. Graph neural\nnetworks with continual learning for fake news detection from social media.\narXiv preprint arXiv:2007.03316 (2020).\n[24] Dhruv Khattar, Jaipal Singh Goud, Manish Gupta, and Vasudeva Varma. 2019.\nMvae: Multimodal variational autoencoder for fake news detection. In The World\nWide Web Conference . 2915\u20132921.\n[25] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph\nconvolutional networks. arXiv preprint arXiv:1609.02907 (2016).\n[26] Elena Kochkina, Maria Liakata, and Arkaitz Zubiaga. 2018. PHEME dataset\nfor Rumour Detection and Veracity Classification. https://doi.org/10.6084/m9.\nfigshare.6392078.v1\n[27] Sejeong Kwon, Meeyoung Cha, Kyomin Jung, Wei Chen, and Yajun Wang. 2013.\nProminent features of rumor propagation in online social media. In 2013 IEEE\n13th international conference on data mining . IEEE, 1103\u20131108.\n[28] Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and\ndocuments. In International conference on machine learning . PMLR, 1188\u20131196.\n[29] Jing Ma, Wei Gao, Prasenjit Mitra, Sejeong Kwon, Bernard J Jansen, Kam-Fai\nWong, and Meeyoung Cha. 2016. Detecting rumors from microblogs with recur-\nrent neural networks. (2016).\n[30] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.\nJournal of machine learning research 9, Nov (2008), 2579\u20132605.\n[31] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient\nestimation of word representations in vector space. arXiv preprint arXiv:1301.3781\n(2013).\n[32] Van-Hoang Nguyen, Kazunari Sugiyama, Preslav Nakov, and Min-Yen Kan. 2020.\nFANG: Leveraging social context for fake news detection using graph represen-\ntation. In Proceedings of the 29th ACM International Conference on Information &\nKnowledge Management . 1165\u20131174.\n[33] James W Pennebaker, Ryan L Boyd, Kayla Jordan, and Kate Blackburn. 2015. The\ndevelopment and psychometric properties of LIWC2015 . Technical Report.\n[34] Ver\u00f3nica P\u00e9rez-Rosas, Bennett Kleinberg, Alexandra Lefevre, and Rada Mihalcea.\n2018. Automatic Detection of Fake News. In Proceedings of the 27th International\nConference on Computational Linguistics . 3391\u20133401.\n[35] Martin Potthast, Johannes Kiesel, Kevin Reinartz, Janek Bevendorff, and Benno\nStein. 2018. A Stylometric Inquiry into Hyperpartisan and Fake News. In Proceed-\nings of the 56th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers) . 231\u2013240.\n[36] Shengsheng Qian, Jinguang Wang, Jun Hu, Quan Fang, and Changsheng Xu. 2021.\nHierarchical multi-modal contextual attention network for fake news detection.\nInProceedings of the 44th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval . 153\u2013162.\n[37] Yuxiang Ren, Bo Wang, Jiawei Zhang, and Yi Chang. 2020. Adversarial active\nlearning based heterogeneous graph neural network for fake news detection. In\n2020 IEEE International Conference on Data Mining (ICDM) . IEEE, 452\u2013461.\n[38] Victoria L Rubin and Tatiana Lukoianova. 2015. Truth and deception at the\nrhetorical structure level. Journal of the Association for Information Science and\nTechnology 66, 5 (2015), 905\u2013917.\n[39] Natali Ruchansky, Sungyong Seo, and Yan Liu. 2017. Csi: A hybrid deep model for\nfake news detection. In Proceedings of the 2017 ACM on Conference on Information\nand Knowledge Management . 797\u2013806.\n[40] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan\nTitov, and Max Welling. 2018. Modeling relational data with graph convolutional\nnetworks. In European semantic web conference . Springer, 593\u2013607.\n[41] Kai Shu, Deepak Mahudeswaran, Suhang Wang, Dongwon Lee, and Huan Liu.\n2018. FakeNewsNet: A Data Repository with News Content, Social Context and\nDynamic Information for Studying Fake News on Social Media. arXiv preprint\narXiv:1809.01286 (2018).\n[42] Kai Shu, Suhang Wang, and Huan Liu. 2019. Beyond news contents: The role\nof social context for fake news detection. In Proceedings of the twelfth ACM\ninternational conference on web search and data mining . 312\u2013320.\n[43] Kai Shu, Xinyi Zhou, Suhang Wang, Reza Zafarani, and Huan Liu. 2019. The\nrole of user profiles for fake news detection. In Proceedings of the 2019 IEEE/ACM\ninternational conference on advances in social networks analysis and mining . 436\u2013\n439.\n[44] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2019. Rotate: Knowl-\nedge graph embedding by relational rotation in complex space. arXiv preprint\narXiv:1902.10197 (2019).\n[45] Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint\narXiv:1710.10903 (2017).\n[46] Nguyen Vo and Kyumin Lee. 2021. Hierarchical Multi-head Attentive Network\nfor Evidence-aware Fake News Detection. In Proceedings of the 16th Conference\nof the European Chapter of the Association for Computational Linguistics: Main\nVolume . 965\u2013975.\n[47] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu.\n2019. Heterogeneous graph attention network. In The World Wide Web Conference .\n9\nAnonymous Submission to The Web Conference, 2022 Jian Cui, Kwanwoo Kim, Seung Ho Na, and Seungwon Shin\n2022\u20132032.\n[48] Yaqing Wang, Fenglong Ma, Zhiwei Jin, Ye Yuan, Guangxu Xun, Kishlay Jha, Lu\nSu, and Jing Gao. 2018. Eann: Event adversarial neural networks for multi-modal\nfake news detection. In Proceedings of the 24th acm sigkdd international conference\non knowledge discovery & data mining . 849\u2013857.\n[49] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge\ngraph embedding by translating on hyperplanes. In Proceedings of the AAAI\nConference on Artificial Intelligence , Vol. 28.\n[50] Claire Wardle and Hossein Derakhshan. 2017. Information disorder: Toward an\ninterdisciplinary framework for research and policy making. Council of Europe\nreport 27 (2017), 1\u2013107.\n[51] Chuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, and Nitesh V\nChawla. 2019. Heterogeneous graph neural network. In Proceedings of the 25th\nACM SIGKDD International Conference on Knowledge Discovery & Data Mining .\n793\u2013803.\n[52] Xinyi Zhou and Reza Zafarani. 2020. A survey of fake news: Fundamental\ntheories, detection methods, and opportunities. ACM Computing Surveys (CSUR)\n53, 5 (2020), 1\u201340.A NOTATIONS\nTable 8: Notations used in the paper.\nNotation Meaning\nG=(V,E) Heterogeneous graph of news\nV A set of nodes in the graph G\nE A set of edges in the graph G\nR A set of relations between two nodes (type of edge)\n\ud835\udc34 A set of types of nodes \ud835\udc34={\ud835\udc34\ud835\udc5d,\ud835\udc34\ud835\udc5b,\ud835\udc34\ud835\udc62}\n\ud835\udc34\ud835\udc5d Node type: publisher\n\ud835\udc34\ud835\udc5b Node type: news\n\ud835\udc34\ud835\udc62 Node type: user\nP A set of Meta-Path\n\ud835\udc43\ud835\udc48 Meta-Path:\ud835\udc41\ud835\udc52\ud835\udc64\ud835\udc60\u2212\ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f\u2212\ud835\udc41\ud835\udc52\ud835\udc64\ud835\udc60\n\ud835\udc43\ud835\udc46 Meta-Path:\ud835\udc41\ud835\udc52\ud835\udc64\ud835\udc60\u2212\ud835\udc43\ud835\udc62\ud835\udc4f\ud835\udc59\ud835\udc56\ud835\udc60\u210e\ud835\udc52\ud835\udc5f\u2212\ud835\udc41\ud835\udc52\ud835\udc64\ud835\udc60\n\ud835\udc5d A Meta-Path instance\nW\ud835\udc34 Type-specific transformation matrix\nx\ud835\udc34\ud835\udc63 Initial feature of the node \ud835\udc63of type\ud835\udc34\nh\ud835\udc34\ud835\udc63 Transformed feature of the node \ud835\udc63of type\ud835\udc34\n\ud835\udc53\ud835\udc52\ud835\udc5b\ud835\udc50 Meta-Path instance encoding function\n\u2225 Concatenation operator\n\u2299 Element-wise product\n\u02dcv Reshape the vector vin a 2D form\nB DESCRIPTION OF EXISTING METHODS\nThe bench-marked fake news detection methods can be categorized\ninto text-based approaches and graph-based approaches. Hetero-\nSCAN is also compared with representative graph embedding meth-\nods made for the homogeneous and heterogeneous graph. The detail\nof the methods we compared is listed below.\nText-based Methods:\n\u2022TF-IDF + SVM : TF-IDF is short for term frequency-inverse\ndocument frequency. It is intended to represent the importance\nof a word in a document. Feature vectors were extracted based\non news article contents with TF-IDF, and SVM is applied to it.\n\u2022LIWC [33]+ SVM : LIWC stands for Linguistic Inquiry and\nWord Count. It is widely used to extract words falling into psy-\nchologically meaningful categories, and these words can be used\nto compose a feature vector.\n\u2022Doc2Vec [28]+ SVM : Doc2Vec is a paragraph embedding tech-\nnique based on Word2Vec [ 31]. It uses skip-gram and CBOW\nmodel to learn the representation vector. Doc2Vec is consid-\nered as an unsupervised learning approach to learn the latent\nrepresentation of a document.\nGraph-based Methods:\n\u2022SAFER [13]: SAFER uses GCN and pre-trained RoBERTa model\nto embed news nodes in the heterogeneous graph. They con-\ncatenate two vectors and apply Logistic Regression to classify\nthe news embeddings.\n\u2022CSI[39]: CSI is a hybrid deep learning based framework that\naims to model the response, text, and user engagement of the\nnews. The representation of response and text is concatenated\nwith the user vector and score.\n10\nMeta-Path-based Fake News Detection Leveraging Multi-level Social Context Information Anonymous Submission to The Web Conference, 2022\nAlgorithm 1 Learning Algorithm\nInput: Heterogeneous Graph of News G=(V,E),\nnode feature{xv,\u2200\ud835\udc63\u2208V}\nnode typesA={\ud835\udc34\ud835\udc5d,\ud835\udc34\ud835\udc5b,\ud835\udc34\ud835\udc62}\nlabel\ud835\udc66\nOutput: Learn-able parameters \ud835\udf03\nforeach epoch do\nfornode type\ud835\udc34\u2208A do\n# Node feature transformation\nh\ud835\udc34\ud835\udc63=W\ud835\udc34\u00b7x\ud835\udc34\ud835\udc63\nend for\nforMeta-Path schema \ud835\udc43\u2208Pdo\nh\ud835\udc5d=\ud835\udc53\ud835\udf03(h\ud835\udc62,\ud835\udc5f,h\ud835\udc64,\ud835\udc5f\u22121)\nif\ud835\udc43==P\ud835\udc46then\n# Calculate the weight coefficient \ud835\udefc\ud835\udc5dfor each Meta-Path\n# instance.\nh\ud835\udc43\ud835\udc63=\u2225\ud835\udc3e\n\ud835\udc58=1\ud835\udf0e(\u00cd\n\ud835\udc62\u2208N\ud835\udc43\ud835\udc63[\ud835\udefc\ud835\udc5d]\ud835\udc58\u00b7h\ud835\udc5d)\nend if\nif\ud835\udc43==P\ud835\udc48then\nAllh\ud835\udc5d\ud835\udc56are sorted chronologically\nhP\ud835\udc48\ud835\udc63=GRU(h\ud835\udc5d1,h\ud835\udc5d2,...,h\ud835\udc5d\ud835\udc5b),\ud835\udc5d\ud835\udc56\u2208PU\nend if\n\ud835\udc60\ud835\udc43=1\n|V|\u00cd\n\ud835\udc63\u2208V\ud835\udc61\ud835\udc4e\ud835\udc5b\u210e(M\ud835\udc34\u00b7h\ud835\udc43\ud835\udc63+b\ud835\udc34)\nend for\nCalculate the weight coefficient \ud835\udefd\ud835\udc43for each Meta-Path.\nh\ud835\udc63=\u00cd\n\ud835\udc43\u2208P\ud835\udefd\ud835\udc43\u00b7\u210e\ud835\udc43\ud835\udc63\n# a fully connected layer for new classification.\nz\ud835\udc63=\ud835\udc4a\ud835\udc50\u00b7h\ud835\udc63\n[P\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc59,P\ud835\udc53\ud835\udc4e\ud835\udc58\ud835\udc52]=\ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65(z\ud835\udc63)\nL=\u2212\u00cd(\ud835\udc66\ud835\udc59\ud835\udc5c\ud835\udc54(P\ud835\udc53\ud835\udc4e\ud835\udc58\ud835\udc52))+( 1\u2212\ud835\udc66)\ud835\udc59\ud835\udc5c\ud835\udc54(P\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc59)\n\ud835\udf03\u2190\ud835\udc35\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc5d\ud835\udc4e\ud835\udc54\ud835\udc4e\ud835\udc61\ud835\udc52(L)\nend for\n\u2022FANG [32]: FANG divides the detection task into several sub-\ntasks, such as textual encoding and stance detection. The final\ndetection object is optimized by defining loss functions for those\nsub-tasks.\n\u2022AA-HGNN [37]: AA-HGNN uses active learning to tackle the\nlimited training data problem and extends GAT [ 45] to learn the\nnews representation in the graph.\nGNN baselines :\n\u2022GCN [25]: GCN is a deep learning based method on a graph-\nstructured data. Each node is learned by aggregating the feature\ninformation from its neighbors and the feature of itself.\n\u2022GAT [45]: GAT is similar to GCN, but it introduces the atten-\ntion mechanism to replace the statically normalized convolution\noperation in GCN.\n\u2022GraphSAGE [22]: GraphSAGE is a general inductive framework\nthat learns a node representation by sampling its neighbors and\naggregating features of sampled nodes.\n\u2022R-GCN [40]: R-GCN is an application of the GCN framework\nfor modeling relational data. In R-GCN, edges can represent\ndifferent relations.\n\u2022HAN [47]: HAN is an extension of GAT on the heterogeneous\ngraph. Meta-Path extraction strategy and attention mechanism\nare adopted to learn the representation of a node.C VALIDATION LOSS DURING TRAINING\nIn Section 5.3, to see the impact of temporal information in Hetero-\nSCAN , we replace the RNN with attention mechanism. In other\nwords, we compare the Hetero-SCAN trained with and without\ntemporal information. These two Hetero-SCAN are trained with\ntwo dataset, and corresponding validation loss during the training\nis shown in Figure 6. The Hetero-SCAN trained with temporal in-\nformation has faster convergence speed than Hetero-SCAN trained\nwithout temporal information in FANG dataset; In the HealthStory\ndataset, however, two models have no significant difference. Con-\nsidering that fake news in FANG dataset is disinformation, and fake\nnews in HealthStory is misinformation, temporal information can\naccelerates the convergence speed of training when identifying\ndisinformation.\n0 100 200\nEpoch0.450.500.550.600.650.70Validation Lossw/\nw/o\n(a) FANG\n0 50 100 150\nEpoch0.600.620.640.660.68# of engaged usersw/\nw/o (b) HealthStory\nFigure 6: Validation loss during training. (Red line indicates\nthe validation loss of Hetero-SCAN with temporal informa-\ntion, blue line indicates the validation loss of Hetero-SCAN\nwithout temporal information.)\nD ABLATION STUDY ON META-PATH\nINSTANCE ENCODING METHODS\nIn Section 4.3.2, we propose to use knowledge triple embedding\nmethods to encode Meta-Path instances, and we adopt TransE in\nHetero-SCAN . We wanted to examine the performance differences\nby changing the Meta-Path encoding method to other knowledge\ntriple embedding methods, RotatE and ConvE. Descriptions of the\nthree encoding methods are introduced below.\n\u2022TransE [49]: The TransE model represents relations as transla-\ntions and aims to model the inversion and composition patterns.\nIt defines each relation as a translation from the subject entity\nto the object entity.\n\u2022RotatE [44]: The RotatE model maps the entities and relations to\nthe complex vector space and defines each relation as a rotation\nfrom the subject entity to the object entity.\n\u2022ConvE [17]: The ConvE model uses 2D convolution over embed-\nding and multiple layers of nonlinear features to mode knowl-\nedge graphs. They reshape the embedding of subject and predi-\ncates in a 2D form and apply convolution calculations on it.\nTo show the performance differences when different knowledge\ntriple embedding methods are applied, F1 score, Accuracy, and\nAUC score were measured on two datasets: FANG and HealthStory.\nTable 9 indicates that TransE gives better results than the others.\nThis reason can be drawn from the fact that TransE requires fewer\nparameters and operations than RotatE and ConvE. With limited\ntraining data, complex models are easy to suffer from over-fitting,\nwhich will cause performance degradation.\n11\nAnonymous Submission to The Web Conference, 2022 Jian Cui, Kwanwoo Kim, Seung Ho Na, and Seungwon Shin\nTable 9: Performance of detection result when apply differ-\nent Meta-Path encoding method. Bold texts indicate the best\nencoding method in Hetero-SCAN .\nF1 Score Accuracy AUC\nTransE 0.831\u00b10.037 0.831\u00b10.037 0.900\u00b10.057\nRotatE 0.799\u00b10.035 0.799\u00b10.036 0.862\u00b10.035\nConvE 0.532\u00b10.174 0.526\u00b10.079 0.665\u00b10.021\nE T-SNE VISUALIZATION\nTo show that the news representation produced by Hetero-SCAN is\nbetter than the existing methods, t-SNE was adopted to visualize\nnews representation in a two-dimensional plane (Figure 7). The\nt-SNE technique is a well-known method to visualize the high-\ndimensional data in a two-dimensional plane [ 30]. As can be seen\nin Figure 7, the representations of Hetero-SCAN are clustered tighter\nthan the other methods, implying a significant improvement over\nexisting methods.\nfake\nreal\n(a) FANG\nfake\nreal (b) SAFER\nfake\nreal\n(c)Hetero-SCAN\n(w/o temporal information)\nfake\nreal(d)Hetero-SCAN\n(w/ temporal information)\nFigure 7: t-SNE visualization of news representations.\n12", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Meta-path-based fake news detection leveraging multi-level social context information", "author": ["J Cui", "K Kim", "SH Na", "S Shin"], "pub_year": "2022", "venue": "Proceedings of the 31st ACM international \u2026", "abstract": "Fake news, false or misleading information presented as news, has a significant impact on  many aspects of society, such as in politics or healthcare domains. Due to the deceiving"}, "filled": false, "gsrank": 497, "pub_url": "https://dl.acm.org/doi/abs/10.1145/3511808.3557394", "author_id": ["eepEd2kAAAAJ", "uVwgkBQAAAAJ", "TGGj5moAAAAJ", "DbAmqd8AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:Wd34J5JIEQsJ:scholar.google.com/&output=cite&scirp=496&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D490%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=Wd34J5JIEQsJ&ei=YLWsaK7sAqzWieoPic2ZoAU&json=", "num_citations": 34, "citedby_url": "/scholar?cites=797498401594334553&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:Wd34J5JIEQsJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2109.08022"}}]