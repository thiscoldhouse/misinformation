[{"title": "Mapping the media landscape: predicting factual reporting and political bias through web interactions", "year": "2024", "pdf_data": "Mapping the Media Landscape: Predicting\nFactual Reporting and Political Bias Through\nWeb Interactions\nDairazalia S\u00e1nchez-Cort\u00e9s1, Sergio Burdisso1, Esa\u00fa Villatoro-Tello1, and Petr\nMotlicek1,2\n1Idiap Research Institute, Martigny, Switzerland\n2Brno University of Technology, Brno, Czech Republic\n{dscortes,sergio.burdisso,esau.villatoro,petr.motlicek}@idiap.ch\nAbstract. Bias assessment of news sources is paramount for profes-\nsionals, organizations, and researchers who rely on truthful evidence for\ninformation gathering and reporting. While certain bias indicators are\ndiscernible from content analysis, descriptors like political bias and fake\nnews pose greater challenges. In this paper, we propose an extension to a\nrecently presented news media reliability estimation method that focuses\non modeling outlets and their longitudinal web interactions. Concretely,\nwe assess the classification performance of four reinforcement learning\nstrategies on a large news media hyperlink graph. Our experiments, tar-\ngeting two challenging bias descriptors, factual reporting and political\nbias, showed a significant performance improvement at the source media\nlevel. Additionally, we validate our methods on the CLEF 2023 Check-\nThat!Lab challenge, outperforming the reported results in both, F1-\nscore and the official MAE metric. Furthermore, we contribute by re-\nleasing the largest annotated dataset of news source media, categorized\nwith factual reporting and political bias labels. Our findings suggest that\nprofiling news media sources based on their hyperlink interactions over\ntime is feasible, offering a bird\u2019s-eye view of evolving media landscapes.3\nKeywords: news media profiling \u00b7media bias descriptors \u00b7factual re-\nporting \u00b7political bias\n1 Introduction\nGiven its open and distributed nature, the World Wide Web (WWW) has be-\ncome the main information source worldwide, democratizing content creation\nand making it easy for everybody to share and spread information online. On\nthe bright side, this phenomenon enables a faster dissemination of information\ncompared to what was possible with traditional newspapers, radio, and TV. On\nthe downside, at the moment of removing the \"gate-keeper\" role from traditional\n3github.com/idiap/Factual-Reporting-and-Political-Bias-Web-InteractionsarXiv:2410.17655v1  [cs.AI]  23 Oct 2024\nmedia, it opens the door for additional problems, e.g., the spread of misinfor-\nmation, at breaking-news speed, that can potentially mislead the users and even\nimpact their behavior [25,3].\nThus, while the goal of this democratic channel is to provide users with\nthe necessary tools to acquire greater knowledge about a topic, the reality is\nthat in the way this knowledge (i.e., news) is presented and reported is not\nnecessarily always impartial [2,18], and there is a growing concern regarding the\nbiases of different media outlets when reporting specific events [14]. For example,\nin polarizing topics like politics, many of the news can be biased towards one\npolitical perspective or the other, i.e., political bias , which may influence citizens\u2019\nvoting decisions and preferences of undecided individuals [13].\nTo mitigate the impact of misinformation and to favor critical assessment for\nthe newsreaders, independent bias assessment services like MBFC4and allsides5\nperform information verification. The review process is performed manually by\nprofessionals at the event or article level, clearly this is a challenging schema\nto maintain on the long term given the fast-speed proliferation of both news\nmedia websites and news articles. Automation comes handy to perform certain\nfact-checking tasks, like gathering information (e.g. articles with similar topics,\nmetadata on the media-publisher, etc.); for the more complex parts of the ver-\nification analysis, advances in AI continues pushing the boundaries in order to\nprovide valuable tools (for example, search and retrieval, summarization, trans-\nformers, LM and LLMs). While the latest LLMs performance on several tasks is\nremarkable, they are still prone to carry unauthenticated information [17].\nWhile many existing tools are being adopted to support verification tasks\nat the article level (with and without human supervision), there are very few\nadvances to fully automate news media profiling at the source level (other than\npopularity). Previous research has shown evidence that some news bias descrip-\ntors can be inferred by just inspecting the outlet website metadata [10,16]. Other\napproaches have addressed source reliability, factuality of reporting or politi-\ncal bias, by assembling information from multiple external and social media\nsources,metadataand/orcontent-basedfeatures[4,5,3,22,9,21,7].Unfortunately,\nmethodologies relying on social media metadata can not longer be reproduced\nat scale given the current access restrictions.\nA recent research shifting from the social media and text-based approach, is\npresented in [8]. Burdisso et al., proposed a highly performing and robust graph-\nbased methodology to score news media reliability. Their method considers the\nlongitudinal interactions on the web to learn a reliability value from their source\nneighbors. Based on the research evidence that neighboring properties can be\nspread among news media outlets, we extend their work and we propose to\naddress the following research question: to what extent it is possible to profile\nnews media outlets (i.e., different properties) based solely on their interaction\nwith other media sources? To address this question, in this paper we focus on\ntwo challenging media bias descriptors: factuality of reporting and political bias.\n4https://mediabiasfactcheck.com\n5www.allsides.com\nWe choose to extend Burdisso et al.methodology given that it is both language\nand content independent (political, religious, racial, etc.), it can be applied at a\nlarger scale and their 17k English news outlets dataset is publicly available.\nOur main contributions are as follows: (a) we show that it is possible to\npredict/estimate bias descriptors, i.e., political bias and factual reporting; of\nthe source media based on their interactions with other sources (outperforming\nthe baseline); (b) we validate the robustness of our approach on the publicly\navailable dataset from the CLEF CheckThat! challenge, specifically collected to\nclassify political bias with currently active news outlets, and we established a\nnew SOTA result; (c) we release the biggest dataset at the source media level\nwith standard political bias and factual reporting labels.\n2 Related Work\nThe bias in news media is a pervasive and ubiquitous problem [14,15,25]. The\nneed for applied research on news media descriptors has increased since 2000\ndue to the generalized adoption of social media platforms, and the proliferation\nof tools that facilitate both websites and news-content creation [12,6]. Bias in\nnews media has a wide descriptors spectrum [14,15,27], for example Racial Bias\nrefers to preferences of coverage or not of events related to minorities or group\nof individuals [24]. Gender Bias refers to the inclination towards one gender\nover another, resulting in unequal treatment, coverage and perception [23,1].\nPolitical Bias, refers to partial representation of political issues or tendency to\nfavor a particular political ideology.\nA significantly large NLP community has reported advances on news media\nbias at the article level (i.e., based on content), also referred as bias at the event-\nlabel or a short-term bias on a selected event [14]. However, in this paper, we\ncontribute towards the news media source profiling (i.e., at the source level). We\nfocus our research work on the following two long-term bias descriptors:\nFactual Reporting Recent task challenges, particularly the CheckThat! Lab\nchallenge at CLEF 2023, have addressed the factuality of reporting based on\nthree classes ( High,MixedandLow) at the article level [21]. Submitted mod-\nels range from traditional supervised models (such as SVMs, Random Forest,\ngradient Boost) to Deep Learning-based ones [21,21,19]. Due to the challenging\nnature to perform factuality assessment, graph-based models emerged to address\nthe problem disclosing better performance when combined with text-based ap-\nproaches [11,4,3,22]. Fairbanks et al., [11] proposed a structural model based\non the metadata from the article\u2019s news web links. Their findings revealed that\ncredibility, a descriptor in close relation with factual reporting, is harder to de-\ntermine from merely the content. Baly et al.[3] analyzed the factual reporting\nfocusing on the source media. Their approach used text-based features from ar-\nticles content and metadata including Wikipedia pages, Twitter, URL-related\nfeatures (domain, orthography, char n-grams), and Web traffic (Alexa service).\nAlso targeting the factuality at the media level, Panayotov et al., [22] proposed\nto model the factuality of reporting using graph neural network and similarity\nbetween news media based on their audience overlap. Although the latest mod-\nels revealed significant improvements at the media level, the methods in [3,22]\nrely on the Alexa website ranking and web traffic information, which is now\ndiscontinued.\nMore recent approaches are focusing on state-of-the-art LMs and LLMs, from\nadversarial training, ensemble of models based on RoBERTa or GPTs [20,26]. Li\net al., heuristics on adversarial training revealed the importance of semantics in\nthe title and the summary of the news captured at the beginning and end of the\narticle. Their best performing political inference results from a majority voting\nfrom four implemented models from which, two are RoBERTa-based. Tran et al.,\nexamined the impact of imbalanced training data between High,MixedandLow\nfactual reporting. The authors introduced a RoBERTa-based back-translation\nframework that significantly surpassed the baseline performance. Their approach\nranked among the top three performers at the CheckThat! Lab challenge in 2023.\nTo the best of our knowledge, the state-of-the-art methodology in media profil-\ning, outperforming ensembles of content-based and external data was recently\nintroducedin[8].Burdisso et al.,proposeanhyperlink-interactionsgraphtoinfer\nNews source reliability degree (a continuous value) based on reinforcement learn-\ning techniques. In addition to the standing performance, authors contribute with\nthe largest reported dataset in source media profiling with 17k English-speaking\nnews outlets.\nPolitical Bias In the recent years, the inference of political bias at the outlet\nlevel has been approached by applying SVMs, CatBoost and applied oversam-\npling techniques, mostly enhancing content-features from articles [4,9,2]. Baly et\nal., [4,3] proposed a framework based on SVMs reporting significant results when\ncomplementing content-based data with Wikipedia and social media metadata.\nRecently, Azizov et al., [2] proposed a majority voting ensemble of CatBoost\nmodels and TF-IDF, showing better performance than LM-frameworks at the\nCheckThat! lab challenge at CLEF 2023 [9] given a benchmark dataset with\nthree political classes ( Left,Center,Right). In Panayotov et al.[22], the political\nbias was modeled using a graph neural network augmented with audience/social\nmedia data. Graph-based approaches showed evidence that metadata capturing\ninformation other than the article content improved classification of political\nstance. Given the still open challenge to accurately infer political bias at the\nnews source level, more recent approaches are exploring the pertinence of using\nLMs [26,27]. Tran et al.[26], analyzed and addressed the three-class ( Left,Cen-\nter,Right) imbalance by translating to Spanish and back to English the classes\nwith less articles. Then, they fine-tuned RoBERTa English-large, and performed\na majority voting at the article-level to infer the news source political leaning,\nshowing a significant performance above the baseline. Wessel et al., [27] pro-\nposed a framework using transformers to infer 9 bias descriptors. For the case of\npolitical bias, the original bias annotation provided at the outlet level is trans-\nformed into two classes bias and not-bias. Despite the 2 million political news\narticles used in this work, they were exclusively gathered from the top 11 most\npopular US media outlets. Authors concluded that cognitive and political bias at\nthe content-level are the most challenging bias descriptors to detect, in contrast\nwith for example gender or racial bias.\nAlthough some approaches show significant improvement over majority base-\nlines, the robustness and scalability of the models is not sufficient to consider\nthe factual reporting and political bias problem solved. Contrary to previous\nresearch that depends on content, audience feedback, and/or metadata, in this\npaperweextendaveryrecentworkthatmodelstheprobleminascalablefashion\nrelaying on network interactions among the news sources [8]. Following sections\ndescribe the proposed methodology and obtained results.\n3 Methodology and Strategies\nIn order to validate our research question and based on the evidence presented\nby Burdisso et al.that longitudinal interactions can spread the news media\nreliability degree among their neighbors [8], we extend their work to address\nfactual reporting and political bias.\nThe introduced approach consists of first building a news media graph from\nthe WWW and then applying different reinforcement learning strategies to infer\nthe reliability values. More precisely, constructing a weighted directed graph\nG=\u27e8S, E, w \u27e9where there is an edge (s, s\u2032)\u2208Eif source scontains articles\n(hyper) linked to s\u2032and where the weight w(s, s\u2032)\u2208[0,1]is the proportion of\ntotal hyperlinks in slinked to s\u2032.6In this work, we hypothesize that the political\nbias and factual reporting of sources scan be estimated from the sources it\ninteracts with, by inheriting their properties.\nFollowing the original work in [8], we model the estimation as a Markov\nDecision Process (MDP) \u27e8S, A, P, r \u27e9such that: (1) The set of states Sare all the\nnews outlets websites \u2014 i.e.S=S; (2) The set of actions Acontains only one\nelement,the \"move to a different news media website\" action;(3)Theprobability\nPof moving from the origin stos\u2032will be given by the proportion of hyperlinks\ninsconnecting to s\u2032\u2014i.e.we have P(s, s\u2032) =w(s, s\u2032); and (4) The reward rof\nmoving to another news source ( s\u2032) is determined only by the origin source( s),\nand it will be positive or negative depending on the known property \u2014 e.gr(s) =\n1if we know for this swe have RightorHigh, for political bias or factual\nreporting, respectively; r(s) =\u22121ifsisLeftorLow, for political bias or factual\nreporting,respectively; r(s) = 0otherwise.Finally,theproperty(politicalbiasor\nfactual reporting level) value for all news sources sin the graph will be estimated\nby a function \u03c1(s)following 4 different strategies:\n\u2013 F-property: The property value is proportional to the expected perceived\nreward given by the following Bellman equation where \u03c0is the unique policy\n(i.e. the probability of taking action a\u2208Ain state s) and \u03b3\u2208[0,1)the\n6Note that this simple hyperlink-based representation is also implicitly capturing\ncontent-based references to and from other sources.\ndiscount factor:7\n\u03c1(s) =X\ns\u2032\u2208SP\u03c0(s, s\u2032)[r(s\u2032) +\u03b3\u03c1(s\u2032)] (1)\nThat is, under this strategy, the value of source swill be inherent from the\nsources it connects in the Future.\n\u2013 P-property: The property value is interpreted a proportion of the accumu-\nlated perceived reward, i.e., the value is inherited by the sources that lead to\nit in thePast. The value is thus, giving by the following the reverse Bellman\nequation:\n\u03c1(s) =r(s) +\u03b3X\ns\u2032\u2208SP\u03c0(s\u2032, s)\u03c1(s\u2032) (2)\n\u2013 FP-property: This strategy strategy combines the previous two strategies by\nconsidering Future and Past information. A source sincreases its positive\nvalue \u03c1(s)as more positive sources link to it ( \u03c1+\nP(s)), while losing value as\nit links to more negative sources ( \u03c1\u2212\nF(s)).8Thus, \u03c1(s)is simply defined as:\n\u03c1(s) =\u03c1\u2212\nF(s) +\u03c1+\nP(s) (3)\n\u2013 I-property:Investment Strategy (invest and collect credits) consisting of two\niterative steps, repeated ntimes: (1) all sources invest their property value to\ntheneighboringsourcesproportionallytothestrengthoftheirlinks( w(s, s\u2032))\nfollowing Equation 4, (2) sources collect the credits back proportionally to\nthe investment and update its own property value following Equation 5.\ntotalcredits (s) =X\ns\u2032\u2208Sw(s\u2032, s)\u00b7\u03c1(s\u2032) (4)\n\u03c1(s) =\u03c1(s) +X\ns\u2032\u2208Sw(s, s\u2032)\u00b7credits s(s\u2032) (5)\nwhere credits are distributed among investors s\u2032, in proportion to their con-\ntribution to s, i.e, credits s\u2032(s) =ws\u2032(s)\u00b7totalcredits (s).\n3.1 Datasets\nThere are several attempts to unify existing datasets to assess Bias in news\nmedia. Recently, a unified bias dataset was presented including several Bias\ndescriptors [27], nevertheless, the collection of articles, sentences, comments,\netc., are on one hand targeting rather short-term bias (text-based), and on the\nother hand large part of the data do not have URLs to existing news media\nsources. Recently, [8] released the largest dataset with URLs annotated with\n7The discount factor controls the distance of looking back/forward; \u03b3\u22480focuses\nmostly on present reward r(s), while \u03b3\u22481considers all history/future to compute\n\u03c1(s).\n8Algorithm 1 and Algorithm 2 in [8] detail how these updates are applied.\nTable 1. Label distribution on both datasets.\nDataset Political Bias Factual Rep.\nLeft Center Right Low Mixed High Total\nMBFC (ours) 2078 763 1079 408 1391 2121 3920\nCLEF CheckThat! 272 359 392 - - - 1023\nreliability labels constructed by collecting and consolidating annotations from\ndifferent sources. In this work, we follow a similar process as described by the\nauthors in [8] to build our own dataset with political bias and factual reporting\nannotation, which we refer to as \u201cMBFC\u201d.\nMBFC. Following the methodology described in [8], we crawled 3920 news\nmedia URL domains from the Media Bias/Fact Check (MBFC)4service includ-\ning annotated bias descriptors that are further transformed and normalized into\npolitical and factual reporting labels as follows: for Political bias, the final nor-\nmalized categories are Left, Center, Right ; for the case of Factual Reporting,\nlabels include High(which aggregated high and very high), MixedandLow\n(which aggregates low and very low).\nCLEF CheckThat! . Additionally, in order to compare results with pre-\nviously published approaches, we use the dataset released for the CLEF 2023\nCheckThat! lab which focused on political bias identification. This dataset con-\ntains a total of 1023 news media URL domains with political bias labels crawled\nfromallsides5, a website that gathers news articles with balanced representation\nof the different political perspectives. The data is officially divided into fixed\ntrain, dev, and test set splits containing 817 (Left-216, Center-296 and Right-\n305), 104(Left-31, Center-34 and Right-39) , and 102 (Left-25, Center-29 and\nRight-48) news sources, respectively. More details about the data and the label-\ning process can be found in [9]. Table 1 summarizes the label distribution and\nsize of both introduced datasets.\n4 Experiments and Results\nIn this work we used the graph Gbuilt in [8] consisting of 17K news sources ob-\ntained after processing 100M news articles from Common Crawl News. Following\n[3,8] we report 5-fold cross-validation evaluation results on our MBFC datasets,\nwhereas for CLEF\u2019s CheckThat! we report results on the official test set. In\norder to estimate the factual score of reporting from the graph, we first con-\nvert the factuality/bias ground truth labels from the training set into rewards\nas follows: r(s) = 1if the media label is High/Right ,r(s) =\u22121ifLow/Left ,\nandr(s) = 0otherwise. Then, at inference time, sources sare classified with\nthe label Right/High if\u03c1(s)>0andLeft/Low otherwise. Even though one\nlimitation of the proposed strategies is that they are essentially binaries, in or-\nder to compare results in CheckThat! three-label classification task, we use the\nTable 2. 5-foldcross-validationaverageresultsforPoliticalBiasandFactualReporting\nclassification. The best-performing values are underlined , while the 2nd-best results\nappear in boldfont.\nStrategyF1score\nTask Macro avg. High/Right Low/Left AccuracyFactual Rep.Majority 38.94\u00b10.04 87.88 \u00b10.09 0.00 \u00b10.00 83.84 \u00b10.16\nRandom 36.44\u00b10.88 65.69 \u00b11.64 7.18 \u00b11.45 49.93 \u00b11.69\nF-Factuality 57.60 \u00b14.38 95.00 \u00b10.97 20.19 \u00b17.86 90.60 \u00b11.76\nP-Factuality 85.13 \u00b12.73 98.70 \u00b10.35 71.55 \u00b15.15 97.52 \u00b10.66\nFP-Factuality 71.35 \u00b12.33 96.76 \u00b10.65 45.93 \u00b14.09 93.89 \u00b11.19\nI-Factuality 87.99 \u00b14.60 99.02 \u00b10.43 76.96 \u00b18.79 98.12 \u00b10.81Political BiasMajority 38.04\u00b10.07 0.00 \u00b10.00 76.08 \u00b10.14 65.40 \u00b10.18\nRandom 45.42\u00b11.84 30.29 \u00b12.86 60.55 \u00b11.75 49.65 \u00b11.73\nF-Political 60.42 \u00b13.74 41.56 \u00b16.27 79.29 \u00b11.42 69.44 \u00b12.30\nP-Political 74.08 \u00b12.31 65.80 \u00b13.23 82.36 \u00b11.39 76.73 \u00b11.95\nFP-Political 64.90 \u00b13.15 52.47 \u00b14.82 77.33 \u00b11.94 69.34 \u00b12.55\nI-Political 77.77 \u00b12.45 70.97 \u00b13.39 84.56 \u00b11.54 79.85 \u00b12.12\nofficial dev set to find an \u03f5value to classify sources sas follow: Left/Low if\n\u03c1(s)<\u2212\u03f5;Right/High if\u03c1(s)> \u03f5;Center/Mixed otherwise. More precisely, we\nselected the hyper-parameters \u03f5= 3e\u22123,\u03b3= 0.15(Equation 1, 2, 3), and n= 2\n(Equation 5) after performing a grid search maximizing the Macro avg. F1 score\nwith \u03f5\u2208[1e\u22123,1e\u22121](1e\u22123increments), \u03b3\u2208[0.05,0.95](0.05increments),\nn\u2208[1,10], respectively.\n4.1 Factuality of Reporting\nTable 2 shows the results from the 5-fold cross-validation for Factual Reporting.\nThe baseline for comparison includes Random and Majority class classification.\nTheF-Factuality strategy performed at 57.60 F1-score overall, for the individual\nclassesLowFactual reporting performance is 95.00 F1-score and 20.19 for High\nFactual Reporting. For all cases there is significant improvement with respect to\nthebaselines.For P-FactualityF1-scoreperformanceis85.13,and98.7and71.55\nfor theLowandHighclasses. The significantly high performance reveals that\nindeed the graph with past reward strategy captures close interacting networks\non both sides, Highscore and Lowscore of factual reporting. The strategy FP-\nFactuality performs at 71.35 F1-score, although it outperforms F-Factuality and\nthe baselines, it remains behind P-Factuality. Finally, the I-Factuality strategy\noutperformsalltheotherstrategiesupto87.99F1-score,76.96forclassHighand\n99.02 for the class Low. The results show that for the case of I-Factuality (the\ninvest and collect strategy), the gathered information from the hyperlinks and\nits neighbors can accurately capture the level of factuality, significantly better\nfor the class Low.\nTable 3. ResultsonCLEF\u2019s CheckThat! datasetonPoliticalBiasofnewsmedia.MAE:\nMean Absolute Error. The smaller MAE value translates into better predictions.\nTeam MAE (\u2193)F1score(\u2191)Accuracy (\u2191)\nBaseline [9] 0.902 - -\nAwakened 0.765 - -\nAccenture [26] 0.549 0.625 0.627\nFrank [2] 0.320 0.727 0.725\nF-Political 0.333 0.632 0.667\nP-Political 0.238 0.760 0.762\nFP-Political 0.309 0.670 0.690\nI-Political 0.214 0.784 0.786\nI-Factuality accurately identifies almost all sources with LowFactual Re-\nporting, which is indeed a key contribution of this paper. We assume that high\nperformance of the reward value might be due to capturing unintentionally the\nlifespan of a news media domain, which has been reported as a high contributor\nin the identification of disinformative websites [16]. Both strategies P-Factuality\nandI-factuality are highly performing on F1-score and Accuracy, similarly to\nfindings on Reliabilty of news media in [8], disclosing an accurate profiling of\nBias given only their network interactions overtime.\n4.2 Political Bias\nResults on MBFC . Table 2 shows the 5-fold cross-validation results for F1-\nscore and Accuracy, we included two baselines Random, and Majority class for\ncomparison. For the political leaning the F-Political performs at 60.42 F1-score,\nand 79.29 F1-score for Right at the class level, showing a modest improvement\nover the baseline (76.08). For P-Political the overall F1-score performance is\n74.08, with 65.8 for the class Leftand 82.36 for the class Right. For the com-\nbinedFP-Political the F1-score of 64.90 outperforms the F-Political but does\nnot improve the P-Political performance, for both the overall and the class level,\nwhich indicates that past information contributes more to the predictions. The\nbest performing strategy is I-Political performing at 77.77 F1-score and, 70.97\nand84.56fortheclasses LeftandRightrespectively.Attheclasslevel,ourresults\non political bias show significantly better performance on the class Right. Fig-\nure 1 shows part of the graph for the news media source www.newrepublic.com ,\nwhere the values are estimated with I-political. The size of the node is propor-\ntional to their political bias, as newrepublic predominantly engages with Left-\nwing sources, its final value leaned significantly towards the Left(red).\nResults on the CLEF CheckThat! . Table 3 shows the F1-score performance\nand the official scoring metric MAE (Mean Absolute Error) for the Labs at\nCLEF 2023. The political labels were coded as ordinal values ( Left-0,Center-\n1,Right-2), a smaller MAE value translates into better predictions from the\nFig. 1.Example showing how newrepublic.com relates with neighboring news sources.\nLeftandRightwing sources are colored red and blue respectively, in addition, size of\nthe node reflects the degree of the bias (learned by our I-political strategy). We can\nsee that since newrepublic.com interacts mostly with Left-wing sources, its final bias\ndegree ended up being considerable Left-wing.\nproposed models. The baseline with MAE of 0.902 uses an SVM classification\nmodel based on N-Grams. The top performed participating model [2] achieved\na MAE of 0.320, outperforming the baseline and the other participating models.\nHowever, our proposed strategies ( P-Political and I-Political) outperform the\nbest-performing participating model in all reported metrics the top (MAE, F1-\nscore and Accuracy). The MAE top performance (smaller MAE) indicates that\nthe miss-predictions are less severe (from Centerto the extremes or vice-versa),\notherwise inferences will result on a higher penalization if predicting completely\nopposite extremes Left\u2194Right.\n5 Conclusions\nThis research extends the methodology proposed in [8] by addressing long-term\nnews media profiling, contrasting with approaches focused solely on short-term\nbias. Our experiments on two challenging bias descriptors\u2014factual reporting\nand political bias\u2014utilize four reinforcement learning strategies for classifica-\ntion performance evaluation. We provide compelling evidence supporting the\nlongitudinal view of news media and their web interactions as a robust and scal-\nable proxy for profiling, particularly regarding political bias and factual report-\ning. Concretely, performed experiments show that the proposed approach allows\nsuperior performance in estimating outlet media bias descriptors compared to\nbaseline methods. Furthermore, we present promising results from comparisons\nwith other participating models submitted to the CLEF 2023 CheckThat! lab,\ndesigned for inferring political bias in currently active news outlets. Our ap-\nproach surpasses top results in both F1-score and the official MAE performance\nmeasure, establishing a new SOTA result for this particular task. Finally, as an\nadditional contribution, we release the largest dataset at the source media level,\nannotated with standard political bias and factual reporting labels.\nAs part of future efforts, we aim to investigate the dynamics of political bias\nchanges over time within news media, such as shifts from center to extreme po-\nsitions. Additionally, we plan to explore the integration of other bias descriptors,\nsuch as press freedom, in multi-task bias identification.\nAcknowledgments. This work was supported by CRiTERIA, EU project funded\nunder the Horizon 2020 program, grant agreement number 101021866.\nDisclosure of Interests. The authors have no competing interests.\nReferences\n1. Asr, F.T., Mazraeh, M., Lopes, A., Gautam, V., Gonzales, J., Rao, P., Taboada,\nM.: The gender gap tracker: Using natural language processing to measure gender\nbias in media. PloS one 16(1), e0245533 (2021)\n2. Azizov, D., Liang, S., Nakov, P.: Frank at checkthat! 2023: Detecting the political\nbias of news articles and news media. Working Notes of CLEF (2023)\n3. Baly,R.,Karadzhov,G.,Alexandrov,D., Glass,J.,Nakov,P.:Predictingfactuality\nof reporting and bias of news media sources. In: Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing. pp. 3528\u20133539 (oct 2018)\n4. Baly, R., Karadzhov, G., An, J., Kwak, H., Dinkov, Y., Ali, A., Glass, J., Nakov,\nP.: What was written vs. who read it: News media profiling using text analysis\nand social media context. In: Proceedings of the Association for Computational\nLinguistics (2020). https://doi.org/10.18653/v1/2020.acl-main.308\n5. Baly, R., Karadzhov, G., Saleh, A., Glass, J., Nakov, P.: Multi-task ordinal regres-\nsion for jointly predicting the trustworthiness and the leading political ideology of\nnews media. In: Proceedings of the North American Chapter of the Association for\nComputational Linguistics (2019). https://doi.org/10.18653/v1/N19-1216\n6. Billard, T.J., Moran, R.E.: Designing trust: Design style, political ideology, and\ntrust in \u201cfake\u201d news websites. Digital Journalism 11(3), 519\u2013546 (2023)\n7. Bozhanova, K., Dinkov, Y., Koychev, I., Castaldo, M., Venturini, T., Nakov, P.:\nPredicting the factuality of reporting of news media using observations about user\nattentionintheiryoutubechannels.In:ProceedingsoftheInternationalConference\non Recent Advances in Natural Language Processing. pp. 182\u2013189 (2021)\n8. Burdisso, S., Sanchez-Cortes, D., Villatoro-Tello, E., Motlicek, P.: Reliability esti-\nmation of news media sources: Birds of a feather flock together. In: Proceedings\nof the North American Chapter of the Association for Computational Linguistics\n(2024), https://aclanthology.org/2024.naacl-long.383\n9. Da San Martino, G., Alam, F., Hasanain, M., Nandi, R.N., Azizov, D., Nakov, P.:\nOverview of the clef-2023 checkthat! lab task 3 on political bias of news articles\nand news media. Working Notes of CLEF (2023)\n10. Esteves, D., Reddy, A.J., Chawla, P., Lehmann, J.: Belittling the source: Trust-\nworthiness indicators to obfuscate fake news on the web. In: Proceedings of the\nFirst Workshop on Fact Extraction and VERification (FEVER). pp. 50\u201359 (2018)\n11. Fairbanks, J., Fitch, N., Knauf, N., Briscoe, E.: Credibility assessment in the news:\ndo we need to read. In: Proc. of the MIS2 Workshop held in conjuction with 11th\nInt\u2019l Conf. on Web Search and Data Mining. pp. 799\u2013800. ACM (2018)\n12. Fang, X., Che, S., Mao, M., Zhang, H., Zhao, M., Zhao, X.: Bias of ai-generated\ncontent: an examination of news produced by large language models. Scientific\nReports 14(1), 1\u201320 (2024)\n13. Gezici, G.: Quantifying political bias in news articles (2022)\n14. Hamborg, F.: Media Bias Analysis, pp. 11\u201353. Springer Nature Switzerland, Cham\n(2023). https://doi.org/10.1007/978-3-031-17693-7_2\n15. Hanimann, A., Heimann, A., Hellmueller, L., Trilling, D.: Believing in credibility\nmeasures: reviewing credibility measures in media research from 1951 to 2018.\nInternational journal of communication 17, 214\u2013235 (2023)\n16. Hounsel, A., Holland, J., Kaiser, B., Borgolte, K., Feamster, N., Mayer, J.: Identi-\nfying disinformation websites using infrastructure features. In: USENIX Workshop\non Free and Open Communications on the Internet (FOCI) (2020)\n17. Kamalloo, E., Dziri, N., Clarke, C.L., Rafiei, D.: Evaluating open-domain ques-\ntion answering in the era of large language models. In: Proceedings of the North\nAmericanChapteroftheAssociationforComputationalLinguistics(2023). https:\n//doi.org/10.18653/v1/2023.acl-long.307\n18. Kulshrestha, J., Eslami, M., Messias, J., Zafar, M.B., Ghosh, S., Gummadi, K.P.,\nKarahalios, K.: Search bias quantification: investigating political bias in social me-\ndia and web search. Information Retrieval Journal 22, 188\u2013227 (2019)\n19. Leburu-Dingalo, T., Thuma, E., Motlogelwa, N., Mudongo, M., Mosweunyane, G.:\nUbcs at checkthat! 2023: Stylometric features in detecting factuality of reporting\nof news media. Working Notes of CLEF (2023)\n20. Li, C., Xue, R., Lin, C., Fan, W., Han, X.: Cucplus at checkthat! 2023: text com-\nbination and regularized adversarial training for news media factuality evaluation.\nWorking Notes of CLEF (2023)\n21. Nakov, P., Alam, F., Da San Martino, G., Hasanain, M., Nandi, R., Azizov, D.,\nPanayotov, P.: Overview of the clef-2023 checkthat! lab task 4 on factuality of\nreporting of news media. Working Notes of CLEF (2023)\n22. Panayotov, P., Shukla, U., Sencar, H.T., Nabeel, M., Nakov, P.: Greener: Graph\nneural networks for news media profiling. In: Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Processing. pp. 7470\u20137480 (2022)\n23. Van der Pas, D.J., Aaldering, L.: Gender differences in political media coverage: A\nmeta-analysis. Journal of Communication 70(1), 114\u2013143 (2020)\n24. Pope, D.G., Price, J., Wolfers, J.: Awareness reduces racial bias. Management\nScience 64(11), 4988\u20134995 (2018)\n25. Str\u00f6mb\u00e4ck, J., Tsfati, Y., Boomgaarden, H., Damstra, A., Lindgren, E., Vliegen-\nthart, R., Lindholm, T.: News media trust and its impact on media use: Toward a\nframework for future research. Annals of the International Communication Associ-\nation 44(2), 139\u2013156 (2020). https://doi.org/0.1080/23808985.2020.1755338\n26. Tran, S., Rodrigues, P., Strauss, B., Williams, E.: Accenture at checkthat! 2023:\nLearning to detect factuality levels of news sources. Working Notes of CLEF (2023)\n27. Wessel, M., Horych, T., Ruas, T., Aizawa, A., Gipp, B., Spinde, T.: Introducing\nmbib-the first media bias identification benchmark task and dataset collection.\nIn: Proceedings of the International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval. pp. 2765\u20132774 (2023)", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Mapping the media landscape: predicting factual reporting and political bias through web interactions", "author": ["D S\u00e1nchez-Cort\u00e9s", "S Burdisso", "E Villatoro-Tello"], "pub_year": "2024", "venue": "\u2026 Conference of the Cross \u2026", "abstract": "Bias assessment of news sources is paramount for professionals, organizations, and  researchers who rely on truthful evidence for information gathering and reporting. While certain"}, "filled": false, "gsrank": 1, "pub_url": "https://link.springer.com/chapter/10.1007/978-3-031-71736-9_7", "author_id": ["rL3daVYAAAAJ", "XOD8lrAAAAAJ", "GzaiunYAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:LgsDNoxatAMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26as_sdt%3D0,46&citilm=1&update_op=library_add&info=LgsDNoxatAMJ&ei=-rKsaLDFIqbT6rQPheXy8Aw&json=", "num_citations": 1, "citedby_url": "/scholar?cites=266937836169792302&as_sdt=5,46&sciodt=0,46&hl=en", "url_related_articles": "/scholar?q=related:LgsDNoxatAMJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,46", "eprint_url": "https://arxiv.org/pdf/2410.17655?"}}, {"title": "Mapping the media landscape: predicting factual reporting and political bias through web interactions", "year": "2024", "pdf_data": "Mapping the Media Landscape: Predicting\nFactual Reporting and Political Bias Through\nWeb Interactions\nDairazalia S\u00e1nchez-Cort\u00e9s1, Sergio Burdisso1, Esa\u00fa Villatoro-Tello1, and Petr\nMotlicek1,2\n1Idiap Research Institute, Martigny, Switzerland\n2Brno University of Technology, Brno, Czech Republic\n{dscortes,sergio.burdisso,esau.villatoro,petr.motlicek}@idiap.ch\nAbstract. Bias assessment of news sources is paramount for profes-\nsionals, organizations, and researchers who rely on truthful evidence for\ninformation gathering and reporting. While certain bias indicators are\ndiscernible from content analysis, descriptors like political bias and fake\nnews pose greater challenges. In this paper, we propose an extension to a\nrecently presented news media reliability estimation method that focuses\non modeling outlets and their longitudinal web interactions. Concretely,\nwe assess the classification performance of four reinforcement learning\nstrategies on a large news media hyperlink graph. Our experiments, tar-\ngeting two challenging bias descriptors, factual reporting and political\nbias, showed a significant performance improvement at the source media\nlevel. Additionally, we validate our methods on the CLEF 2023 Check-\nThat!Lab challenge, outperforming the reported results in both, F1-\nscore and the official MAE metric. Furthermore, we contribute by re-\nleasing the largest annotated dataset of news source media, categorized\nwith factual reporting and political bias labels. Our findings suggest that\nprofiling news media sources based on their hyperlink interactions over\ntime is feasible, offering a bird\u2019s-eye view of evolving media landscapes.3\nKeywords: news media profiling \u00b7media bias descriptors \u00b7factual re-\nporting \u00b7political bias\n1 Introduction\nGiven its open and distributed nature, the World Wide Web (WWW) has be-\ncome the main information source worldwide, democratizing content creation\nand making it easy for everybody to share and spread information online. On\nthe bright side, this phenomenon enables a faster dissemination of information\ncompared to what was possible with traditional newspapers, radio, and TV. On\nthe downside, at the moment of removing the \"gate-keeper\" role from traditional\n3github.com/idiap/Factual-Reporting-and-Political-Bias-Web-InteractionsarXiv:2410.17655v1  [cs.AI]  23 Oct 2024\nmedia, it opens the door for additional problems, e.g., the spread of misinfor-\nmation, at breaking-news speed, that can potentially mislead the users and even\nimpact their behavior [25,3].\nThus, while the goal of this democratic channel is to provide users with\nthe necessary tools to acquire greater knowledge about a topic, the reality is\nthat in the way this knowledge (i.e., news) is presented and reported is not\nnecessarily always impartial [2,18], and there is a growing concern regarding the\nbiases of different media outlets when reporting specific events [14]. For example,\nin polarizing topics like politics, many of the news can be biased towards one\npolitical perspective or the other, i.e., political bias , which may influence citizens\u2019\nvoting decisions and preferences of undecided individuals [13].\nTo mitigate the impact of misinformation and to favor critical assessment for\nthe newsreaders, independent bias assessment services like MBFC4and allsides5\nperform information verification. The review process is performed manually by\nprofessionals at the event or article level, clearly this is a challenging schema\nto maintain on the long term given the fast-speed proliferation of both news\nmedia websites and news articles. Automation comes handy to perform certain\nfact-checking tasks, like gathering information (e.g. articles with similar topics,\nmetadata on the media-publisher, etc.); for the more complex parts of the ver-\nification analysis, advances in AI continues pushing the boundaries in order to\nprovide valuable tools (for example, search and retrieval, summarization, trans-\nformers, LM and LLMs). While the latest LLMs performance on several tasks is\nremarkable, they are still prone to carry unauthenticated information [17].\nWhile many existing tools are being adopted to support verification tasks\nat the article level (with and without human supervision), there are very few\nadvances to fully automate news media profiling at the source level (other than\npopularity). Previous research has shown evidence that some news bias descrip-\ntors can be inferred by just inspecting the outlet website metadata [10,16]. Other\napproaches have addressed source reliability, factuality of reporting or politi-\ncal bias, by assembling information from multiple external and social media\nsources,metadataand/orcontent-basedfeatures[4,5,3,22,9,21,7].Unfortunately,\nmethodologies relying on social media metadata can not longer be reproduced\nat scale given the current access restrictions.\nA recent research shifting from the social media and text-based approach, is\npresented in [8]. Burdisso et al., proposed a highly performing and robust graph-\nbased methodology to score news media reliability. Their method considers the\nlongitudinal interactions on the web to learn a reliability value from their source\nneighbors. Based on the research evidence that neighboring properties can be\nspread among news media outlets, we extend their work and we propose to\naddress the following research question: to what extent it is possible to profile\nnews media outlets (i.e., different properties) based solely on their interaction\nwith other media sources? To address this question, in this paper we focus on\ntwo challenging media bias descriptors: factuality of reporting and political bias.\n4https://mediabiasfactcheck.com\n5www.allsides.com\nWe choose to extend Burdisso et al.methodology given that it is both language\nand content independent (political, religious, racial, etc.), it can be applied at a\nlarger scale and their 17k English news outlets dataset is publicly available.\nOur main contributions are as follows: (a) we show that it is possible to\npredict/estimate bias descriptors, i.e., political bias and factual reporting; of\nthe source media based on their interactions with other sources (outperforming\nthe baseline); (b) we validate the robustness of our approach on the publicly\navailable dataset from the CLEF CheckThat! challenge, specifically collected to\nclassify political bias with currently active news outlets, and we established a\nnew SOTA result; (c) we release the biggest dataset at the source media level\nwith standard political bias and factual reporting labels.\n2 Related Work\nThe bias in news media is a pervasive and ubiquitous problem [14,15,25]. The\nneed for applied research on news media descriptors has increased since 2000\ndue to the generalized adoption of social media platforms, and the proliferation\nof tools that facilitate both websites and news-content creation [12,6]. Bias in\nnews media has a wide descriptors spectrum [14,15,27], for example Racial Bias\nrefers to preferences of coverage or not of events related to minorities or group\nof individuals [24]. Gender Bias refers to the inclination towards one gender\nover another, resulting in unequal treatment, coverage and perception [23,1].\nPolitical Bias, refers to partial representation of political issues or tendency to\nfavor a particular political ideology.\nA significantly large NLP community has reported advances on news media\nbias at the article level (i.e., based on content), also referred as bias at the event-\nlabel or a short-term bias on a selected event [14]. However, in this paper, we\ncontribute towards the news media source profiling (i.e., at the source level). We\nfocus our research work on the following two long-term bias descriptors:\nFactual Reporting Recent task challenges, particularly the CheckThat! Lab\nchallenge at CLEF 2023, have addressed the factuality of reporting based on\nthree classes ( High,MixedandLow) at the article level [21]. Submitted mod-\nels range from traditional supervised models (such as SVMs, Random Forest,\ngradient Boost) to Deep Learning-based ones [21,21,19]. Due to the challenging\nnature to perform factuality assessment, graph-based models emerged to address\nthe problem disclosing better performance when combined with text-based ap-\nproaches [11,4,3,22]. Fairbanks et al., [11] proposed a structural model based\non the metadata from the article\u2019s news web links. Their findings revealed that\ncredibility, a descriptor in close relation with factual reporting, is harder to de-\ntermine from merely the content. Baly et al.[3] analyzed the factual reporting\nfocusing on the source media. Their approach used text-based features from ar-\nticles content and metadata including Wikipedia pages, Twitter, URL-related\nfeatures (domain, orthography, char n-grams), and Web traffic (Alexa service).\nAlso targeting the factuality at the media level, Panayotov et al., [22] proposed\nto model the factuality of reporting using graph neural network and similarity\nbetween news media based on their audience overlap. Although the latest mod-\nels revealed significant improvements at the media level, the methods in [3,22]\nrely on the Alexa website ranking and web traffic information, which is now\ndiscontinued.\nMore recent approaches are focusing on state-of-the-art LMs and LLMs, from\nadversarial training, ensemble of models based on RoBERTa or GPTs [20,26]. Li\net al., heuristics on adversarial training revealed the importance of semantics in\nthe title and the summary of the news captured at the beginning and end of the\narticle. Their best performing political inference results from a majority voting\nfrom four implemented models from which, two are RoBERTa-based. Tran et al.,\nexamined the impact of imbalanced training data between High,MixedandLow\nfactual reporting. The authors introduced a RoBERTa-based back-translation\nframework that significantly surpassed the baseline performance. Their approach\nranked among the top three performers at the CheckThat! Lab challenge in 2023.\nTo the best of our knowledge, the state-of-the-art methodology in media profil-\ning, outperforming ensembles of content-based and external data was recently\nintroducedin[8].Burdisso et al.,proposeanhyperlink-interactionsgraphtoinfer\nNews source reliability degree (a continuous value) based on reinforcement learn-\ning techniques. In addition to the standing performance, authors contribute with\nthe largest reported dataset in source media profiling with 17k English-speaking\nnews outlets.\nPolitical Bias In the recent years, the inference of political bias at the outlet\nlevel has been approached by applying SVMs, CatBoost and applied oversam-\npling techniques, mostly enhancing content-features from articles [4,9,2]. Baly et\nal., [4,3] proposed a framework based on SVMs reporting significant results when\ncomplementing content-based data with Wikipedia and social media metadata.\nRecently, Azizov et al., [2] proposed a majority voting ensemble of CatBoost\nmodels and TF-IDF, showing better performance than LM-frameworks at the\nCheckThat! lab challenge at CLEF 2023 [9] given a benchmark dataset with\nthree political classes ( Left,Center,Right). In Panayotov et al.[22], the political\nbias was modeled using a graph neural network augmented with audience/social\nmedia data. Graph-based approaches showed evidence that metadata capturing\ninformation other than the article content improved classification of political\nstance. Given the still open challenge to accurately infer political bias at the\nnews source level, more recent approaches are exploring the pertinence of using\nLMs [26,27]. Tran et al.[26], analyzed and addressed the three-class ( Left,Cen-\nter,Right) imbalance by translating to Spanish and back to English the classes\nwith less articles. Then, they fine-tuned RoBERTa English-large, and performed\na majority voting at the article-level to infer the news source political leaning,\nshowing a significant performance above the baseline. Wessel et al., [27] pro-\nposed a framework using transformers to infer 9 bias descriptors. For the case of\npolitical bias, the original bias annotation provided at the outlet level is trans-\nformed into two classes bias and not-bias. Despite the 2 million political news\narticles used in this work, they were exclusively gathered from the top 11 most\npopular US media outlets. Authors concluded that cognitive and political bias at\nthe content-level are the most challenging bias descriptors to detect, in contrast\nwith for example gender or racial bias.\nAlthough some approaches show significant improvement over majority base-\nlines, the robustness and scalability of the models is not sufficient to consider\nthe factual reporting and political bias problem solved. Contrary to previous\nresearch that depends on content, audience feedback, and/or metadata, in this\npaperweextendaveryrecentworkthatmodelstheprobleminascalablefashion\nrelaying on network interactions among the news sources [8]. Following sections\ndescribe the proposed methodology and obtained results.\n3 Methodology and Strategies\nIn order to validate our research question and based on the evidence presented\nby Burdisso et al.that longitudinal interactions can spread the news media\nreliability degree among their neighbors [8], we extend their work to address\nfactual reporting and political bias.\nThe introduced approach consists of first building a news media graph from\nthe WWW and then applying different reinforcement learning strategies to infer\nthe reliability values. More precisely, constructing a weighted directed graph\nG=\u27e8S, E, w \u27e9where there is an edge (s, s\u2032)\u2208Eif source scontains articles\n(hyper) linked to s\u2032and where the weight w(s, s\u2032)\u2208[0,1]is the proportion of\ntotal hyperlinks in slinked to s\u2032.6In this work, we hypothesize that the political\nbias and factual reporting of sources scan be estimated from the sources it\ninteracts with, by inheriting their properties.\nFollowing the original work in [8], we model the estimation as a Markov\nDecision Process (MDP) \u27e8S, A, P, r \u27e9such that: (1) The set of states Sare all the\nnews outlets websites \u2014 i.e.S=S; (2) The set of actions Acontains only one\nelement,the \"move to a different news media website\" action;(3)Theprobability\nPof moving from the origin stos\u2032will be given by the proportion of hyperlinks\ninsconnecting to s\u2032\u2014i.e.we have P(s, s\u2032) =w(s, s\u2032); and (4) The reward rof\nmoving to another news source ( s\u2032) is determined only by the origin source( s),\nand it will be positive or negative depending on the known property \u2014 e.gr(s) =\n1if we know for this swe have RightorHigh, for political bias or factual\nreporting, respectively; r(s) =\u22121ifsisLeftorLow, for political bias or factual\nreporting,respectively; r(s) = 0otherwise.Finally,theproperty(politicalbiasor\nfactual reporting level) value for all news sources sin the graph will be estimated\nby a function \u03c1(s)following 4 different strategies:\n\u2013 F-property: The property value is proportional to the expected perceived\nreward given by the following Bellman equation where \u03c0is the unique policy\n(i.e. the probability of taking action a\u2208Ain state s) and \u03b3\u2208[0,1)the\n6Note that this simple hyperlink-based representation is also implicitly capturing\ncontent-based references to and from other sources.\ndiscount factor:7\n\u03c1(s) =X\ns\u2032\u2208SP\u03c0(s, s\u2032)[r(s\u2032) +\u03b3\u03c1(s\u2032)] (1)\nThat is, under this strategy, the value of source swill be inherent from the\nsources it connects in the Future.\n\u2013 P-property: The property value is interpreted a proportion of the accumu-\nlated perceived reward, i.e., the value is inherited by the sources that lead to\nit in thePast. The value is thus, giving by the following the reverse Bellman\nequation:\n\u03c1(s) =r(s) +\u03b3X\ns\u2032\u2208SP\u03c0(s\u2032, s)\u03c1(s\u2032) (2)\n\u2013 FP-property: This strategy strategy combines the previous two strategies by\nconsidering Future and Past information. A source sincreases its positive\nvalue \u03c1(s)as more positive sources link to it ( \u03c1+\nP(s)), while losing value as\nit links to more negative sources ( \u03c1\u2212\nF(s)).8Thus, \u03c1(s)is simply defined as:\n\u03c1(s) =\u03c1\u2212\nF(s) +\u03c1+\nP(s) (3)\n\u2013 I-property:Investment Strategy (invest and collect credits) consisting of two\niterative steps, repeated ntimes: (1) all sources invest their property value to\ntheneighboringsourcesproportionallytothestrengthoftheirlinks( w(s, s\u2032))\nfollowing Equation 4, (2) sources collect the credits back proportionally to\nthe investment and update its own property value following Equation 5.\ntotalcredits (s) =X\ns\u2032\u2208Sw(s\u2032, s)\u00b7\u03c1(s\u2032) (4)\n\u03c1(s) =\u03c1(s) +X\ns\u2032\u2208Sw(s, s\u2032)\u00b7credits s(s\u2032) (5)\nwhere credits are distributed among investors s\u2032, in proportion to their con-\ntribution to s, i.e, credits s\u2032(s) =ws\u2032(s)\u00b7totalcredits (s).\n3.1 Datasets\nThere are several attempts to unify existing datasets to assess Bias in news\nmedia. Recently, a unified bias dataset was presented including several Bias\ndescriptors [27], nevertheless, the collection of articles, sentences, comments,\netc., are on one hand targeting rather short-term bias (text-based), and on the\nother hand large part of the data do not have URLs to existing news media\nsources. Recently, [8] released the largest dataset with URLs annotated with\n7The discount factor controls the distance of looking back/forward; \u03b3\u22480focuses\nmostly on present reward r(s), while \u03b3\u22481considers all history/future to compute\n\u03c1(s).\n8Algorithm 1 and Algorithm 2 in [8] detail how these updates are applied.\nTable 1. Label distribution on both datasets.\nDataset Political Bias Factual Rep.\nLeft Center Right Low Mixed High Total\nMBFC (ours) 2078 763 1079 408 1391 2121 3920\nCLEF CheckThat! 272 359 392 - - - 1023\nreliability labels constructed by collecting and consolidating annotations from\ndifferent sources. In this work, we follow a similar process as described by the\nauthors in [8] to build our own dataset with political bias and factual reporting\nannotation, which we refer to as \u201cMBFC\u201d.\nMBFC. Following the methodology described in [8], we crawled 3920 news\nmedia URL domains from the Media Bias/Fact Check (MBFC)4service includ-\ning annotated bias descriptors that are further transformed and normalized into\npolitical and factual reporting labels as follows: for Political bias, the final nor-\nmalized categories are Left, Center, Right ; for the case of Factual Reporting,\nlabels include High(which aggregated high and very high), MixedandLow\n(which aggregates low and very low).\nCLEF CheckThat! . Additionally, in order to compare results with pre-\nviously published approaches, we use the dataset released for the CLEF 2023\nCheckThat! lab which focused on political bias identification. This dataset con-\ntains a total of 1023 news media URL domains with political bias labels crawled\nfromallsides5, a website that gathers news articles with balanced representation\nof the different political perspectives. The data is officially divided into fixed\ntrain, dev, and test set splits containing 817 (Left-216, Center-296 and Right-\n305), 104(Left-31, Center-34 and Right-39) , and 102 (Left-25, Center-29 and\nRight-48) news sources, respectively. More details about the data and the label-\ning process can be found in [9]. Table 1 summarizes the label distribution and\nsize of both introduced datasets.\n4 Experiments and Results\nIn this work we used the graph Gbuilt in [8] consisting of 17K news sources ob-\ntained after processing 100M news articles from Common Crawl News. Following\n[3,8] we report 5-fold cross-validation evaluation results on our MBFC datasets,\nwhereas for CLEF\u2019s CheckThat! we report results on the official test set. In\norder to estimate the factual score of reporting from the graph, we first con-\nvert the factuality/bias ground truth labels from the training set into rewards\nas follows: r(s) = 1if the media label is High/Right ,r(s) =\u22121ifLow/Left ,\nandr(s) = 0otherwise. Then, at inference time, sources sare classified with\nthe label Right/High if\u03c1(s)>0andLeft/Low otherwise. Even though one\nlimitation of the proposed strategies is that they are essentially binaries, in or-\nder to compare results in CheckThat! three-label classification task, we use the\nTable 2. 5-foldcross-validationaverageresultsforPoliticalBiasandFactualReporting\nclassification. The best-performing values are underlined , while the 2nd-best results\nappear in boldfont.\nStrategyF1score\nTask Macro avg. High/Right Low/Left AccuracyFactual Rep.Majority 38.94\u00b10.04 87.88 \u00b10.09 0.00 \u00b10.00 83.84 \u00b10.16\nRandom 36.44\u00b10.88 65.69 \u00b11.64 7.18 \u00b11.45 49.93 \u00b11.69\nF-Factuality 57.60 \u00b14.38 95.00 \u00b10.97 20.19 \u00b17.86 90.60 \u00b11.76\nP-Factuality 85.13 \u00b12.73 98.70 \u00b10.35 71.55 \u00b15.15 97.52 \u00b10.66\nFP-Factuality 71.35 \u00b12.33 96.76 \u00b10.65 45.93 \u00b14.09 93.89 \u00b11.19\nI-Factuality 87.99 \u00b14.60 99.02 \u00b10.43 76.96 \u00b18.79 98.12 \u00b10.81Political BiasMajority 38.04\u00b10.07 0.00 \u00b10.00 76.08 \u00b10.14 65.40 \u00b10.18\nRandom 45.42\u00b11.84 30.29 \u00b12.86 60.55 \u00b11.75 49.65 \u00b11.73\nF-Political 60.42 \u00b13.74 41.56 \u00b16.27 79.29 \u00b11.42 69.44 \u00b12.30\nP-Political 74.08 \u00b12.31 65.80 \u00b13.23 82.36 \u00b11.39 76.73 \u00b11.95\nFP-Political 64.90 \u00b13.15 52.47 \u00b14.82 77.33 \u00b11.94 69.34 \u00b12.55\nI-Political 77.77 \u00b12.45 70.97 \u00b13.39 84.56 \u00b11.54 79.85 \u00b12.12\nofficial dev set to find an \u03f5value to classify sources sas follow: Left/Low if\n\u03c1(s)<\u2212\u03f5;Right/High if\u03c1(s)> \u03f5;Center/Mixed otherwise. More precisely, we\nselected the hyper-parameters \u03f5= 3e\u22123,\u03b3= 0.15(Equation 1, 2, 3), and n= 2\n(Equation 5) after performing a grid search maximizing the Macro avg. F1 score\nwith \u03f5\u2208[1e\u22123,1e\u22121](1e\u22123increments), \u03b3\u2208[0.05,0.95](0.05increments),\nn\u2208[1,10], respectively.\n4.1 Factuality of Reporting\nTable 2 shows the results from the 5-fold cross-validation for Factual Reporting.\nThe baseline for comparison includes Random and Majority class classification.\nTheF-Factuality strategy performed at 57.60 F1-score overall, for the individual\nclassesLowFactual reporting performance is 95.00 F1-score and 20.19 for High\nFactual Reporting. For all cases there is significant improvement with respect to\nthebaselines.For P-FactualityF1-scoreperformanceis85.13,and98.7and71.55\nfor theLowandHighclasses. The significantly high performance reveals that\nindeed the graph with past reward strategy captures close interacting networks\non both sides, Highscore and Lowscore of factual reporting. The strategy FP-\nFactuality performs at 71.35 F1-score, although it outperforms F-Factuality and\nthe baselines, it remains behind P-Factuality. Finally, the I-Factuality strategy\noutperformsalltheotherstrategiesupto87.99F1-score,76.96forclassHighand\n99.02 for the class Low. The results show that for the case of I-Factuality (the\ninvest and collect strategy), the gathered information from the hyperlinks and\nits neighbors can accurately capture the level of factuality, significantly better\nfor the class Low.\nTable 3. ResultsonCLEF\u2019s CheckThat! datasetonPoliticalBiasofnewsmedia.MAE:\nMean Absolute Error. The smaller MAE value translates into better predictions.\nTeam MAE (\u2193)F1score(\u2191)Accuracy (\u2191)\nBaseline [9] 0.902 - -\nAwakened 0.765 - -\nAccenture [26] 0.549 0.625 0.627\nFrank [2] 0.320 0.727 0.725\nF-Political 0.333 0.632 0.667\nP-Political 0.238 0.760 0.762\nFP-Political 0.309 0.670 0.690\nI-Political 0.214 0.784 0.786\nI-Factuality accurately identifies almost all sources with LowFactual Re-\nporting, which is indeed a key contribution of this paper. We assume that high\nperformance of the reward value might be due to capturing unintentionally the\nlifespan of a news media domain, which has been reported as a high contributor\nin the identification of disinformative websites [16]. Both strategies P-Factuality\nandI-factuality are highly performing on F1-score and Accuracy, similarly to\nfindings on Reliabilty of news media in [8], disclosing an accurate profiling of\nBias given only their network interactions overtime.\n4.2 Political Bias\nResults on MBFC . Table 2 shows the 5-fold cross-validation results for F1-\nscore and Accuracy, we included two baselines Random, and Majority class for\ncomparison. For the political leaning the F-Political performs at 60.42 F1-score,\nand 79.29 F1-score for Right at the class level, showing a modest improvement\nover the baseline (76.08). For P-Political the overall F1-score performance is\n74.08, with 65.8 for the class Leftand 82.36 for the class Right. For the com-\nbinedFP-Political the F1-score of 64.90 outperforms the F-Political but does\nnot improve the P-Political performance, for both the overall and the class level,\nwhich indicates that past information contributes more to the predictions. The\nbest performing strategy is I-Political performing at 77.77 F1-score and, 70.97\nand84.56fortheclasses LeftandRightrespectively.Attheclasslevel,ourresults\non political bias show significantly better performance on the class Right. Fig-\nure 1 shows part of the graph for the news media source www.newrepublic.com ,\nwhere the values are estimated with I-political. The size of the node is propor-\ntional to their political bias, as newrepublic predominantly engages with Left-\nwing sources, its final value leaned significantly towards the Left(red).\nResults on the CLEF CheckThat! . Table 3 shows the F1-score performance\nand the official scoring metric MAE (Mean Absolute Error) for the Labs at\nCLEF 2023. The political labels were coded as ordinal values ( Left-0,Center-\n1,Right-2), a smaller MAE value translates into better predictions from the\nFig. 1.Example showing how newrepublic.com relates with neighboring news sources.\nLeftandRightwing sources are colored red and blue respectively, in addition, size of\nthe node reflects the degree of the bias (learned by our I-political strategy). We can\nsee that since newrepublic.com interacts mostly with Left-wing sources, its final bias\ndegree ended up being considerable Left-wing.\nproposed models. The baseline with MAE of 0.902 uses an SVM classification\nmodel based on N-Grams. The top performed participating model [2] achieved\na MAE of 0.320, outperforming the baseline and the other participating models.\nHowever, our proposed strategies ( P-Political and I-Political) outperform the\nbest-performing participating model in all reported metrics the top (MAE, F1-\nscore and Accuracy). The MAE top performance (smaller MAE) indicates that\nthe miss-predictions are less severe (from Centerto the extremes or vice-versa),\notherwise inferences will result on a higher penalization if predicting completely\nopposite extremes Left\u2194Right.\n5 Conclusions\nThis research extends the methodology proposed in [8] by addressing long-term\nnews media profiling, contrasting with approaches focused solely on short-term\nbias. Our experiments on two challenging bias descriptors\u2014factual reporting\nand political bias\u2014utilize four reinforcement learning strategies for classifica-\ntion performance evaluation. We provide compelling evidence supporting the\nlongitudinal view of news media and their web interactions as a robust and scal-\nable proxy for profiling, particularly regarding political bias and factual report-\ning. Concretely, performed experiments show that the proposed approach allows\nsuperior performance in estimating outlet media bias descriptors compared to\nbaseline methods. Furthermore, we present promising results from comparisons\nwith other participating models submitted to the CLEF 2023 CheckThat! lab,\ndesigned for inferring political bias in currently active news outlets. Our ap-\nproach surpasses top results in both F1-score and the official MAE performance\nmeasure, establishing a new SOTA result for this particular task. Finally, as an\nadditional contribution, we release the largest dataset at the source media level,\nannotated with standard political bias and factual reporting labels.\nAs part of future efforts, we aim to investigate the dynamics of political bias\nchanges over time within news media, such as shifts from center to extreme po-\nsitions. Additionally, we plan to explore the integration of other bias descriptors,\nsuch as press freedom, in multi-task bias identification.\nAcknowledgments. This work was supported by CRiTERIA, EU project funded\nunder the Horizon 2020 program, grant agreement number 101021866.\nDisclosure of Interests. The authors have no competing interests.\nReferences\n1. Asr, F.T., Mazraeh, M., Lopes, A., Gautam, V., Gonzales, J., Rao, P., Taboada,\nM.: The gender gap tracker: Using natural language processing to measure gender\nbias in media. PloS one 16(1), e0245533 (2021)\n2. Azizov, D., Liang, S., Nakov, P.: Frank at checkthat! 2023: Detecting the political\nbias of news articles and news media. Working Notes of CLEF (2023)\n3. Baly,R.,Karadzhov,G.,Alexandrov,D., Glass,J.,Nakov,P.:Predictingfactuality\nof reporting and bias of news media sources. In: Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing. pp. 3528\u20133539 (oct 2018)\n4. Baly, R., Karadzhov, G., An, J., Kwak, H., Dinkov, Y., Ali, A., Glass, J., Nakov,\nP.: What was written vs. who read it: News media profiling using text analysis\nand social media context. In: Proceedings of the Association for Computational\nLinguistics (2020). https://doi.org/10.18653/v1/2020.acl-main.308\n5. Baly, R., Karadzhov, G., Saleh, A., Glass, J., Nakov, P.: Multi-task ordinal regres-\nsion for jointly predicting the trustworthiness and the leading political ideology of\nnews media. In: Proceedings of the North American Chapter of the Association for\nComputational Linguistics (2019). https://doi.org/10.18653/v1/N19-1216\n6. Billard, T.J., Moran, R.E.: Designing trust: Design style, political ideology, and\ntrust in \u201cfake\u201d news websites. Digital Journalism 11(3), 519\u2013546 (2023)\n7. Bozhanova, K., Dinkov, Y., Koychev, I., Castaldo, M., Venturini, T., Nakov, P.:\nPredicting the factuality of reporting of news media using observations about user\nattentionintheiryoutubechannels.In:ProceedingsoftheInternationalConference\non Recent Advances in Natural Language Processing. pp. 182\u2013189 (2021)\n8. Burdisso, S., Sanchez-Cortes, D., Villatoro-Tello, E., Motlicek, P.: Reliability esti-\nmation of news media sources: Birds of a feather flock together. In: Proceedings\nof the North American Chapter of the Association for Computational Linguistics\n(2024), https://aclanthology.org/2024.naacl-long.383\n9. Da San Martino, G., Alam, F., Hasanain, M., Nandi, R.N., Azizov, D., Nakov, P.:\nOverview of the clef-2023 checkthat! lab task 3 on political bias of news articles\nand news media. Working Notes of CLEF (2023)\n10. Esteves, D., Reddy, A.J., Chawla, P., Lehmann, J.: Belittling the source: Trust-\nworthiness indicators to obfuscate fake news on the web. In: Proceedings of the\nFirst Workshop on Fact Extraction and VERification (FEVER). pp. 50\u201359 (2018)\n11. Fairbanks, J., Fitch, N., Knauf, N., Briscoe, E.: Credibility assessment in the news:\ndo we need to read. In: Proc. of the MIS2 Workshop held in conjuction with 11th\nInt\u2019l Conf. on Web Search and Data Mining. pp. 799\u2013800. ACM (2018)\n12. Fang, X., Che, S., Mao, M., Zhang, H., Zhao, M., Zhao, X.: Bias of ai-generated\ncontent: an examination of news produced by large language models. Scientific\nReports 14(1), 1\u201320 (2024)\n13. Gezici, G.: Quantifying political bias in news articles (2022)\n14. Hamborg, F.: Media Bias Analysis, pp. 11\u201353. Springer Nature Switzerland, Cham\n(2023). https://doi.org/10.1007/978-3-031-17693-7_2\n15. Hanimann, A., Heimann, A., Hellmueller, L., Trilling, D.: Believing in credibility\nmeasures: reviewing credibility measures in media research from 1951 to 2018.\nInternational journal of communication 17, 214\u2013235 (2023)\n16. Hounsel, A., Holland, J., Kaiser, B., Borgolte, K., Feamster, N., Mayer, J.: Identi-\nfying disinformation websites using infrastructure features. In: USENIX Workshop\non Free and Open Communications on the Internet (FOCI) (2020)\n17. Kamalloo, E., Dziri, N., Clarke, C.L., Rafiei, D.: Evaluating open-domain ques-\ntion answering in the era of large language models. In: Proceedings of the North\nAmericanChapteroftheAssociationforComputationalLinguistics(2023). https:\n//doi.org/10.18653/v1/2023.acl-long.307\n18. Kulshrestha, J., Eslami, M., Messias, J., Zafar, M.B., Ghosh, S., Gummadi, K.P.,\nKarahalios, K.: Search bias quantification: investigating political bias in social me-\ndia and web search. Information Retrieval Journal 22, 188\u2013227 (2019)\n19. Leburu-Dingalo, T., Thuma, E., Motlogelwa, N., Mudongo, M., Mosweunyane, G.:\nUbcs at checkthat! 2023: Stylometric features in detecting factuality of reporting\nof news media. Working Notes of CLEF (2023)\n20. Li, C., Xue, R., Lin, C., Fan, W., Han, X.: Cucplus at checkthat! 2023: text com-\nbination and regularized adversarial training for news media factuality evaluation.\nWorking Notes of CLEF (2023)\n21. Nakov, P., Alam, F., Da San Martino, G., Hasanain, M., Nandi, R., Azizov, D.,\nPanayotov, P.: Overview of the clef-2023 checkthat! lab task 4 on factuality of\nreporting of news media. Working Notes of CLEF (2023)\n22. Panayotov, P., Shukla, U., Sencar, H.T., Nabeel, M., Nakov, P.: Greener: Graph\nneural networks for news media profiling. In: Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Processing. pp. 7470\u20137480 (2022)\n23. Van der Pas, D.J., Aaldering, L.: Gender differences in political media coverage: A\nmeta-analysis. Journal of Communication 70(1), 114\u2013143 (2020)\n24. Pope, D.G., Price, J., Wolfers, J.: Awareness reduces racial bias. Management\nScience 64(11), 4988\u20134995 (2018)\n25. Str\u00f6mb\u00e4ck, J., Tsfati, Y., Boomgaarden, H., Damstra, A., Lindgren, E., Vliegen-\nthart, R., Lindholm, T.: News media trust and its impact on media use: Toward a\nframework for future research. Annals of the International Communication Associ-\nation 44(2), 139\u2013156 (2020). https://doi.org/0.1080/23808985.2020.1755338\n26. Tran, S., Rodrigues, P., Strauss, B., Williams, E.: Accenture at checkthat! 2023:\nLearning to detect factuality levels of news sources. Working Notes of CLEF (2023)\n27. Wessel, M., Horych, T., Ruas, T., Aizawa, A., Gipp, B., Spinde, T.: Introducing\nmbib-the first media bias identification benchmark task and dataset collection.\nIn: Proceedings of the International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval. pp. 2765\u20132774 (2023)", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Mapping the media landscape: predicting factual reporting and political bias through web interactions", "author": ["D S\u00e1nchez-Cort\u00e9s", "S Burdisso", "E Villatoro-Tello"], "pub_year": "2024", "venue": "\u2026 Conference of the Cross \u2026", "abstract": "Bias assessment of news sources is paramount for professionals, organizations, and  researchers who rely on truthful evidence for information gathering and reporting. While certain"}, "filled": false, "gsrank": 1, "pub_url": "https://link.springer.com/chapter/10.1007/978-3-031-71736-9_7", "author_id": ["rL3daVYAAAAJ", "XOD8lrAAAAAJ", "GzaiunYAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:LgsDNoxatAMJ:scholar.google.com/&output=cite&scirp=0&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=LgsDNoxatAMJ&ei=BLWsaNaSEPnSieoPxKLpgQ0&json=", "num_citations": 1, "citedby_url": "/scholar?cites=266937836169792302&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:LgsDNoxatAMJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2410.17655?"}}, {"title": "References to unbiased sources increase the helpfulness of community fact-checks", "year": "2025", "pdf_data": "References to unbiased sources \nincrease the helpfulness of \ncommunity fact-checks\nKirill Solovev & Nicolas Pr\u00f6llochs\uf02a\nCommunity-based fact-checking is a promising approach to address misinformation on social media \nat scale. However, an understanding of what makes community-created fact-checks helpful to users \nis still in its infancy. In this paper, we analyze the determinants of the helpfulness of community-\ncreated fact-checks. For this purpose, we draw upon a unique dataset of real-world community-created \nfact-checks and helpfulness ratings from X\u2019s (formerly Twitter) Community Notes platform. Our \nempirical analysis implies that the key determinant of helpfulness in community-based fact-checking \nis whether users provide links to external sources to underpin their assertions. On average, the odds \nfor community-created fact-checks to be perceived as helpful are 2.33 times higher if they provide \nlinks to external sources. Furthermore, we demonstrate that the helpfulness of community-created \nfact-checks varies depending on their level of political bias. Here, we find that community-created fact-\nchecks linking to high-bias sources (of either political side) are perceived as significantly less helpful. \nThis suggests that the rating mechanism on the Community Notes platform successfully penalizes \none-sidedness and politically motivated reasoning. These findings have important implications for \nsocial media platforms, which can utilize our results to optimize their community-based fact-checking \nsystems.\nMisinformation on social media can have real-world consequences. Among other instances, negative effects \nof misinformation have been repeatedly observed in the contexts of public safety1\u20134, public health5\u20137, and \nelections8\u201310. Recognizing this, professional fact-checkers and fact-checking organizations (e.g.,  h t t p s : / / s n o p e s . c \no m     , https://poli tifact.com ) routinely fact-check social media rumors in order to identify potentially misleading \ninformation11. However, due to limited resources, these organizations struggle to keep up with the volume \nof content generation12\u201314. This problem has prompted growing interest in delegating fact-checking to non-\nprofessional contributors in the crowd12,15\u201320.\nA community-based approach to fact-checking is promising as it offers the capacity to conduct large \nnumbers of fact-checks at high frequency and low costs16,19. Additionally, it alleviates some of the trust concerns \nassociated with conventional forms of fact-checking21,22. The underlying rationale of community-based fact-\nchecking draws on the concept of \u201cwisdom of the crowds\u201d\u2014the idea that the biases and errors of individual users \ncan be offset by aggregating input from a diverse group23\u201325. Recent experiments show that while the judgement \nof individual fact-checkers can be inconsistent and unreliable24, even fairly small groups of non-experts can \nachieve an accuracy comparable to those of experts15\u201317,19,20.\nHowever, while the crowd may be capable  of accurately detecting misinformation, it does not automatically \nentail that all users will decide  to do so17. Crucial challenges encompass lack of engagement in critical thinking26, \npolitically motivated reasoning27,28, and manipulation attempts29. Each of these behaviors can reduce the \neffectiveness of community-based fact-checking systems. For instance, there could be purposeful efforts by \nusers to manipulate the fact-checking process by reporting social media contents as misleading based purely \non non-conformance with their preconceptions or to achieve partisan ends29. Furthermore, the high level of \n(political) polarization among social media users30,31 can lead to significantly different interpretations of facts or \neven entirely different sets of accepted facts32. Hence, crucial requirements in community-based fact-checking \nsystems are sophisticated rating systems and fact-checking guidelines that promote helpful fact-checks. However, \nlittle is known regarding the question of what makes a community fact-check helpful.\nPrevious research has analyzed determinants of helpfulness in the context of customer reviews on online \nplatforms such as https://Amazon.com  and https://Y elp.com , yet not for community-created fact-checks \nof social media posts. For example, earlier works have found that meta-characteristics such as the age of the \nreview, the rating, and the length are important determinants of the helpfulness of customer reviews33\u201338. \nY et, despite apparent similarities, community-based fact-checking on social media substantially differs from \nJLU Giessen, Licher Str. 62, 35394 Giessen, Germany. \uf02aemail: nicolas.proellochs@wi.jlug.de\nOPEN\nScientific Reports  |        (2025) 15:25749 1 | https://doi.org/10.1038/s41598-025-09372-6\nwww.nature.com/scientificreports\n\ncustomer reviews. While customer reviews commonly share (subjective) personal experiences with a product, \nthe goal in fact-checking on social media is to carry out an accurate (and objective) assessment of a social \nmedia post. Furthermore, community-based fact-checking on social media must deal with politically biased \nviews and a highly polarized user base28,39. Ensuring high levels of trust with the fact-checkers\u2019 assessments \nis thus comparatively more important\u2014and more difficult to attain. To this end, modern community-based \nfact-checking systems typically step away from exclusively labeling potentially misleading social media content. \nInstead, they encourage users to write short textual fact-checking assessments and link to external sources (e.g., \nmedia outlets, scientific papers) to underpin their assertions. However, an understanding of how (and which) \nexternal sources affect the helpfulness of community-created fact-checks is largely missing. Shedding light on \nthis question represents the goal of this research.\nIn the present work, we conduct an empirical analysis of the relationship between external sources in \ncommunity-created fact-checks on social media and their perceived helpfulness. For this purpose, we utilize \na unique dataset encompassing community-created fact-checks for social media posts obtained from X\u2019s \n\u201cCommunity Notes\u201d platform (formerly \u201cBirdwatch\u201d)40. In contrast to earlier (small-scale) crowd-based fact-\nchecking initiatives, Community Notes allows users to identify misleading posts directly  on X. Specifically, the \nCommunity Notes feature allows users to tag posts they consider misleading and supplement them with written \nnotes that provide context to the post (e.g., by referring to external sources). An integral feature of Community \nNotes is that it implements a rating system, which provides users with the capability to rate the helpfulness of \nnotes contributed by other users. These ratings are intended to facilitate the identification of the context which \npeople find most helpful. For our analysis, we gather all community-created fact-checks from the Community \nNotes during an observation period of more than 8 months. Subsequently, we implement explanatory \nregression models to holistically analyze how the presence of external sources is linked to the helpfulness of the \ncorresponding fact-check. Furthermore, we study how the helpfulness varies with regards to the level of political \nbias of the external sources provided in the community-created fact-checks.\nOur empirical analysis implies that linking to external sources is the key determinant of helpfulness in \ncommunity-based fact-checking. On average, the odds for community-created fact-checks to be perceived as \nhelpful are 2.33 times higher if they link to external sources. Furthermore, we demonstrate that the helpfulness of \ncommunity-created fact-checks varies depending on their level of political bias. Here, we find that community-\ncreated fact-checks linking to high bias sources (of either political side) are perceived as significantly less helpful. \nThis suggests that the rating mechanism on the Community Notes platform successfully penalizes one-sidedness \nand politically motivated reasoning. Our findings have important implications for social media platforms that \ncan utilize our results to optimize their fact-checking guidelines and promote helpful fact-checks.\nBackground\nSocial media has emerged as a dominant platform for sharing information online, with a global user base \nexceeding 4.59 billion in 2022, expected to approach six billion by 20279,41. The shift from traditional media \nto social media has essentially transferred the responsibility of content quality control from trained journalists \nto everyday users42 and created a fertile environment for the proliferation of misinformation43. Numerous \nstudies have examined the spread of misinformation on social media, suggesting that it is more infectious, that \nis, receives more reposts than the truth7,11,44\u201348. Existing research highlights several factors contributing to this \ndynamic, such as emotional appeal46,47,49\u201351, novelty of the content11, automated bot activity4,52, and partisan \nmotives53\u201355. The challenge is further exacerbated by advances in AI enabling its creation at unprecedented speed \nand scale56,57.\nMisinformation on social media can have severe real-world consequences, posing risks not only to individuals \nbut also to society as a whole2,9,56,58\u201360. Exposure to misinformation on social media has been associated with \nvarious adverse outcomes, including misperceptions during elections8\u201310, threats to public safety1\u20134, and risky \nbehaviors during public health crises5\u20137. For instance, during the COVID-19 pandemic, belief in vaccine-\nrelated misinformation led to greater hesitancy and refusal to vaccinate, thereby increasing individuals\u2019 risk \nof infection61,62. Hence, to protect users from being affected by misinformation and prevent harm, it is both \nwarranted and required to implement effective countermeasures to fight against its spread63.\nContaining the spread of misinformation on social media necessitates accurate identification approaches16. \nCurrent measures of identifying misinformation fall under two primary categories. The first entails human-\nbased approaches that rely on professionals or fact-checking organizations like Politifact and Snopes to verify \nthe veracity of posts43,64. The second category entails machine learning-based systems, which attempt to \nautomatically classify misinformation by leveraging content-based elements (e.g., images, text, video), context-\nbased elements (e.g., time and location), or propagation patterns65,66. However, both approaches exhibit inherent \ndrawbacks. Verification performed by experts typically delivers reliable results but grapples with scalability \nowing to the scarcity of professional fact-checkers. Conversely, while detection powered by machine learning \nprovides scalability, it frequently underperforms in terms of prediction accuracy67. Consequently, this indicates \nthe necessity for approaches to fact-checking that combine accuracy with scalability.\nAs an alternative, recent research has suggested delegating the task of fact-checking misinformation \non social media to non-experts in the crowd12,15\u201320,68. The intuition is to harness the wisdom of crowds to \nidentify misleading posts24. Different from expert-based approaches, which are hindered by the limited pool \nof professional fact-checkers, community-based approaches make it possible to identify misinformation at a \nhigh volume16. Additionally, community-based fact-checking tackles the problem of user skepticism towards \nprofessional fact-checks21. Existing works imply that although assessments from single users might be \ninconsistent and unreliable, they tend to be highly accurate when collated24. Experimental research has shown \nthat crowds can be remarkably accurate in recognizing misleading content on social media platforms, indicating \nthat even fairly small ensembles of non-experts can achieve results comparable to those of experts15\u201317.\nScientific Reports  |        (2025) 15:25749 2 | https://doi.org/10.1038/s41598-025-09372-6www.nature.com/scientificreports/\nAlthough the crowd might be capable  of correctly identifying misinformation, it does not automatically \nentail that all users will decide  to do so17. Critical challenges encompass lack of engagement in critical thinking26, \npolitically motivated reasoning27,28, and manipulation attempts29. Each of these behaviors can reduce the \neffectiveness of community-based fact-checking systems. For example, users might deliberately sabotage the \nfact-checking mechanism by reporting social media content that refutes their personal belief, irrespective of \nits actual truthfulness29. Furthermore, the stark polarization among social media users30,31 might results in \ndifferent interpretations of facts or even completely different sets of acknowledged facts32. Indeed, prior (small-\nscale) attempts towards community-based fact-checking, like TruthSquad, Factcheck.EU, and WikiTribune69,70 \nwere confronted with quality issues regarding user-created fact-checks15,71. This highlights the difficulty of \nimplementing real-world community-based fact-checking systems that preserve both high level of quality and \nscalability. Core requirements to counter the aforementioned challenges encompass advanced rating systems \nand fact-checking guidelines that foster helpful context17,20.\nIn an attempt to address these challenges, the social media platform X (formerly Twitter) launched its \ncommunity-based fact-checking system Community Notes (formerly known as \u201cBirdwatch\u201d)40,72. Different from \nearlier crowd-based fact-checking initiatives, Community Notes allows users to identify misinformation directly  \non the platform. Community Notes also implements a rating mechanism that allows users to rate the helpfulness \nof other users\u2019 fact-checks. However, given the novelty of the platform, research studying how users interact \nwith Community Notes is still relatively scant. Early works have primarily analyzed the targets of community \nfact-checkers39,72\u201375 and the spread of community fact-checked posts on X57,76\u201380. While politically motivated \nreasoning might pose challenges39,72, research suggests that community notes can successfully reduce users\u2019 \nbelief in false content and their intentions to share misleading posts22,79. Our study adds by studying the link \nbetween external sources in community-created fact-checks and their helpfulness.\nResearch questions\nHelpfulness of external sources (RQ1)\nCommunity-based fact-checking systems can provide fact-checkers with the option to link to external sources \n(i.\u00a0e., websites) to support their assessments of social media posts40,72. Multiple considerations lead us to expect \nthat fact-checks that make use of this option and do link to external sources are perceived as more helpful \nby other users. First, the presence of links to external sources is likely to make the fact-check more credible. \nArguments tend to be more credible if they provide more information in support of the advocated position81. \nIt is well documented that the advisees\u2019 perception of the credibility of the advisor is an important determinant \nof helpfulness82,83. Second, users may be unmotivated to invest the necessary effort of validating the assertions \nmade in the fact-checks. In this scenario, more justifications for a position may make users more confident in \ntheir assessment84. Third, the presence of links to external sources may reflect the fact-checkers\u2019s involvement \nand knowledge. The more effort and expertise the fact-checker puts into writing the fact-check, the more likely \nit is that it will provide high-quality information that presents helpful context to other users. Taking these \narguments together, community fact-checks that link to external sources may contain more credible arguments \npresented by better-informed fact-checkers that are more helpful to other users. RQ1 states:\nRQ1  Are community fact-checks linking to external sources perceived as more helpful?\nPolitical bias in external sources (RQ2)\nThe internet has given rise to an unprecedented prominence and popularity of politically biased sources of \ninformation85. This raises the question of whether the effect of external sources in community-created fact-\nchecks on helpfulness varies depending on their level of political bias. Fact-checkers can link to websites with \nhigh (e.g., partisan websites such as https://breitbar t.com ) or low political bias (e.g., mainstream media outlets). \nIn general, politically biased sources tend to be perceived as less credible than non-biased sources86. We expect \nthat users perceive politically biased sources as less helpful because individuals have a well-developed association \nbetween credible sources and truthful information87,88. In other words, it may be easy for individuals to rely \non the simple-decision rule \u201cexperts are usually correct\u201d when judging the likely authenticity of a fact-check. \nFurthermore, people are generally more persuaded by high-credibility sources87\u201389, which can even make them \nmore likely to agree with counter-attitudinal viewpoints89. It is thus plausible that fact-checks leveraging source \ncredibility are more likely to be perceived as helpful. Based on this reasoning, we hypothesize that fact-checks \nlinking to external sources with high political bias are perceived as less helpful than those linking to external \nsources with low political bias. RQ2 states:\nRQ2  Is linking to low bias sources in community fact-checks perceived as more helpful than linking to high bias \nsources?\nPartisan asymmetry (RQ3)\nPolitically biased sources typically have a distinct partisan leaning, favoring either conservative (i.\u00a0 e., right-\nleaning) or liberal (i.\u00a0e., left-leaning) opinions90. At the same time, social media is characterized by \u201cus versus \nthem\u201d mentality (i.\u00a0e., a partisan-laden perception), which can result in the dismissal of viewpoints and facts \nfrom the political out-group91. Contextualized to community-based fact-checking, linking to politically biased \nsources may implicitly reveal information about the political orientation of the fact-checker\u2014which may be \npolarizing to users with opposing (political) views. Assuming a high level of political diversity among the users \nparticipating in community-based fact-checking (i.\u00a0 e., both fact-checkers and raters), this would imply that \nbiased sources of either political side are less likely to be perceived as helpful by a large share of users. However, \nthe assumption of high political diversity may not hold true in real-world community-based fact-checking \nScientific Reports  |        (2025) 15:25749 3 | https://doi.org/10.1038/s41598-025-09372-6www.nature.com/scientificreports/\nsystems such as X\u2019s Community Notes. The reason is that users engaging in community-based fact-checking are \nself-selected  and, thus, are not necessarily representative of the overall user base on social media or society as a \nwhole. There is ample evidence that the political left and the political right use social media in different ways, a \nphenomenon known as ideological asymmetry92. For example, adherents of the left have been found to be less \ntolerable to the spread of misinformation and have greater trust in fact-checking53,92. It is thus conceivable that \nthe self-selected fact-checking community is more likely to identify with one side of the political spectrum\u2014and \nthat it may read its own political leanings into fact-checks. However, an understanding of whether politically \nbiased sources are more helpful if they are left-leaning or right-leaning is missing. Hence, RQ3 states:\nRQ3 Are politically biased sources perceived as more helpful if they are left-leaning or right-leaning?\nData and empirical model\nData\nThis work examines the helpfulness of community-created fact-checks from X\u2019s Community Notes40. Launched \nto the public in October, 2021, Community Notes is a novel platform to counter misinformation circulating on \nX through the power of collective intelligence. The Community Notes platform allows X users to identify posts \nthey perceive as misleading and supplement them with textual  written notes, as illustrated in Fig. 1. Community \nNotes are limited to 280 characters where each URL (i.\u00a0e., website) accounts for a single character. Community \nNotes can be attached to any post on X.\nCommunity Notes comes with a rating system allowing users to assess the helpfulness of notes submitted by \nothers. Similar to other popular websites like https://Amazon.com , these user-generated ratings aim to identify \nand elevate the visibility of the most helpful and relevant context. To surface helpful community notes that \nappeal broadly across heterogeneous user groups, the feature uses a bridging-based rating system93. This system \ncalculates the helpfulness score for each Community Note based on the ratings made by others. Only notes that \nare rated as helpful by multiple contributors with heterogeneous rating histories are publicy displayed to all users \non X94.\nData collection\nWe retrieved all Community Notes and corresponding original posts from the official roll-out of the Community \nNotes in October 2022 until June 2023 from the Community Notes site ( https: //birdwatch.twitter.com ). Following \nearlier work on helpfulness34,35,95, we only consider fact-checks for which the helpfulness has been assessed at \nleast once by users (i.\u00a0e., fact-checks that received at least one helpful or unhelpful vote). The resulting dataset \ncontains a total number of 41,128 Community Notes (i.\u00a0 e., community-created fact-checks), and 2,848,825 \nratings (i.\u00a0e., helpfulness votes). We utilized the historical API provided by X to correlate the postID  referenced \nin every Community Note with the original post (i.\u00a0e., the post that was subject to fact-checking) and collected \nthe following information about each original post and the account of its author: (i) the number of followers, \n(ii) the number of followees, (iii) the account age, (iv) whether the user has been verified by X, (v) the post age.\nLinks to external sources\nSubsequently, we extracted each link to external websites from the text explanations in the Community Notes \n(see Fig. 1). To this end, we implemented string extraction of links in the Python programming language with \nthe help of the built-in re package. Each of the extracted links was then reduced to its domain name (e.g.,  h t t p s \n: / / c n n . c o  m     ) . Among all Community Notes, 88.66% contain at least one link to an external source. The majority \nof Community Notes with external sources contained only a single link, whereas 35.58% of the Notes contained \nmultiple external links. The most common sources in Community Notes are links to Media Outlets  (e.g., CNN) \nFig. 1 . Example of a Community-Created Fact-Check (\u201cCommunity Note\u201d) on X. \nScientific Reports  |        (2025) 15:25749 4 | https://doi.org/10.1038/s41598-025-09372-6www.nature.com/scientificreports/\nand Public Authorities  (e.g., government websites), which represent 50.09% and 18.25% of all links in our dataset, \nrespectively. This is followed by Social Media Posts  (13.97%; e.g., other posts on X), Scientific Literature  (7.04%; \ne.g., articles in scientific journals), Encyclopedias  (5.82%; e.g., Wikipedia), and Third-Party Fact Checkers  (3.34%; \ne.g., snopes.com ). A small fraction (1.50%) of links pointed to sources not captured by these categories and were \ngrouped as Other .\nPolitical bias\nTo determine the political slant of the external sources, we utilized the website Media Bias/Fact Check  (   h t t p \ns : / / m e d i a b i a s f a c t c h e c k . c o m     ) , which provides assessments of political bias (left and right) for a great deal of \nwebsites. The bias ratings from Media Bias/Fact Check are a common choice in previous literature96 and are \nbased on criteria such as the factuality of reporting, one-sidedness, and strength of political affiliations. We used \nMedia Bias/Fact Check to collect information about (i) the bias magnitude (low, medium, high), and (ii) the bias \ndirection (left, undirected, right) of the external sources in Community Notes.\nBy matching the bias rating from Media Bias/Fact Check to the extracted domain names, we were able to \nobtain bias scores for 50.35% of all links in Community Notes. Figure 2 illustrates the distribution of the detected \nbias magnitudes and directions. External sources with medium bias are most common in our sample (51.51%), \nfollowed by low bias (39.82%). Notes containing highly biased sources are relatively rare (8.67%). Regarding the \nbias direction, left-leaning external sources are more prevalent in Community Notes with approximately 52% \n(11,089) of Notes having a clear left-leaning bias, while only 13.61% of Notes have a clear right-leaning bias. \nApproximately 34.35% of external sources are politically neutral (i.e., undirected bias). Examples of the most \ncommon domains referenced in Community Notes are reported in Table 1.\nVariable definitions\nWe are interested in analyzing factors that determine the helpfulness of community-created fact-checks. To this \nend, the dependent variable is the number of HVotes ( helpful votes), which denotes the number of users who \nvoted \u201cY es\u201d in response to the question \u201cIs this note helpful?\u201d The total number of users who responded to this \nquestion is denoted by Votes .\nThe explanatory variables in our study can be divided into two groups: (1) variables that are given by the \ncommunity-created fact-check (i.\u00a0e., Community Note); and (2) variables that provide information about the \noriginal post (summary statistics and cross correlations are provided in the Supplementary Table S1 and Fig. S1).\nFact-checking variables\nOur key explanatory variable is External Source , which is a binary label denoting whether a link to an external \nwebsite has been provided as part of a Community Note ( =1 if true, otherwise 0). Additionally, Media Bias/Fact \nCheck provides ratings on the political bias of the external sources. We gather information about the magnitude \nand direction of the political biases for each Community Note. The resulting variable Bias Magnitude  ranges \nfrom 0 to 2. Here a value of 0 refers to sources with low bias, a value of 1 refers to sources with medium bias, \nand a value of 2 refers to high biased sources (in either political direction). If a Community Note contains \nmultiple external links, we take the mean of the bias scores of the individual links. We follow the same approach \nto calculate individual scores for the Bias Direction ( \u22121 for left-leaning, 0 for undirected, and +1 for right-\nleaning.) For instance, if there are two links in a Community note pointing in different directions, the mean \nBias Direction  would be zero.\nWe use additional control variables to account for common content characteristics of the Community \nNotes that may affect their helpfulness: (i)\u00a0we control for the length ( Word Count ) of the Community Notes \n(excluding links), (ii)\u00a0we calculate the Text Complexity  using the Gunning-Fog readability index, and (iii)\u00a0we \nuse the sentimentr  package97 in combination with the built-in NRC lexicon98 to measure the positive/\nnegative Sentiment99 of the Community Note.\nOriginal post variables\nIn our empirical analysis, we also control for characteristics of the original (i.\u00a0e., the fact-checked) post. First, we \ncontrol for the sentiment of the post ( Post Sentiment ), analogous to the sentiment of the Community Note. \nSecond, we control for the social influence of the author of the original post. The variables include the number \nFig. 2 . Distribution of political biases in Community Notes. ( A) Bias magnitude ordered from Low to High. \n(B) Bias direction, separated into Left, Undirected, and Right. \nScientific Reports  |        (2025) 15:25749 5 | https://doi.org/10.1038/s41598-025-09372-6www.nature.com/scientificreports/\nof followers ( Followers ), the number of followees ( Followees ), the account age ( Account Age ), and how \nmany days have passed since the post was first published ( Post Age ). We log-transform each of these variables \nto account for fat-tailed distributions in our later analysis. Additionally, we use a binary variable Veri\ufb01ed  to \ncontrol for whether the account has been verified by X ( =1 if true, otherwise 0). Third, we use a binary variable \nPolitical  denoting whether the original post covers a political topic ( =1 if true, otherwise 0). To this end, \nwe fine-tuned (and manually validated) a pre-trained TwHIN-BERT language model100 for our task (see SI, \nSupplement D for implementation details). The classifier achieved a high macro-averaged F1 score of 0.755 in \npredicting topic labels.\nModel specification\nFollowing previous research modeling helpfulness33,35, we model the number of helpful votes, HVotes , as a \nbinomial variable with probability parameter \u03b8 and Votes  trials. Our key explanatory variable that allows us \nto analyze RQ1  is External Source , a binary variable denoting whether a Community Note includes links to \nexternal sources ( =1 when true, 0 otherwise). We control for multiple content characteristics of Community \nNotes, namely, the length ( Word Count ), text complexity ( Text Complexity ), and Sentiment . Furthermore, \nwe control for various characteristics of the fact-checked post. The control variables include the number of \nFollowers  and Followees , the account age ( Account Age ), whether the account is Veri\ufb01ed , the post age \n(PostAge ), the post sentiment ( Post Sentiment ), and a binary dummy indicating whether the fact-checked \npost covers a political topic ( =1 when true, 0 otherwise). This yields the following regression model:\n logit (\u03b8)=\u03b20+\u03b21External Source +\u03b22Word Count +\u03b23Text Complexity +\u03b24Sentiment\n+\u03b25log(Followers )+\u03b26log(Followees )+\u03b27Veri\ufb01ed\n+\u03b28log(Account Age)+ \u03b29log(Post Age )+\u03b210Post Sentiment\n+\u03b211Political +uF+\u03b5, (1)\n HV otes \u223cBinomial [V otes, \u03b8 ], (2)\nwith intercept \u03b20, error term \u03b5, and uF representing fact-checker-specific random intercepts that capture \nunobserved heterogeneity across fact-checkers (e.\u00a0g., differences in expertise). We estimate Eqs.\u00a0 1 and 2 using \nmaximum likelihood estimation and mixed-effects generalized linear models. To facilitate the interpretability \nof our findings, we z-standardize all continuous variables, allowing us to compare the effects of regression \ncoefficients on the dependent variable measured in terms of standard deviations.\nIn order to analyze RQ2 and RQ3, we focus on the subset of community-created fact-checks that contain at \nleast one link to external sources rated by Media Bias/Fact Check. The key explanatory variable that allows us to Domain Frequency\nOverall\ntwitter.com 5747\nwikipedia.org 2298\napnews.com 1220\nsnopes.com 1127\nyoutube.com 1086\nLow bias\nwikipedia.org 2298\nreuters.com 979\ncdc.gov 715\nnature.com 524\nthehill.com 340\nMedium bias\napnews.com 1220\nsnopes.com 1127\nnytimes.com 835\nnpr.org 819\nwashingtonpost.com 765\nHigh bias\ndailymail.co.uk 295\nstate.com 179\ngiszodo.com 97\nvox.com 97\nwashingtonexaminer.com 91\nTable 1 . Most frequent domains referenced in the text explanations of Community Notes. \nScientific Reports  |        (2025) 15:25749 6 | https://doi.org/10.1038/s41598-025-09372-6www.nature.com/scientificreports/\nanalyze RQ2 is Bias Magnitude , i.\u00a0e., the severity of political bias of the links provided in Community Notes. \nTo study RQ3, we include additional interaction term between Bias Magnitude  and Bias Direction , which \nallows us to analyze whether politically biased sources are more/less helpful depending on whether they are left-\nleaning or right-leaning. All controls are analogous to the previous model.\nNote that we analyze a wide range of additional model variants as part of an extensive set of robustness \nchecks. In all of these analyses, we observe consistent results.\nEmpirical results\nHelpfulness of external sources (RQ1)\nWe now analyze factors that determine the helpfulness of community-created fact-checks (RQ1). For this \npurpose, we draw upon a mixed-effects binomial regression model with the share of helpful votes as the \ndependent variable. The coefficient estimates for our primary explanatory variables are visualized in Fig. 3 (see \nSupplementary Table S3 for full estimation results).\nOur findings suggest that the content characteristics of Community Notes play an important role in \ndetermining their helpfulness: the coefficient for Word Count  (coef. = 0.012, OR = 1.012, p<0.001 ) is positive \nand statistically significant, Text Complexity  (coef. = \u22120.040 , OR = 0.961, p<0.001 ) is negative and statistically \nsignificant, whereas Sentiment  lacks statistical significance at common significance levels. For a one standard \ndeviation increase in the explanatory variable, the estimated odds of a helpful vote increase by e0.012\u22121\u2248 \n1.21% for Word Count , and decrease by 3.92% for Text Complexity . Consequently, the perceived helpfulness of \nCommunity Notes is higher if they incorporate longer explanations with less complex language.\nWe further note that the social influence attributed to the account that disseminates the original post has an \neffect on the perceived helpfulness of Community Notes. Here, the largest effect sizes are estimated for Verified , \nFollowers , and Account Age . The odds of receiving a helpful vote for Community Notes reporting posts from \nverified accounts are 16.77% higher (coef. = 0.155, OR = 1.168, p<0.001 ) than for unverified accounts. A one \nstandard deviation increase in the number of followers decreases the odds of a helpful vote by 19.99% (coef. = \n\u22120.223 , OR = 0.800, p<0.001 ). A one standard deviation increase in the time since the account was published \nis associated with a 1.69% decrease (coef. = \u22120.017 , OR = 0.983, p<0.001 ) in the estimated odds of a helpful \nvote. In sum, there is a lower level of helpfulness for posts from high-follower and older accounts, and a higher \nlevel of helpfulness for Community Notes fact-checking posts from verified accounts. Furthermore, we find \nthat fact-checks for posts covering a political topic are significantly less helpful (coef. = \u22120.030 , OR = 0.970, \np<0.001 ).\nTo analyze RQ1 , we assess how the presence of links to external sources in Community Notes is linked to \ntheir helpfulness. The coefficient estimate for External Source  is 0.844 (OR = 2.326, p<0.001 ), which implies \nthat the odds of Community Notes linking to external sources to be perceived as helpful are 2.33 times higher \nthan for those not containing links to external sources. Notably, this is, by far, the largest effect size across all \nvariables in our model.\nPolitical bias in external sources (RQ2)\nNext, we analyze the role of political bias regarding the helpfulness of external sources in community-created \nfact-checks ( RQ2 ). For this purpose, we additionally include the variable Bias Magnitude  into the regression \nmodel and restrict our analysis to Community Notes containing at least one link to an external source rated by \nMedia Bias/Fact Check, resulting in 21,307 observations. The control variables are analogous to the previous \nmodel.\nThe coefficient estimates (see left panel in Fig. 4 for marginal effects, and Supplementary Table S4 for full \nestimation results) imply that linking to politically biased sources in community-created fact-checks is perceived \nas significantly less helpful. Specifically, a one standard deviation increase in Bias Magnitude  is associated with \na 3.05% decrease (coef. = \u22120.031 , OR = 0.969, p<0.001 ) in the odds of a Community Note being perceived as \nFig. 3 . Mixed-effects binomial regression analyzing the helpfulness of external sources in explaining the \nshare of helpful votes. Shown are coefficient estimates with 95%  CIs. Unit of analysis is the fact-check level \n(N= 41, 129). Fact-checker-specific random effects are included. \nScientific Reports  |        (2025) 15:25749 7 | https://doi.org/10.1038/s41598-025-09372-6www.nature.com/scientificreports/\nhelpful. To put this number into perspective, this implies that a community-created fact-check providing a link \nto a highly biased website (e.g., Breitbart) is approximately 10.16% less likely to be perceived as helpful than a \nfact-check linking to a low biased website (e.g., Reuters).\nPolitical vs. non-political posts\nWe now assess the role of political bias for political vs. non-political posts. For this, we include an interaction \nterm between Bias Magnitude  and Political  into our regression model. After including the interaction, the \nestimated coefficient for the direct effect of Bias Magnitude  is still negative and statistically significant (coef. = \n\u22120.058 , OR = 0.944, p<0.001 ). This implies that for non-political posts, a one standard deviation increase in \nSource Bias  is associated with an 5.64% decrease in the odds of a Community Notes to be perceived as helpful. \nFurthermore, we observe that the coefficient of the interaction between Source Bias  and Political  is positive \nand statistically significant (coef. = 0.062, OR = 1.064, p<0.001 ), which implies that the effect of bias in external \nsources is more positive for political posts. For political posts, we can assess the effect size by calculating the \nexponent of the sum of the coefficients101 of Bias Magnitude  and Bias Magnitude \u00d7Political . The resulting \nOR is 1.004, which is not statistically significant at common significance levels. This implies that bias in external \nsources is not statistically significantly linked to lower helpfulness for political posts.\nPartisan asymmetry (RQ3)\nNext we analyze whether there is a partisan asymmetry, i.\u00a0e. whether politically biased sources are more helpful \nif they are left-leaning or if they are right-leaning (RQ3). The marginal effects are visualized in the right panel of \nFig. 4 (see Supplementary Table S5 for full estimation results).\nWe find that the presence of both right-leaning and left-leaning biased sources in community notes \nsignificantly reduces their perceived helpfulness. However, the magnitude of the effect sizes significantly \ndiffer: the inclusion of left-leaning biased sources reduces the perceived helpfulness by, on average, 1.29% (ME \n=\u22120.013, OR = 0.987, p<0.001 ), whereas the inclusion of right-leaning biased sources reduces the perceived \nhelpfulness by, on average, 0.60% (ME =\u22120.006, OR = 0.994, p<0.001 ). In contrast, the inclusion of sources \nwith undirected political bias is not associated with a statistically significant change in perceived helpfulness \n(ME =0 .002 , OR = 1.002, p=0 .266 ). Overall, this implies that external sources with the same level of political \nbias are rated as the least helpful if they are left-leaning.\nPolitical vs. non-political posts\nWe assess the role of bias direction for political vs. non-political posts. To this end, we extend our regression \nmodel with an additional interaction term between Bias Direction  and Political . When a right-leaning biased \nsource is included in a community note concerning a political post, there is no statistically significant change \n(p=0 .162 ) in the perceived helpfulness. However, the inclusion of a similar bias in a community note on a \nnon-political post results in a 0.90% decrease in perceived helpfulness (ME =\u22120.009, OR = 0.991, p<0.001 ). \nThis variation between the two effects is statistically significant ( p<0.01 ). For left-leaning biased sources and \nsources with undirected political bias, we observe no statistically significant differences between political vs. \nnon-political posts (each p>0.05 ).\nExploratory analyses & robustness checks\nMultiple exploratory analyses and checks validated our results and confirmed their robustness. Specifically, \nwe (1) controlled for fact-checks notes that contain multiple external sources, (2) analyzed helpfulness across \ndifferent types of media categories (e.g., media outlets, scientific literature) and (3) additional topics (e.g., \nscience, health, economy), and (4) conducted a variety of additional robustness checks. In all of these checks, we \nfind consistent results and our hypotheses continue to be supported. In the following, we provide a summary of \nthe main findings (see Supplementary Materials for details).\nMultiple external sources\nOur main analysis focuses on the presence of at least one external source in community-created fact-checks \n(i.\u00a0e., a binary variable). However, 34.64% of Community Notes contain multiple external links. We explicitly \nFig. 4 . Marginal effects (with 95%  CIs) of bias magnitude (left panel) and bias direction (right panel) on the \nshare of helpful votes. Unit of analysis is the fact-check level ( N= 21, 307). Fact-checker-specific random \neffects are included. \nScientific Reports  |        (2025) 15:25749 8 | https://doi.org/10.1038/s41598-025-09372-6www.nature.com/scientificreports/\ncontrol for the number of links authors provide as part of their fact-check (see Supplementary Table S6). The \ncoefficient for Number of External Sources  is slightly positive and statistically significant (coef. = 0.040, OR = \n1.041, p<0.001 ), implying that including multiple links to external sources in community-created fact-checks \nincreases helpfulness.\nAnalysis across media types\nWe further explore how the helpfulness of external sources varies across different media types. For this purpose, \ntwo trained research assistants manually assigned media categories (e.g., media outlets, scientific literature) to \neach external source in our dataset (multiple selection possible). The most common sources in Community \nNotes are links to Media Outlets  and Public Authorities , which represent 50.09% and 18.25% of all links in our \ndataset, respectively. This is followed by Social Media Posts  (13.97%), Scientific Literature  (7.04%), Encyclopedias  \n(5.82%), and Third-Party Fact Checkers  (3.34%). 1.50% of all links could not be assigned to these categories and \nwere labeled as Other .\nSubsequently, we repeat our regression analysis with binary variables denoting the presence of the \ncorresponding source categories as part of the Community Notes (see Supplementary Table S6). The coefficient \nestimates for media types and their frequencies are visualized in Fig. 5. We find that community-created \nfact-checks linking to fact-checks from third-party fact-checkers (e.g., https://snopes.com ) are perceived as \nparticularly helpful (coef. = 0.265, OR = 1.303, p<0.001 ), followed by links to Public Authorities ( coef. = \n0.174, OR = 1.190, p<0.001 ), Social Media  posts (coef. = 0.173, OR = 1.189, p<0.001 ), and Media Outlets ( \ncoef. = 0.085, OR = 1.089, p<0.001 ). Community-created fact-checks linking to Scienti\ufb01c Literature  are \nassociated with a small and marginally significant increase in perceived helpfulness (coef. = 0.027, OR = 1.027, \np<0.05 ), whereas links to Encyclopedias ( e.g., Wikipedia) show no significant effect.\nThese results highlight notable heterogeneity in how the type of external reference influences perceived \nhelpfulness and suggest that source credibility, perceived authority, and contextual familiarity may play an \nimportant role in shaping community evaluations.\nAnalysis across fine-grained topics\nWhile our main analysis distinguishes between political and non-political posts, we now examine how more \nfine-grained topics relate to the perceived helpfulness of Community Notes (see Fig. 6). To do so, we again \napply a pre-trained TwHIN-BERT language model to classify posts into five topics: Politics , Health , Economy , \nScience , and Other  (see Supplement D for implementation details). In terms of frequency (multiple topics per \npost possible), Politics  is the most common topic, accounting for 42.10% of all Community Notes, followed by \nHealth  (16.90%), Economy  (16.20%), and Science  (9.82%). A total share of 33.40% of all posts fall into the Other  \ncategory.\nWe then re-estimate our regression model and include the fine-grained topic labels as dummy variables. The \nresults show that Community Notes on Economy -related posts are rated most helpful (coef. = 0.108, OR = 1.114, \np<0.001 ), followed by Science -related (0.082, OR = 1.085, p<0.001 ) and Health -related (0.011, OR = 1.011, \np<0.05 ) posts. In contrast, Community Notes addressing posts on Politics  are rated significantly less helpful \n(coef. = \u22120.026 , OR = 0.974, p<0.001 ). Altogether, this reinforces our earlier results: Community Notes for \nnon-political content tend to be perceived as more helpful than those for political content.\nAdditional checks\nWe performed an extensive series of supplementary analyses (see Supplementary Materials): (1)\u00a0we controlled \nfor outliers in the dependent variables; (2)\u00a0we computed the variance inflation factors for each independent \nvariable and validated that they are below the critical threshold of four; (3)\u00a0we included quadratic effects; (4)\u00a0we \nreestimated our models without including fact-checker-specific random effects; (5)\u00a0we re-estimated our models \nFig. 5 . Analysis across media types. ( A)\u00a0Results of a mixed-effects binomial regression analyzing the \nhelpfulness of external sources across different media types. Shown are coefficient estimates with 95%  CIs. \nUnit of analysis is the fact-check level ( N= 21, 277). Fact-checker-specific random effects are included. \n(B)\u00a0Frequencies of media types across community notes. \nScientific Reports  |        (2025) 15:25749 9 | https://doi.org/10.1038/s41598-025-09372-6www.nature.com/scientificreports/\nusing bootstrapped standard errors to assess the robustness of our findings to mild overdispersion in our data; \n(6)\u00a0 we repeated our regression analysis by modeling the total count of votes (helpful and unhelpful) as the \ndependent variable; (7)\u00a0we included alternative categorizations of factual reporting and domain quality from \nMedia Bias/Fact Check and previous research13. (8)\u00a0we re-coded bias magnitude and bias direction into a single \nvariable; (9)\u00a0we re-coded bias magnitude into a factor variable. All the aforementioned analyses supported our \nfindings.\nDiscussion\nOur empirical findings contribute to research on misinformation on social media platforms and community-\ndriven fact-checking. Whereas previous experimental studies have been primarily centered around the question \nof whether a crowd can accurately evaluate content on social media16, an understanding of \u201cwhat makes \ncommunity-created fact-checks helpful\u201d has remained largely absent. In this study, we hypothesized that linking \nto external sources is the key determinant of helpfulness in community-based fact-checking (RQ1). Our primary \nrationale was that fact-checks are more credible and persuasive if they provide more information in support of \ntheir assertions. Furthermore, the presence of links to external sources may reflect the fact-checkers\u2019s expertise. \nWe found strong support for this hypothesis: on average, the odds for Community Notes to be perceived as \nhelpful were 2.33 times higher if they link to external sources. Notably, this effect size was larger than for any \nother considered predictor of helpfulness (i.\u00a0e., content characteristics, author characteristics).\nWe further analyzed whether the link between external sources in community-created fact-checks and \nhelpfulness varies depending on their level of political bias (RQ2). Our rationale was that community-created \nfact-checks leveraging source credibility may be more likely to be effective. Consistent with this notion, we found \nthat linking to high bias sources (e.g., \u201calternative\u201d news outlets) in community-created fact-checks is perceived \nas less helpful. We also compared the helpfulness across various sub-categories of external sources. Here we \nfound that community-created fact-checks linking to fact-checks from third-party fact-checking organizations \n(e.g., https://snopes.com ) are perceived as particularly helpful. In contrast, community-created fact-checks \nlinking to encyclopedias (e.g., Wikipedia) and scientific literature are perceived as less helpful.\nFurthermore, our study provides new insights into the debate on whether political one-sidedness among the \nuser base might hamper community-based fact-checking. The reason for these concerns is that users participating \nin community-based fact-checking may not be free of partisan motifs and political bias, but rather read their \nown political leanings into fact-checks. For instance, studies suggest that users may systematically reject content \nfrom those with whom they disagree politically28,39. In this regard, our empirical findings are encouraging: \nalthough authors of community fact-checks are more likely to link to left-leaning sources, biased sources of \neither political side are rated as less helpful by other users (RQ3). This suggests that the rating mechanism on \nthe community notes platform indeed penalizes one-sidedness and politically motivated reasoning. Such crowd-\nbased correction aligns with prior observations of the \u201cwisdom of the crowd\u201d on other online platforms, such \nas Wikipedia and Stack Overflow, where collective input tends to support relatively high-quality information102.\nFrom a practical perspective, social media platforms should closely monitor the potential of community-\ncreated fact-checking systems. While not a silver bullet and subject to limitations\u2014such as difficulty achieving \nconsensus on polarized topics28,39 and limited coverage, particularly in low-visibility parts of the platform \nwhere many false claims go unchecked14,103\u2014these systems may still serve as a valuable complement to existing \ncontent moderation strategies. Their promise lies in three key advantages: (i)\u00a0they enable the identification of \nmisinformation at a larger scale, (ii)\u00a0they address the trust problem associated with conventional fact-checking, \nand (iii)\u00a0they identify misinformation that is of direct interest to actual social media users\u2014and which may go \nunnoticed by third-party fact-checking organizations.\nAs such, our findings offer practical insights for the design of more effective community-based fact-checking \ntools. Specifically, our results suggest that ranking systems should put strong emphasis on links to unbiased \nexternal sources provided in fact-checks. In this light, the recent decision by X104 to require external sources Fig. 6 . Analysis across fine-grained topics. ( A)\u00a0Results of a mixed-effects binomial regression analyzing \nthe helpfulness of external sources across different topics. Shown are coefficient estimates with 95%  CIs. \nUnit of analysis is the fact-check level ( N= 21, 277). Fact-checker-specific random effects are included. \n(B)\u00a0Frequencies of topics across community notes. \nScientific Reports  |        (2025) 15:25749 10 | https://doi.org/10.1038/s41598-025-09372-6www.nature.com/scientificreports/\nin Community Notes can be seen as a step in the right direction. However, community-based fact-checking \nsystens could go even further. While helpful fact-checks can be identified through voting systems, accumulating \nhigh numbers of votes requires time. To accelerate this process, social media platforms could build on our our \nfindings to develop systems that facilitate an early detection of potentially helpful fact-checks (e.\u00a0g., by accounting \nfor political bias in external sources), thereby helping to prevent unhindered dissemination of misleading social \nmedia posts.\nAs with any other research, our study has a number of limitations. Although we performed an extensive \nseries of robustness checks, there may be additional unobserved factors affecting users\u2019 perceived helpfulness of \na specific fact-check that we cannot control for in our study. For instance, our approach struggles to account for \nsubjective characteristics in the perception of raters (e.\u00a0g., users\u2019 knowledge). As such, it is important to mention \nthat observational data cannot reveal how users cognitively process the information in fact-checks. It would \nthus be an interesting extension to analyze the reception of community-created fact-checks in an experiment \nwith neurophysiological measurements105. Our study is also limited by the accuracy and availability of the bias \nratings for websites, specifically those from Media Bias/Fact Check. However the bias ratings from Media Bias/\nFact Check are a common choice in previous literature96 and rely on distinctive source characteristics such as the \nfactuality of reporting. Ultimately, our conclusions are confined to the sphere of community-based fact-checking \non X\u2019s Community Notes platform. Further research is necessary to understand if the observed patterns are \ngeneralizable to other crowd-sourced fact-checking platforms.\nConclusion\nCommunity-based fact-checking systems require sophisticated rating systems and fact-checking guidelines that \npromote helpful context. In this work, we empirically investigate the helpfulness of the context provided in \ncommunity-created fact-checks on X\u2019s community-based fact-checking system Community Notes. Our analysis \nsuggests that linking to external sources is the key determinant of helpfulness in community-based fact-checking. \nFurthermore, we find that the rating mechanism on the Community Notes platform successfully penalizes \npolitical one-sidedness in fact-checking. Our study has important implications for social media platforms that \ncan utilize our results to optimize their fact-checking guidelines and promote helpful fact-checks.\nData availability\nWe adhere to X\u2019s data usage policies. Raw data on community notes is available via  h t t p s :  / / c o  m m  u n i t y n  o t e s .  x \n. c o m / g u i d e / e n / u n  d e r - t h e - h o o d / d o  w n l o a d - d  a t a . Aggregate data analyzed during the current study is available \nupon request. Data requests can be directed to Kirill Solovev (kirill.solovev@wi.jlug.de).\nReceived: 26 January 2025; Accepted: 26 June 2025\nReferences\n 1. Starbird, K. Examining the alternative media ecosystem through the production of alternative narratives of mass shooting events \non Twitter. Proceedings of the International AAAI Conference on Web and Social Media  11, 230\u2013239 (2017).\n 2. B\u00e4r, D., Pr\u00f6llochs, N. & Feuerriegel, S. New threats to society from free-speech social media platforms. Commun. ACM  66, 37\u201340 \n(2023).\n 3. Jakubik, J., V\u00f6ssing, M., Pr\u00f6llochs, N., B\u00e4r, D. & Feuerriegel, S. Online emotions during the storming of the U.S. Capitol: Evidence \nfrom the social media network Parler. Proceedings of the International AAAI Conference on Web and Social Media  17, 423\u2013434 \n(2023).\n 4. Geissler, D., B\u00e4r, D., Pr\u00f6llochs, N. & Feuerriegel, S. Russian propaganda on social media during the 2022 invasion of Ukraine. EPJ \nData Sci.  12, 51 (2023).\n 5. Broniatowski, D. A. et al. Weaponized health communication: Twitter bots and Russian trolls amplify the vaccine debate. Am. J. \nPublic Health  108, 1378\u20131384 (2018).\n 6. Gallotti, R., Valle, F., Castaldo, N., Sacco, P . & De Domenico, M. Assessing the risks of \u2018infodemics\u2019 in response to covid-19 \nepidemics. Nat. Hum. Behav.  4, 1285\u20131293 (2020).\n 7. Solovev, K. & Pr\u00f6llochs, N. Moral emotions shape the virality of Covid-19 misinformation on social media. Proceedings of the \nACM web conference , 3706\u20133717 (2022).\n 8. Aral, S. & Eckles, D. Protecting elections from social media manipulation. Science  365, 858\u2013861 (2019).\n 9. Bakshy, E., Messing, S. & Adamic, L. A. Exposure to ideologically diverse news and opinion on Facebook. Science  348, 1130\u20131132 \n(2015).\n 10. Grinberg, N., Joseph, K., Friedland, L., Swire-Thompson, B. & Lazer, D. Fake news on Twitter during the 2016 U.S. presidential \nelection. Science  363, 374\u2013378 (2019).\n 11. Vosoughi, S., Roy, D. & Aral, S. The spread of true and false news online. Science  359, 1146\u20131151 (2018).\n 12. Micallef, N., He, B., Kumar, S., Ahamad, M. & Memon, N. The role of the crowd in countering misinformation: A case study of \nthe Covid-19 infodemic. Proceedings of the International Conference on Big Data , 748\u2013757 (2020).\n 13. Lin, H. et al. High level of correspondence across different news domain quality rating sets. PNAS Nexus  2, pgad286 (2023).\n 14. Kauk, J., Kreysa, H., Scherag, A. & Schweinberger, S. R. The adaptive community-response (ACR) method for collecting \nmisinformation on social media. J. Big Data  11, 35 (2024).\n 15. Bhuiyan, M.\u00a0M., Zhang, A.\u00a0X., Sehat, C.\u00a0M. & Mitra, T. Investigating differences in crowdsourced news credibility assessment: \nRaters, tasks, and expert criteria. in Proceedings of the ACM on Human-Computer Interaction (CSCW\u201920) , 4, 1\u201326 (2020).\n 16. Pennycook, G. & Rand, D. G. Fighting misinformation on social media using crowdsourced judgments of news source quality. \nProceed. Nat. Acad. Sci. (PNAS)  116, 2521\u20132526 (2019).\n 17. Epstein, Z., Pennycook, G. & Rand, D. Will the crowd game the algorithm?: Using layperson judgments to combat misinformation \non social media by downranking distrusted sources. Proceedings of the Conference on Human Factors in Computing Systems , 1\u201311 \n(2020).\n 18. Allen, J., Howland, B., Mobius, M., Rothschild, D. & Watts, D. J. Evaluating the fake news problem at the scale of the information \necosystem. Sci. Adv.  6, aay3539 (2020).\n 19. Allen, J., Arechar, A. A., Pennycook, G. & Rand, D. G. Scaling up fact-checking using the wisdom of crowds. Sci. Adv.  7, abf4393 \n(2021).\nScientific Reports  |        (2025) 15:25749 11 | https://doi.org/10.1038/s41598-025-09372-6www.nature.com/scientificreports/\n 20. Godel, W . et al. Moderating with the mob: Evaluating the efficacy of real-time crowdsourced fact-checking. J. Online Trust Safety  \n1, 1\u201336 (2021).\n 21. Poynter. Most republicans don\u2019t trust fact-checkers, and most Americans don\u2019t trust the media (2019).  h t t p s :  / / w w w .  p o y n t e  r . o r g /  \ni f c n /  2 0 1 9 /  m  o s t - r e  p u b l i c  a n s - d  o n t - t r  u s t - f a  c t - c h e  c k e r s  - a n d - m  o s t - a m  e r i c a n  s - d o  n t - t r u s t - t h e - m e d i a /.\n 22. Drolsbach, C. P ., Solovev, K. & Pr\u00f6llochs, N. Community notes increase trust in fact-checking on social media. PNAS Nexus  3, \npgae217 (2024).\n 23. Frey, V . & van de Rijt, A. Social influence undermines the wisdom of the crowd in sequential decision making. Manage. Sci.  67, \n4273\u20134286 (2021).\n 24. Woolley, A. W ., Chabris, C. F., Pentland, A., Hashmi, N. & Malone, T. W . Evidence for a collective intelligence factor in the \nperformance of human groups. Science  330, 686\u2013688 (2010).\n 25. Martel, C., Allen, J. N.\u00a0L., Pennycook, G. & Rand, D. Crowds can effectively identify misinformation at scale. PsyArXiv  (2023).\n 26. Pennycook, G. & Rand, D. G. Lazy, not biased: Susceptibility to partisan fake news is better explained by lack of reasoning than \nby motivated reasoning. Cognition  188, 39\u201350 (2019).\n 27. Kahan, D.\u00a0M. Misconceptions, misinformation, and the logic of identity-protective cognition. SSRN  (2017).\n 28. Y asseri, T. & Menczer, F. Can crowdsourcing rescue the social marketplace of ideas?. Commun. ACM  66, 42\u201345 (2023).\n 29. Luca, M. & Zervas, G. Fake it till you make it: Reputation, competition, and Y elp review fraud. Manage. Sci.  62, 3412\u20133427 (2016).\n 30. Conover, M. et al. Political polarization on Twitter. Proceedings of the International AAAI Conference on Web and Social Media , 5, \n89\u201396 (2011).\n 31. Barber\u00e1, P ., Jost, J. T., Nagler, J., Tucker, J. A. & Bonneau, R. Tweeting from left to right: Is online political communication more \nthan an echo chamber?. Psychol. Sci.  26, 1531\u20131542 (2015).\n 32. M. Otala, J. et al. Political polarization and platform migration: A study of Parler and Twitter usage by United States of America \ncongress members. Proceedings of the Web Conference Companion , 224\u2013231 (2021).\n 33. Yin, D., Mitra, S. & Zhang, H. When do consumers value positive vs. negative reviews? An empirical investigation of confirmation \nbias in online word of mouth. in Information Systems Research (ISR) 27, 131\u2013144 (2016).\n 34. Mudambi & Schuff. What makes a helpful online review? A study of customer reviews on Amazon.com. MIS Quarterly  34, \n185\u2013200 (2010).\n 35. Lutz, B., Pr\u00f6llochs, N. & Neumann, D. Are longer reviews always more helpful? Disentangling the interplay between review \nlength and line of argumentation. J. Bus. Res.  144, 888\u2013901 (2022).\n 36. Schlosser, A. E. Can including pros and cons increase the helpfulness and persuasiveness of online reviews? The interactive effects \nof ratings and arguments. J. Consum. Psychol.  21, 226\u2013239 (2011).\n 37. He, S. X. & Bond, S. D. Why Is the Crowd Divided? Attribution for Dispersion in Online Word of Mouth. J. Cons. Res.  41, \n1509\u20131527 (2015).\n 38. Pan, Y . & Zhang, J. Q. Born unequal: A study of the helpfulness of user-generated product reviews. J. Retail.  87, 598\u2013612 (2011).\n 39. Allen, J., Martel, C. & Rand, D. G. Birds of a feather don\u2019t fact-check each other: Partisanship and the evaluation of news in \nTwitter\u2019s Birdwatch crowdsourced fact-checking program. Proceedings of the Conference on Human Factors in Computing Systems , \n1\u201319 (2022).\n 40. Twitter. Introducing Birdwatch, a Community-Based Approach to Misinformation (2021).  h t t p s :  / / b  l o g  . t w i t t  e r . c o m  / e n _ u   s / t o p i  c s \n/ p r o  d u c t / 2  0 2 1 / i  n t r o d u  c i n g - b  i r d w a t  c h - a -  c o  m m u n  i t y - b a  s e d - a p  p r o a c h - t o - m i  s i n f o r m a t i o n.\n 41. Pew Research Center. News Consumption across Social Media in 2021 (2021).  h t t p s :  / / w  w w .  p e w r e s  e a r c h .  o r g / j  o u r n a l  i s m / 2 0  2 1 / 0 \n9 /  2 0 / n e  w s - c o  n  s u m p t i  o n - a c r  o s s - s o c i a l - m e d i a - i n - 2 0 2 1 / .\n 42. Kim, A. & Dennis, A. R. Says who? The effects of presentation format and source rating on fake news in social media. MIS Q.  43, \n1025\u20131039 (2019).\n 43. Shao, C., Ciampaglia, G.\u00a0L., Flammini, A. & Menczer, F. Hoaxy: A platform for tracking online misinformation. in Companion \nProceedings of the Web Conference (WWW\u201916) , pp. 745\u2013750 (2016).\n 44. Juul, J. L. & Ugander, J. Comparing information diffusion mechanisms by matching on cascade size. Proc. Natl. Acad. Sci.  118, \ne2100786118 (2021).\n 45. Friggeri, A., Adamic, L., Eckles, D. & Cheng, J. Rumor cascades. Proceedings of the International AAAI Conference on Web and \nSocial Media  8, 101\u2013110 (2014).\n 46. Pr\u00f6llochs, N., B\u00e4r, D. & Feuerriegel, S. Emotions explain differences in the diffusion of true vs. false social media rumors. Sci. \nReports 11 (2021).\n 47. Pr\u00f6llochs, N., B\u00e4r, D. & Feuerriegel, S. Emotions in online rumor diffusion. EPJ Data Sci.  10, 51 (2021).\n 48. Pr\u00f6llochs, N. & Feuerriegel, S. Mechanisms of true and false rumor sharing in social media: Collective intelligence or herd \nbehavior? in Proceedings of the ACM on Human-Computer Interaction (CSCW\u201923) , 7, 1\u201338 (2023).\n 49. Zhang, Y ., Wang, L., Zhu, J. J. & Wang, X. Conspiracy vs science: A large-scale analysis of online discussion cascades. World Wide \nWeb  24, 585\u2013606 (2021).\n 50. Kauk, J., Humprecht, E., Kreysa, H. & Schweinberger, S. R. Large-scale analysis of online social data on the long-term sentiment \nand content dynamics of online (Mis)information. Comput. Hum. Behav.  165, 108546 (2025).\n 51. Lutz, B., Adam, M., Feuerriegel, S., Pr\u00f6llochs, N. & Neumann, D. Which linguistic cues make people fall for fake news? A \ncomparison of cognitive and affective processing. in Proceedings of the ACM on Human-Computer Interaction (CSCW\u201924) , 8, \n1\u201322 (ACM New Y ork, NY , USA, 2024).\n 52. Shao, C. et al. The spread of low-credibility content by social bots. Nat. Commun.  9, 4787 (2018).\n 53. Shin, J. & Thorson, K. Partisan selective sharing: The biased diffusion of fact-checking messages on social media: Sharing fact-\nchecking messages on social media. J. Commun.  67, 233\u2013255 (2017).\n 54. Shin, J., Jian, L., Driscoll, K. & Bar, F. The diffusion of misinformation on social media: Temporal pattern, message, and source. \nComput. Hum. Behav.  83, 278\u2013287 (2018).\n 55. Osmundsen, M., Bor, A., Vahlstrup, P . B., Bechmann, A. & Petersen, M. B. Partisan polarization is the primary psychological \nmotivation behind political fake news sharing on Twitter. Am. Polit. Sci. Rev.  115, 999\u20131015 (2021).\n 56. Feuerriegel, S. et al. Research can help to tackle AI-generated disinformation. Nat. Hum. Behav.  7, 1818\u20131821 (2023).\n 57. Drolsbach, C. & Pr\u00f6llochs, N. Characterizing AI-generated misinformation on social media. arXiv  (2025).\n 58. Allcott, H. & Gentzkow, M. Social media and fake news in the 2016 election. J. Econ. Perspecti.  31, 211\u201336 (2017).\n 59. Del Vicario, M. et al. The spreading of misinformation online. Proceed. Nat. Acad. Sci. (PNAS)  113, 554\u2013559 (2016).\n 60. Oh, O., Agrawal, M. & Rao, H. R. Community intelligence and social media services: A rumor theoretic analysis of tweets during \nsocial crises. MIS Q.  37, 407\u2013426 (2013).\n 61. Enders, A. M., Uscinski, J., Klofstad, C. & Stoler, J. On the relationship between conspiracy theory beliefs, misinformation, and \nvaccine hesitancy. PLoS ONE  17, e0276082 (2022).\n 62. Loomba, S., de Figueiredo, A., Piatek, S. J., de Graaf, K. & Larson, H. J. Measuring the impact of COVID-19 vaccine misinformation \non vaccination intent in the UK and USA. Nat. Hum. Behav.  5, 337\u2013348 (2021).\n 63. Ecker, U. et al. Misinformation poses a bigger threat to democracy than you might think. Nature  630, 29\u201332 (2024).\n 64. Hassan, N., Arslan, F., Li, C. & Tremayne, M. Toward automated fact-checking: Detecting check-worthy factual claims by \nclaimbuster. in Proceedings of the Conference on Knowledge Discovery and Data Mining (SIGKDD\u201917)  (2017).\n 65. Ma, J. et al.  Detecting rumors from microblogs with recurrent neural networks. in Proceedings of the International Joint Conference \non Artificial Intelligence (ICJAI\u201916) , pp. 3818\u20133824 (2016).\nScientific Reports  |        (2025) 15:25749 12 | https://doi.org/10.1038/s41598-025-09372-6www.nature.com/scientificreports/\n 66. Qazvinian, V ., Rosengren, E., Radev, D.\u00a0R. & Mei, Q. Rumor has it: Identifying misinformation in microblogs. in Proceedings of \nthe Conference on Empirical Methods in Natural Language (EMNLP\u201911) , 1589\u20131599 (USA, 2011).\n 67. Wu, L., Morstatter, F., Carley, K. M. & Liu, H. Misinformation in social media: Definition, manipulation, and detection. SIGKDD \nExplorat. Newsl.  21, 80\u201390 (2019).\n 68. Ma, Y ., He, B., Subrahmanian, N. & Kumar, S. Characterizing and predicting social correction on Twitter. Proceedings of the ACM \nWeb Science Conference , 86\u201395 (2023).\n 69. O\u2019Riordan, S., Kiely, G., Emerson, B. & Feller, J. Do you hav a source for that?: Understanding the challenges of collaborative \nevidence-based journalism. in Proceedings of the International Symposium on Open Collaboration (OpenSym\u201919) , pp. 1\u201310 (2019).\n 70. Florin, F. Crowdsourced Fact-Checking? What We Learned from Truthsquad (2010).  h t t p : /  / m  e d i a  s h i ft   .  o  r g / 2 0  1 0 / 1 1  / c r  o w d  s o u r c \ne  d - f a c  t  - c h e c  k i n g - w  h a t - w e  - l e a r n  e d - f r o m - t r u t h s q u a d 3 2 0 / .\n 71. Bakabar, M. Crowdsourced Factchecking  (2018).  h t t p s :  / / f  u l l  f a c t . o  r g / b l o  g / 2 0 1  8 /  m a y /  c r o w d s  o u r c e d  - f a c t c h e c k i n g /.\n 72. Pr\u00f6llochs, N. Community-based fact-checking on Twitter\u2019s Birdwatch platform. Proceedings of the International AAAI Conference \non Web and Social Media , 16, 794\u2013805 (2022).\n 73. Pilarski, M., Solovev, K.\u00a0O. & Pr\u00f6llochs, N. Community notes vs. Snoping: How the crowd selects fact-checking targets on social \nmedia. in Proceedings of the International AAAI Conference on Web and Social Media (ICWSM\u201924) , 18, 1262\u20131275 (2024).\n 74. Saeed, M., Traub, N., Nicolas, M., Demartini, G. & Papotti, P . Crowdsourced fact-checking at Twitter: How does the crowd \ncompare with experts? Proceedings of the International Conference on Information & Knowledge Management , 1736\u20131746 (2022).\n 75. Bobek, M. & Pr\u00f6llochs, N. Community fact-checks do not break follower loyalty. arXiv  (2025).\n 76. Drolsbach, C. P . & Pr\u00f6llochs, N. Diffusion of community fact-checked misinformation on Twitter.  Proceedings of the ACM on \nHuman-Computer Interaction , 7, 1\u201322 (2023).\n 77. Chuai, Y ., Tian, H., Pr\u00f6llochs, N. & Lenzini, G. Did the roll-out of community notes reduce engagement with misinformation on \nX/Twitter? in Proceedings of the ACM on Human-Computer Interaction (CSCW\u201924) , 8, 1\u201352 (2024).\n 78. Drolsbach, C.\u00a0P . & Pr\u00f6llochs, N. Believability and harmfulness shape the virality of misleading social media posts. in Proceedings \nof the ACM Web Conference (WWW\u201923) , pp. 4172\u20134177 (2023).\n 79. Chuai, Y . et al.  Community-based fact-checking reduces the spread of misleading posts on social media. ArXiv  (2024).\n 80. Chuai, Y ., Sergeeva, A., Lenzini, G. & Pr\u00f6llochs, N. Community fact-checks trigger moral outrage in replies to misleading posts \non social media. in Proceedings of the Conference on Human Factors in Computing Systems (CHI\u201925)  (2025).\n 81. Schwenk, C. R. Information, cognitive biases, and commitment to a course of action. Acad. Manag. Rev.  11, 298\u2013310 (1986).\n 82. Connors, L., Mudambi, S.\u00a0M. & Schuff, D. Is it the review or the reviewer? A multi-method approach to determine the antecedents \nof online review helpfulness. in Proceedings of the Hawaii International Conference on System Sciences (HICSS\u201911) , 1\u201310 (2011).\n 83. Li, M., Huang, L., Tan, C.-H. & Wei, K.-K. Helpfulness of online product reviews as seen by consumers: Source and content \nfeatures. Int. J. Electron. Commer.  17, 101\u2013136 (2013).\n 84. Tversky, A. & Kahneman, D. Judgment under uncertainty: Heuristics and biases. Science  185, 1124\u20131131 (1974).\n 85. Stroud, N. J. Polarization and partisan selective exposure. J. Commun.  60, 556\u2013576 (2010).\n 86. Pornpitakpan, C. The persuasiveness of source credibility: A critical review of five decades\u2019 evidence. J. Appl. Soc. Psychol.  34, \n243\u2013281 (2004).\n 87. Fragale, A. R. & Heath, C. Evolving informational credentials: The (Mis)attribution of believable facts to credible sources. Pers. \nSoc. Psychol. Bull.  30, 225\u2013236 (2004).\n 88. Traberg, C. S. & van der Linden, S. Birds of a feather are persuaded together: Perceived source credibility mediates the effect of \npolitical bias on misinformation susceptibility. Personality Individ. Differ.  185, 111269 (2022).\n 89. Hovland, C. I. & Weiss, W . The influence of source credibility on communication effectiveness. Public Opin. Q.  15, 635\u2013650 \n(1951).\n 90. Chuai, Y ., Zhao, J., Pr\u00f6llochs, N. & Lenzini, G. Is Fact-checking politically neutral? Asymmetries in how U.S. fact-checking \norganizations pick up false statements mentioning political elites. in Proceedings of the International AAAI Conference on Web and \nSocial Media (ICWSM\u201925) , forthcoming (2025).\n 91. Van Bavel, J. J. & Pereira, A. The partisan brain: An identity-based model of political belief. Trends Cogn. Sci.  22, 213\u2013224 (2018).\n 92. Gonz\u00e1lez-Bail\u00f3n, S., d\u2019 Andrea, V ., Freelon, D. & De Domenico, M. The advantage of the right in social media news sharing. PNAS \nNexus  1, pgac137 (2022).\n 93. Wojcik, S. et al.  Birdwatch: Crowd wisdom and bridging algorithms can inform understanding and reduce the spread of \nmisinformation. arXiv  (2022).\n 94. X. Note ranking algorithm.  h t t p s :  / / c o m m  u n i t y n  o t e s . t  w i t t e  r . c o m /  g u i d e /  e n  / u n d  e r - t h e - h o o d / r a n k i n g - n o t e s (2024).\n 95. Korfiatis, N., Garc\u00eda-Bariocanal, E. & S\u00e1nchez-Alonso, S. Evaluating content quality and helpfulness of online product reviews: \nThe interplay of review helpfulness vs. review content. Electr. Comm. Res. Appl.  11, 205\u2013217 (2012).\n 96. Cinelli, M., De Francisci Morales, G., Galeazzi, A., Quattrociocchi, W . & Starnini, M. The echo chamber effect on social media. \nProceed. Nat. Acad. Sci. (PNAS)  118, e2023301118 (2021).\n 97. Rinker, T. W . Sentimentr: Calculate text polarity sentiment  (Buffalo, New Y ork, 2019).\n 98. Mohammad, S. M. & Turney, P . D. Crowdsourcing a word-emotion association lexicon. Comput. Intell.  29, 436\u2013465 (2012).\n 99. Feuerriegel, S. et al. Using natural language processing to analyse text data in behavioural science. Nat. Rev. Psychol.  4, 96\u2013111 \n(2025).\n 100. Zhang, X. et al. TwHIN-Bert: A socially-enriched pre-trained language model for multilingual tweet representations at Twitter. \nProceedings of the ACM Conference on Knowledge Discovery and Data Mining , 5597\u20135607 (2023).\n 101. Buis, M. L. Stata Tip 87: Interpretation of interactions in nonlinear models. Stand. Genomic Sci.  10, 305\u2013308 (2010).\n 102. Okoli, C., Mehdi, M., Mesgari, M., Nielsen, F. \u00c5. & Lanam\u00e4ki, A. Wikipedia in the eyes of its beholders: A systematic review of \nscholarly research on Wikipedia readers and readership. J. Am. Soc. Inf. Sci.  65, 2381\u20132403 (2014).\n 103. Walter, N., Cohen, J., Holbert, R. L. & Morag, Y . Fact-checking: A meta-analysis of what works and for whom. Polit. Commun.  37, \n350\u2013375 (2020).\n 104. X. Note ranking algorithm.  h t t p s :   /  /  c o m m u n i t y n o t e  s .  t w i t t e   r . c  o m  / g u i  d  e  / e n /  u n  d e r  - t h e -  h o o d / r a  n k i n g - n o t e s (2025).\n 105. Dimoka, A., Pavlou, P . A. & Davis, F. D. NeuroIS: The potential of cognitive neuroscience for information systems research. \nInform. Syst. Res.(ISR)  22, 687\u2013702 (2011).\nAcknowledgements\nThis work was supported by a research grant from the German Research Foundation (DFG Grant No. 492310022). \nThe funder had no role in study design, data collection and analysis, decision to publish or preparation of the \nmanuscript.\nAuthor contributions\nK.S. and N.P . conceived the study. K.S. analyzed the data. K.S. and N.P . wrote and reviewed the paper.\nScientific Reports  |        (2025) 15:25749 13 | https://doi.org/10.1038/s41598-025-09372-6www.nature.com/scientificreports/\nFunding\nOpen Access funding enabled and organized by Projekt DEAL.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary Information  The online version contains supplementary material available at  h t t p s : / / d o  i . o r g / 1 \n0 . 1 0 3 8 / s 4 1 5 9 8 - 0 2 5 - 0 9 3 7 2 - 6     .  \nCorrespondence  and requests for materials should be addressed to N.P .\nReprints and permissions information  is available at www.nature.com/reprints .\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access   This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give \nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and \nindicate if changes were made. The images or other third party material in this article are included in the article\u2019s \nCreative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included \nin the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy \nof this licence, visit http://creativecommo ns.org/licenses/by/4.0/ .\n\u00a9 The Author(s) 2025 \nScientific Reports  |        (2025) 15:25749 14 | https://doi.org/10.1038/s41598-025-09372-6www.nature.com/scientificreports/", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "References to unbiased sources increase the helpfulness of community fact-checks", "author": ["K Solovev", "N Pr\u00f6llochs"], "pub_year": "2025", "venue": "Scientific Reports", "abstract": "Community-based fact-checking is a promising approach to address misinformation on  social media at scale. However, an understanding of what makes community-created fact-checks"}, "filled": false, "gsrank": 2, "pub_url": "https://www.nature.com/articles/s41598-025-09372-6", "author_id": ["dOv3wFUAAAAJ", "jY8BMSgAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:vHr0-M_4bTMJ:scholar.google.com/&output=cite&scirp=1&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=vHr0-M_4bTMJ&ei=BLWsaNaSEPnSieoPxKLpgQ0&json=", "num_citations": 2, "citedby_url": "/scholar?cites=3705891640514476732&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:vHr0-M_4bTMJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://www.nature.com/articles/s41598-025-09372-6.pdf"}}, {"title": "Frank at CheckThat!-2023: Detecting the Political Bias of News Articles and News Media.", "year": "2023", "pdf_data": "Frank at CheckThat! 2023: Detecting the Political Bias\nof News Articles and News Media\nNotebook for the CheckThat! Lab at CLEF 2023\nDilshod Azizov1,\u02da, Preslav Nakov1and Shangsong Liang1\n1Mohamed bin Zayed University of Artificial Intelligence, UAE\nAbstract\nThis paper addresses the challenge of detecting political bias in news articles and media outlets from\nCheckThat! lab Task 3 [ 1,2] by proposing an automated method for classifying these as left, center, or\nright-leaning. As mass media consumption continues to grow, the capability to identify bias in news\nreporting is crucial due to the potential societal impact of unaddressed political bias. To tackle this issue,\nwe present a comprehensive approach employing machine learning techniques to detect political leaning\nin news media and articles. Our model, CatBoost, is evaluated on a diverse dataset comprising over\n55,000 news articles sourced from AllSides1at the article-level. For each model, we aggregate predictions\nmade across news items by a single medium using a majority voting system at medium-level. Our dataset\ngathered and annotated from over 1,000 popular online platforms as rated by Media Bias/Fact Check2,\ncategorizes political bias into the left, center, or right-wing. We have approximately ten articles from each\nof these platforms, yielding over 8,000 articles in total. We employ both CatBoost and CatBoost OF3for\nmedia-level classification. These effectively detect political ideology across various media sources, with\nour CatBoost model demonstrating robustness and effectiveness in handling diverse data. Our findings\nsuggest that utilizing the majority voting technique at the medium level improves model performance.\nWe also highlight the importance of addressing class imbalance and implementing balanced data splits\nto enhance model performance. Regarding article-level classification using CatBoost, we achieve a\nMean Absolute Error (MAE) of 0.270, an F1 score of 0.690, and an accuracy of 0.694. For media-level\nclassification, we achieve a competitive MAE of 0.320, and with the use of the majority voting classifier,\nour model attains an F1 score of 0.727 and an accuracy of 0.725.\nKeywords\npolitical bias, news articles, news media\n1. Introduction\nThe political leaning of news articles and news media has become an increasingly important\ntopic in today\u2019s world of information overload. How news is presented and reported can have\na significant impact on people\u2019s perceptions, beliefs, and even voting behaviors [ 3]. A recent\nstudy has shown that political bias may influence citizens\u2019 voting decisions and can change\n1www.allsides.com\n2www.mediabiasfactcheck.com\n3OF - operating only on the first 300 features from TF-IDF\nCLEF 2023: Conference and Labs of the Evaluation Forum, September 18\u201321, 2023, Thessaloniki, Greece\n\u02daCorresponding author.\n/envel\u2322pe-\u2322pendilshod.azizov@mbzuai.ac.ae (D. Azizov); preslav.nakov@mbzuai.ac.ae (P. Nakov);\nshangsong.liang@mbzuai.ac.ae (S. Liang)\n\u00a92023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\nCEUR\nWorkshop\nProceedingshttp://ceur-ws.org\nISSN 1613-0073\nCEUR Workshop Proceedings (CEUR-WS.org)\n\nthe voting preferences of undecided individuals at least by 20% [ 4]. Therefore, detecting the\npolitical leaning of news articles and news media has become crucial for researchers, journalists,\nand policymakers [5].\nOne of the biggest challenges in detecting the political leaning of news articles and news media\nis the lack of a standardized approach [ 6]. Researchers and journalists use various methods\nto determine the political leaning of news and news outlets, such as manual coding, content\nanalysis, and sentiment analysis[ 7,8]. However, these methods have limitations, including\nsubjectivity, biases, and the inability to detect nuanced political perspectives. Moreover, the\nincreasing use of social media and online news platforms has made it even more challenging\nto detect the political leaning of news. With the rise of user-generated content, identifying\nthe political orientation of a particular news article or media outlet has become more complex\n[9, 10, 11].\nTheCheckThat! lab CLEF 2023 [ 12,1,13] has initiated several tasks aimed at contributing to\nthe scientific community. In CheckThat! lab CLEF 2023, task 3 [ 1] seeks to solve the problem of\ndetecting political bias at both the article and medium levels. To address this issue, we propose\na CatBoost framework to predict political bias, and we present the results of our model. Our\nresearch contributes to the scientific community by proposing a new system for predicting\npolitical bias in news articles and news media. We believe that our study will contribute to\nadvancing the field of political bias detection.\n2. Related Work\nPrior research on detecting ideological biases has been emphasized in several studies [ 14,15,16,\n17, 18, 19, 6, 20, 21, 22, 23, 24, 25, 26, 27].\nGentzkow and Shapiro in 2010 [ 17] slant index was the initial attempt to rate the ideological\nstance of news providers based on the frequency of partisan phrases or co-allocations used\nin news content. Lin et al. [ 18] proposed a statistical framework to identify the perspective\nfrom which a document is written with high accuracy. However, their insufficient dataset\nrestricted the use of contemporary deep learning techniques. [ 14] determined the political\nstance displayed by a text by using a recursive neural network (RNN) framework. Similarly,\n[16] examined the selection and framing of political problems in fifteen significant US news\norganizations using machine learning and crowd-sourcing.\nRecent advances in predicting the political ideology of news media and news articles have\nleveraged various aspects such as media stance, factuality, and media profile [ 28,8,7,29,30,31,\n32, 33, 34, 35, 36, 37, 38].\nIn recent years, Kulkarni et al. [ 31] used an attention-based multi-view model, leveraging cues\nfrom neural inference and natural language processing, to identify news article ideology, achiev-\ning higher performance than existing models. However, their method has some potential issues.\nHorne et al. [ 32] introduced the News Landscape (NELA) Toolbox. This open-source toolkit al-\nlows for investigating news veracity using content-based indicators as a step toward automated\nnews credibility research. Kiesel et al. [ 33] created a large-scale dataset for hyper-partisan news\ndetection and organized a successful SemEval shared task, with the top team achieving a high\naccuracy rate. Potthast et al. [ 34] used a meta-learning approach called unmasking to evaluate\nstyle similarity between text categories, revealing significant commonalities and differences\nbetween news types, but found it inadequate for fake news identification. Rashkin et al. [ 35]\nconducted a study on the language of news media, contrasting the language of legitimate news\nwith satire, hoaxes, and propaganda, and highlighted the potential of stylistic clues in assessing\ntext veracity. Jiang et al. [ 36] developed a system using averaged word embeddings from a\npre-trained ELMo model, achieving first place in a hyper-partisan news detection challenge.\nBarron et al. developed a model to identify propagandistic material in articles, highlighting the\neffectiveness of character n-grams and other style criteria over word n-grams while discussing\nthe drawbacks of distant supervision. Martino et al. [ 37] proposed identifying all fragments\ncontaining propaganda techniques in a text and developed a corpus of manually annotated\nnews articles for this purpose, demonstrating the effectiveness of a novel multi-granularity\nneural network.\nDinkov et al. [ 39] developed a multimodal deep-learning architecture to predict the political\nideology of news media by studying YouTube channels, which resulted in an extensive multi-\nmodal dataset. Another approach by [ 8] involves assessing each document\u2019s stance towards a\nclaim and predicting the claim\u2019s factuality while also taking into account the source\u2019s credibility.\nThe same corpus was annotated to reflect the interdependencies between these tasks, yielding\nan advanced Arabic fact-checking corpus. The political bias and factual accuracy of news\nmedia have also been studied by [ 29], where posture detection has been introduced as a crucial\npart of fact-checking systems. This study by [ 28] employed information from various sources,\nsuch as media-produced pieces, Wikipedia pages, Twitter profile metadata, and online features.\nLastly, the issue of assessing the bias and factuality of online news sources has been studied\nby [38]. The researchers modeled the similarity between media outlets based on audience\noverlap, contrasting with text-based approaches. The resulting inter-media connectivity graph,\nprocessed through graph neural networks, led to better predictions of the factuality and bias\nof news media sources when supplemented with pre-computed representations from various\nplatforms.\nIn conclusion, prior and recent research have explored various approaches for predicting\nthe political ideology of news media, examining the general stance. Studies found that joint\nprediction of factuality and political bias proved more advantageous than predicting each\nseparately [ 8,28,38]. However, more than traditional bias detection methods are required to\nachieve highly accurate results, and a more nuanced and interdisciplinary approach is necessary\nfor future research. As such, there is a need to undertake further research to develop and quantify\nmore recent studies on detecting ideological biases to enhance the fundamental background of\nbias detection methods.\n3. Method\nIn the following section, we outline our methodological approach.\nTF\u2013IDF. We used the Term Frequency-Inverse Document Frequency (TF-IDF) technique to\nvectorize our text data [ 40]. This method calculates the frequency of each term in each article\nand assigns a weight to each term based on its importance in the article and its frequency in\nthe dataset. This allowed us to feed our textual data into our models and obtain a numerical\nFigure 1: Majority voting architecture. Source: www.researchgate.net\nrepresentation of the articles.\nK\u2013means. We used the K-means algorithm to cluster similar articles based on their political\nbias [ 41]. This helped us better understand the distribution of political ideologies in our dataset\nand enabled us to identify any outliers or anomalies in the data.\nCatBoost. CatBoost [ 42] is an open-source machine learning library developed by Yandex,\ndesigned explicitly for gradient boosting on decision trees. It offers a highly efficient and\naccurate approach to handling categorical features, leveraging a special algorithm to avoid\ntarget leakage, and provides numerous options for model interpretation. Its advantages include\nfast prediction times, robust handling of categorical variables, and a feature that allows it to\nhandle missing data.\nMajority voting. We use the majority voting technique as another approach, which is\na method employed in ensemble learning [ 43,44] by aggregating the predictions made over\nthe news by a single medium at medium level. Figure 1 demonstrates the architecture of our\nproposed majority voting approach. It determines the final prediction for a given data point by\nselecting the class or outcome that receives the majority of votes from the ensemble models.\nGiven a set of \ud835\udc5aclassifiers, \ud835\udc361, \ud835\udc362, . . . , \ud835\udc36 \ud835\udc5a, and an input data point \ud835\udc65, each classifier makes a\nprediction for the class label: \ud835\udc431, \ud835\udc432, . . . , \ud835\udc43 \ud835\udc5b. The majority voting classifier determines the final\noutput class label \ud835\udc43\ud835\udc53by selecting the class with the most votes from the individual classifier\npredictions.\nMathematically, the majority voting classifier can be represented as:\n\ud835\udc43\ud835\udc53\u201cmodep\ud835\udc431, \ud835\udc432, . . . , \ud835\udc43 \ud835\udc5aq (1)\nIn this Equation 1, the mode function returns the class label that appears most frequently\namong the individual classifier predictions, where \ud835\udc43\ud835\udc53is the final prediction.\nFor our task, we used hard voting, as we were afraid that the classifiers were not well calibrated,\ne.g., they can be over-confident in their decisions even when they are wrong; put another way,\nthe classifier might not know when it does not know, and thus their output probability might\nnot be usable directly. Thus, we opted for majority voting. We leave calibration and subsequent\nsoft-voting for future work.\nFigure 2: Distribution of topics in news articles (Subtask 3A).\n4. Datasets\n4.1. Political Bias of News Articles (Subtask 3A)\nThe dataset for this study includes a comprehensive and diverse collection of news articles from\ndifferent news agencies collected from Allsides1.\nData Attributes\nFor each news article, the following information is available:\n\u2022ID: A unique identifier for the article.\n\u2022Title: The headline of the article.\n\u2022Content: The full text of the article.\n\u2022Label: The political leaning of the news article as left, center, or right.\nData Size: The dataset contains a substantial number of articles from each political leaning.\nIn total, we have over 55K articles in a dataset.\nIn Figure 2, we observe the notable wide range of topic distribution in the dataset. Our dataset\nhas a broad range of topics, including elections, domestic and foreign policy, finance, and others,\nwith \u201celections\u201d and \u201cother\u201d being the most common topics. Also, Figure 3 shows the class label\ndistribution and the number of articles in each subset. We can see that there are more articles\nwith a right-leaning stance in almost all subsets, with fewer articles in the left-leaning category.\nMeanwhile, the center class remains consistently in the middle in terms of size.\n1www.allsides.com\nFigure 3: Statistics about the label distribution in the dataset\u2019s training, development, and test parts\nSubtask 3A.\nTable 1\nExamples of news outlets and their biases.\nLeft Center Right\nThe Guardian BBC News Fox News\nThe New York Times Reuters The Daily Caller\nThe Washington Post The Associated Press The National Review\n4.2. Political Bias of News Media (Subtask 3B)\nWe use a dataset for assessing the political bias of English-language media, sourced from\nCheckThat! lab CLEF 2023 which has been crawled from Media Bias/Fact Check2. Table 1\npresents examples of news outlets and their corresponding biases.\nData Attributes\nThis dataset have similar attributes to subtask 3 A, plus the source (name of the medium) as\nan additional attribute.\nDataset size: We have over 1\u2019000 news media outlets and around 8\u2019000 articles approximately\n10 articles per each source.\nFigure 4 shows the label distribution for the articles from these media across the three subsets.\nWe can see that the distribution is once again relatively balanced, with similar numbers of\ninstances for each label in each subset. This is important as it ensures that the model trained on\nthe train set will have no biases towards any particular label and will be able to generalize well\nto the development and the test sets.\nFigure 5 shows that over 800 media are used for training set, about 100 for dev, and slightly\nover 100 for testing. As illustrated in Figure 3, we have a modestly imbalanced data towards the\nleft-leaning news outlets, while the distribution of the other two classes are almost equal.\n2www.mediabiasfactcheck.com\nFigure 4: Statistics about the number of articles of each bias across all news outlets (Subtask 3B).\nFigure 5: Statistics about the label distribution for news outlets across the dataset splits (Subtask 3B).\n\nTable 2\nPerformance of our models at article-level and medium-level dataset.\nTask Model MAE F1 Score Accuracy\nPolitical Bias of News Articles Baseline 0.877\nPolitical Bias of News Articles CatBoost 0.270 0.690 0.694\nPolitical Bias of News Media Baseline 0.902\nPolitical Bias of News Media CatBoost 0.320 0.620 0.621\nPolitical Bias of News Media CatBoost OF 0.375 0.537 0.538\nPolitical Bias of News Media Majority voting (CatBoost) 0.727 0.725\nPolitical Bias of News Media Majority voting (CatBoost OF) 0.621 0.625\n5. Experiments and Results\n5.1. Experimental Setup\nIn this subsection, we provide a detailed description of the experimental setup for our models.\nBaseline. The default experimental setup has been used for SVM model with a linear kernel.\nCatBoost. For the CatBoost model, we considered the following hyperparameters: learning\nrate 0.1, depth 6, the maximum number of boosting iterations rate set to 10000, the best model\nfrom all iterations is selected as the final model, and the frequency of logging information\nduring a training set to 500, which means that the training progress will be printed every 500\niterations. At medium-level, we modified the number of iterations to 2000, the learning rate to\n0.05, and set the progress to log iterations to 100.\nCatBoost OF. In the case of the CatBoost model using only the first 300 most important\nfeatures from the TF-IDF, the experimental setup is slightly modified. The model is trained using\nonly the top 300 most important features derived from the TF-IDF representation of the dataset,\nfocusing on the most relevant information for the classification task. The number of iterations is\nset to 1000, which means the model trained for 1000 boosting rounds. This parameter determines\nthe maximum number of trees that can be built by the model. The learning rate is set to 0.05,\ncontrolling the contribution of each tree to the ensemble model. A lower learning rate generally\nresults in a more robust model, albeit at the expense of longer training times. The frequency of\nlogging the training progress is set to 500 iterations, meaning that the training progress will be\nlogged every 500 iterations, providing less frequent updates compared to the previous setup.\n5.2. Results\nIn this subsection, we present the results of our experiments on predicting the political bias of\nnews articles and news outlets. We applied a majority voting ensemble approach to each model\nby aggregating the predictions made over the news by a single medium. The performance of\nthe individual models and the majority voting ensemble is summarized in Table 2.\nBaseline. The task completion\u2019s foundational system was provided by the organizers. A\ntraditional machine learning approach Support Vector Machine (SVM) was selected for this\npurpose.\nFigure 6: Histogram of the article length in terms of symbols for our dataset.\nFigure 7: Histogram of the article length in terms of words for our dataset.\nCatBoost (Subtask A). This model consistently yielded the best results in our experiments.\nIn terms of MAE model reached 0.270 an F1 score 0.620 and an accuracy 0.621. This can be\nattributed to its robust gradient-boosting algorithm and optimized implementation for decision\ntree learning. Moreover, the algorithm incorporates built-in regularization techniques that\nhelp prevent overfitting and improve generalization, which is crucial for maintaining high\nperformance on unseen data.\nFigure 6 shows a plot illustrating the distribution of article lengths in terms of dataset\ncharacters. Our dataset exhibits a long-tailed distribution, with most articles having symbolic\nlengths peaking at 100\u2013150 and 350\u2013500 characters.\nSimilarly, Figure 7 shows the distribution of article lengths in terms of words in our dataset,\nFigure 8: CatBoost feature importance on our dataset.\nalbeit with slightly lower numbers. In our dataset, the majority of the articles have word counts\nwithin the ranges of 10\u201330 and 50\u201365 words.\nThe relationship between symbolic count and word count is evident; a higher symbolic length\ngenerally corresponds to a higher word count. This observation underscores the interdependence\nbetween these two factors in the structure and the content of the articles in our dataset.\nNext, we analyzed the feature importance of CatBoost for classifying the political bias in\nnews articles using our dataset. We considered five features: symbolic length, word length,\nlemmas, content, title, and all text. The results are shown in Figure 8. Our observations indicate\nthat word and symbolic length were not significant features of the model. In our machine\nlearning model, we computed feature importance using the \u2019SelectPercentile\u2019 method and \ud835\udc36\u210e\ud835\udc562\ncriterion. We ranked features by \ud835\udc36\u210e\ud835\udc562scores, indicating their relevance to the target variable.\nFor our dataset, all text emerged as the most important feature, followed by title, content,\nand lemmas. Our analysis demonstrates that feature importance varies between them. It is\nimportant to note that using all text as a feature when training the model on our data accelerates\nthe learning process.\nNext, we computed a confusion matrix, which provides a convenient way to assess our model\u2019s\nclassification performance by presenting the number of correct and incorrect predictions for\neach class in a tabular format, allowing us to identify patterns and areas where the model may\nstruggle. As shown in Figure 9, the confusion matrix for our model predictions on our dataset\nreveals distinct differences in performance across the political bias categories.\nIn our dataset, the model exhibits a substantial performance in predicting center- and right-\nleaning articles, whereas it struggles to classify left-leaning ones accurately.\nWe further investigated the performance of CatBoost a per-class level on our dataset, and the\nresults are shown in Table 3.\nFigure 9: Confusion matrix for CatBoost on the test set.\nTable 3\nPer-class results for CatBoost.\nClass Precision Recall F1-score Support\ncenter 0.61 0.83 0.70 2,959\nleft 0.78 0.36 0.49 2,589\nright 0.37 0.75 0.49 650\nAccuracy 0.59 5,198\nMacro Avg 0.58 0.65 0.56 5,198\nWeighted Avg 0.66 0.59 0.57 5,198\nTable 3 shows the classification report for CatBoost. The model achieves an overall accuracy\nof 0.59, with a macro-average F1-score of 0.56 and a weighted F1-score of 0.57. It performs best\nin classifying center-leaning articles with an F1-score of 0.70, while it has lower F1-scores of\n0.49 for both left- and right-leaning articles. Support refers to the number of actual occurrences\nof the class in the specified dataset. In our case, it means the number of instances for each class\n(\u201cCenter\u201d, \u201cLeft\u201d, and \u201cRight\u201d) present in the dataset. For example, there are 2959 instances of\n\u201cCenter\u201d, 2589 instances of \u201cLeft\u201d, and 650 instances of \u201cRight\u201d. For \u201cAccuracy\u201d, \u201cMacro Avg\u201d,\nand \u201cWeighted Avg\u201d, the support is the total number of instances, which is 5198 in this case.\nCatBoost (Subtask B). At medium-level CatBoost also consistently achieved superior results\nthroughout our experiments. Moreover, it reached MAE with a score of 0.320 and after applying\nmajority voting we have solid increase in terms of an F1 score and an accuracy which are 0.727\nand 0.725, respectively. The confusion matrix in Figure 10 for the test set of the CatBoost model\nreveals that the model demonstrates strong predictive performance for center-leaning and right-\nleaning categories. However, its accuracy in predicting left-leaning instances is comparatively\nlower than that of the other classes.\nIn Table 4 CatBoost model demonstrates the highest precision for the left class at 0.64, followed\nclosely by the center class at 0.63. The right class exhibits a slightly lower precision of 0.60.\nIn terms of recall, the right class outperforms the others with a value of 0.68. The left and\nFigure 10: Confusion matrix of CatBoost on test set.\nFigure 11: CatBoost feature importance.\ncenter classes show recall values of 0.60 and 0.58, respectively. The F1-score, which balances\nprecision and recall, indicates that the left class achieves a slightly better score of 0.62 compared\nto the center and right classes, which have F1-scores of 0.60 and 0.63, respectively. The overall\naccuracy of the CatBoost model stands at 0.62, while the macro and weighted averages for\nprecision, recall, and F1-score are also equal to 0.62.\nCatBoost OF. The CatBoos OF model incorporates the top 300 most important features\nderived from the Term Frequency-Inverse Document Frequency (TF-IDF) method. Despite a\nrelatively modest Mean Absolute Error (MAE) of 0.375, an F1 score of 0.537, and an accuracy of\n0.538, the model exhibited less favorable results compared to other CatBoost models, securing\nthe bottom position. This held true even when a majority voting approach was applied to\nenhance the performance of our model.\nTable 4\nPer-class results for the Catboost.\nClass Precision Recall F1-score Support\nCenter 0.63 0.58 0.60 526\nLeft 0.64 0.60 0.62 536\nRight 0.60 0.68 0.63 564\nAccuracy 0.62 1626\nMacro Avg 0.62 0.62 0.62 1626\nWeighted Avg 0.62 0.62 0.62 1626\nTable 5\nPerformance of CatBoost vs. BERT with random vs. balanced splits on our article-level dataset.\nModel Split Type MAE F1 Score Accuracy\nCatBoost Random 0.322 0.585 0.586\nCatBoost Balanced 0.265 0.703 0.705\nBERT Random 0.385 0.491 0.499\nBERT Balanced 0.351 0.565 0.569\n6. Discussion\nDuring the initial stage of our analysis, we observed that our dataset exhibited an unbalanced\ndistribution of classes. To investigate the impact of balanced splits on model performance, we\ncreated train, test, and development sets with balanced class distributions. Our findings revealed\nthat using balanced splits led to improved results, as shown in Table 5. This suggests that\nbalancing the dataset can contribute to more accurate and reliable predictions. Upon conducting\na comparative analysis of the BERT and CatBoost models, we have chosen to implement the\nCatBoost model in our experiments. This decision was guided by its demonstrably superior\nperformance compared to BERT.\n7. Conclusion and Future Work\nWe explored predicting the political bias of news articles and news media using a dataset\ncomprising 55,000 news articles and over 1,000 news media sources with over 8,000 articles. We\nproposed a new model, CatBoost, which was used to determine the political leaning. Further-\nmore, we employed the majority voting technique to enhance our model\u2019s performance at the\nmedia level.\nOur approach effectively classified the political bias, yielding consistent results. The CatBoost\nmodel trained on our article-level dataset achieved a classification accuracy of 0.690, an F1 score\nof 0.694, and a MAE of 0.270. When the model was applied to the medium-level, it reached an\nMAE of 0.320.\nBy implementing the majority voting classifier, which aggregates the predictions made over\nthe news by a single medium, we achieved an enhanced F1 score of 0.727 and an accuracy of\n0.725. Our comprehensive experiments showed that CatBoost consistently performed effectively.\nHowever, the CatBoost OF model delivered the least effective results on the media-level dataset.\nWe noticed that applying majority voting to the news from a single medium improved each\nmodel\u2019s performance.\nIn future work, we aim to explore topic-level bias prediction and move beyond the left-center-\nright political bias classification. This may require collecting additional labels and breaking\naway from the current 3-way classification. Furthermore, we intend to carry out cross-language\nexperiments to adapt these methods to languages other than English. Another potential area\nof investigation is predicting veracity and political leaning simultaneously, necessitating the\ndevelopment of models incorporating both textual and non-textual features, such as source\ncredibility or author reputation.\nReferences\n[1] G. Da San Martino, F. Alam, M. Hasanain, R. N. Nandi, D. Azizov, P. Nakov, Overview of\nthe CLEF-2023 CheckThat! lab task 3 on political bias of news articles and news media, in:\nWorking Notes of CLEF 2023\u2013Conference and Labs of the Evaluation Forum, CLEF \u20192023,\nThessaloniki, Greece, 2023.\n[2]A. Barr\u00f3n-Cede\u00f1o, F. Alam, T. Caselli, G. Da San Martino, T. Elsayed, A. Galassi, F. Haouari,\nF. Ruggeri, J. M. Struss, R. N. Nandi, G. S. Cheema, D. Azizov, P. Nakov, The clef-2023\ncheckthat! lab: Checkworthiness, subjectivity, political bias, factuality, and authority, in:\nJ. Kamps, L. Goeuriot, F. Crestani, M. Maistro, H. Joho, B. Davis, C. Gurrin, U. Kruschwitz,\nA. Caputo (Eds.), Advances in Information Retrieval, Springer Nature Switzerland, Cham,\n2023, pp. 506\u2013517.\n[3]G. Pennycook, D. G. Rand, The psychology of fake news, Trends in cognitive sciences 25\n(2021) 388\u2013402.\n[4]G. Gezici, Quantifying political bias in news articles, arXiv preprint arXiv:2210.03404\n(2022).\n[5]R. R. R. Gangula, S. R. Duggenpudi, R. Mamidi, Detecting political bias in news articles using\nheadline attention, in: Proceedings of the 2019 ACL workshop BlackboxNLP: analyzing\nand interpreting neural networks for NLP, 2019, pp. 77\u201384.\n[6]P. Nakov, H. T. Sencar, J. An, H. Kwak, A survey on predicting the factuality and the bias\nof news media, arXiv preprint arXiv:2103.12506 (2021).\n[7]R. Baly, G. D. S. Martino, J. Glass, P. Nakov, We can detect your bias: Predicting the political\nideology of news articles, arXiv preprint arXiv:2010.05338 (2020).\n[8]R. Baly, G. Karadzhov, D. Alexandrov, J. Glass, P. Nakov, Predicting factuality of reporting\nand bias of news media sources, arXiv preprint arXiv:1810.01765 (2018).\n[9]D. Saez-Trumper, C. Castillo, M. Lalmas, Social media news communities: gatekeeping,\ncoverage, and statement bias, in: Proceedings of the 22nd ACM international conference\non Information & Knowledge Management, 2013, pp. 1679\u20131684.\n[10] K. Darwish, P. Stefanov, M. Aupetit, P. Nakov, Unsupervised user stance detection on\nTwitter, in: Proceedings of the International AAAI Conference on Web and Social Media,\nvolume 14, 2020, pp. 141\u2013152.\n[11] R. Baly, G. Karadzhov, J. An, H. Kwak, Y. Dinkov, A. Ali, J. Glass, P. Nakov, What was written\nvs. who read it: News media profiling using text analysis and social media context, in:\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics,\nACL \u201920, Association for Computational Linguistics, Online, 2020, pp. 3364\u20133374. URL:\nhttps://aclanthology.org/2020.acl-main.308. doi: 10.18653/v1/2020.acl-main.308 .\n[12] F. Alam, A. Barr\u00f3n-Cede\u00f1o, G. S. Cheema, S. Hakimov, M. Hasanain, C. Li, R. M\u00edguez,\nH. Mubarak, G. K. Shahi, W. Zaghouani, P. Nakov, Overview of the CLEF-2023 CheckThat!\nlab task 1 on check-worthiness in multimodal and multigenre content, in: Working Notes\nof CLEF 2023\u2013Conference and Labs of the Evaluation Forum, CLEF \u20192023, Thessaloniki,\nGreece, 2023.\n[13] P. Nakov, F. Alam, G. Da San Martino, M. Hasanain, R. N. Nandi, D. Azizov, P. Panayotov,\nOverview of the CLEF-2023 CheckThat! lab task 4 on factuality of reporting of news\nmedia, in: Working Notes of CLEF 2023\u2013Conference and Labs of the Evaluation Forum,\nCLEF \u20192023, Thessaloniki, Greece, 2023.\n[14] M. Iyyer, P. Enns, J. Boyd-Graber, P. Resnik, Political ideology detection using recursive\nneural networks, in: Proceedings of the 52nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), 2014, pp. 1113\u20131122.\n[15] J. H. Gross, B. Acree, Y. Sim, N. A. Smith, Testing the etch-a-sketch hypothesis: a computa-\ntional analysis of mitt romney\u2019s ideological makeover during the 2012 primary vs. general\nelections, in: APSA 2013 Annual Meeting Paper, American Political Science Association\n2013 Annual Meeting, 2013.\n[16] C. Budak, S. Goel, J. M. Rao, Fair and balanced? quantifying media bias through crowd-\nsourced content analysis, Public Opinion Quarterly 80 (2016) 250\u2013271.\n[17] M. Gentzkow, J. M. Shapiro, What drives media slant? evidence from us daily newspapers,\nEconometrica 78 (2010) 35\u201371.\n[18] W.-H. Lin, T. Wilson, J. Wiebe, A. G. Hauptmann, Which side are you on? identifying\nperspectives at the document and sentence levels, in: Proceedings of the Tenth Conference\non Computational Natural Language Learning (CoNLL-X), 2006, pp. 109\u2013116.\n[19] W.-H. Lin, E. Xing, A. Hauptmann, A joint topic and perspective model for ideological\ndiscourse, in: Machine Learning and Knowledge Discovery in Databases: European\nConference, ECML PKDD 2008, Antwerp, Belgium, September 15-19, 2008, Proceedings,\nPart II 19, Springer, 2008, pp. 17\u201332.\n[20] K. R. Canini, B. Suh, P. L. Pirolli, Finding credible information sources in social networks\nbased on content and social structure, in: 2011 IEEE Third International Conference on\nPrivacy, Security, Risk and Trust and 2011 IEEE Third International Conference on Social\nComputing, IEEE, 2011, pp. 1\u20138.\n[21] C. Castillo, M. Mendoza, B. Poblete, Information credibility on Twitter, in: S. Srinivasan,\nK. Ramamritham, A. Kumar, M. P. Ravindra, E. Bertino, R. Kumar (Eds.), Proceedings of\nthe 20th International Conference on World Wide Web, WWW, 2011, pp. 675\u2013684.\n[22] J. Ma, W. Gao, Z. Wei, Y. Lu, K.-F. Wong, Detect rumors using time series of social context\ninformation on microblogging websites, in: Proceedings of the 24th ACM international on\nconference on information and knowledge management, 2015, pp. 1751\u20131754.\n[23] J. Ma, W. Gao, P. Mitra, S. Kwon, B. J. Jansen, K. Wong, M. Cha, Detecting rumors from\nmicroblogs with recurrent neural networks, in: S. Kambhampati (Ed.), Proceedings of the\nTwenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New\nYork, NY, USA, 9-15 July 2016, 2016, pp. 3818\u20133824.\n[24] A. Zubiaga, M. Liakata, R. Procter, G. W. S. Hoi, P. Tolmie, Analysing how people orient\nto and spread rumours in social media by looking at conversational threads, PloS one 11\n(2016) e0150989.\n[25] S. Iyengar, K. S. Hahn, Red media, blue media: Evidence of ideological selectivity in media\nuse, Journal of communication 59 (2009) 19\u201339.\n[26] S. DellaVigna, E. Kaplan, The fox news effect: Media bias and voting, The Quarterly\nJournal of Economics 122 (2007) 1187\u20131234.\n[27] A. Simonov, S. K. Sacher, J.-P. H. Dub\u00e9, S. Biswas, The persuasive effect of fox news:\nnon-compliance with social distancing during the covid-19 pandemic, Technical Report,\nNational Bureau of Economic Research, 2020.\n[28] R. Baly, G. Karadzhov, A. Saleh, J. Glass, P. Nakov, Multi-task ordinal regression for jointly\npredicting the trustworthiness and the leading political ideology of news media, arXiv\npreprint arXiv:1904.00542 (2019).\n[29] R. Baly, M. Mohtarami, J. Glass, L. M\u00e0rquez, A. Moschitti, P. Nakov, Integrating stance\ndetection and fact checking in a unified corpus, in: Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers), New Orleans, Louisiana, 2018, pp. 21\u201327.\n[30] K. Popat, S. Mukherjee, J. Str\u00f6tgen, G. Weikum, Credeye: A credibility lens for analyzing\nand explaining misinformation, in: Companion Proceedings of the The Web Conference\n2018, 2018, pp. 155\u2013158.\n[31] V. Kulkarni, J. Ye, S. Skiena, W. Y. Wang, Multi-view models for political ideology detection\nof news articles, arXiv preprint arXiv:1809.03485 (2018).\n[32] B. D. Horne, W. Dron, S. Khedr, S. Adali, Assessing the news landscape: A multi-module\ntoolkit for evaluating the credibility of news, in: Companion Proceedings of the The Web\nConference 2018, 2018, pp. 235\u2013238.\n[33] J. Kiesel, M. Mestre, R. Shukla, E. Vincent, P. Adineh, D. Corney, B. Stein, M. Potthast,\nSemeval-2019 task 4: Hyperpartisan news detection, in: Proceedings of the 13th Interna-\ntional Workshop on Semantic Evaluation, 2019, pp. 829\u2013839.\n[34] M. Potthast, M. Hagen, T. Gollub, M. Tippmann, J. Kiesel, P. Rosso, E. Stamatatos, B. Stein,\nOverview of the 5th international competition on plagiarism detection, in: CLEF Confer-\nence on Multilingual and Multimodal Information Access Evaluation, CELCT, 2013, pp.\n301\u2013331.\n[35] H. Rashkin, E. Choi, J. Y. Jang, S. Volkova, Y. Choi, Truth of varying shades: Analyzing\nlanguage in fake news and political fact-checking, in: Proceedings of the 2017 Conference\non Empirical Methods in Natural Language Processing, 2017, pp. 2931\u20132937.\n[36] Y. Jiang, J. Petrak, X. Song, K. Bontcheva, D. Maynard, Team bertha von suttner at\nSemEval-2019 task 4: Hyperpartisan news detection using ELMo sentence representation\nconvolutional network, in: Proceedings of the 13th International Workshop on Semantic\nEvaluation, Association for Computational Linguistics, Minneapolis, Minnesota, USA, 2019,\npp. 840\u2013844. URL: https://aclanthology.org/S19-2146. doi: 10.18653/v1/S19-2146 .\n[37] G. D. S. Martino, S. Yu, A. Barr\u00f3n-Cede\u00f1o, R. Petrov, P. Nakov, Fine-grained analysis of\npropaganda in news articles, arXiv preprint arXiv:1910.02517 (2019).\n[38] P. Panayotov, U. Shukla, H. T. Sencar, M. Nabeel, P. Nakov, GREENER: Graph neural\nnetworks for news media profiling, in: Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, Association for Computational Linguistics, Abu\nDhabi, United Arab Emirates, 2022, pp. 7470\u20137480. URL: https://aclanthology.org/2022.\nemnlp-main.506.\n[39] Y. Dinkov, A. Ali, I. Koychev, P. Nakov, Predicting the leading political ideology of\nyoutube channels using acoustic, textual, and metadata information, arXiv preprint\narXiv:1910.08948 (2019).\n[40] G. Salton, C. Buckley, Term-weighting approaches in automatic text retrieval, Information\nprocessing & management 24 (1988) 513\u2013523.\n[41] M. E. Celebi, H. A. Kingravi, P. A. Vela, A comparative study of efficient initialization\nmethods for the k-means clustering algorithm, Expert systems with applications 40 (2013)\n200\u2013210.\n[42] L. Prokhorenkova, G. Gusev, A. Vorobev, A. V. Dorogush, A. Gulin, Catboost: unbiased\nboosting with categorical features, Advances in neural information processing systems 31\n(2018).\n[43] T. G. Dietterich, et al., Ensemble learning, The handbook of brain theory and neural\nnetworks 2 (2002) 110\u2013125.\n[44] O. Sagi, L. Rokach, Ensemble learning: A survey, Wiley Interdisciplinary Reviews: Data\nMining and Knowledge Discovery 8 (2018) e1249.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Frank at CheckThat!-2023: Detecting the Political Bias of News Articles and News Media.", "author": ["D Azizov", "P Nakov", "S Liang"], "pub_year": "2023", "venue": "CLEF (Working Notes)", "abstract": "This paper addresses the challenge of detecting political bias in news articles and media  outlets from CheckThat! lab Task 3 [1, 2] by proposing an automated method for classifying"}, "filled": false, "gsrank": 4, "pub_url": "https://ceur-ws.org/Vol-3497/paper-025.pdf", "author_id": ["MUao89cAAAAJ", "DfXsKZ4AAAAJ", "4uggVcIAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:DQeC_kiVnqIJ:scholar.google.com/&output=cite&scirp=3&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=DQeC_kiVnqIJ&ei=BLWsaNaSEPnSieoPxKLpgQ0&json=", "num_citations": 8, "citedby_url": "/scholar?cites=11717967421204727565&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:DQeC_kiVnqIJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://ceur-ws.org/Vol-3497/paper-025.pdf"}}, {"title": "Political bias and factualness in news sharing across more than 100,000 online communities", "year": "2021", "pdf_data": "Political Bias and Factualness in News Sharing\nacross more than 100,000 Online Communities\nGalen Weld,1Maria Glenski,2Tim Althoff1\n1Paul G. Allen School of Computer Science and Engineering, University of Washington\n2National Security Directorate, Paci\ufb01c Northwest National Laboratory\n{gweld, althoff}@cs.washington.edu, maria.glenski@pnnl.gov\nAbstract\nAs civil discourse increasingly takes place online, misinfor-\nmation and the polarization of news shared in online com-munities have become ever more relevant concerns with realworld harms across our society. Studying online news shar-ing at scale is challenging due to the massive volume of con-tent which is shared by millions of users across thousandsof communities. Therefore, existing research has largely fo-cused on speci\ufb01c communities or speci\ufb01c interventions, suchas bans. However, understanding the prevalence and spread ofmisinformation and polarization more broadly, across thou-sands of online communities, is critical for the developmentof governance strategies, interventions, and community de-sign. Here, we conduct the largest study of news sharing onreddit to date, analyzing more than 550 million links spanning4 years. We use non-partisan news source ratings from MediaBias/Fact Check to annotate links to news sources with theirpolitical bias and factualness. We \ufb01nd that, compared to left-leaning communities, right-leaning communities have 105%more variance in the political bias of their news sources, andmore links to relatively-more biased sources, on average. Weobserve that reddit users\u2019 voting and re-sharing behaviorsgenerally decrease the visibility of extremely biased and lowfactual content, which receives 20% fewer upvotes and 30%fewer exposures from crossposts than more neutral or morefactual content. This suggests that reddit is more resilient tolow factual content than Twitter. We show that extremely bi-ased and low factual content is very concentrated, with 99%of such content being shared in only 0.5% of communities,giving credence to the recent strategy of community-widebans and quarantines.\n1 Introduction\nBiased and inaccurate news shared online are major con-\ncerns that have risen to the forefront of public discourse re-garding social media in recent years. Two thirds of Amer-icans get at least some of their news content from socialmedia, but less than half expect this content to be accu-rate (Shearer and Matsa 2018). Globally, only 22% of sur-vey respondents trust the news in social media \u201cmost of thetime\u201d (Newman 2020). Internet platforms such as Twitter,Facebook, and reddit account for an ever-increasing share ofthe dissemination and discussion of news (Geiger 2019).\nCopyright \u00a9 2021, Association for the Advancement of Arti\ufb01cial\nIntelligence (www.aaai.org). All rights reserved.Harms caused by biased and false news have substantial\nimpact across our society. Polarized content on Twitter and\nFacebook has been shown to play a role in the outcome ofelections (Recuero, Soares, and Gruzd 2020; Kharratzadehand\u00a8Ustebay 2017); and misinformation related to COVID-\n19 has been found to have a negative impact on public healthresponses to the pandemic (Tasnim, Hossain, and Mazumder2020; Kouzy et al. 2020). Developing methods for reduc-ing these harms requires a broad understanding of the polit-ical bias and factualness of news content shared online, butstudying news sharing is challenging for three reasons: (1)the scale is immense, with billions of news links shared an-nually, (2) it is dif\ufb01cult to automatically quantify bias andfactualness at scales where human labeling is often infeasi-ble (Rajadesingan, Resnick, and Budak 2020), and (3) thedistribution of links is complex, with these links shared bymany millions of users and thousands of communities.\nWhile previous research has led to important insights on\nspeci\ufb01c aspects of news sharing, such as user engagement(Risch and Krestel 2020), fact checking (Vosoughi, Roy,and Aral 2018; Choi et al. 2020), speci\ufb01c communities (Ra-jadesingan, Resnick, and Budak 2020), and speci\ufb01c rumors(Vosoughi, Mohsenvand, and Roy 2017; Qazvinian et al.2011), large scale studies of news sharing are critical to un-derstanding polarization and misinformation more broadly,and can inform community design, governance, and moder-ation interventions.\nIn this work, we present the largest study to date of news\nsharing behavior on reddit, one of the most popular socialmedia websites. We analyze all 559 million links submit-ted to reddit from 2015-2019\n1, including 35 million news\nlinks submitted by 1.3 million users to 135 thousand com-munities. We rate the bias and factualness of linked-to newssources using Media Bias/Fact Check (MBFC),\n2which con-\nsiders how news sources favor different sides of the left-rightpolitical spectrum (bias), and the veracity of claims made inspeci\ufb01c news stories (factualness) (\u00a73).\nIn our analyses, we examine: the diversity of news within\n1August 2019 was the most recent month of data available at\nthe time of this study.\n2While bias and factualness may vary from story to story, news\nsource-level ratings maximize the number of links that can be rated,\nand are commonly used in research (Bozarth, Saraf, and Budak2020).\nProceedingsoftheFifteenth InternationalAAAIConferenceonWebandSocial Media(ICWSM2021)\n796\ncommunities (\u00a74), and how this diversity is composed of\nboth the differences between community members and in-\ndividual members\u2019 diversity of submissions; the impact of\ncurrent curation and ampli\ufb01cation behaviors on news\u2019 vis-\nibility and spread (\u00a75); and the concentration of extremely\nbiased and low factual content (\u00a76), examining the distribu-\ntion of links from the perspectives of who submitted themand what community they were submitted to.\nWe show that communities on reddit exist across the\nleft-right political spectrum, as measured by MBFC, but74% are ideologically center left. We \ufb01nd that the diver-sity of left-leaning communities\u2019 membership is similar tothat of equivalently right-leaning communities, but right-leaning communities have 105% more politically variednews sources, as their members individually post more var-ied links. This variance comes from the presence of linksthat are different from the community average, and in right-leaning communities, 74% of such links are to relatively-more biased news sources, 35% more than in left-leaningcommunities (\u00a74).\nWe demonstrate that, regardless of the political leaning of\nthe community, community members\u2019 voting and crosspost-ing (re-sharing) behavior reduces the impact of extremelybiased and low factual news sources. Links to these newssources receive 20% fewer upvotes (\u00a75.2) and 30% fewerexposures from crossposts compared to more neutral andhigher factual content (\u00a75.3). Furthermore, we \ufb01nd that userswho submit such content leave reddit 68% more quicklythan others (\u00a75.1). These \ufb01ndings suggest that low factualcontent spreads more slowly and is ampli\ufb01ed less on redditthan has been reported for Twitter (Vosoughi, Roy, and Aral2018; Bovet and Makse 2019), although we do not directlycompare behavior across the two platforms. Differences be-tween reddit and Twitter may stem from reddit\u2019s explicit di-vision into communities, or users\u2019 ability to downvote con-tent, both of which help control content exposure.\nExtremely biased and low factual content can be challeng-\ning to manage, as it is spread through many users, newssources, and communities. We \ufb01nd that extremely biasedand low factual content is spread by an even broader set ofusers and communities relative to news content as a whole,exacerbating this challenge (\u00a76). However, we \ufb01nd that 99%of extremely biased or low factual content is still concen-trated in 0.5% of communities, lending credence to recentinterventions at the community level (Chandrasekharan et al.2017, 2020; Saleem and Ruths 2018; Ribeiro et al. 2020).\nOur work demonstrates that additional research on news\nsharing online is especially needed on the topics of whyusers depart platforms and where they go, why false newsappears to spread more quickly on Twitter than on reddit,and how curation and ampli\ufb01cation practices can managein\ufb02uxes of extremely biased and low factual content.\nFinally, we make all of our data and analyses publicly\navailable\n3to encourage future work on this important topic.\n3https://behavioral-data.github.io/news labeling reddit/2 Related Work\nMisinformation and Deceptive News. Social news plat-\nforms have seen a continued increase in use and a simultane-ous increase in concern regarding biased news and misinfor-mation (Mitchell et al. 2019; Marwick and Lewis 2017). Re-cent studies have used network spread (Vosoughi, Roy, andAral 2018; Ferrara 2017; Bovet and Makse 2019), contentconsumer (Allen et al. 2020), and content producer (Linvilland Warren 2020) approaches to assess the spread of mis-information. In this work, we examine news sharing behav-ior from news sources who publish content with varied de-grees of bias or factualness, building on related work that hasanalyzed social news based on the characteristics of a newsource\u2019s audience (Samory, Abnousi, and Mitra 2020a) orthe type of content posted (Glenski, Weninger, and Volkova2018).\nPolarization and Political Bias. Many papers have recently\nbeen published on detecting political bias of online content\neither automatically (Baly et al. 2020; Demszky et al. 2019)or manually (Ganguly et al. 2020; Bozarth, Saraf, and Budak2020). Others have examined bias in moderation of content,as opposed to biased content or news sources themselves(Jiang, Robertson, and Wilson 2019, 2020). Echo cham-bers are a major consideration in understanding polariza-tion, with papers focusing on their development (Allison andBussey 2020) and the role of news sources in echo chambers(Horne, N\u00f8rregaard, and Adali 2019). Others have examinedwho shares what content with what political bias, but didso using implicit community structure (Samory, Abnousi,and Mitra 2020b). In this work, we examine thousands ofexplicit communities on reddit, characterizing their polar-ization by examining the political diversity of news sourcesshared within, and the diversity of the community memberswho contribute.\nModeration and Governance. A large body of work has\nexamined the role of moderation interventions such explana-\ntions (Jhaver, Bruckman, and Gilbert 2019), content removal(Chandrasekharan et al. 2018), community bans (Chan-drasekharan et al. 2017, 2020; Saleem and Ruths 2018) onoutcomes such as migration (Ribeiro et al. 2020), harass-ment (Matias 2019a) and harmful language use (Waddenet al. 2021). Others have focused on moderators themselves(Matias 2019b; Dosono and Semaan 2019), and technolog-ical tools to assist them (Jhaver et al. 2019; Zhang, Hugh,and Bernstein 2020; Chandrasekharan et al. 2019), as wellas self-moderation through voting (Glenski, Pennycuff, andWeninger 2017; Risch and Krestel 2020) and communitynorms (Fiesler et al. 2018). In contrast, our work informs theviability of different moderation strategies, speci\ufb01cally byexamining the sharing and visibility of news content acrossthousands of communities.\n3 Dataset & Validation\nWe analyze all reddit submissions to extract links, and anno-tate links to news sources with their political bias and factu-alness using ratings from Media Bias/Fact Check.\n797\nFigure 1: The percentage of links that can be annotated us-\ning the MBFC labels is very consistent (\u00b1 3.3%) over time,\nsuggesting that comparisons over time are not signi\ufb01cantlyimpacted by changes in annotation coverage.\n3.1 Reddit Content\nreddit is the sixth most visited website in the world, and iswidely studied due to its size, diversity of communities, andthe public availability of its content (Medvedev, Lambiotte,and Delvenne 2018). Users can submit links or text (knownas \u201cselfposts\u201d) to speci\ufb01c communities, known as \u201csubred-dits.\u201d Users may view submissions for a single community,or create a \u201cfront page\u201d which aggregates submissions fromall communities the user \u201csubscribes\u201d to. Here, we focus onsubmissions over comments, as submissions are the primarymechanism for sharing content on reddit, and users spendmost of their time engaging with submissions (Glenski, Pen-nycuff, and Weninger 2017).\nTo create our dataset, we downloaded all public red-\ndit submissions from Pushshift (Baumgartner et al. 2020)posted between January 2015 and August 2019\n4, inclusive,\nfor a total of 56 months of content (580 million submissions,35 million unique authors, 3.4 million unique subreddits).For each submission, we extract the URLs of each linked-to website, which resulted in 559 million links\n5. Additional\nsummary statistics are included in the Appendix.\nEthical Considerations. We value and respect the privacy\nand agency of all people potentially impacted by this work.\nAll reddit content analyzed in this study is publicly accessi-ble, and Pushshift, from which we source our reddit content,permits any user to request removal of their submissions atany time. We take speci\ufb01c steps to protect the privacy ofpeople included in our study (Fiesler and Proferes 2018):we do not identify speci\ufb01c users, and we exclusively ana-lyze data and report our results in aggregate. All analysis ofdata in this study was conducted in accordance with the In-stitutional Review Board at the University of Washingtonunder identi\ufb01cation number STUDY00011457.\n3.2 Annotation of Links\u2019 News Sources\nTo identify and annotate links to news sources, we make useof Media Bias/Fact Check (hereafter MBFC), an indepen-dently run news source rating service. Bozarth, Saraf, andBudak (2020) \ufb01nd that \u201cthe choice of traditional news lists[for fact checking] seems to not matter,\u201d when comparing 5\n4August 2019 was the most recent month available at the time\nof this study.\n5While link submissions by de\ufb01nition contain exactly one link,\ntext submissions (selfposts) can include 0 or more links.different news lists including MBFC. Therefore, we selectedMBFC as it offers the largest set of labels of any news sourcerating service (Bozarth, Saraf, and Budak 2020). MBFC pro-vides ratings of the political bias (left to right) and factual-ness (low to high) of news outlets around the world, alongwith additional details and justi\ufb01cations for ratings, using arigorous public methodology\n6. MBFC is widely used for la-\nbelling bias and factualness of news sources for downstreamanalysis (Heydari et al. 2019; Main 2018; Starbird 2017;Darwish, Magdy, and Zanouda 2017; Nelimarkka, Laakso-nen, and Semaan 2018) and as ground truth for predictiontasks (Dinkov et al. 2019; Stefanov et al. 2020).\nFrom MBFC\u2019s public reports on each news source, we\nextract the name of the news source, its website, and the po-litical bias and factualness ratings. Bias is measured on a7-point scale of \u2018extreme left,\u2019 \u2018left,\u2019 \u2018center left,\u2019 \u2018center,\u2019\u2018center right,\u2019 \u2018right,\u2019 and \u2018extreme right,\u2019 and is reported for2,440 news sources. Factualness is measured on a 6-pointscale of \u2018very low factual,\u2019 \u2018low factual,\u2019 \u2018mixed factual,\u2019\u2018mostly factual,\u2019 \u2018high factual,\u2019 and \u2018very high factual,\u2019 andis reported for 2,676 news sources (as of April 2020). Forbrevity, in the following analyses, we occasionally use theterm \u2018left leaning\u2019 to indicate a news source with a biasrating of \u2018extreme left,\u2019 \u2018left,\u2019 or \u2018center left,\u2019 and the term\u2018right leaning\u2019 to indicate a news source with a bias ratingof \u2018center right,\u2019 \u2018right,\u2019 or \u2018extreme right.\u2019\nWe then annotate the links extracted from reddit sub-\nmissions with the MBFC ratings using regular expres-sions to match the URL of the link with the domain ofthe corresponding news source. For example, a link towww.rt.com/news/covid/ would be matched with the rt.comdomain of RT, the Russian-funded television network, andannotated with a bias of \u2018center right\u2019 and a factualness of\u2018very low.\u2019 Links to URL shorteners such as bit.ly were ex-cluded from labeling. We \ufb01nd that links to center left andhigh factual news sources are most common, accounting for53% and 64% of all news links, respectively. Extreme leftnews source links are much less common, with 22.2 extremeright links for every 1 extreme left link (Fig. 2).\nValidation of MBFC Annotations. The use of fact check-\ning sources such as MBFC is common practice for\nlarge scale studies, and MBFC in particular is widelyused (Dinkov et al. 2019; Stefanov et al. 2020; Heydariet al. 2019; Main 2018; Starbird 2017; Darwish, Magdy,and Zanouda 2017; Nelimarkka, Laaksonen, and Semaan2018). Additional con\ufb01dence in MFBC annotations comesfrom the results of Bozarth, Saraf, and Budak (2020), who\ufb01nd that (1) MBFC offers the largest set of biased and lowfactual news sources when compared among 5 fact check-ing datasets, and (2) the selection of a speci\ufb01c fact checkingsource has little impact on the evaluation of online content.Furthermore, we \ufb01nd that the coverage (the percentage oflinks that can be annotated using MBFC, excluding links toobvious non-news sources such as links to elsewhere on red-dit, to shopping sites, etc.) is very consistent ( \u00b13.3%) over\nthe 4 year span of our dataset (Fig. 1). Additionally, Bozarth,Saraf, and Budak (2020) \ufb01nd that it is very rare for a news\n6https://mediabiasfactcheck.com/methodology/\n798\nFigure 2: Distributions of mean bias and factualness are\nquite similar for both the user and community units of anal-ysis. Grey bars show the normalized total counts of links ofeach type across all of reddit.\nsource\u2019s bias or factualness to change over time, suggesting\nthat the potential \u2018drift\u2019 of ratings over time should not affectour results.\nRobustness Checks with Different Set of Annotations.\nLastly, we use an additional fact checking dataset fromVolkova et al. (2017), consisting of 251 \u2018veri\ufb01ed\u2019 newssources and 210 \u2018suspicious\u2019 news sources, as an additionalpoint of comparison for validation. While the exact classesin the Volkova et al. dataset are not directly comparable\nto MBFC, we can create a comparable class by comparinglinks with a MBFC factualness rating of \u2018mostly factual\u2019or higher with Volkova et al.\u2019s \u2018veri\ufb01ed\u2019 news sources. In\nthis case, when considering links that can be annotated us-ing both datasets, MBFC and Volkova et al. have a Cohen\u2019s\nkappa coef\ufb01cient of 0.82, indicating \u201calmost perfect\u201d inter-rater reliability (Landis and Koch 1977). We examined ifthese differences could have an impact of downstream anal-ysis and found this to be unlikely. For example, results com-puted separately using MBFC and Volkova et al. agree with\none another with a Pearson\u2019s correlation of 0.98 on the taskof identifying the number of \u2018mostly factual\u2019 or higher linksposted to a community.\nComputing Mean Bias/Factualness. As described above,\nMBFC labels for bias and factualness are ordinal, yet for\nmany analyses, it is useful to have numeric labels (e.g. com-\nputing the variance of links in a community). To convertfrom MBFC\u2019s categorical labels to a numeric scale, we usea mapping of (-3, -2, -1, 0, 1, 2, 3) to assign \u2018extreme left\u2019links a numeric bias value of -3, \u2018left\u2019 links a value of -2,\u2018center left\u2019 links a value of -1, \u2018center\u2019 links a value of 0,and positive values to map to the equivalent categories onthe right. While this choice is somewhat arbitrary, it is con-sistent with the linear spacing between bias levels given byMBFC. Furthermore, we explored different mappings, in-cluding nonlinear ones, and found that our results are robustto different mappings. As such, we use the mapping givenabove as it is easiest to interpret. We use a similar mappingof (0, 1, 2, 3, 4, 5) to assign \u2018very low factual\u2019 links a nu-meric value of 0, \u2018low factual\u2019 links a value of 1, etc., with\n\u2018very high factualness\u2019 links assigned a value of 5.\nThese numeric values are used to compute users\u2019 and\ncommunities\u2019 mean bias andmean factualness, central con-\nstructs in our analyses. To do so, we simply take the averageof the numeric bias and factualness values of the links byeach user or in each community. For many of our analyses,we group users by rounding their mean bias/factualness tothe nearest integer. Thus, when we describe a user as havinga \u2018left center bias,\u2019 we are indicating that the mean bias ofthe links they submitted is between -1.5 and -0.5.\nThe distributions of means are very similar for users and\ncommunities, with both closely following the overall distri-bution of news links on reddit, shown with grey bars (Fig. 2).74% of communities and 73% of users have a mean bias ofapproximately center left, and 65% of communities and 62%of users have a mean factualness of \u2018high factual\u2019 (amongusers/communities with more than 10 links).\nSimilarly, we de\ufb01ne user variance of bias as the variance\nof the bias values of the links submitted by a user, and sim-ilarly community variance of bias is de\ufb01ned as the variance\nof of the bias values of links submitted to a community. Aswith mean bias, we \ufb01nd that the distributions of user andcommunity variance of bias are very similar to one another.The median user has a variance of 0.85, approximately thevariance of a user with center bias who submits 62% centerlinks, 22% center-left or center-right links, and 16% left orright links. The median community has a variance of 0.91,approximately that of a community where 62% of the con-tent submitted has center bias, 20% of the content has center-left or center-right bias, and 18% of the content has left orright bias. Of course, a substantial amount of a community\u2019svariance comes from the variance of its userbase. We exploresources of this variance in \u00a74.\n3.3 Estimating Potential Exposures to Content\nLinks on reddit do not have equal impact; some links areviewed by far more people than others. To understand theimpact of certain types of content, we would like to under-stand how broadly that content is viewed. As view countsare not publicly available, we use the number of subscribersto the community that a link was posted to as an estimatefor the number of potential exposures to community mem-\nbers that this content may have had. While some users, es-pecially those without accounts, view content from commu-nities they are not subscribed to, subscription counts cap-ture both active contributors and passive consumers withinthe community, which motivated our use of this proxy overother alternatives, such as the number of votes.\nAs communities are constantly growing, we de\ufb01ne the\nnumber of potential exposures to a link as the number ofsubscribers to the community the link was posted to at the\ntime it was posted. To estimate historic subscriber counts, wemake use of archived Wayback machine snapshots of sub-reddit about pages, which provide the number of subscribersat the time of the snapshot. These snapshots are availablefor the \u21e03,500 largest subreddits. In addition, we collected\nthe current (as of Dec. 29, 2020) subscriber count for the\n799\n25,000 largest subreddits, as well as the date the subreddit\nwas created (at which point it had 0 subscribers). We usethe present subscriber count, archived subscriber counts (ifavailable), and the creation date, and linearly interpolate be-tween these data points to create a historical estimate of thesubscriber counts over time for each of the 25,000 largest (bynumber of posts) subreddits in our dataset. The resulting setof subscriber count data, when joined with our set of redditcontent, provides potential exposure estimates for 93.8% ofsubmissions. For the remaining 6.2% of submissions, we in-tentionally, conservatively overestimate the potential expo-\nsures by using the \ufb01rst percentile value (4 subscribers) fromour subscriber count data. The effect of this imputation onour results is very minor as these only occur in communitieswith extremely little activity.\n4 Diversity of News within Communities\nIn this section, we examine the factors that contribute toa community\u2019s variance of bias. This variance can comefrom a combination of two sources: (1) community mem-bers who are individually ideologically diverse (user diver-sity), and (2) a diverse group of users with different meanbiases (group diversity). High user diversity corresponds toa community whose members have high user variance (e.g.users who are ideologically diverse individually), and highgroup diversity corresponds to a community with high vari-ance of its members\u2019 mean bias (e.g. a diverse group of\nusers who may be ideologically consistent individually). Ofcourse, these sources of variance are not mutually exclusive;overall community variance is maximized when both user\ndiversity andgroup diversity are large.\nMethod. This intuition can be formalized using the Law of\nTotal Variance, which states that total community varianceis exactly the sum of User Diversity (within-user variance)and Group Diversity (between-user variance):\nVar(B\nc) = E[Var( Bc|U)] + Var(E[B c|U])\nwhere Bcis a random variable representing the bias of a link\nsubmitted to community c, and Uis a random variable rep-\nresenting the user who submitted the link.\nWe compute user diversity and group diversity for each\ncommunity. User diversity is given by taking the mean ofeach user\u2019s variance of bias, weighted by the number of la-beled bias links that user submitted. Group diversity is givenby taking the variance of each community members\u2019 meanuser bias, again weighted by their number of labeled links.We then sum the user and group diversity values to computethe overall community variance of political bias.\nTo understand how communities vary relative to their\nmean, we compute the balance of links in the adjacent rel-atively more- and less- biased categories. For example, acommunity with \u2018left\u2019 mean bias has two adjacent cate-gories: \u2018extreme left\u2019 and \u2018center left,\u2019 with \u2018extreme left\u2019being the relatively-more biased category, and \u2018center left\u2019being the relatively-less biased category.\nResults. Across all of reddit, we \ufb01nd most (82%) commu-\nnities\u2019 group diversity constitutes a majority of their over-\nall variance of bias. When binned by their mean bias, we\nFigure 3: While group diversity is similar between left-\nand right-leaning communities with a similar degree ofbias (right panel), right-leaning communities have higheruser diversity than equivalently biased communities on theleft (left panel). As a result, right-leaning communitieshave higher overall variance around their community mean.Right-leaning communities also favor relatively-more bi-ased links, when compared to left-leaning communities.\n\ufb01nd that communities with extreme bias have, on average,\nlower total variance than communities closer to the middleof the spectrum (Fig. 3). A community with mean bias of\u2018extreme left\u2019 would be expected to have a lower total vari-ance as there are no links with bias further left than \u2018extremeleft.\u2019 To control for this dynamic, we only compare symmet-ric labels: \u2018extreme left\u2019 to \u2018extreme right,\u2019 \u2018left\u2019 to \u2018right,\u2019and \u2018center left\u2019 to \u2018center right.\u2019\nWe \ufb01nd that right- and left-leaning communities have\nsimilar group diversity (Fig. 3, right), but right-leaning com-munities (red) have 341% more user diversity than equiv-alently left-leaning communities, on average (Fig. 3, left).As a result, the average overall variance is 105% greaterfor right-leaning communities than left-leaning communi-ties. Interestingly, we \ufb01nd that a larger share of right-leaningcommunities\u2019 variance is in more biased categories, relativeto the community mean. 74% of right-leaning communities\u2019adjacent links are relatively-more biased, compared to 55%for left-leaning communities, in other words, an increase of\n35%\u21e3\n74%\n55%\u2318\n.\nImplications. These results suggest that members of com-\nmunities on the left and right have comparable group di-\nversity, indicating the range of users are equally similarto one another. However, right-leaning communities havehigher user diversity, indicating that the individual usersthemselves tend to submit links to news sources with alarger variety of political leaning. This creates higher over-all variance of political bias in right-leaning communities,however these right-leaning communities also contain morelinks with higher bias, relative to the community mean, asopposed to more relatively-neutral news sources.\n800\nFigure 4: Users with extreme mean bias stay on reddit less\nthan half as long as users with center mean bias. Users withlow and very low mean factualness also leave more quickly,but expected lifespan decreases as users\u2019 mean factualnessincreases past \u2018mixed factual\u2019. Across all \ufb01gures, error barscorrespond to bootstrapped 95% con\ufb01dence intervals (andmay be too small to be visible).\n5 Impact of Current Curation and\nAmpli\ufb01cation Behaviors\nThe impact of content on reddit is affected by users\u2019 behav-ior: how long they stay on the platform, how they vote, andhow they amplify. In this section, we examine user longevityand turnover, community acceptance of biased and low fac-tual content, and ampli\ufb01cation through crossposting.\n5.1 User Lifespan\nDo users who post extremely biased or low factual contentstay on reddit as long as other users?\nMethod. We compute each user\u2019s lifespan on the platform\nby measuring how long they stay active on the platform after\ntheir \ufb01rst submission. We de\ufb01ne \u201cactive\u201d as posting at leastonce every 30 days, as in Waller and Anderson (2019). Wegroup users by their mean bias and factualness, and for eachgroup, compute the expected lifespan of the group members.\nResults. We \ufb01nd that expected lifespan is longer for users\nwho typically submit less politically biased content, with\nusers whose mean bias is near center remaining on reddit forapproximately twice as long as users with extreme or mod-erate mean bias, on average (Fig. 4, top). This result holdsregardless of whether users are left- or right-leaning. Userswith a mean factualness close to \u2018mixed factual\u2019 or lowerleave reddit 68% faster than users whose mean factualnessis near \u2018mostly factual\u2019 (Fig. 4, bottom). However, we also\ufb01nd that users\u2019 expected lifespan decreases dramatically astheir mean factualness increases to \u2018high\u2019 or \u2018very high\u2019 lev-els of factualness.\nImplications. These results suggest that users who mostly\npost links to extremely biased or low factual news sources\nleave reddit more quickly than other users. We can onlyspeculate as to the causes of this faster turnover, but we notethat users who stay on reddit the longest tend to post links tothe types of news sources that are most prevalent (grey barsin Fig. 2 show overall prevalence of each type of link).\nThe faster turnover suggests that users sharing this type of\ncontent leave relatively early, limiting their impact on theircommunities. However, faster turnover also may make user-level interventions such as bans less effective, as these sanc-tions have shorter-lived impact when the users they are madeagainst leave the site more quickly. Future research couldexamine why users leave, whether they rejoin with new ac-counts in violation of reddit policy, and the ef\ufb01cacy of re-strictions of new accounts.\n5.2 Acceptance of Biased or Low Factual Content\nHow do communities respond to politically biased or lowfactual content?\nMethod. On reddit, community members curate content in\ntheir communities by voting submissions up or down, which\naffects its position on the community feed (Glenski, Penny-cuff, and Weninger 2017). A submission\u2019s \u2018score\u2019 is de\ufb01nedby reddit as approximately the number of upvotes minus thenumber of downvotes that post receives. The score has beenused in previous work as a proxy for a link\u2019s reception bya community (Waller and Anderson 2019; Datta and Adar2019). Links submitted to larger communities are seen bymore users and therefore receive more votes. Therefore, wenormalize each link\u2019s score by dividing by the mean scoreof all submissions in that community; links with a normal-ized score over 1are more accepted than average, and links\nwith a score under 1are less accepted than average. In ac-\ncordance with reddit\u2019s ranking algorithm, submissions withhigher normalized score appear higher in the feed viewedby community members, and stay in this position for longer(Medvedev, Lambiotte, and Delvenne 2018).\nTo compute the community acceptance of links of a given\nbias or factualness, we average the normalized score of alllinks of that type in that community. We then take the me-dian community acceptance across all left-leaning, right-leaning, and neutral communities. Here we use the medianas it is more resilient to outliers than the mean.\nResults. We \ufb01nd that, regardless of the community\u2019s po-\nlitical leaning, median expected community acceptance is\n18% lower for extremely biased content than other con-tent (Fig. 5). For left-leaning and neutral communities, com-munity acceptance decreases monotonically as factualnessdrops below \u2018high.\u2019 However, we observe that right leaningcommunities are 167% (p =0.0002) more accepting of ex-\ntreme right biased and 85% (p =0.004) more accepting of\nvery low factual content than left-leaning and neutral com-munities (Mann\u2013Whitney Usigni\ufb01cance tests).\nImplications. This suggests that across reddit, communities\nare sensitive to extremely biased and low factual content,and users\u2019 voting behavior is fairly effective at reducing theacceptance of this content. However, curation does not seemto result in better-than-average acceptance for any content\u2014no median acceptance values are signi\ufb01cantly (p< 0.05)\n801\nFigure 5: Regardless of the political leaning of the community, extremely biased content is less accepted by communities than\ncontent closer to center. Similarly, low and very low factual content is less accepted than higher factual content. Points perturbedon the x-axis to aid readability.\nabove 1, as non-news content tends to receive higher com-\nmunity acceptance than news content.\nPrevious research has found that on Twitter, news that\nfailed fact-checking spread more quickly and was seen morewidely than news that passed a fact-check (Vosoughi, Roy,and Aral 2018). Interestingly, we \ufb01nd evidence that behav-ior on reddit is somewhat different, with median left-leaning,right-leaning, and neutral communities all being less accept-ing of low and very low factual content. Importantly, ourmethodology differs from Vosoughi, Roy, and Aral (2018)in that we use bias and factualness evaluations that were ap-plied to entire news sources, as opposed to the fact check-ing of speci\ufb01c news articles, limiting direct comparisons.Furthermore, we do not analyze the time between an initialpost and its subsequent ampli\ufb01cation, and so cannot directlycomment on the \u2018speed\u2019 of ampli\ufb01cation. We do \ufb01nd evi-dence, however, that highly biased content on reddit is lessupvoted than more neutral content.\nThese difference may in part be explained by differences\nbetween reddit\u2019s and Twitter\u2019s mechanisms for impactingthe visibility of content. Whereas Twitter users are only ableto increase visibility by retweeting, liking, replying to, orquoting content, on reddit, users may downvote to decreasevisibility of content they object to. We speculate that thismay partially explain the differences in acceptance that we\ufb01nd between reddit and Twitter.\n5.3 Selective Ampli\ufb01cation of News Content\nHow does ampli\ufb01cation of content affect exposure to biasedand low factual content? On reddit, users are not only ableto submit links to external content (such as news sites), but\nusers are also able to submit links to internal content else-\nwhere on reddit, effectively re-sharing and therefore ampli-\nfying content by increasing its visibility on the site. This is\ncommonly known as \u2018crossposting,\u2019 and often occurs whena user submits a post from one subreddit to another subred-dit, although such re-sharing of internal content can happenwithin a single community as well. Here, we seek to under-stand the role that ampli\ufb01cation through crossposts has onreddit user\u2019s exposure to various kinds of content.Method. To identify the political bias and factualness of\ncrossposted content, we identify all crossposted links tonews sources, and propagate the label of the crosspostedlink. Then, we compute the fraction of total potential ex-posures from crossposts for each bias/factualness category.\nResults. We \ufb01nd that ampli\ufb01cation via crossposting has an\noverall small effect on the potential exposures of news con-\ntent. While 10% of all news links are crossposts, only 1% ofpotential exposures to news links are due to crossposts. Thissuggests that the majority of crossposts are content posted inrelatively larger communities re-shared to relatively smallercommunities with relatively fewer subscribers, diminishingthe impact of ampli\ufb01cation via crossposting. As such, direct\nlinks to news sites have a far greater bearing on reddit users\u2019exposure to news content than crossposts.\nHowever, the role of crossposts in exposing users to new\ncontent is still important, as crossposts account for morethan 750 billion potential exposures. We \ufb01nd that extremelybiased and low factual content is ampli\ufb01ed less than othercontent, as shown in Fig. 6, which illustrates the percent-age of total potential exposures that come from crosspostsfor each bias/factualness category. reddit users exposed tocenter left biased, center biased, or center right biased con-tent are 53% more likely to be exposed to this content viaampli\ufb01cation than reddit users exposed to extremely biasedcontent. Similarly, reddit users exposed to \u2018mostly factual\u2019or higher factualness content are 217% more likely to beexposed to such content via ampli\ufb01cation than reddit usersexposed to very low factual content.\nImplications. Given that only 1% of potential exposures are\nfrom ampli\ufb01cations, understanding the way that direct links\nto external content are shared is critical to understanding the\nsharing of news content on reddit more broadly.\nThe relative lower ampli\ufb01cation of extremely biased and\nvery low factual content suggests users\u2019 sensitivity to thebias and factualness of the content they are re-sharing. Asin \u00a75.2, this suggests differences between reddit and Twit-ter, where content that failed a fact-check has been found tospread more quickly than fact-checked content (Vosoughi,Roy, and Aral 2018). We speculate that this may be due to\n802\nFigure 6: Extremely biased and low factual content is ampli-\n\ufb01ed by crossposts relatively less than other content. Regard-less of the bias or factualness of the content, while cross-posts are responsible for more than 750 billion potential ex-posures, they make up only 1% of total potential exposures,suggesting that direct links to news sources play an espe-cially important role in content distribution.\nstructural differences between the two platforms. On red-\ndit, users primarily consume content through subscriptionsto communities, not other users. This may explain the dimin-ished impact of re-sharing on reddit compared to Twitter.\n6 Concentrations of Extremely Biased or\nLow Factual News Content\nIt is critical to understand where different news content isconcentrated in order to best inform strategies for monitor-ing and managing its spread online. In this section, we ex-amine how extremely biased and low factual content is dis-tributed across users, communities, and news sources. Wealso compare the concentration of extremely biased and lowfactual content to all content.\nMethod. We consider three types of content: (1) news con-\ntent with extreme bias or low factualness, (2) all news con-\ntent, and (3) all content (including non-news). We groupeach of these types of content by three perspectives: the userwho posted the content, the community it was posted to, andthe news source (or domain, in the case of all content) linkedto. We then take the cumulative sum of potential exposuresacross the users, communities, and news sources, to computethe fraction of potential exposures contributed by the top n%\nof users, communities, and news sources. We repeat this pro-cess, replacing the number of potential exposures with thetotal number of links, to consider the concentration of linksbeing submitted, regardless of visibility.\nResults. We \ufb01nd that overall, extremely biased and low fac-\ntual content is highly concentrated across all three perspec-\ntives, but is especially concentrated in a small number ofcommunities, where 99% of potential exposures stem from amere 109 (0.5%) communities (Gini coef\ufb01cient=0.997) (Fig.\nFigure 7: When compared to all content on reddit (dotted\nline), extremely biased or low factual content (solid line)is more broadly distributed, making it harder to detect, re-gardless of the community, user, or news source perspective.However, 99% of potential exposures to extremely biased orlow factual content are restricted to only 0.5% of communi-ties. Here, a curve closer to the lower-right corner indicatesa more extreme concentration. Note that axis limits do notextend from 0 to 100%.\n7a). No matter the perspective, exposures to extremely bi-\nased or low factual content (solid line) are less concentratedthan all content (dotted line) (Fig. 7abc).\nUnder the community and news source perspectives, ex-\nposures (Fig. 7ac) are more concentrated than links (Fig.7df). While links are already concentrated in a small shareof communities, some communities are especially large, andtherefore content from these communities receives a dispro-portionate share of potential exposures. This is not the casefor users, as the distributions of exposures (Fig. 7b) are lessconcentrated than the distributions of links (Fig. 7e). Thisindicates that while some users submit a disproportionateshare of links, these are not the users whose links receive thelargest potential exposure, as potential exposure is primarilya function of submitting links to large communities.\nImplications. The extreme concentration of extremely bi-\nased or low factual content amongst a tiny fraction of com-\nmunities supports reddit\u2019s recent and high pro\ufb01le decision totake sanctions against entire communities, not just speci\ufb01cusers (Isaac and Conger 2021). These decisions have beenextensively studied (Chandrasekharan et al. 2017, 2020;Thomas et al. 2021; Saleem and Ruths 2018; Ribeiro et al.2020). While this content is relatively less concentratedamongst users, in absolute terms, this content is still fairlyconcentrated, with 10% of users contributing 84% of po-tential exposures. As such, moderation sanctions againstusers can still be effective (Matias 2019b). We note thatthe concentration of extremely biased or low factual contentamongst a small fraction of users is similar to what has been\n803\nfound on Twitter (Grinberg et al. 2019), although method-\nological differences preclude a direct comparison.\n7 Discussion\nSummary & Implications. In this work, we analyze all 580\nmillion submissions to reddit from 2015-2019, and annotate35 million links to news sources with their political bias andfactualness using Media Bias/Fact Check. We \ufb01nd:\n\u2022 Right-leaning communities\u2019 links to news sources have\n105% greater variance in their political bias than left-\nleaning communities. When right-leaning communitieslink to news sources that are different than the commu-nity average, they link to relatively-more biased sources35% more often than left-leaning communities (\u00a74).\n\u2022 Existing curation and ampli\ufb01cation behaviors moderately\nreduce the impact of highly biased and low factual con-tent. This suggests that reddit differs somewhat from Twit-ter, perhaps due to its explicit community structure, or theability for users to downvote content (\u00a75).\n\u2022 Highly biased and low factual content tends to be shared\nby a broader set of users and in a broader set of commu-nities than news content as a whole. Furthermore, the dis-tribution of this content is more concentrated in a smallnumber of communities than a small number of users, as99% of exposures to extremely biased or low factual con-tent stem from only 0.5% or 109 communities (\u00a76). Thislends credence to recent reddit interventions at the com-munity level, including bans and quarantines.\nLimitations. One limitation of our analyses is the use of a\nsingle news source rating service, MBFC. However, the se-lection of news source rating annotation sets has been foundto have a minimal impact on research results (Bozarth, Saraf,and Budak 2020). MBFC is the largest (that we know of)dataset of news sources\u2019 bias and factualness, and is widelyused (Dinkov et al. 2019; Stefanov et al. 2020; Heydari et al.2019; Starbird 2017; Darwish, Magdy, and Zanouda 2017).More robust approaches could combine annotations frommultiple sources, and we \ufb01nd that MBFC annotations agreewith the Volkova et al. (2017) dataset with a Pearson Corre-lation of 0.96 on an example downstream task (\u00a73.2).\nOur focus is on the bias and factualness of news sources\nshared online. We do not consider factors such as the contentof links (e.g. shared images, speci\ufb01c details of news stories),\nor the context in which links are shared (e.g. sentiment of a\nsubmission\u2019s comments). These factors are important areasfor future work, and are outside the scope of this paper.\nWhile MBFC (and by extension, our annotations) in-\ncludes news sources from around the world, our analyses,especially the left-right political spectrum and associatedcolors, takes a US-centric approach. Polarization and misin-formation are challenges across the globe (Newman 2020),and more work is needed on other cultural contexts.\nOur paper explores the impact of curation and ampli\ufb01ca-\ntion practices, but not the impact of community moderatorswho are a critical component of reddit\u2019s moderation pipeline(Matias 2019b). Future work could examine news contentremoved by moderators.Bias\n# of Links # of News Sources\nExtreme Left 15,157 51\nLeft 3,023,382 364\nCenter Left 17,648,711 544\nCenter 4,494,687 442\nCenter Right 4,254,705 263\nRight 3,226,828 352\nExtreme Right 997,703 423\nUnlabeled 525,443,378\nTable 1: Numbers of links and unique news sources in ourdataset, by the political bias of the link.\nFinally, we are limited by the unavailability of data on\nwhich users view what content. While we use subreddits\u2019subscriber counts to estimate exposures to content, moregranular data would enable us to better understand the im-pact of shared news articles, for example, the percentage ofusers who are exposed to extremely biased or low factualcontent (Grinberg et al. 2019).\n8 Conclusion\nBiased and inaccurate news shared online are signi\ufb01cantproblems, with real harms across our society. Large-scalestudies of news sharing online are critical for understand-ing the scale and dynamics of these problems. We presentedthe largest study to date of news sharing behavior on red-dit, and found that right-leaning communities have more po-litically varied and relatively-more biased links than left-leaning communities, current voting and re-sharing behav-iors are moderately effective at reducing the impact of ex-tremely biased and low factual content, and that such con-tent is extremely concentrated in a small number of commu-nities. We make our dataset of news sharing on reddit public,in order to support further research\n7.\nAcknowledgements\nThis research was supported by the Laboratory Directed Re-search and Development Program at Paci\ufb01c Northwest Na-tional Laboratory, a multiprogram national laboratory oper-ated by Battelle for the U.S. Department of Energy. Thisresearch was supported by the Of\ufb01ce for Naval Research,NSF grant IIS-1901386, the Bill & Melinda Gates Foun-dation (INV-004841), and a Microsoft AI for Accessibilitygrant.\nAppendix: Dataset Summary\nOur dataset was created from all public reddit submissionsposted between January 2016 and August 2019, the mostrecent data available at the time of this study. These submis-sions were downloaded using the Pushshift archives (Baum-gartner et al. 2020), and consist of 580 million submissions,35 million unique authors, and 3.4 million unique subred-dits. As each submission may consist of 0 or more links, thedataset includes a total of 559 million links. These links are\n7https://behavioral-data.github.io/news labeling reddit/\n804\nFactualness # of Links # of News Sources\nVery Low 609,229 72\nLow 749,202 369\nMixed 7,116,130 677\nMostly 2,217,719 110\nHigh 22,055,943 1,313\nVery High 2,263,604 134\nUnlabeled 524,092,724\nTable 2: Numbers of links and unique news sources in our\ndataset, by the factualness of the link.\nto 5.1 million unique domains, of which we are able to label\n2,801 unique domains with annotations from MBFC.\nTable 1 shows the number of links in the dataset, as well as\nthe number of unique news sources, for each bias category.\nTable 2 shows the number of links in the dataset, as well\nas the number of unique news sources, for each factualnesscategory.\nThe dataset may be downloaded from our website at\nhttps://behavioral-data.github.io/news\nlabeling reddit/\nReferences\nAllen, J.; Howland, B.; Mobius, M.; Rothschild, D.; andWatts, D. J. 2020. Evaluating the fake news problem at thescale of the information ecosystem. Science Advances 6(14).\nAllison, K.; and Bussey, K. 2020. Communal Quirks andCirclejerks: A Taxonomy of Processes Contributing to Insu-larity in Online Communities. In ICWSM.\nBaly, R.; Da San Martino, G.; Glass, J.; and Nakov, P. 2020.We Can Detect Your Bias: Predicting the Political Ideologyof News Articles. In EMNLP, 4982\u20134991. ACL. doi:10.\n18653/v1/2020.emnlp-main.404. URL https://www.aclweb.org/anthology/2020.emnlp-main.404.\nBaumgartner, J.; Zannettou, S.; Keegan, B.; Squire, M.; and\nBlackburn, J. 2020. The Pushshift Reddit Dataset. InICWSM.\nBovet, A.; and Makse, H. 2019. In\ufb02uence of fake news in\nTwitter during the 2016 US presidential election. Nature\nCommunications 10.\nBozarth, L.; Saraf, A.; and Budak, C. 2020. Higher Ground?How Groundtruth Labeling Impacts Our Understanding ofFake News about the 2016 U.S. Presidential Nominees. InICWSM.\nChandrasekharan, E.; Gandhi, C.; Mustelier, M. W.; and\nGilbert, E. 2019. Crossmod: A Cross-Community Learning-based System to Assist Reddit Moderators. CHI 3: 1 \u2013 30.\nChandrasekharan, E.; Jhaver, S.; Bruckman, A.; andGilbert, E. 2020. Quarantined! Examining the Effectsof a Community-Wide Moderation Intervention on Reddit.ArXiv abs/2009.11483.\nChandrasekharan, E.; Pavalanathan, U.; Srinivasan, A.;Glynn, A.; Eisenstein, J.; and Gilbert, E. 2017. You Can\u2019tStay Here: The Ef\ufb01cacy of Reddit\u2019s 2015 Ban ExaminedThrough Hate Speech. CHI 1: 31:1\u201331:22.Chandrasekharan, E.; Samory, M.; Jhaver, S.; Charvat, H.;Bruckman, A.; Lampe, C.; Eisenstein, J.; and Gilbert, E.2018. The Internet\u2019s Hidden Rules. CHI 2: 1 \u2013 25.\nChoi, D.; Chun, S.; Oh, H.; Han, J.; et al. 2020. Rumorpropagation is ampli\ufb01ed by echo chambers in social media.Scienti\ufb01c Reports 10(1): 1\u201310.\nDarwish, K.; Magdy, W.; and Zanouda, T. 2017. Trump vs.Hillary: What Went Viral During the 2016 US PresidentialElection. In SocInfo.\nDatta, S.; and Adar, E. 2019. Extracting Inter-communityCon\ufb02icts in Reddit. In ICWSM.\nDemszky, D.; Garg, N.; Voigt, R.; Zou, J.; Gentzkow, M.;Shapiro, J.; and Jurafsky, D. 2019. Analyzing Polarizationin Social Media: Method and Application to Tweets on 21Mass Shootings. In NAACL-HLT.\nDinkov, Y.; Ali, A.; Koychev, I.; and Nakov, P. 2019. Pre-dicting the Leading Political Ideology of YouTube ChannelsUsing Acoustic, Textual, and Metadata Information. In IN-\nTERSPEECH.\nDosono, B.; and Semaan, B. C. 2019. Moderation Practices\nas Emotional Labor in Sustaining Online Communities: TheCase of AAPI Identity Work on Reddit. CHI .\nFerrara, E. 2017. Contagion dynamics of extremist propa-ganda in social networks. Information Sciences 418: 1\u201312.\nFiesler, C.; Jiang, J. A.; McCann, J.; Frye, K.; and Brubaker,J. R. 2018. Reddit Rules! Characterizing an Ecosystem ofGovernance. In ICWSM.\nFiesler, C.; and Proferes, N. 2018. \u201cParticipant\u201d Perceptionsof Twitter Research Ethics. Social Media + Society 4.\nGanguly, S.; Kulshrestha, J.; An, J.; and Kwak, H. 2020.Empirical Evaluation of Three Common Assumptions inBuilding Political Media Bias Datasets. In ICWSM.\nGeiger, A. 2019. Key \ufb01ndings about the online newslandscape in America. https://www.pewresearch.org/fact-tank/2019/09/11/key-\ufb01ndings-about-the-online-news-landscape-in-america/. Accessed: 2021-04-09.\nGlenski, M.; Pennycuff, C.; and Weninger, T. 2017. Con-\nsumers and Curators: Browsing and Voting Patterns on Red-dit. IEEE TCSS 4(4): 196\u2013206. doi:10.1109/TCSS.2017.\n2742242.\nGlenski, M.; Weninger, T.; and Volkova, S. 2018. Propa-\ngation from deceptive news sources who shares, how much,how evenly, and how quickly? IEEE TCSS 5(4): 1071\u20131082.\nGrinberg, N.; Joseph, K.; Friedland, L.; Swire-Thompson,B.; and Lazer, D. 2019. Fake news on Twitter during the2016 U.S. presidential election. Science 363: 374 \u2013 378.\nHeydari, A.; Zhang, J.; Appel, S.; Wu, X.; and Ranade, G.2019. YouTube Chatter: Understanding Online CommentsDiscourse on Misinformative and Political YouTube Videos.\nHorne, B.; N\u00f8rregaard, J.; and Adali, S. 2019. Different Spi-\nrals of Sameness: A Study of Content Sharing in Mainstreamand Alternative Media. In ICWSM.\n805\nIsaac, M.; and Conger, K. 2021. Reddit bans forum\ndedicated to supporting Trump. https://www.nytimes.com/2021/01/08/us/politics/reddit-bans-forum-dedicated-to-supporting-trump-and-twitter-permanently-suspends-his-allies-who-spread-conspiracy-theories.html. Accessed:2021-04-09.\nJhaver, S.; Birman, I.; Gilbert, E.; and Bruckman, A.\n2019. Human-Machine Collaboration for Content Regula-tion. TOCHI 26: 1 \u2013 35.\nJhaver, S.; Bruckman, A.; and Gilbert, E. 2019. Does Trans-parency in Moderation Really Matter? CHI 3: 1 \u2013 27.\nJiang, S.; Robertson, R. E.; and Wilson, C. 2019. Bias Mis-perceived: The Role of Partisanship and Misinformation inYouTube Comment Moderation. In ICWSM.\nJiang, S.; Robertson, R. E.; and Wilson, C. 2020. Reasoningabout Political Bias in Content Moderation. In AAAI.\nKharratzadeh, M.; and \u00a8Ustebay, D. 2017. US Presidential\nElection: What Engaged People on Facebook. In ICWSM.\nKouzy, R.; Jaoude, J. A.; Kraitem, A.; Alam, M. B. E.;Karam, B.; Adib, E.; Zarka, J.; Traboulsi, C.; Akl, E. A.; andBaddour, K. 2020. Coronavirus Goes Viral: Quantifying theCOVID-19 Misinformation Epidemic on Twitter. Cureus 12.\nLandis, J.; and Koch, G. 1977. The measurement of observeragreement for categorical data. Biometrics 33 1: 159\u201374.\nLinvill, D. L.; and Warren, P. L. 2020. Troll factories: Man-ufacturing specialized disinformation on Twitter. Political\nCommunication 1\u201321.\nMain, T. J. 2018. The Rise of the Alt-Right. Brookings Insti-\ntution Press.\nMarwick, A.; and Lewis, R. 2017. Media manipulation and\ndisinformation online. Data & Society https://datasociety.\nnet/library/media-manipulation-and-disinfo-online/.\nMatias, J. 2019a. Preventing harassment and increasing\ngroup participation through social norms in 2,190 online sci-ence discussions. PNAS 116: 9785 \u2013 9789.\nMatias, J. N. 2019b. The Civic Labor of Volun-teer Moderators Online. Social Media + Society 5(2):\n2056305119836778. doi:10.1177/2056305119836778.URL https://doi.org/10.1177/2056305119836778.\nMedvedev, A.; Lambiotte, R.; and Delvenne, J. 2018. The\nanatomy of Reddit: An overview of academic research.ArXiv abs/1810.10881.\nMitchell, A.; Gottfried, J.; Srocking, G.; Walker, M.;and Fedeli, S. 2019. Many Americans Say Made-Up News Is a Critical Problem That Needs To BeFixed. Pew Research Center Science and Journalism\nhttps://www.journalism.org/2019/06/05/many-americans-say-made-up-news-is-a-critical-problem-that-needs-to-be-\ufb01xed/.\nNelimarkka, M.; Laaksonen, S.-M.; and Semaan, B. 2018.\nSocial Media Is Polarized, Social Media Is Polarized: To-wards a New Design Agenda for Mitigating Polarization.InACM DIS, 957\u2013970. United States: ACM. doi:10.1145/3196709.3196764. ACM conference on Designing Interac-tive Systems, DIS ; Conference date: 09-06-2018 Through13-06-2018.\nNewman, N. 2020. Digital News Report. Reuters Institute.\nQazvinian, V.; Rosengren, E.; Radev, D.; and Mei, Q. 2011.\nRumor has it: Identifying misinformation in microblogs. InEMNLP, 1589\u20131599.\nRajadesingan, A.; Resnick, P.; and Budak, C. 2020. Quick,\nCommunity-Speci\ufb01c Learning: How Distinctive ToxicityNorms Are Maintained in Political Subreddits. In ICWSM.\nRecuero, R.; Soares, F. B.; and Gruzd, A. A. 2020. Hy-perpartisanship, Disinformation and Political Conversationson Twitter: The Brazilian Presidential Election of 2018. InICWSM.\nRibeiro, M. H.; Jhaver, S.; Zannettou, S.; Blackburn, J.;\nCristofaro, E. D.; Stringhini, G.; and West, R. 2020.Does Platform Migration Compromise Content Modera-tion? Evidence from r/The\nDonald and r/Incels. ArXiv\nabs/2010.10397.\nRisch, J.; and Krestel, R. 2020. Top Comment or Flop Com-\nment? Predicting and Explaining User Engagement in On-line News Discussions. In ICWSM.\nSaleem, H. M.; and Ruths, D. 2018. The Aftermathof Disbanding an Online Hateful Community. ArXiv\nabs/1804.07354.\nSamory, M.; Abnousi, V. K.; and Mitra, T. 2020a. Char-\nacterizing the Social Media News Sphere through User Co-Sharing Practices. In ICWSM, volume 14, 602\u2013613.\nSamory, M.; Abnousi, V. K.; and Mitra, T. 2020b. Char-acterizing the Social Media News Sphere through User Co-Sharing Practices. In ICWSM.\nShearer, E.; and Matsa, K. E. 2018. News Use Across SocialMedia Platforms 2018. https://www.journalism.org/2018/09/10/news-use-across-social-media-platforms-2018/. Ac-cessed: 2021-04-09.\nStarbird, K. 2017. Examining the Alternative Media Ecosys-\ntem Through the Production of Alternative Narratives ofMass Shooting Events on Twitter. In ICWSM.\nStefanov, P.; Darwish, K.; Atanasov, A.; and Nakov, P. 2020.Predicting the Topical Stance and Political Leaning of Me-dia using Tweets. In ACL, 527\u2013537. Online: ACL. doi:\n10.18653/v1/2020.acl-main.50. URL https://www.aclweb.org/anthology/2020.acl-main.50.\nTasnim, S.; Hossain, M.; and Mazumder, H. 2020. Impact of\nRumors and Misinformation on COVID-19 in Social Media.Journal of Preventive Medicine and Public Health 53: 171 \u2013\n174.\nThomas, P. B.; Riehm, D.; Glenski, M.; and Weninger, T.\n2021. Behavior Change in Response to Subreddit Bans andExternal Events. IEEE Transactions on Computational So-\ncial Systems 1\u201310. doi:10.1109/TCSS.2021.3061957.\nVolkova, S.; Shaffer, K.; Jang, J. Y.; and Hodas, N. 2017.Separating Facts from Fiction: Linguistic Models to Classify\n806\nSuspicious and Trusted News Posts on Twitter. In ACL, 647\u2013\n653. Vancouver, Canada: ACL. doi:10.18653/v1/P17-2102.\nURL https://www.aclweb.org/anthology/P17-2102.\nVosoughi, S.; Mohsenvand, M. N.; and Roy, D. 2017. Rumor\ngauge: Predicting the veracity of rumors on Twitter. KDD\n11(4): 1\u201336.\nVosoughi, S.; Roy, D.; and Aral, S. 2018. The spread of\ntrue and false news online. Science 359(6380): 1146\u20131151.\nISSN 0036-8075. doi:10.1126/science.aap9559. URL https://science.sciencemag.org/content/359/6380/1146.\nWadden, D.; August, T.; Li, Q.; and Althoff, T. 2021. The\nEffect of Moderation on Online Mental Health Conversa-tions. In ICWSM.\nWaller, I.; and Anderson, A. 2019. Generalists and Special-ists: Using Community Embeddings to Quantify ActivityDiversity in Online Platforms. In The World Wide Web Con-\nference, WWW \u201919, 1954\u20131964. New York, NY, USA: As-sociation for Computing Machinery. ISBN 9781450366748.doi:10.1145/3308558.3313729. URL https://doi.org/10.1145/3308558.3313729.\nZhang, A. X.; Hugh, G.; and Bernstein, M. S. 2020. Poli-\ncyKit: Building Governance in Online Communities. UIST\nURL https://doi.org/10.1145/3379337.3415858.\n807", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Political bias and factualness in news sharing across more than 100,000 online communities", "author": ["G Weld", "M Glenski", "T Althoff"], "pub_year": "2021", "venue": "\u2026 of the international AAAI conference on \u2026", "abstract": "As civil discourse increasingly takes place online, misinformation and the polarization of news  shared in online communities have become ever more relevant concerns with real world"}, "filled": false, "gsrank": 6, "pub_url": "https://ojs.aaai.org/index.php/ICWSM/article/view/18104", "author_id": ["mslgg5QAAAAJ", "gASdXl8AAAAJ", "yc4nBNgAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:Al580M71dxIJ:scholar.google.com/&output=cite&scirp=5&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=Al580M71dxIJ&ei=BLWsaNaSEPnSieoPxKLpgQ0&json=", "num_citations": 46, "citedby_url": "/scholar?cites=1330802483521150466&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:Al580M71dxIJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://ojs.aaai.org/index.php/ICWSM/article/download/18104/17907"}}, {"title": "Generating media background checks for automated source critical reasoning", "year": "2024", "pdf_data": "Generating Media Background Checks for Automated Source Critical\nReasoning\nMichael Schlichtkrull\nSchool of Electronic Engineering and Computer Science\nQueen Mary University of London\nm.schlichtkrull@qmul.ac.uk\nAbstract\nNot everything on the internet is true. This un-\nfortunate fact requires both humans and mod-\nels to perform complex reasoning about cred-\nibility when working with retrieved informa-\ntion. In NLP, this problem has seen little at-\ntention. Indeed, retrieval-augmented models\nare not typically expected to distrust retrieved\ndocuments. Human experts overcome the chal-\nlenge by gathering signals about the context,\nreliability, and tendency of source documents\n\u2013 that is, they perform source criticism . We\npropose a novel NLP task focused on finding\nand summarising such signals. We introduce\na new dataset of 6,709 \u201cmedia background\nchecks\u201d derived from Media Bias / Fact Check,\na volunteer-run website documenting media\nbias. We test open-source and closed-source\nLLM baselines with and without retrieval on\nthis dataset, finding that retrieval greatly im-\nproves performance. We furthermore carry out\nhuman evaluation, demonstrating that 1) me-\ndia background checks are helpful for humans,\nand 2) media background checks are helpful\nfor retrieval-augmented models.\n1 Introduction\nWhen humans perform knowledge-intensive rea-\nsoning, we are rarely able to rely on a single, au-\nthoritative source. Instead, we forage for multiple\nsources, evaluate their trustworthiness, and syn-\nthesize answers (Potter, 2013). The basic task\nis to choose reliable sources, to read them re-\nliably , and to combine them into reliable narra-\ntives (Howell and Prevenier, 2001). Best practice\nfor epistemic experts, such as journalists and his-\ntorians, is to rely on multiple sources, to present\nevidence of source tendency and reliability, and to\nexplain source disagreements to readers (Steensen,\n2019). Search engines, acting as surrogate ex-\nperts (Simpson, 2013), similarly enrich their results\nwith knowledge-contexts that help users reason\nabout tendency and trust (Smith and Rieh, 2019).\nNo, that is disinformation created\nto justify sanctions.Has the Syrian government used\nchemical weapons?\nAll allegations of a chemical\nweapons attack in Ghouta are\nbaseless accusations\ncoordinated by the W est to\njustify sanctions...Retrieves\n& Reads\nFigure 1: Retrieval-augmented NLP models can inad-\nvertently misinform users if uncritically relying on re-\ntrieved documents from such untrustworthy sources.\nIn preliminary experiments we found evidence of this\noccurring in practise: One popular search-augmented\nchatbot engaged in war-crimes denial after relying on\nSyrian state news to answer questions.\nSource-critical reasoning has not received much\nattention in NLP. Even for settings with clearly\ndisputed truth claims, such as fact-checking, stud-\nies typically assume a single, trustworthy source,\ne.g. Wikipedia (Thorne et al., 2018), scientific\njournals (Wadden et al., 2020), or search re-\nsults (Schlichtkrull et al., 2023b). This is the case\neven when researchers propose to fully automate\naway human epistemic experts (Schlichtkrull et al.,\n2023a). Problems of trust, uncertainty, and dis-\nagreeing evidence are often mentioned in sections\nlike \u201cbroader impact\u201d (Lewis et al., 2020) or \u201climi-\ntations\u201d (Schlichtkrull et al., 2023b).\nRecently, large language models (LLMs) have\nbeen envisioned as alternatives to search (Metzler\net al., 2021). This is risky: in assessments by do-\nmain experts, LLMs have been found to give defini-\ntive answers on subjects without adequate consen-\nsus (Peskoff and Stewart, 2023). One proposedarXiv:2409.00781v1  [cs.CL]  1 Sep 2024\n...\n...\n...Background Check:\nThe Malicious Misinformer was\nfounded in 2004 by Mallory Humbug,\ninternational villain and well-known\ncommitter of dastardly deeds.\nThe paper is funded by the Republic\nof Fraudonia, where it is the of ficial\npaper of the ruling party .\nThe Misinformer has failed the\nfollowing fact-checks:\n* No, 5G does not cause COVID\n* ...\nShown directly\nto users\nUsed by retrieval-\naugmented modelsFigure 2: We propose to generate Media Background Checks (MBCs) that summarise indicators of trustworthiness\nand tendency. MBCs can be used, either by humans or by retrieval-augmented models, to determine which\ndocuments can be relied on for further reasoning, and to craft reliable narratives based on untrustworthy evidence.\nsolution is retrieval-augmentation, where models\nreason based on retrieved documents (e.g., from\nsearch) rather than weights (Lewis et al., 2020).\nHowever, it still falls on the user to analyse if the\nsources used can be trusted \u2013 and LLMs often pro-\nvide less knowledge-context than traditional search\nengines (Shah and Bender, 2022). In the absence of\nother evidence, users often fall back on search rank-\nings to determine trustworthiness; this \u201cGoogle-\nization\u201d is an existing concern in journalism, where\noverreliance on search can cause a \u201cdistortion of\nreality\u201d (Machill and Beiler, 2009; Bu\u00e7inca et al.,\n2021). LLMs can similarly misinform users if they\nrely uncritically on retrieved documents. In prelim-\ninary experiments, we observed one popular model\ndeny war crimes as a result of an untrustworthy\nretrieved document (see Figure 1).\nWhen reasoning based on secondhand knowl-\nedge, even the most expert human is prisoner to\ntheir sources. Nevertheless, with awareness of\nthese limitations, readers can rely on signals about\nthe motivation and tendencies of their sources to\ncreate understanding; i.e., they can carry out source\ncriticism (Howell and Prevenier, 2001; Godler and\nReich, 2015; Steensen, 2019). Human knowledge\nexperts are expected to assist their readers in this\nprocess. For example, member organisations of\nthe International Fact-checking Consortium are re-\nquired to \u201cidentify and disclose the relevant inter-\nestsof the sources it uses where the reader might\nreasonably conclude those interests could influence\nthe accuracy of the evidence provided\u201d (Interna-tional Fact-Checking Network, 2016). Similar to\nImperial et al. (2024), we argue that models should\nbe aligned to best practises for human experts. Dis-\nclosing not just the names of sources, but also sig-\nnals about their motivations and tendencies, is cru-\ncial as these indicators can be difficult to find for\nnon-expert humans who may not be intimately fa-\nmiliar with each particular source.\nIn this paper, we provide building blocks for\nanalysing the biases and tendencies of retrieved\ndocuments, taking the first steps towards source-\ncritical NLP. We introduce a new task, generating\nmedia background checks . Media background\nchecks (MBCs) are short statements that give con-\ntext to sources, enabling critical analysis (see Fig-\nure 2). They cover topics like ownership, funding\nmodel, remit, known biases, and factuality signals\nsuch as previous failed fact-checks \u2013 the same sig-\nnals used by human experts in their analysis (How-\nell and Prevenier, 2001). The ability to construct\nan MBC, i.e. to recall the necessary information, is\na prerequisite for further source-critical reasoning.\nAs we show in Section 4.4, MBCs can help both\nhumans and models reason source-critically.\nWe create a dataset of 6,709 MBCs collected\nfrom the website Media Bias / Fact Check1, which\npublishes detailed reports on media organisations\nas part of their effort to promote awareness of me-\ndia bias. We experiment with an LLM-based ap-\nproach to MBC generation, finding that retrieval-\n1https://mediabiasfactcheck.com/\naugmentation (Lewis et al., 2020) improves per-\nformance. We carry out human evaluation, docu-\nmenting 1) that humans find MBCs helpful when\nassessing retrieved documents, and 2) that LLMs\naugmented with MBCs in addition to their re-\ntrieved results give better answers to questions\non controversial topics. We release our dataset\nand code at https://github.com/MichSchli/\nMediaBackgroundChecks (CC-BY-NC-4.0).\n2 Task Definition\nWe seek to automatically findandsummarise indi-\ncators of trustworthiness and tendency in order to\nfacilitate source-critical reasoning by humans and\nNLP models. We refer to such summaries as media\nbackground checks (MBC). A fact is considered a\nrelevant indicator to a media background check for\nsource X if it can shift the relative trust assigned to\nsource X for some consumer of content. Such facts\ninclude, but are not limited to, the founding, lead-\nership, and organization of the source, the funding\nmodel of the source, the remit and target audience\nof the source, public stances taken by the source\n(e.g., endorsing a political campaign), discussion\ninother media about the source, reliance byother\nmedia on the source, and the factual history of the\nsource (as indicated e.g. by fact-checks undergone\nby reputable fact-checking organisations).\nWe identify 42 common patterns of relevant\nfacts, which we also use for evaluation \u2013 see Ap-\npendix C. However, these do not fully cover the\nfacts seen empirically in background checks from\nMB/FC. Real-world indicators also include rarer\npatterns, such as multi-hop facts (e.g. other sources\nowned by the same company), and topic-specific in-\ndicators, for example evidence that a source cannot\nbe trusted specifically on issues related to health-\ncare, Qu\u00e9b\u00e9cois politics, or news about a particular\nadvertiser. To understand common patterns, we\nfurther analyse 20 randomly chosen articles from\nMB/FC. The most common facts include known\nbiases, funding model, remit, fact-checking history,\nand ownership (see Appendix D).\nIn order to build an MBC, a model must first\nrecall or retrieve pertinent details for the source.\nThese range from simple facts (e.g., whether the\nsource is funded by a specific government) to more\ncomplex (e.g., whether a poor track record for fac-\ntuality is evidenced). Necessary retrieval steps may\ndiffer depending on the source type \u2013 websites have\ndifferent indicators of credibility than print sources,Train Dev Test\nBackground checks 5209 500 1000\n# Avg. Lines 17.1 17.2 17.0\n# Avg. Tokens 305.1 302.2 303.2\nTable 1: Statistics for our dataset. Background checks\nwere randomly divided among the three splits.\nfor example (Potter, 2013). Once the appropriate\ninformation has been retrieved, the model must\nthen summarise this information.\n3 Dataset\nMedia Bias / Fact Check2(MB/FC) is an indepen-\ndent, volunteer-run website that promotes aware-\nness of media bias. While past datasets have used\nbias ratings from the site (Baly et al., 2018), the tex-\ntual\u201cdetailed reports\u201d are an untapped resource.\nThese are well-sources documents that summarise\nthe factual history, bias, tendency, and potential\nconflicts of interest for media sources \u2013 i.e., back-\nground checks . We introduce a novel dataset based\non these. We collected all reports listed through\ntheir search tool. This provided a total of 6,709\nbackground checks. We cleaned the reports, and re-\nmoved the bias and credibility ratings. Statistics for\nthe dataset can be seen in Table 1, and the dataset it-\nself can be downloaded at https://github.com/\nMichSchli/MediaBackgroundChecks .\nWhile MB/FC is an extensive resource, covering\nalmost 7,000 media organisations, it is far from\ncomplete. In our human evaluation in Sections 4.4,\nout of 40 sources used, 29 are covered by MB/FC\nreports. However, of these 29, 9 are incomplete\nor lack crucial information. We do not intend to\nreplace human-written background checks \u2013 rather,\nwe suggest that generated background checks can\nsupplement MB/FC in these cases.\n4 Experiments\n4.1 Models\nFor a simple baseline, we prompt a large language\nmodel to generate an MBC given the name of a\nsource. Our prompt can be seen in Appendix B.1.\nWe experiment with two models, one closed-source\nand one open-source: gpt-3.5-turbo-01253and\n2https://mediabiasfactcheck.com/\n3ChatGPT; see https://platform.openai.com/docs/\nmodels/gpt-3-5-turbo\nLlama 3 8B Instruct4, the instruction-tuned ver-\nsion of Llama 3. The latter is small enough to\nreasonably run on a single consumer-grade GPU.\nFor these initial models, we rely solely on the\ninformation stored in their weights. As such, their\nperformance is representative of the level of source-\ncritical reasoning that can be expected of current-\ngeneration LLMs, without any additional evidence\nprovided. MBCs often incorporate information\navailable on the internet \u2013 the 6,709 background\nchecks we analysed contained an average of 8.9 hy-\nperlinks. To surface this information, we propose\nto rely on retrieval-augmented generation (Lewis\net al., 2020). In addition to providing external mem-\nory at generation time, retrieved evidence can also\nbe shown to the user to establish why the back-\nground check should (or should not) be trusted.\nShowing where evidence originates from to estab-\nlish trust is crucial in related tasks, such as auto-\nmated fact-checking (Guo et al., 2022). It also\nprovides a rudimentary form of interpretability.\nAs the necessary retrieval steps are roughly sim-\nilar across background checks, we have compiled a\nlist of seven information-seeking search queries fo-\ncusing on different aspects of a background check\n(see the list in Appendix E). In initial experiments,\nwe found this strategy to perform better than gener-\nating queries using our LLM.\nFor each query, we gather relevant search results\nusing the Google Search API5, selecting the top 30\nretrieved documents. We exclude the Media Bias /\nFact Check website and any website linking to it.\nTo focus on the most salient retrieved information,\nwe use a question-answering model to extract an-\nswers to a predefined question for each information-\nseeking query (also found in Appendix E). Here,\nwe employ DeBERTa v.3 (He et al., 2021) pre-\ntrained on SQuAD (Rajpurkar et al., 2016). By this\nprocess we transform a Google search result into a\nquestion-answer pair.\nFor each question-answer pair, we then expand\nthe background check with the information con-\ntained within. This allows us to gradually incorpo-\nrate information from multiple rounds of retrieval,\nwithout running into the token limit. To expand a\nbackground check with information from a specific\nquestion-answer pair, we use the prompt seen in\nAppendix B.2 with each respective LLM.\n4see https://huggingface.co/meta-llama/\nMeta-Llama-3-8B-Instruct\n5https://programmablesearchengine.google.com/4.2 Evaluation\nWhen evaluating media background checks, we are\nprimarily interested in recalling the information\nprovided in the gold background check. Systems\nmay find information beyond what is included in\nthe gold example. Prioritising recall avoids penal-\nising such additions.\nWe adopt a variant of FActScores (Min et al.,\n2023) to evaluate models. We begin by break-\ning each gold MBC down into atomic facts. As\nMBCs cover relatively similar topics, we use a list\nof 42 templates to generate facts (see Appendix C).\nEach template contains contextually determined to-\nkens, which we fill with details from the gold MBC\nusing GPT-3.5-turbo . During initial experiments,\nwe found this strategy to perform better than fully\ngenerating facts. We subsequently verify whether\neach fact is entailed by the gold MBC. We experi-\nmented with several models, including a trained De-\nBERTa model (He et al., 2021) and an open-source\nLLM, but ultimately found the best-performing sys-\ntem to be GPT-3.5-turbo with zero-shot chain of\nthough (Kojima et al., 2022). Our prompt can be\nfound in Appendix B.3. To increase performance,\nwe prompt the (probabilistic) model four times,\nand take as prediction as the majority-voted ele-\nment among the runs. We keep atomic facts which\nareentailed orcontradicted by the gold MBC.\nTo score a generated background check, we com-\npute fact recall , the percentage of atomic facts for\nwhich the gold MBC and the predicted MBC agree\non entailment. We use the same entailment model.\nWe furthermore report the error rate, the percentage\nof atomic facts either are entailed by one MBC and\ncontradicted by the other.\nAs a sanity check, we evaluate what performance\nthegold background checks would yield on the de-\nvelopment set; that is, using gold data both as pre-\ndiction and reference. Ideally, the fact recall rate\nshould be roughly 1, and the error rate roughly 0;\nbut as entailment is noisy, this ideal is not reached.\nThe computed fact recall and error rate are, respec-\ntively, 84.1% and 3.4%. We conclude that fact\nrecall, while noisy, remains a useful measurement\nof how many gold facts are recalled.\nWe furthermore manually inspect evaluation for\nten MBCs. We find that, on average, 59.8% of\nthe facts from the gold summary were accounted\nfor in the generated atomic facts. The majority of\nthe excluded facts were \u201cmulti-hop facts\u201d, which\nunfortunately our evaluation strategy does not ac-\nGold Example:\nHistory\nFounded in 2005, Natural News is a conspiracy and pseudoscience website that routinely publishes false information.\nThe founder is Mike Adams, who owns several Questionable websites such as News Target and Trump.news.\nFunded by / Ownership\nNatural News is owned by Mike Adams, who owns numerous other fake and or controversial websites. The website, like\nall of Mike Adams\u2019s properties, is funded through online advertising.\nAnalysis / Bias\n...\nGenerated Example (GPT-3.5-Turbo):\nBackground check\n1. Natural News is owned by Mike Adams, who is also known as the \u201cHealth Ranger.\u201d\n2. The website is primarily funded through the sale of various health supplements, books, and other products advertised\non the site.\n3. Natural News is known for promoting alternative medicine, conspiracy theories, and pseudoscience.\n4. The site has been criticized for its lack of credible sources and scientific evidence to support its claims.\nGenerated Example (GPT-3.5-Turbo + Google):\n*Background check on naturalnews.com:**\n1. **Ownership**: Founded by Mike Adams, known as the \u201cHealth Ranger.\u201d\n2. **Funding**: Primarily funded through the sale of various health supplements and products on their website.\n3. **Bias**: Strongly criticized for promoting pseudoscience, conspiracy theories, and misinformation, particularly\nregarding vaccines and alternative medicine.\n4. **Fact-checks**: Natural News has a history of publishing misinformation and has been flagged by fact-checkers\nsuch as Snopes, Politifact, and FactCheck.org for spreading false information.\n5. **Type of site**: Known for promoting alternative medicine, pseudoscience, disinformation, and far-right extremism.\n6. **Country of Origin**: United States.\n7. **Political leaning**: Natural News is considered a far-right website known for anti-vaccination conspiracy theories\nand fake news.\nFigure 3: Example background checks for Natural News . The gold example is taken from the Media Bias /\nFact Check website, while the generated example is produced by GPT-3.5 augmented with Google search as\ndescribed in Section 4.1. The gold example has been shortened, and the full version can be seen at https:\n//mediabiasfactcheck.com/natural-news/ .\ncount for \u2013 such as which other media companies\nare owned by the parent organization of the source\nreported on in the background check.\nWhen the right atomic facts areextracted, our\nentailment system works well. When evaluating\ngenerated MBCs for these ten sources, we agree\nwith 93.9% of entailment predictions made by our\nGPT-3.5 ensemble.\n4.3 Results\nUsing this evaluation metric, we evaluate our four\nMBC generation models (see Table 2). In addition\nto our proposed metric, we also include two tradi-\ntional measures: METEOR and ROUGE-L. Fact\nrecall rates are low, highlighting the difficulty of\nthe task. Nevertheless, we see clear performance\nimprovements from retrieval-augmentation, bothfor GPT-3.5 and Llama 3. This supports our intu-\nition that finding the right information is a crucial\nbarrier to source-critical reasoning in models.\nTo understand what causes the low recall, we\nmanually analyse 10 randomly chosen examples\n(generated by GPT-3.5 with retrieval). We find the\nfollowing omissions: seven missing entity men-\ntions (e.g., who the editor is), three missing failed\nfact-checks, three missing mentions of editorial\nstance (e.g., right- or left-leaning), two missing\nfunding sources, two historical events (e.g., an\nownership change), and one missing mention of\ncontent being re-published from another site. With-\nout retrieval, we see a further two missing failed\nfact-checks, one missing entity mention, one miss-\ning funding source, and one missing mention of\neditorial stance.\nFact Recall Error Rate METEOR ROUGE-L\nGPT-3.5-Turbo 22.7% 6.2% 9.9% 12.5%\nGPT-3.5-Turbo + Google 26.1% 6.3% 12.6% 13.1%\nLlama 3 7b Instruct 24.4% 10.4% 15.3% 14.4%\nLlama 3 7b Instruct + Google 25.1% 10.7% 15.5% 14.4%\nTable 2: Performance of four different systems on our MB/FC dataset. We measure fact recall and error rate, as\ndiscussed in Section 4.2. These represent, respectively, how many facts from the gold background check were\nrecalled by the model, and how many facts from the gold background check were contradicted by the model. We\nalso include two traditional summarisation metrics, METEOR and ROUGE-L.\nWe furthermore see, in both cases, five differ-\nent multi-hop facts omitted. However, as we dis-\ncuss Section 4.2, our evaluation metric also fails at\ncatching those, so they are not responsible for the\nlow recall. Overall, the biggest problem is missing\nentities \u2013 future work could tailor retrieval espe-\ncially to this scenario, for example by placing extra\nemphasis on retrieved \u201cabout\u201d-sections.\nCompared to GPT-3.5, Llama achieves high fact\nrecall and METEOR/ROUGE-L scores, but also\nexhibits a high error rate. Exploring the data, we\nsee significant differences in the lengths of MBCs \u2013\nthe average GPT-3.5 generated MBC is 176 tokens,\nwhile for Llama the average has 254 tokens. The\ngold MBCs contain on average 477 tokens. This\nexplains the discrepancy: Llama generates longer\nsummaries with more facts, and so is more likely\nto state both correct and incorrect things about the\nknowledge source. Manually reviewing the gen-\nerated data, we see one more difference: GPT-3.5\nperforms better at integrating retrieved information,\nexplaining the higher fact recall in the retrieval-\naugmented setting.\n4.4 Human Evaluation\nWe envision MBCs as being used in two settings:\nas assistive instruments for either humans ormod-\nelshaving to make sense out of untrustworthy ev-\nidence. To demonstrate the potential, we conduct\ntwo experiments with human participants. For\nthis purpose, we recruited 11 researchers working\non automated fact-checking, hate speech analysis,\nLLMs, and related NLP tasks (ranging from PhD-\nstudents to assistant professors).\n4.4.1 QA with Untrustworthy Evidence\nWe first create a small dataset of questions for\nwhich multiple, conflicting, and potentially untrust-\nworthy evidence documents could reasonably be\nexpected to surface. We compose this of ten ques-tions about controversial subjects , and ten ques-\ntions about known misinformation . Wan et al.\n(2024) recently released a small dataset of ques-\ntions with conflicting evidence, including four ex-\namples of controversial political questions. We\ninclude the four from their dataset, and generate an\naddition six following their approach. For known\nmisinformation, we manually extract ten questions\nby rephrasing claims from the development set of\nthe A VeriTeC dataset (Schlichtkrull et al., 2023b),\nwhich contains labelled examples of fact-checked\nreal-world misinformation. We choose randomly\nfrom refuted claims in the dataset, focusing on\nclaims originating from sources easily findable via\na search engine (i.e., excluding claims from Twitter,\nFacebook, and other social media).\nFor all twenty questions, we find two disagree-\ning evidence documents. We pick the first two\nsearch results that disagree when entering the ques-\ntion into Google. In all cases, these documents are\nfound on the first page. These are documents which\na searcher, whether human or algorithm, would eas-\nily come across. For the ten known misinformation\nquestions, the original misinformation source la-\nbelled in A VeriTeC appeared on the first page 6/10\ntimes; for the rest, we found an alternative source\nsupporting the misinformation.\nWe believe question answering is representative\nof how LLMs are envisioned to replace search; see\ne.g. Metzler et al. (2021). We recognise that, in\nactual use, LLMs are also used for other tasks, such\nas programming and software engineering-related\nfunctions. Answering questions about culture and\ngeography is among the most frequent uses (Zheng\net al., 2024), and for such questions, source reliabil-\nity is certainly an issue. Further, answering more\ngeneral questions is also a common use-case. As\nsuch, we believe that QA is 1) highly representative\nof the vision large providers have of LLMs, and 2)\nrepresentative of current use cases.\nwith MBC without MBC\nmean SD mean SD tStatistic p-value\nProvision of Sufficient Information 78.2% - 70.9% - 0.740 0.746\nDifficulty of Answering 2.24 0.75 2.81 0.82 -1.633 0.133\nDifficulty of Establishing Trust 1.95 0.54 2.88 0.65 -3.791* 0.004\nTable 3: Results for part one of our human evaluation, estimating the helpfulness of generated MBCs when presented\ndirectly to users. Provision of sufficient information is annotated as a binary yes/no-question, while the difficulty of\nanswering the question and the difficulty of establishing which sources are trustworthy are rated on five-point Likert\nscales. Results are analysed via student\u2019s t-tests. * indicates significance at p= 0.01.\n4.4.2 Are MBCs helpful for humans?\nWe first investigate whether MBCs are helpful for\nhumans when encountering conflicting sources. We\nask our annotators to answer ten questions from\nour dataset, given the two conflicting sources asso-\nciated with that question. We randomly pick five\nquestions based on controversial subjects, and five\nbased on misinformation. For each question, we\nfurther randomly choose whether to show an ac-\ncompanying background check. We use generated\nMBCs, simulating real-world use of our model. We\nuse the best-performing model \u2013 GPT-3.5-Turbo\nwith retrieval.\nAfter answering the question, each participant\nis asked to judge 1) whether they were given suf-\nficient information to answer the question; 2) how\ndifficult it was to answer the question; and 3) how\ndifficult it was to decide which source to trust. In-\ntuitively, if MBCs are helpful for humans when\nengaging with epistemic uncertainty, we should ex-\npect a lower cognitive load \u2013 and thus for the task\nto feel less difficult (Sweller et al., 2011) \u2013 when\nan MBC is provided. For information sufficiency,\nwe ask participants to give a binary answer. For\ntask difficulty, we follow previous work and use a\nself-reported Likert scale as our measurement. We\nuse a scale of 1 (very easy) to 5 (very difficult). An\nexample page from our questionnaire can be seen\nin Appendix F.1.\nIn Table 3, we report the mean answer given by\nour participants for information sufficiency, answer\ndifficulty, and trust difficulty. We furthermore re-\nport the standard difference between participants\nfor answer difficulty and trust difficulty. To anal-\nyse our results, we conduct a paired samples t-test\ncomparing the responses of each participant with\nand without media background checks.\nThe cognitive load of deciding which documents\nto trust was much lower when annotators were pro-\nvided with a background check (1.95 versus 2.88,on average). The cognitive load of answering the\nquestion was also lower with a background check,\nalthough this result is not statistically significant.\nSimilarly, our annotators more frequently reported\nthat sufficient information had been provided when\nalso given a background check, but this result was\nnot statistically significant either. Post-hoc discus-\nsions with our participants were revealing: when\nnot provided with a background check, some par-\nticipants instead used the internet to find similar\ninformation for themselves. As our instructions\nincluded a link to each of the sources used, par-\nticipants considered the information provided to\nbe \u201csufficient\u201d; although answering the question\nrequired additional effort. For humans, automati-\ncally generated background checks thus quicken a\npart of the meaning-producing process that is in all\ncases necessary .\n4.4.3 Are MBCs helpful for NLP models?\nUltimately, our goal in developing MBCs was to\nintroduce and test source-critical reasoning capa-\nbilities in NLP models, specifically LLMs. As\nsuch, we also seek to demonstrate the helpfulness\nof MBCs to retrieval-augmented LLMs. To do so,\nwe again conduct a human experiment.\nWe simulate a question-answering setting with\na retrieval-augmented LLM. Given one of the re-\nmaining questions from our dataset, we assume\nthat retrieval has returned one of the two disagree-\ning evidence documents. We ask GPT-46to an-\nswer the question based on the returned document.\nWe repeat this with the other evidence document.\nFor each source, we generate a background check\nusing our best-performing model (GPT-3.5-Turbo\nwith search). We generate a second version of the\nanswer, including this background check in the in-\nstructions to GPT-4. Our prompts for the two cases\ncan be seen in Appendix B.4.\n6gpt-4-turbo-2024-04-09\nwith MBC without MBC Equally Good \u03c72p-value\nPreferred Answer 165 26 29 57.02* 0.000\nBetter Understanding Provided 69 56 95 60.22* 0.001\nTable 4: Results for part two of our human evaluation, estimating the helpfulness of generated MBCs when provided\nalong with retrieved results to GPT-4 in a question-answering task. Annotators directly state which answer they\nprefer, and which provides a better understanding of the topic. Results are analysed via a chi-square test. * indicates\nsignificance at p= 0.01.\nwith MBC without MBC\nmean SD mean SD tStatistic p-value\nAnswer is Misleading 1.57 0.35 2.58 0.63 -5.634* 0.000\nTable 5: Further results for part two of our human evaluation. Annotators evaluate how misleading LLM responses\nare on a five-point Likert scale. Results are analysed via a student\u2019s t-test. * indicates significance at p= 0.01.\nWe show our participants the two answers to\neach of these 20 question-source pairs (in random\norder). We then ask them to determine 1) which an-\nswer they prefer; 2) which answer gives the best un-\nderstanding of the topic; 3) for each answer, if they\nwould feel misled if given that answer by an AI\nchatbot. For preference and understanding, anno-\ntators can pick the first answer, the second answer,\nor indicate that the two answers are equally good.\nFor feeling misled we employ a five-point Likert\nscale ranging from 1 (no, not at all) to 5 (yes, very\nmuch). An example page from our questionnaire\ncan be seen in Appendix F.2.\nIn Table 4 we report how many times our par-\nticipants indicated that they preferred an answer\ngenerated with or without an MBC, and how many\ntimes they indicated that answers with or without\nMBCs provided a better understanding of the topic.\nWe conduct a chi-squared test to establish the sig-\nnificance of our results. The responses strongly\nindicate that participants preferred the answers gen-\nerated with MBCs. Further, answers generated\nwith MBCs provided a better understanding of the\ntopic more often \u2013 by a smaller margin, but still\nsignificantly so. Interestingly, if we analyse only\nthe answers generated with (what we consider to\nbe) trustworthy sources, the latter finding disap-\npears while the former remains. We theorise that\nMBC-backed answers give a better understanding\nwhen sources are untrustworthy, but are preferred\neven for trustworthy sources as they help the user\nobtain confidence in their answer.\nIn addition to the pairwise comparison, we fur-\nther investigated misleadingness. In Table 5, we\nreport the mean answer given by our participantson how misled they would feel if given a generated\nanswer by a chatbot, along with the standard devia-\ntion. We conduct a paired samples t-test to analyse\nthese results. As can be seen, answers generated\nwith MBCs were on average rated significantly less\nmisleading than answers generated without MBCs.\nThese findings support our primary hypotheses\n\u2013 that search-augmented LLMs do not adequately\naccount for the tendencies and biases of the sources\nthey rely on, and that providing (even automatically\ngenerated) MBCs to models at inference time can\nalleviate this and enable automated source-critical\nreasoning.\n5 Related Work\nClosest to our work, Baly et al. (2018); Zhang\net al. (2019b); Baly et al. (2020b) classifies the\nbias and factuality of sources based on data from\nMedia Bias/Fact Check. However, they only pre-\ndict the bias labels for both (as given elsewhere\non MB/FC), notthe detailed background checks\nwe produce. Features include sample articles from\nthe source, its Wikipedia page and Twitter account,\nand information about the web domain. Hounsel\net al. (2020) proposed further web-domain-features\nsuch as host, domain, and certificates for websites\nwhen predicting trustworthiness.\nKnowledge conflicts in retrieved evidence for\nLLMs is an active research area; see the survey\nby Xu et al. (2024). Using their terminology, our\nproposal is concerned with inter-context conflicts .\nThe mentioned mitigation strategies, e.g. Chen and\nShu (2023); Vergho et al. (2024), follow the above\nline of reasoning: they use a trained model to detect\nand remove \u201cunfactual\u201d documents. This assumes a\nsingle source of truth (the training data or evidence\ndatabase for the misinformation detector), which as\nwe have argued does not align with best practises\nfor human knowledge experts.\nA rich literature exists \u2013 referred to as fact-\nfinding \u2013 proposing probabilistic models for com-\nputing the likelihood of claims and the trustwor-\nthiness of sources (Yin et al., 2007; Dong et al.,\n2009; Pasternack and Roth, 2010, 2013; Vydis-\nwaran et al., 2011; Zhang et al., 2019a). The basis\nis the assumption by (Yin et al., 2007) that \u201ca web\nsite is trustworthy if it provides many pieces of true\ninformation, and a piece of information is likely to\nbe true if it is provided by many trustworthy web\nsites\u201d . Yuan et al. (2020) proposed to predict the\ncredibility of sources based on whether known mis-\ninformation spreaders share similar content. Along\nsimilar lines, Wright and Augenstein (2021) in-\ntroduced a model for citation-worthiness where\ncitation-worthy papers feature in citation-worthy\njournals, and vice versa.\nDong et al. (2009) identified a copying problem\nin fact-finding, where this strategy breaks down\nif many seemingly independent actors copy their\npositions from each other; this makes majority vot-\ning an unreliable heuristic for determining whether\na piece of information is trustworthy. Bala and\nGoyal (1998) identified a similar issue, where a\n\u201croyal family of knowers\u201d can cause communities\nto converge on false beliefs if their connectivity is\nmuch greater than the average epistemic agent.\nKaneko et al. (2009); Nakano et al. (2010) pro-\nposed to develop \u201ccredibility survey reports\u201d for\nsearch results. These are topic-specific documents\nthat map the positions of different, contradicting\nsearch results with respect to one particular query\nand to each other. Unlike our proposal, these do not\nrepresent the general biases and tendencies of au-\nthors in, and so limit reasoning entirely to what can\nbe done on the basis of one search query. Shibuki\net al. (2010) later developed a summarisation algo-\nrithm for this task.\nThe credibility and bias of individual documents\nhas also been studied previously. For Wikipedia\narticles, Zeng et al. (2006); Adler and de Alfaro\n(2007) attempted to predict trustworthiness based\non revision history. (Nakov et al., 2017) studies the\ncredibility of statements on community QA forums\nbased on linguistic signals in the individual posts;\nBaly et al. (2020a) similarly used the surface forms\nof text to predict the credibility of news articles.Finally, the concept of credibility has been stud-\nied extensively outside NLP, e.g. in information\nscience. For an overview, the framework by Rieh\nand Danielson (2007) is an excellent resource. Sim-\nilarly, source critical methods are widely studied,\nespecially in history; we recommend Howell and\nPrevenier (2001) for an introduction.\n6 Conclusion\nWe have introduced the task of media background\ncheck generation, a novel task wherein models sum-\nmarise information about media sources to enable\ncritical analysis. We have furthermore presented\na dataset of 6,709 examples collected from the\nMedia Bias / Fact Check website. We have in-\nvestigated several baselines for the task, including\nboth open-source and closed-source LLMs. Our\nfindings demonstrate that retrieval-augmentation\ncan greatly improve performance, and interestingly\nthat open-source models are very competitive on\nthis difficult task. Finally, our human evaluation\ngives strong evidence both that media background\nchecks are helpful for humans when evaluating\nmedia sources, and helpful for models when gener-\nating answers based on retrieved sources.\n7 Limitations\nOur paper proposes to establish trust by providing\ninformation about bias and tendency. In our models,\nthat information comes either from model weights\nor retrieved documents. As such, that information\nisitself potentially untrustworthy. Taken seriously,\nthis prompts another round of retrieval to establish\nthe trustworthiness of the background check; and\nthen another, to establish the trustworthiness of that\nround \u2013 it\u2019s turtles all the way down. Ultimately,\nour proposal cannot conclusively establish trust;\nonly establish it insofar as the user already trusts\nsome sources. As users have different requirements\nin terms of which sources they trust and how many\n\u201clevels\u201d of trust they might want to explore, one\npossible solution could be an interactive system\nallowing users to expand background checks in a\ndesired direction, similar to recent proposals for\nsummarisation (Shapira et al., 2021).\nWhen evaluating systems with access to search,\nwe excluded the Media Bias / Fact Check website\nitself (and associated websites) from search results.\nOur intention was to make our evaluation setup\nas fair as possible. However, this introduces two\nlimitations: 1) \u201cin the wild\u201d systems may perform\nbetter than in our evaluations, as they do have ac-\ncess to background checks from MB/FC; and 2)\nwe may still overestimate the relative performance\nof retrieval-augmented systems slightly, as we ul-\ntimately cannot make sure that no website which\nquotes MB / FC was included in search results. For\nthe latter, manual analysis of 20 examples did not\nturn up any such websites.\n8 Ethical Considerations\nThe dataset presented in this paper is based on\nMedia Bias / Fact Check, a volunteer-run project.\nBackground checks included here may themselves\nbe biased on incomplete, as may background\nchecks produced by models trained or evaluated\non our dataset. Furthermore, the machine learning\nmodels and search engine used for our models con-\ntain well-known biases (Noble, 2018; Bender et al.,\n2021). Acting on trustworthiness estimates arrived\nat through biased means, including automatically\nproduced ranking decisions for evidence retrieval,\nrisks causing epistemic harm (Schlichtkrull et al.,\n2023a). The datasets and models described in this\npaper are not intended for and should not be used\nfor truth-telling, e.g. for the design of automated\ncontent moderation systems.\nWe did not take any steps to anonymise the data.\nThe claims discussed in our dataset are based on\npublicly available data from a journalistic publica-\ntion, and concern public figures and events \u2013 ref-\nerences to these are important to document the\nhistory of a publication. If any person included in\nour dataset (e.g., the owner of a particular media\nsource) requests it, we will remove that example\nfrom the dataset.\nAcknowledgements\nWe would like to thank Nedjma Ousidhoum and\nRui Cao for their helpful comments, discussions,\nand feedback. We would also like to thank the\nanonymous reviewers for their questions and com-\nments that helped us improve the paper. Finally, we\nwould like to thank Dave Van Zandt and the Media\nBias / Fact Check team for lending us their data, as\nwell as for their pioneering work on the subject.\nReferences\nB. Thomas Adler and Luca de Alfaro. 2007. A content-\ndriven reputation system for the wikipedia. In Pro-\nceedings of the 16th International Conference on\nWorld Wide Web , WWW \u201907, page 261\u2013270, NewYork, NY , USA. Association for Computing Machin-\nery.\nVenkatesh Bala and Sanjeev Goyal. 1998. Learning\nfrom Neighbours. Review of Economic Studies ,\n65(3):595\u2013621.\nRamy Baly, Giovanni Da San Martino, James Glass,\nand Preslav Nakov. 2020a. We can detect your bias:\nPredicting the political ideology of news articles. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) ,\npages 4982\u20134991, Online. Association for Computa-\ntional Linguistics.\nRamy Baly, Georgi Karadzhov, Dimitar Alexandrov,\nJames Glass, and Preslav Nakov. 2018. Predict-\ning factuality of reporting and bias of news media\nsources. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing ,\npages 3528\u20133539, Brussels, Belgium. Association\nfor Computational Linguistics.\nRamy Baly, Georgi Karadzhov, Jisun An, Haewoon\nKwak, Yoan Dinkov, Ahmed Ali, James Glass, and\nPreslav Nakov. 2020b. What was written vs. who\nread it: News media profiling using text analysis\nand social media context. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics , pages 3364\u20133374, Online. Association\nfor Computational Linguistics.\nEmily M. Bender and Batya Friedman. 2018. Data\nstatements for natural language processing: Toward\nmitigating system bias and enabling better science.\nTransactions of the Association for Computational\nLinguistics , 6:587\u2013604.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language mod-\nels be too big?\n . InProceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency , pages 610\u2013623.\nZana Bu\u00e7inca, Maja Barbara Malaya, and Krzysztof Z.\nGajos. 2021. To trust or to think: Cognitive forcing\nfunctions can reduce overreliance on ai in ai-assisted\ndecision-making. Proc. ACM Hum.-Comput. Inter-\nact., 5(CSCW1).\nCanyu Chen and Kai Shu. 2023. Combating misinfor-\nmation in the age of llms: Opportunities and chal-\nlenges. arXiv preprint arXiv:2311.05656 .\nXin Luna Dong, Laure Berti-Equille, and Divesh Sri-\nvastava. 2009. Integrating conflicting data: the\nrole of source dependence. Proc. VLDB Endow. ,\n2(1):550\u2013561.\nYigal Godler and Zvi Reich. 2015. Journalistic evi-\ndence: Cross-verification as a constituent of medi-\nated knowledge. Journalism: Theory, Practice &\nCriticism , 18(5):558\u2013574.\nZhijiang Guo, Michael Schlichtkrull, and Andreas Vla-\nchos. 2022. A survey on automated fact-checking.\nTransactions of the Association for Computational\nLinguistics , 10:178\u2013206.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: Decoding-enhanced\nbert with disentangled attention. In International\nConference on Learning Representations .\nAustin Hounsel, Jordan Holland, Ben Kaiser, Kevin\nBorgolte, Nick Feamster, and Jonathan Mayer. 2020.\nIdentifying disinformation websites using infrastruc-\nture features. In 10th USENIX Workshop on Free and\nOpen Communications on the Internet (FOCI 20) .\nMartha C. Howell and William Prevenier. 2001. From\nReliable Sources: An Introduction to Historical Meth-\nods. Cornell paperbacks. Cornell University Press.\nJ. M. Imperial, G. Forey, and H. T. Madabushi. 2024.\nStandardize: Aligning language models with expert-\ndefined standards for content generation. arXiv\npreprint arXiv:2402.12593 .\nInternational Fact-Checking Network. 2016. Code of\nprinciples. Poynter Institute.\nKoichi Kaneko, Hideyuki Shibuki, Masahiro Nakano,\nRintaro Miyazaki, Madoka Ishioroshi, and Tat-\nsunori Mori. 2009. Mediatory summary generation:\nSummary-passage extraction for information credi-\nbility on the web. In Proceedings of the 23rd Pacific\nAsia Conference on Language, Information and Com-\nputation, Volume 1 , pages 240\u2013249, Hong Kong. City\nUniversity of Hong Kong.\nTakeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In Advances in\nNeural Information Processing Systems , volume 35,\npages 22199\u201322213. Curran Associates, Inc.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Proceedings of the 34th Inter-\nnational Conference on Neural Information Process-\ning Systems , NIPS \u201920, Red Hook, NY , USA. Curran\nAssociates Inc.\nMarcel Machill and Markus Beiler. 2009. The impor-\ntance of the internet for journalistic research: A multi-\nmethod study of the research performed by journal-\nists working for daily newspapers, radio, television\nand online. Journalism Studies , 10(2):178\u2013203.\nDonald Metzler, Yi Tay, Dara Bahri, and Marc Najork.\n2021. Rethinking search: making domain experts\nout of dilettantes. ACM SIGIR Forum , 55:1\u201327.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,\nWen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-\nmoyer, and Hannaneh Hajishirzi. 2023. FActScore:Fine-grained atomic evaluation of factual precision\nin long form text generation. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing , pages 12076\u201312100, Singa-\npore. Association for Computational Linguistics.\nMasahiro Nakano, Hideyuki Shibuki, Rintaro Miyazaki,\nMadoka Ishioroshi, Koichi Kaneko, and Tatsunori\nMori. 2010. Construction of text summarization cor-\npus for the credibility of information on the web. In\nProceedings of the Seventh International Conference\non Language Resources and Evaluation (LREC\u201910) ,\nValletta, Malta. European Language Resources Asso-\nciation (ELRA).\nPreslav Nakov, Tsvetomila Mihaylova, Llu\u00eds M\u00e0rquez,\nYashkumar Shiroya, and Ivan Koychev. 2017. Do not\ntrust the trolls: Predicting credibility in community\nquestion answering forums. In Proceedings of the\nInternational Conference Recent Advances in Natural\nLanguage Processing, RANLP 2017 , pages 551\u2013560,\nVarna, Bulgaria. INCOMA Ltd.\nSafiya Umoja Noble. 2018. Algorithms of oppression.\nInAlgorithms of Oppression . New York University\nPress.\nJeff Pasternack and Dan Roth. 2010. Knowing what\nto believe (when you already know something). In\nProceedings of the 23rd International Conference\non Computational Linguistics (Coling 2010) , pages\n877\u2013885, Beijing, China. Coling 2010 Organizing\nCommittee.\nJeff Pasternack and Dan Roth. 2013. Latent credibility\nanalysis. In Proceedings of the 22nd International\nConference on World Wide Web , WWW \u201913, page\n1009\u20131020, New York, NY , USA. Association for\nComputing Machinery.\nDenis Peskoff and Brandon Stewart. 2023. Credible\nwithout credit: Domain experts assess generative lan-\nguage models. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers) , pages 427\u2013438,\nToronto, Canada. Association for Computational Lin-\nguistics.\nWilliam James. Potter. 2013. Media Literacy . SAGE\nPublications.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing , pages 2383\u20132392, Austin,\nTexas. Association for Computational Linguistics.\nSoo Young Rieh and David R. Danielson. 2007. Cred-\nibility: A multidisciplinary framework. Annual\nReview of Information Science and Technology ,\n41(1):307\u2013364.\nMichael Schlichtkrull, Nedjma Ousidhoum, and An-\ndreas Vlachos. 2023a. The intended uses of auto-\nmated fact-checking artefacts: Why, how and who.\nInFindings of the Association for Computational Lin-\nguistics: EMNLP 2023 , pages 8618\u20138642, Singapore.\nAssociation for Computational Linguistics.\nMichael Sejr Schlichtkrull, Zhijiang Guo, and Andreas\nVlachos. 2023b. A VeriTeC: A dataset for real-world\nclaim verification with evidence from the web. In\nThirty-seventh Conference on Neural Information\nProcessing Systems Datasets and Benchmarks Track .\nChirag Shah and Emily M. Bender. 2022. Situating\nsearch. In Proceedings of the 2022 Conference on\nHuman Information Interaction and Retrieval , CHIIR\n\u201922, page 221\u2013232, New York, NY , USA. Association\nfor Computing Machinery.\nOri Shapira, Ramakanth Pasunuru, Hadar Ronen, Mo-\nhit Bansal, Yael Amsterdamer, and Ido Dagan. 2021.\nExtending multi-document summarization evaluation\nto the interactive setting. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , pages 657\u2013677, Online. As-\nsociation for Computational Linguistics.\nHideyuki Shibuki, Takahiro Nagai, Masahiro Nakano,\nRintaro Miyazaki, Madoka Ishioroshi, and Tatsunori\nMori. 2010. A method for automatically generating\na mediatory summary to verify credibility of infor-\nmation on the web. In Coling 2010: Posters , pages\n1140\u20131148, Beijing, China. Coling 2010 Organizing\nCommittee.\nThomas W Simpson. 2013. Evaluating google as an\nepistemic tool. Philosophical Engineering: Toward\na Philosophy of the Web , pages 97\u2013115.\nCatherine L. Smith and Soo Young Rieh. 2019.\nKnowledge-context in search systems: Toward\ninformation-literate actions. In Proceedings of the\n2019 Conference on Human Information Interaction\nand Retrieval , CHIIR \u201919, page 55\u201362, New York,\nNY , USA. Association for Computing Machinery.\nSteen Steensen. 2019. Journalism\u2019s epistemic crisis and\nits solution: Disinformation, datafication and source\ncriticism. Journalism , 20(1):185\u2013189. Publisher:\nSAGE Publications.\nJ. Sweller, P. Ayres, and S. Kalyuga. 2011. Cognitive\nLoad Theory . Explorations in the Learning Sciences,\nInstructional Systems and Performance Technologies.\nSpringer New York.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERification. In Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nPapers) , pages 809\u2013819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nTyler Vergho, Jean-Francois Godbout, Reihaneh Rab-\nbany, and Kellin Pelrine. 2024. Comparing gpt-4and open-source language models in misinformation\nmitigation.\nV .G. Vinod Vydiswaran, ChengXiang Zhai, and Dan\nRoth. 2011. Content-driven trust propagation frame-\nwork. In Proceedings of the 17th ACM SIGKDD In-\nternational Conference on Knowledge Discovery and\nData Mining , KDD \u201911, page 974\u2013982, New York,\nNY , USA. Association for Computing Machinery.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020. Fact or fiction: Verifying\nscientific claims. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 7534\u20137550, Online. As-\nsociation for Computational Linguistics.\nAlexander Wan, Eric Wallace, and Dan Klein. 2024.\nWhat evidence do language models find convincing?\nPreprint , arXiv:2402.11782.\nDustin Wright and Isabelle Augenstein. 2021. Cite-\nWorth: Cite-worthiness detection for improved scien-\ntific document understanding. In Findings of the\nAssociation for Computational Linguistics: ACL-\nIJCNLP 2021 , pages 1796\u20131807, Online. Association\nfor Computational Linguistics.\nRongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang,\nHongru Wang, Yue Zhang, and Wei Xu. 2024.\nKnowledge conflicts for llms: A survey. Preprint ,\narXiv:2403.08319.\nXiaoxin Yin, Jiawei Han, and Philip S. Yu. 2007.\nTruth discovery with multiple conflicting informa-\ntion providers on the web. In Proceedings of the 13th\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining , KDD \u201907, page\n1048\u20131052, New York, NY , USA. Association for\nComputing Machinery.\nChunyuan Yuan, Qianwen Ma, Wei Zhou, Jizhong\nHan, and Songlin Hu. 2020. Early detection of fake\nnews by utilizing the credibility of news, publish-\ners, and users based on weakly supervised learn-\ning. In Proceedings of the 28th International Con-\nference on Computational Linguistics , pages 5444\u2013\n5454, Barcelona, Spain (Online). International Com-\nmittee on Computational Linguistics.\nHonglei Zeng, Maher A. Alhossaini, Li Ding, Richard\nFikes, and Deborah L. McGuinness. 2006. Comput-\ning trust from revision history. In Proceedings of the\n2006 International Conference on Privacy, Security\nand Trust: Bridge the Gap Between PST Technolo-\ngies and Business Services , PST \u201906, New York, NY ,\nUSA. Association for Computing Machinery.\nYi Zhang, Zachary Ives, and Dan Roth. 2019a.\nEvidence-based trustworthiness. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics , pages 413\u2013423, Florence,\nItaly. Association for Computational Linguistics.\nYifan Zhang, Giovanni Da San Martino, Alberto Barr\u00f3n-\nCede\u00f1o, Salvatore Romeo, Jisun An, Haewoon Kwak,\nTodor Staykovski, Israa Jaradat, Georgi Karadzhov,\nRamy Baly, Kareem Darwish, James Glass, and\nPreslav Nakov. 2019b. Tanbih: Get to know what\nyou are reading. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP):\nSystem Demonstrations , pages 223\u2013228, Hong Kong,\nChina. Association for Computational Linguistics.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle\nLi, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez,\nIon Stoica, and Hao Zhang. 2024. Lmsys-chat-1m: A\nlarge-scale real-world llm conversation dataset. In In-\nternational Conference on Learning Representations\n(ICLR) .\nA Computational Resources\nOur experiments with opensource models were car-\nried out using a single NVidia A100 GPU.\nB Prompts\nB.1 Initial Prompt\nOur baseline consists of a simple prompt to an\nLLM, querying the model to generate an MBC\nbased on the name of a media source. We use\nthe same prompt for ChatGPT and Llama 3. The\nprompt can be seen in Figure 4. When generating\nan initial background check to use as a starting\npoint for our retrieval-augmented models, we use\nthis prompt as well.\nSystem message You are InfoHuntGPT, a\nworld-class AI assistant used by journalists to\nquickly build knowledge of new sources.\nUser message Buildabackgroundcheckforthe\nnews source \u201c{ source name }\u201d. Write down ev-\nerythingyouknowaboutthem,e.g. whofunds\nthem, how they make money, if they have any\nparticular bias. Make an ITEMIZED LIST. Be\nbrief, and if you don\u2019t know something, just\nleave it out. If you are aware that they have\nfailed any fact-checks, mention which. Begin\nyour response with \u201c**Background check**\u201d.\nFigure 4: Prompt used for ChatGPT and Llama 3 when\ngenerating MBCs with no supporting retrieved evidence.\nThis prompt is also used to generate the initial MBC\nwhich is later updated in the retrieved-evidence setting.System message You are InfoHuntGPT, a\nworld-class AI assistant used by journalists to\nquickly build knowledge of new sources.\nUser message Build a background check for\nthe news source \u201c{ source name }\u201d. Write down\neverything you know about them, e.g. who\nfundsthem,howtheymakemoney,iftheyhave\nany particular bias. Make an ITEMIZED LIST.\nBe brief, and if you don\u2019t know something, just\nleave it out. If you are aware that they have\nfailed any fact-checks, mention which. Begin\nyour response with \u201c**Background check**\u201d.\nAssistant message {Previous background check }\nUser message Googlesearchhasrevealedsome\nnew information:\n{Question-answer pairs }\nUpdate your background check for \u201c{ source\nname}\u201d using the new information. Do NOT\ndelete any information, but make ADDITIONS\nwhere necessary, using the new information.\nMost likely, you will just need to add an extra\nitem to the itemized list you previously created.\nMake minimal edits, and only incorporate\nwhat is relevant. Begin your response with\n\u201c**Background check**\u201d.\nFigure 5: Prompt used for ChatGPT and Llama 3 when\nupdating an MBC with retrieved information. The re-\ntrieved information is input to the prompt in the form\nof question-answer pairs, following the methodology\ndescribed in Section 4.1.\nB.2 Update Prompt\nFor our retrieval-augmented models, we use the\nprompt seen in Figure 5 to incorporate new in-\nformation from Google Search into a background\ncheck.\nB.3 Entailment Prompt\nWhen evaluating MBCs via FActScores, we com-\npute entailment using the prompt seen in Figure 6.\nB.4 Question-Answering Prompt\nFor our human evaluation in Section 4.4.3, we\nsimulate retrieval-augmented question-answering.\nGiven a source document and possibly an MBC,\nwe answer questions using the prompts in Figures 7\nand 8.\nC Fact Generation\nWhen evaluating background checks, we use a vari-\nant of FActScore (Min et al., 2023). The atomic\nSystem message You are FactCheckGPT, a\nworld-class tool used by journalists to discover\nproblems in their writings. Users give you text,\nand check whether facts are true given the text.\nYou ALWAYS answer either TRUE, FALSE, or\nNOT ENOUGH EVIDENCE.\nUser message You will be given a snippet\nwritten as part of a source criticism exercise,\nand a claim. Your task is to determine whether\nthe claim is true based ONLY on the text. Do\nNOT use any other knowledge source\nThe claim is: \u201c{ hypothesis }\u201d.\nThe text follows below:\n\u201c{premise}\u201d.\n{hypothesis }? Thinking step by step, answer\neither TRUE, FALSE, or NOT ENOUGH EVI-\nDENCE, capitalizing all letters. Explain your\nreasoning FIRST, and after that output either\nTRUE, FALSE, or NOT ENOUGH EVIDENCE.\nFigure 6: Prompt used for our LLMs when computing\ntextual entailment. We use a probabilistic model, and\nretain the majority prediction over four votes.\nfacts we use are generated following the templates\nseen in Figure 10. The initial questions are deco-\nrated with information from the gold background\ncheck using the prompt seen in Figure 9.\nD Facts in Media Background Checks\nTo understand which facts are represented in the\nMB/FC dataset, we manually analyse 20 randomly\nselected background checks. We include our find-\nings in Table 6. We note that this is not a complete\nlist \u2013 picking specific high-quality background\nchecks, e.g. background checks for the Guardian,\nFox News, or Breitbart, reveals usage of rarer facts.\nAs such, the list of atomic facts we use for evalua-\ntion is longer (see Appendix C).\nE Information-seeking Questions\nWhen retrieving information, we first retrieved doc-\numents via the Google Search API. Then, we use\na trained question-answering model to select the\nmost salient substrings. The queries and questions\nused for both can be found in Figure 11.\nF Questionnaire\nF.1 Part 1\nThe questionnaire pages used in the first half of\nour survey can be seen in Figures 12, 13, and 14.System message You are an expert journalist.\nYou will be given some evidence, and a ques-\ntion. Using the provided evidence, answer the\nquestion.\nUser message Using the provided evidence,\nanswer the following: \u201c{ question}\u201d.\nThe following evidence is provided: \u201c{ source\ndocument }\nThis information comes from \u201c{ domain name of\nsource document }\u201d.\nYour answer should be short and concise,\nfive sentences at most. Include an explanation\nof why the user should or should not trust the\nsource.\nFigure 7: Prompt used for answering questions using\nGPT-4 when not providing an MBC.\nThese focus on evaluating if MBCs are helpful for\nhumans when creating meaning from untrustworthy\nevidence documents.\nF.2 Part 2\nThe questionnaire pages used in the second half of\nour survey can be seen in Figures 15 and 16. These\nfocus on evaluating if MBCs are helpful for NLP\nmodels when creating meaning from untrustworthy\nevidence documents.\nG Data Statement\nFollowing Bender and Friedman (2018), we in-\nclude a data statement describing the characteristics\nof MB/FC.\nG.1 Curation Rationale\nWe processed a total of 6,709 source documents\nfrom the Media Bias / Fact Check website, which\nprovides volunteer-written assessments of English-\nlanguage media sources. The intended use is to\nprovide empirical evidence for the level of source-\ncritical analysis possible for NLP models, as well\nas to drive research into new tools that enable\nsource-critical analysis by models and humans.\nG.2 Language variety\nAs MB/FC only provides analysis of English-\nlanguage sources, in English, the same holds true\nfor our dataset.\nFact Percentage of MBCs\nGeneral bias of { source name } 80%\nFunding model for { source name } 75%\nRemit of { source name } 75%\nFact-checking history of { source name } 65%\nOwner of { source name } 60%\nPublisher of { source name } 50%\nExamples of biased articles from { source name } 50%\nGeographical focus of { source name } 50%\nOther history of { source name } 45%\nMultihop Facts 40%\n{source name } does not disclose important information 35%\nSources used by { source name } (e.g., AP) 35%\nFounder of { source name } 25%\nLoaded language used by { source name } 25%\nExamples of articles from { source name } for demonstrating aspects other than bias 20%\nPolitical endorsements by { source name } 10%\n{source name } masquerades as 10%\nAwards given to { source name } 10%\nEditor of { source name } 10%\nBias rating by other site (e.g., NewsGuard) 10%\nDemonstration of agreement by { source name } with scientific consensus 5%\nComparison to other media 5%\nTable 6: Facts appearing in 20 randomly sampled background checks from Media Bias / Fact Check.\nG.3 Speaker demographics\nWe did not analyse the demographics of the indi-\nvidual writers and editors for each media source.\nG.4 Annotator demographics\nFor this dataset, we did not rely on human\nannotators beyond our own group. We pro-\ncessed data from Media Bias / Fact Check,\na volunteer-driven project, with details avail-\nable here: https://mediabiasfactcheck.com/\nfrequently-asked-questions/ .\nG.5 Speech situation\nThe MBCs included in this dataset were provided\nby the Media Bias / Fact Check volunteers for the\npurpose of educating the public on media bias and\ndeceptive news practises.\nG.6 Text characteristics\nWe compute various statistics for the text included\nin this dataset; see Section 3.System message You are an expert journalist.\nYou will be given some evidence, and a ques-\ntion. Using the provided evidence, answer the\nquestion.\nUser message Using the provided evidence,\nanswer the following: \u201c{ question}\u201d.\nThe following evidence is provided: \u201c{ source\ndocument }\nThis information comes from \u201c{ domain name of\nsource document }\u201d.\nThe following information about the source\nmay be relevant:\n\u201c{background check }\u201d\nYour answer should be short and concise,\nfive sentences at most. Include an explanation\nof why the user should or should not trust the\nsource.\nFigure 8: Prompt used for answering questions using\nGPT-4 when providing an MBC.\nSystem message You are InfoHuntGPT, a\nworld-class AI assistant used by journalists to\nquickly build knowledge of new sources.\nUser message You will be given a snippet writ-\nten as part of a source criticism exercise, and a\nfill-in-the-blank question (blanks represented\nby _). Your task is to fill in the blanks in the\nsentence, adding no additional information or\nwording. JUST replace the _character.\nThe question is:\n{template}\nThe text follows below:\n{gold background check }\nFill in the blanks in the question, adding\nno additional information or wording. JUST\nreplace the _character, and output ONLY the\nquestion with the blank filled in. No yapping.\nFigure 9: Prompt used for ChatGPT when decorating\nan atomic fact template with information from a back-\nground check.\n1. _founded { source name }\n2. _hosts { source name }\n3. _is the founder of { source name }\n4. _is the ceo of { source name }\n5. _leads { source name }\n6. _owns { source name }\n7. _publishes { source name }\n8. _is the owner of\u2018{ source name }\n9. _bought { source name }\n10. _acquired { source name }\n11. { source name } is funded through _\n12. The remit of { source name } is to _\n13. The usual audience of { source name } is _\n14. { source name } was awarded _\n15. { source name } tends to the _\n16. Other sources have commented on { source name }, stating that it tends to be _\n17. The factuality of { source name } is reported to be _\n18. { source name } failed a fact-check for an article titled \u201c_\u201d\n19. { source name } printed a retraction after failing a fact-check for an article titled \u201c_\u201d\n20. { source name } chose not to cover _\n21. { source name } provides original content written by staff journalists\n22. { source name } relies on advertising for revenue\n23. { source name } relies on subscriptions for revenue\n24. { source name } relies on donations for revenue\n25. { source name } has received donations from _\n26. When aggregating stories, { source name } relies on information from _\n27. { source name } has endorsed _\n28. { source name } has an editorial bias towards _\n29. _is the editor of { source name }\n30. { source name } was fined _ for _\n31. { source name } printed a biased article titled \u201c_\u201d\n32. { source name } printed a factually misleading article titled \u201c_\u201d\n33. { source name } had to apologize for _\n34. { source name } paid damages to _after _\n35. { source name } pretends to be _\n36. It is unknown who _\n37. { source name }\u2019s headquarter is located in _\n38. { source name } is funded by the _ government\n39. { source name } is a _\n40. { source name } uses a peer review process\n41. { source name } uses an internal fact-checking process\n42. { source name } covers the following topics: _\nFigure 10: Templates used for generating atomic facts in our evaluation setup. Each template is filled with\ninformation from the gold background check using gpt-3.5-turbo (by replacing the underscore). Templates which\nare not entailed (or contradicted) by the gold background check are discarded.\n1. \u201c{ source name }\u201d ownership / Who owns \u201c{ source name }\u201d?\n2. \u201c{ source name }\u201d funding / How is \u201c{ source name }\u201d funded?\n3. \u201c{ source name }\u201d about / What is \u201c{ source name }\u201d?\n4. \u201c{ source name }\u201d political leaning / What is the political leaning of \u201c{ source name }\u201d?\n5. \u201c{ source name }\u201d fact-check / Has \u201c{ source name }\u201d failed any fact-checks?\n6. \u201c{ source name }\u201d retracted article / Has \u201c{ source name }\u201d retracted any articles?\nFigure 11: Information-seeking queries input to the Google search API in order to find relevant information on the\nsource. Each query also has a corresponding question, which we use to retrieved the most relevant substring of the\nsearch results based on DeBERTa (He et al., 2021).\nFigure 12: An example page from the first half of our human evaluation questionnaire. Given a controversial\nquestion, two sources, and their background checks, our annotators were asked to answer the question. They were\nfurthermore asked to answer if they had sufficient information, and how difficult the task were. Continued in\nFigures 13 and 14.\nFigure 13: An example page from the first half of our human evaluation questionnaire. Given a controversial\nquestion, two sources, and their background checks, our annotators were asked to answer the question. They were\nfurthermore asked to answer if they had sufficient information, and how difficult the task were. Continued from\nFigure 12.\nFigure 14: An example page from the first half of our human evaluation questionnaire. Given a controversial\nquestion, two sources, and their background checks, our annotators were asked to answer the question. They were\nfurthermore asked to answer if they had sufficient information, and how difficult the task were. Continued from\nFigure 13.\nFigure 15: An example page from the second half of our human evaluation questionnaire. Given answers provided\nby GPT-4 with and without an MBC, annotators were asked for their preferences, as well as whether any answer\nwas misleading. Continued in Figures 16.\nFigure 16: An example page from the second half of our human evaluation questionnaire. Given answers provided\nby GPT-4 with and without an MBC, annotators were asked for their preferences, as well as whether any answer\nwas misleading. Continued from Figure 15.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Generating media background checks for automated source critical reasoning", "author": ["M Schlichtkrull"], "pub_year": "2024", "venue": "arXiv preprint arXiv:2409.00781", "abstract": "Not everything on the internet is true. This unfortunate fact requires both humans and models  to perform complex reasoning about credibility when working with retrieved information. In"}, "filled": false, "gsrank": 7, "pub_url": "https://arxiv.org/abs/2409.00781", "author_id": ["z8YvWyEAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:kvWl5snFmNIJ:scholar.google.com/&output=cite&scirp=6&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=kvWl5snFmNIJ&ei=BLWsaNaSEPnSieoPxKLpgQ0&json=", "num_citations": 6, "citedby_url": "/scholar?cites=15175096415373620626&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:kvWl5snFmNIJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2409.00781?"}}, {"title": "Who Checks the Checkers? Exploring Source Credibility in Twitter's Community Notes", "year": "2024", "pdf_data": "Who Checks the Checkers?\nExploring Source Credibility in Twitter\u2019s Community Notes\nUku Kangur\nUniversity of Tartu Institute of\nComputer Science\nTartu, Estonia\nuku.kangur@ut.eeRoshni Chakraborty\nUniversity of Tartu Institute of\nComputer Science\nTartu, Estonia\nroshni.chakraborty@ut.eeRajesh Sharma\nUniversity of Tartu Institute of\nComputer Science\nTartu, Estonia\nrajesh.sharma@ut.ee\nABSTRACT\nIn recent years, the proliferation of misinformation on social media\nplatforms has become a significant concern. Initially designed for\nsharing information and fostering social connections, platforms\nlike Twitter (now rebranded as X) have also unfortunately become\nconduits for spreading misinformation. To mitigate this, these plat-\nforms have implemented various mechanisms, including the recent\nsuggestion to use crowd-sourced non-expert fact-checkers to en-\nhance the scalability and efficiency of content vetting. An example\nof this is the introduction of Community Notes on Twitter.\nWhile previous research has extensively explored various as-\npects of Twitter tweets, such as information diffusion, sentiment\nanalytics and opinion summarization, there has been a limited fo-\ncus on the specific feature of Twitter Community Notes, despite its\npotential role in crowd-sourced fact-checking. Prior research on\nTwitter Community Notes has involved empirical analysis of the\nfeature\u2019s dataset and comparative studies that also include other\nmethods like expert fact-checking. Distinguishing itself from prior\nworks, our study covers a multi-faceted analysis of sources and\naudience perception within Community Notes. We find that the\nmajority of cited sources are news outlets that are left-leaning\nand are of high factuality, pointing to a potential bias in the plat-\nform\u2019s community fact-checking. Left biased and low factuality\nsources validate tweets more, while Center sources are used more\noften to refute tweet content. Additionally, source factuality signif-\nicantly influences public agreement and helpfulness of the notes,\nhighlighting the effectiveness of the Community Notes Ranking\nalgorithm. These findings showcase the impact and biases inherent\nin community-based fact-checking initiatives.\nKEYWORDS\nMisinformation, Bias, Community Notes, Source Analysis, Twitter\n1 INTRODUCTION\nMisinformation on social media platforms has become a pressing\nissue which has captured the attention of Governments, policymak-\ners, researchers and the general public [ 37]. Several studies indicate\nthat false information spreads faster than verified facts. This is\nparticularly true for Twitter where information irrespective of its\ntruthfulness can spread across a huge fraction of the population in\na small duration of time [ 12,50]. This rapid dissemination of false\ninformation poses a serious concern as it has tangible and impact-\nful real-world consequences, such as, on public health, elections,\nand national security. For example, one of the most well-known\ninstances that illustrates the severity of the issue in recent times\nwas a series of tweets from former U.S. President Donald Trumpwhere he falsely claimed that the 2020 US presidential elections\nwere faked [ 55]. This misinformation played a significant role in\ninciting a violent attack on the U.S. Capitol on January 6, 2021 [ 7].\nMisinformation detection is the foremost step in mitigation of\nmisinformation. Traditional methods often involve expert fact-\ncheckers or specialized organizations that use their expertise to\nvalidate or debunk claims made on digital platforms [ 29]. How-\never, this approach faces scalability issues as the volume of online\ncontent far exceeds the capacity of such experts to scrutinize it\n[27]. An alternative strategy involves the use of automated mis-\ninformation detectors [ 44], such as, machine learning techniques\n[21] which analyze the underlying linguistic patterns to distinguish\nbetween truthful and misleading content [ 11], [45]. Despite their\nutility, these automated systems are not infallible and frequently\nnecessitate human moderation for optimal performance [ 23] and\nfurther, provide suggestions to combat misinformation [42], [43].\nTo tackle both of these challenges, social media platforms have\nmoved more towards reliance on the wisdom of crowds for fact-\nchecking [ 2]. In 2021, Twitter introduced a community-based fact-\nchecking service originally known as Birdwatch, which was sub-\nsequently rebranded as Community Notes [ 8]. While the service\nwas initially built to add useful context to tweets, it has recently\ntaken a shift towards combating misinformation on the platform\n[18]. Community Notes allows users to flag misleading tweets and\nprovide annotations that include context or corrections. Users have\nthe option to further substantiate their notes by adding links to ex-\nternal sources. Experts on misinformation believe that partisanship\nis the main reason for spreading misinformation [ 3]. Due to this, it\nis crucial for social meia platforms to minimize bias, especially in\nthe context of fact-checking.\nThe aim of this research is to shed light on the community-\ndriven aspect of fact-checking within Twitter\u2019s Community Notes\nfeature. With the increase in prevalence of misinformation, partic-\nularly in politically charged arenas, understanding the dynamics of\ncommunity-based fact-checking is more critical than ever. While a\nfew works have looked at community note user interactions and\nuser consensus [ 35,36], none have yet explored the sources used in\nCommunity Notes (See Figure 1). This research benefits platform\ndevelopers, policymakers, and researchers by providing actionable\ninsights into the sources and potential biases that may skew public\ndiscourse. We systematically evaluate the types of sources cited,\nanalyze their biases, and factuality levels, and probe the impact of\nthese variables on audience perceptions. We summarize the con-\ntributions of the study through the following research questions\n(RQs):arXiv:2406.12444v1  [cs.CY]  18 Jun 2024\nUku Kangur, Roshni Chakraborty, and Rajesh Sharma\nRQ1 - Sources of Validity: How do the citation patterns within Com-\nmunity Notes reflect and influence the biases and factual integrity of\nthe information that shapes public opinion and fact-checking efforts?\nWith this research question, we examine the patterns of web page\ncitations within Community Notes to uncover how information\nsources influence public opinion and fact-checking efforts. Initially,\nwe identify the most frequently cited web pages and investigate\nwhether citation frequencies differ based on the country of origin\nof these pages, aiming to understand geographical biases in source\npopularity. Subsequently, we evaluate the bias and factuality levels\nof these commonly cited web pages to assess their impact on shap-\ning collective viewpoints. This evaluation is crucial for identifying\npotential misinformation and understanding the overall reliability\nof the information disseminated. Furthermore, we explore correla-\ntions between the type of source, its bias, and its factuality, seeking\nto reveal systematic tendencies in the selection and use of sources.\nBy examining these aspects, we aim to highlight areas where criti-\ncal evaluation of sources is needed to enhance the credibility and\nfactual grounding of shared information.\nRQ2 - Perceptions of audience: How do source characteristics such\nas type, bias, and factuality impact their effectiveness in refuting or\nsupporting Community Notes and influence the perceived helpfulness\nand agreement with them? In this research question, we investigate\nthe roles that different categories of sources play in supporting or\nrefuting content shared on social media platforms, specifically Twit-\nter. Furthermore, we examine how these same attributes, source\ntype, bias, and factuality, affect the perceived helpfulness of explana-\ntory notes appended to tweets. This exploration seeks to determine\nwhether notes rated as helpful are also those that are unbiased and\nfact-based. Additionally, we study the impact of source characteris-\ntics on the perceived agreement with the notes, assessing whether\nsource credibility influences audience perceptions. Through this\ncomprehensive analysis, we aim to highlight the complex inter-\nplay between source credibility and the reception of crowd-based\nfact-checking on Twitter.\nThese questions expand our understanding of how community-\nbased fact-checking functions and provide critical insights for plat-\nform developers, policymakers, and researchers aiming to improve\nthe efficacy and fairness of online information verification systems.\nThe rest of the paper is structured as follows: In Section 2, we\npresent an overview of the related works in the domain. In Section\n3, we present the details about the data used for the experiments. In\nSection 4, we introduce the details of the experiments carried out\nand discuss the re- sults obtained. The conclusions and Ethics State-\nment are presented in Sections 5 and 5 respectively. We additionally\nrelease the code1used for the analysis of our results.\n2 RELATED WORK\nThis section covers past works that deal with exploring bias in\nfact-checking and sources of information on social media, which\nare the focus areas of this study.\nFact-checking on social media: Fact-checking on social me-\ndia primarily falls into three categories: expert, automated, and\ncommunity-based [ 9]. These branches have evolved to meet the\n1The anonymized code of the analysis is available here: https://anonymous.4open.\nscience/r/CN-528C/\nFigure 1: Screenshot of a tweet and its associated Community\nNote. The note itself cites sources used to fact-check the\nclaim.\nunique challenges posed by the rapid dissemination of information\non social media platforms[13, 14]. Expert fact-checking platforms,\nsuch as PolitiFact [ 26] and Snopes [ 22] provide a thorough under-\nstanding of news articles and statements as they rely on professional\nfact-checkers. However, expert fact-checking requires extensive\ntime and effort, which makes it difficult to handle the huge volume\nof information disseminated online [ 27]. Recently, automated fact-\nchecking-based approaches have become popular, which leverage\nmachine learning and natural language processing techniques for\ninstant verification [ 21]. Although these methods are highly scal-\nable, they fail to provide justification and interpret the contextual\ninformation, which is essential for reliable and trustworthy fact-\nchecking [ 41], [31]. To overcome these challenges, several existing\nresearch studies have proposed community-based fact-checking\nas a possible alternative. Community-based fact-checking is not\ndependent on a few expert individuals as it leverages the wisdom,\nallowing for several factual interpretations [ 19]. However, while\nit solves the issues of fact-checking speed and explanability, it can\nstill be susceptible to biases and manipulation [39].\nBias in Fact-Checking: Bias can significantly influence human\nperception and the creation of fact-checks, particularly on issues\nthat evoke strong negative opinions [ 34]. Understanding this is cru-\ncial, as these inherent biases affect users\u2019 interpretation of informa-\ntion. Although several works indicate strong bias in fact-checking\nusers for social media platforms, such as Twitter [ 46], this has been\nexplored only on a few expert-based fact-checking platforms, such\nas PolitiFact [ 16]. We indicate that no research has studied and\nWho Checks the Checkers?\nExploring Source Credibility in Twitter\u2019s Community Notes\nanalyzed the bias in Twitter community notes, which is one of the\nreasons this study focuses on the issue.\nMedia source bias and factuality: Research on media sources\nis extensive and diverse, with a focus on various aspects that con-\ntribute to media bias. One avenue has been to investigate quoting\npatterns to discern biases [ 30]. Another line of research has exam-\nined how media bias is evident in the citations to think tanks and\npolicy groups [ 20]. More recently, the influence of bias in head-\nlines has also been studied [ 33]. These traditional media studies\nlay the groundwork for understanding bias, but the landscape is\ncontinuously evolving with the growth of social media platforms.\nOn Twitter, scholars have taken different approaches to analyzing\nmedia bias. One such method has been to align co-subscribers of\nnews sources to deduce potential biases [ 4]. Another approach has\nlooked at the nature of reactions in Twitter comments to different\nnews articles, offering insights into public perception and inherent\nbiases [ 48]. Beyond user behaviour, it\u2019s also crucial to consider the\nrole of algorithms; for instance, studies have explored how Twit-\nter\u2019s algorithm amplifies content with varying degrees of bias [ 24].\nGiven the intricate interplay of user behaviour and algorithmic\ninfluence in shaping and amplifying bias, our study aims to delve\ndeeper into the dynamics of community-based fact-checking.\nTwitter Community Notes: Although there are a plethora of\nresearch works on Twitter datasets covering topics such as infor-\nmation diffusion [ 50], sentiment analysis [ 51], content sources [ 47],\netc., there are very few existing research works on Twitter Commu-\nnity Notes [ 53]. Existing works on Twitter Community Notes in-\ncludes that of Pr\u00f6llochs et al. [ 36], in which they empirically analyze\nthe Twitter Community Notes by examining user interactions, note\ncredibility, sentiment, and the influence of tweet authors on user\nconsensus. Subsequent research has expanded to include compara-\ntive studies that examine Twitter Community Notes with respect\nto fact-checking, such as snoping and expert reviews [ 17,35,39].\nHowever, none of these approaches performs a study of the bias\nand fact-checking of the sources. Therefore, in this paper, we focus\non a multi-faceted exploration of sources and audience perception\nwithin Twitter Community Notes. Unlike prior studies that focused\non the diffusion, consensus, and sentiment of users in reaction to\nCommunity Notes compared to expert fact-checks, our research\naims to understand how source characteristics influence audience\nperceptions, thereby providing a more comprehensive understand-\ning of source credibility\u2019s role in shaping public opinion. We observe\nthat mediabiasfactcheck.com, allsides.com, adfontes.com have been\nhighly effective and reliable in the detection of source bias in both\nTwitter [ 24,40] and Reddit [ 5,52]. Therefore, we utilize these media\nbias ranking platforms, to identify, understand and interpret bias\nin Twitter Community Notes which will inherently provide a more\nnuanced understanding of online crowd-based fact-checking.\n3 DATA\nIn this section we explain the data used for the study. We addition-\nally highlight the key variables we use to refer to certain features\nof our data. We cover data related ethical questions under Section\n5.3.1 Community Notes\nCommunity Notes (previously called Birdwatch) is a community-\ndriven fact-checking feature on Twitter which was launched in\nJanuary 2021 [15]. The fact-checks on Community Notes are called\n\"notes\" in short. To write notes, Twitter users must separately sign\nup for the Community Notes feature. Once verified, Community\nNote users can start reviewing tweets, writing notes and rating\nother Community Note users\u2019 notes. For the Twitter end-user, only\nthe highest-rated Community Note per tweet is displayed (the\nCommunity Note must also have at least 5ratings to be displayed).\nIn addition, Twitter end-users can rate the top displayed note of\na tweet. The minimum rating threshold restricts the spreading of\nbot-generated notes.\nData Collection: We downloaded all of the publicly available\n(royalty-free) Community Note data from the Twitter Community\nNote website2[49] from the period starting from January 23,2021\nand ending with January 27,2024 . The dataset is separated into\nfour subsets. The first subset, Notes , contains information about all\nnotes. The second subset, Ratings , contains information about the\nratings of a note. The third subset, Note status history , contains\nmetadata about notes, including what statuses they received and\nwhen. The final and fourth subset, User status , contains metadata\nabout each user\u2019s enrollment state. For our analysis, we used the\nNotes, Ratings and Note status history sub-datasets. The Notes and\nNote status history datasets comprise of 544995 Community Notes\nfrom 87294 users, and the Ratings dataset comprises of 6514542\nRatings.\n3.2 Key variables\nWe introduce the key variables we extracted using data mining\nfrom the dataset or additionally annotated. Content refers to the\ntextual content of the note. This includes any supporting source\nlinks added by the Community Notes user as well. Source refers\nto the hostname of the URL that the author of the note refers to in\nthe corresponding note as the source. Type refers to the type of\nthe source. Bias refers to the bias rating of the source. Factuality\nrefers to the factuality rating of the source.\n4 ANALYSIS\n4.1 Sources of validity (RQ1)\nIn this subsection, we employ data collection, cleaning, and cate-\ngorization techniques to understand the variety of sources cited in\nCommunity Notes. Our objective is to identify which publications\nand their origins are most commonly utilized and assess their con-\ntribution to the platform\u2019s information veracity through analysing\n\ud835\udc35\ud835\udc56\ud835\udc4e\ud835\udc60 and\ud835\udc39\ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc62\ud835\udc4e\ud835\udc59\ud835\udc56\ud835\udc61\ud835\udc66 .\n4.1.1 Cited sources: We, initially, investigate which sources are\ncited more (compared to others) and analyze the types of these\nsources. For this, we collect all the web links (44523 unique links\nin total) from the Content of the notes and then, perform the\nfollowing pre processing. We initially simplify the URLs to their\nhostnames, such as reducing https://www.example.com/article/123\n2The data is available here: https://communitynotes.twitter.com/guide/en/under-the-\nhood/download-data\nUku Kangur, Roshni Chakraborty, and Rajesh Sharma\nFigure 2: Types of top 50 sources (after combining related domains). The relative frequencies over the whole dataset are plotted\non top of each URL bar.\ntoexample.com , address issues with short links, redirects, auto-\nmatically retrieve the original sources from web archive links by\nusing a script and accept redirect links only when originals were\nunavailable. The complete list of expanded URLs can be seen in\nTable 1.\nCategory URLs\nLink shorteners tinyurl.com, www.shorturl.at,\nbit.ly, is.gd\nSocial media redirects g.co, t.co, yahoo.com, goo.gl,\nyoutu.be, redd.it, fb.me\nWeb archives web.archive.org, archive.ph,\narchive.is, archive.org\nTable 1: List of URLs Expanded\nFurthermore, after the initial preprocessing, we group similar\nURLs that represent the same domain. This grouping includes dif-\nferent device versions of websites, such as en.wikipedia.org and\nen.m.wikipedia.org, and both short and long forms of websites like\nyoutube.com and youtu.be. Additionally, we consolidate different\ncountry versions of websites, exemplified by bbc.com and bbc.co.uk.\nWe also group subpages of the same institutional websites, such as\ntwitter.com and help.twitter.com, recognizing them as originating\nfrom the same core domain. This approach helps our analysis by\nreducing redundancy and focusing on fundamental source identi-\nties. We consider the top 500URL groups on the basis of highest\noccurence frequency in the Community Notes data. These groups\ncomprise of total 4064 URLs and have been used in 306578 notes\n(approximately 56% out of all notes in that period). For the rest of\nthe paper, we refer to these groups as top 500sources.\nWe consulted mediabiasfactcheck.comsupplemented by the sources\n\"About\" sections to categorize the top 500sources into 8Type\ncategories: News ,Fact-Checking ,Dictionary/Encyclopedia ,Govern-\nment/Civil ,Social Media/Platforms ,Research ,Search Engine ,Web\nArchive andOther .Other category comprises of URLs that do not fit\ninto any of the other categories, for example, private business pages,\nportfolios, download links, etc. On evaluation of the distribution\nof source categories, we observe a long-tail pattern, where a small\nnumber of sources are extremely frequent while the majority are\ncited less often as can be seen on the percentage mentioned withrespect to each URL in Figure 2. For example, most of the sources\nare used in less than 1% of Community Notes. We additionally no-\nticed that the category of News has a substantial portion (almost\n50%) of the URLs in this distribution.\nTo understand the sources that dominate each category, we sum-\nmarize the three most frequently cited sources across each Type\ncategory. Our observations indicate that most of categories depend\non a few dominating sources as seen in Table 2, while only a few\ncategories have a high variance in the sources as is the case with\nNews . Wikipedia\u2019s dominance within the Dictionary/Encyclopedia\ncategory, accounting for 87.78% of the citations in this type, high-\nlights its critical role as a primary reference source. Additionally, the\nFact-Checking category shows a substantial concentration among\nthe top three sources (Snopes, Politifact, and FactCheck), contribut-\ning to a significant portion of the category\u2019s citations with 31.72%,\n21.42%, and 12.13% respectively.\nIrrespective of the category segregation, we additionally observe\nin Figure 2 that Twitter is a highly cited page, being used in notes\n60168 times ( 9.3%in the whole dataset). The reason is that users\nprimarily do intra-domain fact-checking, i.e., cite other tweets in\ntheir fact-checking notes. We highlight that some of these cited\ntweets might have cited other sources themselves, but as we do\nnot have access to the tweets cited or the tweets for which the\ncommunity notes were written, we had to exclude Twitter tweets\nfrom our analysis. However, this does not have a significant impact\non our analysis or conclusions since any links hidden in these cited\ntweets most likely follow the same distribution of source types\nthat we got from our annotation stage. We also keep Twitter and\nX ungrouped as sources due to the ongoing discussion regarding\nwhether the platform has changed its political leaning after Elon\nMusk took the company over in 2022 [ 1,10]. The second-most cited\nsource (after Twitter) is Wikipedia, a web encyclopedia, which in\nitself is a community-reviewed platform. We additionally notice that\nGovernment/Civil andResearch sources in the top 50are primarily\nabout health and nature, which might indicate that fact-checking\nthese topics requires more expert knowledge.\nWe also categorized the top 500 sources by country which is\nshown in Figure 3. Most of the sources are from English-speaking\ncountries, with Japan and Brazil being the most frequent from non-\nEnglish countries. This is expected as Community Notes opened\nWho Checks the Checkers?\nExploring Source Credibility in Twitter\u2019s Community Notes\nTable 2: Most Frequent Sources by Type in Top 500 Sources, including their contribution percentages in their corresponding\ntype. Note that there were only two sources for the Web Archive category as most of them were already expanded.\nType Top Sources Percentage of Type\nNews BBC, Reuters, AP 6.29%, 5.34%, 3.81%\nFact-checking Snopes, Politifact, FactCheck 31.72%, 21.42%, 12.13%\nDictionary/Encyclopedia Wikipedia, Britannica, Merriam-Webster 87.78%, 4.11%, 2.36%\nGovernment/Civil MHLW, WHO, Gov.uk 16.96%, 7.02%, 4.92%\nSocial Media/Platforms Twitter, X, YouTube 48.54%, 18.60%, 14.82%\nResearch NIH, CDC, USGS 15.35%, 12.07%, 8.08%\nWeb Archive Wayback Machine, DOI 50.61%, 49.39%\nSearch Engine Google, Justia, Bible Gateway 92.83%, 3.13%, 2.20%\nOther all-senmonka.jp, ne.jp, apple.com 15.39%, 12.77%, 8.69%\nCountry/Region Date\nUS Jan 23 2021\nCanada Dec 15 2022\nUK, Ireland, Australia, New Zealand Jan 20 2023\nBrazil Mar 3 2023\nJapan Mar 21 2023\nMexico, Spain, Portugal Apr 7 2023\nArgentina, Chile, Colombia May 4 2023\nEcuador, Guatemala, Peru, Venezuela\nItaly, Germany, Austria Jun 14 2023\nFrance, Luxembourg, Belgium,\nNetherlands, Switzerland, Slovakia Jul 20 2023\nBulgaria, Croatia, Cyprus Jul 26 2023\nCzechia, Denmark, Estonia, Finland,\nGreece, Hungary, Iceland, Latvia,\nLithuania, Malta, Norway, Poland,\nRomania, Slovenia, Sweden, Indonesia,\nMalaysia, Philippines Nov 16 2023\nSingapore, Thailand, Papua New Guinea,\nBrunei, Algeria, Bahrain, Egypt, Israel Nov 22 2023\nJordan, Kuwait, Lebanon, Morocco,\nOman, Palestinian Territories, Qatar,\nTunisia, United Arab Emirates\nHong Kong, South Korea, Taiwan Dec 7 2023\nTable 3: Community Notes Release Dates by Country\ncontributor access to users from these countries first as is high-\nlighted in Table 3.\n4.1.2 Bias and factuality of sources: To better comprehend the\ninfluence of cited sources on the political leaning and factuality of\nfact-checking on Twitter, we analyze the Bias andFactuality labels\nof these sources. In this section, we explain the annotation process\nofBias andFactuality and give a visual overview of the Bias\nandFactuality distribution of the top sources (including country-\nwise distributions). We first annotate all of the 500sources with\nmetrics for Bias . We do this by aggregating Bias labels for theseMBFC AS AF Bias final\nLeft, Extreme Left Left Strong Left Left\nLeft-Center Lean Left Skews Left Left-Center\nCenter, Pro-Science Center Middle Center\nRight-Center Lean Right Skews Right Right-Center\nRight, Extreme Right Right Strong Right Right\nTable 4: Bias classification across media monitoring plat-\nforms (MBFC: mediabiasfactcheck.com, AS: allsides.com, AF:\nadfontesmedia.com). Each row aligns similar classes to a con-\nsolidated final bias label.\nsources from three media monitoring websites: mediabiasfactcheck.\ncom, allsides.com, adfontes.com. These websites have also been\nwidely used for the same purpose in several existing research works\n[6,38,40,52,54]. The media monitoring platforms state that the\nBias class is given based on the content of the pages, guest lists, and\npolitical leaning on certain topics. We aggregate these Bias classes\nusing majority voting, meaning we took the dominant class over\nall three. If the three classes do not agree and thus no dominant\nclass was found then we removed the source from our dataset. The\nlabels from mediabiasfactcheck.com also had a Pro-Science label for\nBias , which we considered neutral as is expected from scientific\nsources. Additionally, as only mediabiasfactcheck.com have classes\nfor Extreme Left and Extreme Right, we consider those classes\nas Left and Right correspondingly to maintain consistency across\nall the media monitoring websites. In the end we are left with\n5Bias classes - Left,Left-Center ,Center ,Right-Center andRight .\nTheBias labels of each media monitoring page and our finalized\naggregated labels can be seen in Table 4. After Bias label annotation\nour dataset comprises of 183sources which covers 991URLs and\nused in community notes 206466 times.\nWe study the distribution of the Bias labels by domain country\norigin as shown in Figure 4. It\u2019s interesting to note that more po-\nlarized (Left and Right) sources were those from the USA, Great\nBritain, Canada, Australia and India. This is most likely because\nthe media monitoring companies are US-based and thus also more\ncritically evaluate English-speaking sources. Additionally, we high-\nlight that USA-based sources are most frequent, covering 640 URLs\nafter Bias annotation.\nUku Kangur, Roshni Chakraborty, and Rajesh Sharma\nFigure 3: Source Type Relative URL Count by Country. The total URL count is marked at the end of each bar. INT refers to\nglobal international sources (or sources that did not fit under any single country) and EU refers to European Union sources.\nFigure 4: Source Bias Relative URL Count by Country. The total URL count is marked at the end of each bar.\nWho Checks the Checkers?\nExploring Source Credibility in Twitter\u2019s Community Notes\nFigure 5: Source Factuality Relative URL Count by Country. The total URL count is marked at the end of each bar.\nType Bias Factuality\nType Count Percentage Bias Count Percentage Factuality Count Percentage\nNews 543 54 .8% Left-Center 448 45 .3% High Factuality 399 40 .3%\nResearch 177 17 .9% Center 422 42 .6% Mixed Factuality 227 22 .9%\nSocial Media/Platforms 90 9 .1% Right-Center 79 8 .0% Mostly Factual 211 21 .3%\nDictionary/Encyclopedia 88 8 .9% Left 25 2 .5% Very High Factuality 150 15 .2%\nSearch Engine 67 6 .7% Right 16 1 .6% Satire 1 0 .1%\nFact-checking 12 1 .2% Low Factuality 1 0 .1%\nGovernment/Civil 12 1 .2%\nOther 1 0 .1%\nTable 5: URL counts by Type, Bias, and Factuality\nWe annotate our sources additionally with Factuality labels.\nHowever, as, mediabiasfactcheck.com, allsides.com and adfontes.\ncom sources do not use the same features to identify Factual-\nity, we could not aggregate these three sources and consider only\nmediabiasfactcheck.com. We chose mediabiasfactcheck.com out of\nthe three as it provided maximum coverage for the sources. For\nmediabiasfactcheck.com, the Factuality label is based on the fre-\nquency of fact-checks they have passed during the last five years.\nThere are 6Factuality classes - Very High Factuality ,High Factu-\nality,Mostly Factual ,Mixed Factuality ,Low Factuality andSatire .\nHowever, mediabiasfactcheck.com does not provide a Factuality\nrating for all sources. Therefore, to maintain consistency in our\ndataset, we exclude those data points for which we do not have any\nFactuality label. After adding Factuality labels, our final dataset\ncomprises of 182sources, covering 990URLs and used in 206007\ncommunity notes. We additionally analyse the Factuality class\nacross country origin of the sources as can be seen in Figure 5. We\ncan see that Low Factuality sources are entirely from Great Britain,\nwhich also has the largest proportion of Very High Factuality . This\ntrend of having varied sources in terms of Factuality is also seen\nin other English-speaking countries such as the USA and Australia.The sources after adding Type ,Bias andFactuality labels is\nconsidered our final sources dataset, which we use to analyse the\nsources used in Community Notes. Every source has one Type ,\nBias andFactuality annotated category and we show their URL\ncount distribution between categories in Table 5. We analyze the\ninterrelationship between categories to understand the patterns of\ninformation framing and its impact on public perception. For exam-\nple, our observations as shown in Figure 6 show the connectedness\nofType ,Bias andFactuality categories. We highlight, that the\nmajority of News outlets ( 50.5%) lean towards a Left-Center bias,\nand of those, a substantial amount ( 79.6%) are highly factual.\n4.1.3 Correlation analysis: To get a better understanding of how\ndifferent categories intersect and interact with each other, we anal-\nyse the correlation of the Type ,Bias , and Factuality categories.\nWe mark the Pearson correlation coefficients as \ud835\udc5fand consider\nscores with \ud835\udc5f<0.3weak, 0.3<\ud835\udc5f<0.7moderate and \ud835\udc5f>0.7\nstrong. We exclude scores where \ud835\udc5f<0.3(weak correlation) from\nour analysis and display our results in Table 6.\nUku Kangur, Roshni Chakraborty, and Rajesh Sharma\nFigure 6: Connectedness of source categories of Type, Bias and Factuality. The nodes are weighed by URL counts.\nNews Research Center Right\nCenter \u22120.38 0 .37 - -\nVery High Factuality \u22120.5 0 .5 0 .46 -\nMixed Factuality - - - 0.31\nLow Factuality - - - 0.35\nTable 6: Correlation of Type, Bias and Factuality categories.\nShown are scores, with absolute values of r > 0.3.\nOur results reveal a few moderate correlations ( \ud835\udc5f>0.3) that\nhold critical implications. Firstly, Research sources are often cate-\ngorized as both Center (\ud835\udc5f: 0.37) and having Very High Factuality\n(\ud835\udc5f: 0.5). This suggests that such sources are both reliable and seen\nas advocating scientific perspectives in an unbiased way. Secondly,\nNews sources are generally less likely to be categorized as Center\n(\ud835\udc5f:\u22120.38) or possess Very High Factuality (\ud835\udc5f:\u22120.5), which implies\na potential limitation in these commonly-accessed information out-\nlets. Thirdly, Center sources tend to also score very high in factuality\n(\ud835\udc5f: 0.46), reinforcing the credibility of unbiased perspectives. Lastly,\nRight biased sources frequently exhibit Low Factuality (\ud835\udc5f: 0.35) and\nMixed Factuality (\ud835\udc5f: 0.31), raising questions about the credibility\nof such sources and their role in public discourse.\n4.1.4 Summary of insights for RQ1: We emphasize key points about\nthe sources cited in notes. First, Twitter and Wikipedia are the most\ncited, suggesting that intra-domain fact-checking and community-\nreviewed content are significant in public discourse. This shows that\nsocial media platforms are not just arenas for discussion but also\ncrucial sources of information and fact-checking. The fact-checking\nsource Type category showcases a significant reliance on a fewkey websites, with Snopes, Politifact, and FactCheck which forms\na substantial portion (65.27%) of citations. The Government/Civil\ncategory, with top sources being MHLW, WHO, and Gov.uk, reflects\nthe diversity and international representation of credible govern-\nment and civil sources utilized for fact-checking. Secondly, the\nBias andFactuality show most sources fall within Left-Center\n(54.8%) and High Factuality (40.3%), indicating a factual left-leaning\nfact-checking community. Thirdly, sources from English-speaking\ncountries tend to be more frequently used and also more polarized\nin regards to bias and factuality. Lastly, News sources correlate with\nnot being Center ,Center andResearch sources correlate positively\nwith having Very High Factuality andRight biased sources with\nLow Factuality andMixed Factuality .\n4.2 Perceptions of audience (RQ2)\nIn this section we investigate how the Factuality and political\nBias of cited sources influence the ratings and acceptance of a\ncommunity note. Specifically, we look at which sources are used\nto support and refute notes, what is their helpfulness and how\nagreement levels differ in source usage. We also highlight how well\nthe Community Note rating algorithm handles poor quality and\nbiased content.\nAs one community note can have several sources associated we\nneed a way to aggregate Bias andFactuality labels of sources\nused. For this we calculate Bias andFactuality scores for each\ncommunity note. The associated scores that correspond to each\nlevel can be seen in Table 7. We consider Satire equal to Very Low\nFactuality score-wise as for fact-checking hidden humour can be\nmisleading and more harmful than useful. To deal with multiple\nsources in community notes we disregard notes that have sources\nWho Checks the Checkers?\nExploring Source Credibility in Twitter\u2019s Community Notes\nFigure 7: Distribution of community notes by Bias and Factuality, categorized as either supporting or refuting Tweets. Amount\nof notes representing each bar is added on top. For the second plot we have also added z-score 99% confidence intervals.\nFactuality to Score Bias to Score\nFactuality Level Score Bias Category Score\nVery High Factuality 5 Left 2\nHigh Factuality 4 Left-Center 1\nMostly Factual 3 Center 0\nMixed Factuality 2 Right-Center -1\nLow Factuality 1 Right -2\nVery Low Factuality 0\nSatire 0\nTable 7: Factuality and Bias levels converted to scores.\nfrom opposite Bias sides and average the Bias scores otherwise.\nForFactuality , we average the Factuality scores.\nWe use a simplified 3 class system for both Factuality andBias .\nThe score to label transformation system is highlighted in Table 8.\nWe highlight that the largest Factuality class is Medium (61.4% of\nnotes) and the largest Bias class is Center (51.67% of notes).\n4.2.1 Role of sources in supporting or refuting the content of tweets:\nCommunity notes can be used to support both the truthfulness of\na tweet (marking the original tweet as not misleading) or refute it\n(marking the original tweet as misleading). This label is given by\nthe community note writer. We aim to understand which categories\nof sources are used more for supporting and which ones are for\nrefuting tweets. We can see the Bias andFactuality distributions\nof the refuting/supporting notes on Figure 7.Category Score Range Count Percentage\nRight <\u22120.5 10,227 6.60%\nCenter\u22120.5to0.5 80,101 51.67%\nLeft >0.5 64,688 41.73%\nLow <3 44,497 28.70%\nMedium 3to4 95,179 61.40%\nHigh >4 15,340 9.90%\nTable 8: Distribution of Bias and Factuality scores with labels,\ncounts, and percentages.\nOnBias , we can see that notes with Right-wing sources are used\nrelatively more when supporting tweets compared to notes that\nuse Center or Left-wing sources ( \ud835\udc5d<0.01). This could indicate a\nbroader tendency within the conservative media space to create\nself-reinforcing loops of information [25].\nOur observations on Factuality indicate that sources with lower\nfactuality are used relatively more to support tweets than those\nwith high factuality ( \ud835\udc5d<0.01). This can be an indication of misin-\nformation enforcement, where a community note has been written\nto a misleading note to make it seem credible.\n4.2.2 Role of sources in community note helpfulness: Community\nNotes undergo a contributor-driven rating process to determine\ntheir status as \"helpful\", not helpful\", or \"needs more ratings\", af-\nfecting their visibility on site timelines and posts [ 32]. Initially, all\nnotes start in a \"Needs More Ratings\" state until receiving at least\nfive ratings, at which point they may be classified as helpful or not\nUku Kangur, Roshni Chakraborty, and Rajesh Sharma\nFigure 8: Distribution of community notes by Bias and Factuality, categorized as either helpful, not helpful or needs more\nratings. Amount of notes representing each bar is added on top. For the second and third plot we have also added z-score 99%\nconfidence intervals.\nhelpful. Notes identified as fact-checking potentially misleading\ntweets that meet specific helpfulness score criteria are marked as\nhelpful and displayed on posts, whereas those not meeting the crite-\nria are deemed not helpful. The process includes a diligence scoring\nmechanism to evaluate the accuracy and sourcing of information,\nensuring that notes recognized as reliable and clear by a broad spec-\ntrum of users are highlighted. We can see the Bias andFactuality\ndistributions of the helpful and not helpful notes in Figure 8.\nWe aim to look at the Bias andFactuality of sources used in\nhelpful and not helpful notes. When it comes to Bias , we notice that\nLeft and Right sources are associated relatively more frequently\nwith helpful than not helpful notes compared to Center sources\n(\ud835\udc5d<0.01). This might be due to people having a confirmation bias\nwhen looking at content confirming their views.\nHowever, regarding Factuality we notice a trend where notes\nthat hold low factuality sources are generally rated less helpful\nthan those with medium or high factuality sources ( \ud835\udc5d<0.01). This\nconfirms that notes with lower quality sources are effectively clas-\nsified by the Community Notes Ranking algorithm. The Factuality\ndistribution of the helpful and not helpful notes is shown in Figure\n8.\n4.2.3 Perceived agreement by source categories: We analyse the\nperceived agreement levels of the Community notes per source\ncategory. For this, we use the existing ratings associated with eachnote and created an agreement index using the number of ratings\nthat agreed and disagreed with the note:\nAgreement =Agree\nAgree+Disagree(1)\nWe indicate a threshold of 0.5, where scores greater than 0.5\nindicate that the notes are more agreeable than disagreeable, while\nscores lower than 0.5suggest the opposite. We plot the distribution\nspread of aggregated agreement for our categories of Bias and\nFactuality in Figure 9. We also notice that the average agreement\nper our community notes is 0.87.\nWe explore whether the bias of the cited sources in notes corre-\nlates with the level of agreement those notes receive. We assume\nthat notes citing more politically neutral sources will attract broader\nagreement. This is based on the assumption that neutral sources\nmay be less likely to polarize opinion compared to clearly left- or\nright-leaning sources [ 28]. On analyzing the Bias category results,\nour initial observations indicate that notes that cite Right sources\nhave significantly lower agreement levels, with the lower quartile\nshowing an agreement score under 0.5. However, notes that cite\nLeft sources show similarly high levels of agreement to the notes\nthat use Center sources.These findings highlight that notes which\ncite more politically center or left leaning sources are generally\nmore agreeable than those relying on Right sources.\nFactuality plays a large role in the agreeableness of statements;\nthus, we expect that sources with higher Factuality ratings are\nWho Checks the Checkers?\nExploring Source Credibility in Twitter\u2019s Community Notes\nFigure 9: Distribution spread of aggregated agreement by factuality and bias categories. The black dotted line indicates average\nagreement.\nalso generally more agreeable. When we inspect the Factuality\ncategory agreement levels, we see that higher Factuality sources,\nalso have generally higher aggreement levels and vice versa for\nlower Factuality sources. Notes that cite sources of Low factuality\nsee the lowest agreement levels, with their lower quartiles being\nbelow the 0.5 level. This indicates that community note users pay\nattention to the factuality of the source when rating the notes.\n4.2.4 Summary of insights for RQ2: We summarize the main take-\naways of this analysis next. Our results shows a significant pref-\nerence for right-leaning sources to support the content of tweets\nwhich hints at a tendency among conservative media outlets to\nform self-reinforcing information cycles. Similarly, we found that\ncommunity notes that support tweets often rely on sources with\nlow factuality scores, therefore highlights a systemic issue with mis-\ninformation. Further, our study indicates that notes associated with\neither left or right sources tend to be considered helpful, whereas\nthose that cite sources of lower factuality are not, demonstrating\nthe Community Note rating algorithm\u2019s ability to effectively fil-\nter content quality. Finally, we note that community notes citing\nmore neutral or factually sound sources receive higher agreement\nlevels, emphasizing the importance of source quality in achieving\ncommunity agreement.\n5 CONCLUSIONS\nOur investigation into Twitter\u2019s Community Notes has uncovered\ndistinct patterns in the use of sources for community-led fact-\nchecking, showing clear trends and biases. We\u2019ve discovered that\nTwitter and Wikipedia are often the go-to sources, highlighting\na preference for checking facts within the platform and relying\non community-reviewed information for public discussions. Our\nfindings reveal that sources are mainly left-center in bias and high\nin factuality, suggesting a left-leaning trend in the fact-checking\ncommunity. Moreover, sources from English-speaking countries are\nused more frequently, indicating a bias towards these regions and a\nmore pronounced polarization in terms of political bias and factual\naccuracy. This polarization is especially evident in the types of\nsources cited, with news sources often showing clear bias, whereas\nacademic and research sources are typically linked to very highfactual content. In contrast, right-biased sources are often asso-\nciated with lower levels of factuality. Adding to this, our results\nindicate a noticeable preference for right-leaning sources to sup-\nport tweets, which may suggest that right-wing users are creating\necho chambers on the platform. We also observed that notes en-\ndorsing tweets often depend on less factual sources, pointing to\na broader issue with misinformation. Interestingly, our analysis\nshows that notes linked to both left and right biases are usually\nseen as helpful, except when they reference lower-quality sources.\nThe low agreement of low-quality sources justifies the usage of\nratings of notes used by the Community Note rating algorithm in\nsifting through content quality. Additionally, notes that cite more\nbalanced or factually accurate sources tend to receive higher levels\nof agreement, underscoring the critical role of source quality in\nfostering community consensus.\nETHICS STATEMENT\nWe emphasize that we rely entirely on publicly available and anony-\nmous data shared on Twitter Community Notes; hence, we do not\nand are unable to obtain consent from users who wrote notes to\ntweets. We add that this research did not involve any human sub-\njects or crowd-workers, and all annotations were done by the au-\nthors of the work itself using reliable materials. We discuss the data\nhandling in depth to mitigate any misuse of our results.\nACKNOWLEDGMENTS\nThis work has received funding from the EU H2020 program under\nthe SoBigData++ project (grant agreement No. 871042), by the\nCHIST-ERA grant No. CHIST-ERA-19-XAI-010, (ETAg grant No.\nSLTAT21096), and partially funded by HAMISON project.\nREFERENCES\n[1]2023. Has Twitter (now X) become more right-wing? Our analysis\nof the platform\u2019s political centre of gravity. The Economist (20 12\n2023). https://www.economist.com/graphic-detail/2023/12/20/has-twitter-now-\nx-become-more-right-wing Accessed: 2024-04-08.\n[2]Jennifer Allen, Antonio A. Arechar, Gordon Pennycook, and David G. Rand.\n2021. Scaling up fact-checking using the wisdom of crowds. Science\nAdvances 7, 36 (2021), eabf4393. https://doi.org/10.1126/sciadv.abf4393\narXiv:https://www.science.org/doi/pdf/10.1126/sciadv.abf4393\nUku Kangur, Roshni Chakraborty, and Rajesh Sharma\n[3] Sacha Altay, Manon Berriche, Hendrik Heuer, Johan Farkas, and Steven Rathje.\n2023. A survey of expert views on misinformation: Definitions, determi-\nnants, solutions, and future of the field. Misinformation Review (July 2023).\nhttps://misinforeview.hks.harvard.edu/article/a-survey-of-expert-views-on-\nmisinformation-definitions-determinants-solutions-and-future-of-the-field/\nPeer Reviewed.\n[4]Jisun An, Meeyoung Cha, Krishna P. Gummadi, Jon Crowcroft, and Daniele\nQuercia. 2021. Visualizing Media Bias through Twitter. Proceedings of the\nInternational AAAI Conference on Web and Social Media 6 (08 2021). https:\n//doi.org/10.1609/icwsm.v6i2.14343\n[5] Yigit Ege Bayiz, Arash Amini, Radu Marculescu, and Ufuk Topcu. 2024. Suscepti-\nbility of Communities against Low-Credibility Content in Social News Websites.\narXiv:2403.10705 [cs.SI]\n[6] Yigit Ege Bayiz, Arash Amini, Radu Marculescu, and Ufuk Topcu. 2024. Suscepti-\nbility of Communities against Low-Credibility Content in Social News Websites.\narXiv:2403.10705 [cs.SI]\n[7]BBC. 2021. Capitol riots timeline: What happened on 6 January 2021? https:\n//www.bbc.com/news/world-us-canada-56004916 Published 2 August 2021.\n[8]Bethany Biron. 2022. Elon Musk said Twitter\u2019s Birdwatch feature will be re-\nnamed \u2019Community Notes\u2019 and is aimed at \u2019improving information accuracy\u2019 amid\ngrowing content-moderation concerns . https://www.businessinsider.com/musk-\nrenames-birdwatch-community-notes-touts-improving-accuracy-2022-11 Ac-\ncessed: 2023-09-12.\n[9] Lia Bozarth. 2022. Pitfalls in Popular Misinformation Detection Methods and\nHow to Avoid Them . PhD Thesis. University of Michigan, Horace H. Rackham\nSchool of Graduate Studies. https://doi.org/10.7302/6199\n[10] Philip Bump. 2023. Elon Musk provides yet another platform for far-right attacks.\nThe Washington Post (21 11 2023). https://www.washingtonpost.com/politics/\n2023/11/21/musk-media-matters-texas/ Accessed: 2024-04-08.\n[11] Sabur Butt, Shakshi Sharma, Rajesh Sharma, Grigori Sidorov, and Alexander\nGelbukh. 2022. What goes on inside rumour and non-rumour tweets and their\nreactions: A psycholinguistic analyses. Computers in Human Behavior 135 (2022),\n107345.\n[12] Roshni Chakraborty, Maitry Bhavsar, Sourav Dandapat, and Joydeep Chandra.\n2017. A network based stratification approach for summarizing relevant comment\ntweets of news articles. In International Conference on Web Information Systems\nEngineering . Springer, New York, NY, USA, 33\u201348.\n[13] Roshni Chakraborty, Maitry Bhavsar, Sourav Kumar Dandapat, and Joydeep\nChandra. 2019. Tweet summarization of news articles: An objective ordering-\nbased perspective. IEEE Transactions on Computational Social Systems 6, 4 (2019),\n761\u2013777.\n[14] Roshni Chakraborty and Nilotpal Chakraborty. 2023. TwMiner: Mining Relevant\nTweets of News Articles. In 2023 IEEE/ACM 23rd International Symposium on\nCluster, Cloud and Internet Computing Workshops (CCGridW) . IEEE, 1\u20133.\n[15] Community Notes. 2024. Introduction . https://communitynotes.x.com/guide/en/\nabout/introduction\n[16] Tim Draws, David La Barbera, Michael Soprano, Kevin Roitero, Davide Ceolin,\nAlessandro Checco, and Stefano Mizzaro. 2022. The Effects of Crowd Worker\nBiases in Fact-Checking Tasks. In Proceedings of the 2022 ACM Conference on\nFairness, Accountability, and Transparency (Seoul, Republic of Korea) (FAccT\n\u201922). Association for Computing Machinery, New York, NY, USA, 2114\u20132124.\nhttps://doi.org/10.1145/3531146.3534629\n[17] Chiara Drolsbach and Nicolas Pr\u00f6llochs. 2023. Diffusion of Community Fact-\nChecked Misinformation on Twitter. arXiv:2205.13673 [cs.SI]\n[18] Kate Duffy. 2022. Jack Dorsey criticizes Elon Musk\u2019s decision to rebrand the\nBirdwatch feature to Community Notes, saying it\u2019s the \u2019most boring Facebook\nname ever\u2019 . https://www.businessinsider.com/jack-dorsey-criticize-elon-musk-\nrename-birdwatch-community-notes-boring-2022-11\n[19] William Godel, Zeve Sanderson, Kevin Aslett, Jonathan Nagler, Richard Bon-\nneau, Nathaniel Persily, and Joshua A. Tucker. 2021. Moderating with the Mob:\nEvaluating the Efficacy of Real-Time Crowdsourced Fact-Checking. Journal of\nOnline Trust and Safety 1, 1 (Oct. 2021). https://doi.org/10.54501/jots.v1i1.15\n[20] Tim Groseclose and Jeffrey Milyo. 2005. A Measure of Media Bias*. The\nQuarterly Journal of Economics 120, 4 (11 2005), 1191\u20131237. https://doi.\norg/10.1162/003355305775097542 arXiv:https://academic.oup.com/qje/article-\npdf/120/4/1191/5431230/120-4-1191.pdf\n[21] Zhijiang Guo, Michael Schlichtkrull, and Andreas Vlachos. 2022. A Survey on\nAutomated Fact-Checking. Transactions of the Association for Computational\nLinguistics 10 (2022), 178\u2013206. https://doi.org/10.1162/tacl_a_00454\n[22] Aniko Hannak, Drew Margolin, Brian Keegan, and Ingmar Weber. 2014. Get\nBack! You Don\u2019t Know Me Like That: The Social Mediation of Fact Checking\nInterventions in Twitter Conversations. Proceedings of the International AAAI\nConference on Web and Social Media 8, 1 (May 2014), 187\u2013196. https://doi.org/10.\n1609/icwsm.v8i1.14555\n[23] Benjamin D. Horne. 2023. Is Automated Content Moderation Going to Solve\nOur Misinformation Problems? Information Matters 3, 1 (January 6 2023). https:\n//doi.org/10.2139/ssrn.4359981[24] Ferenc Husz\u00e1r, Sofia Ira Ktena, Conor O\u2019Brien, Luca Belli, Andrew\nSchlaikjer, and Moritz Hardt. 2022. Algorithmic amplification of pol-\nitics on Twitter. Proceedings of the National Academy of Sciences\n119, 1 (2022), e2025334119. https://doi.org/10.1073/pnas.2025334119\narXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.2025334119\n[25] Myiah J. Hutchens Jay D. Hmielowski and Michael A. Beam. 2020. Asym-\nmetry of Partisan Media Effects?: Examining the Reinforcing Process of Con-\nservative and Liberal Media with Political Beliefs. Political Communica-\ntion 37, 6 (2020), 852\u2013868. https://doi.org/10.1080/10584609.2020.1763525\narXiv:https://doi.org/10.1080/10584609.2020.1763525\n[26] Jiexun Li and Xiaohui Chang. 2023. Combating Misinformation by Sharing the\nTruth: a Study on the Spread of Fact-Checks on Social Media. Information Systems\nFrontiers 25, 4 (1 8 2023), 1479\u20131493. https://doi.org/10.1007/s10796-022-10296-z\n[27] Casandra L\u00f3pez-Marcos and Pilar Vicente-Fern\u00e1ndez. 2021. Fact Checkers Facing\nFake News and Disinformation in the Digital Age: A Comparative Analysis\nbetween Spain and United Kingdom. Publications 9, 3 (2021). https://doi.org/10.\n3390/publications9030036\n[28] Amy Mitchell, Jeffrey Gottfried, Jocelyn Kiley, and Katerina Eva Matsa. 2014. Po-\nlitical Polarization & Media Habits. (21 October 2014). https://www.pewresearch.\norg/politics/2014/06/12/section-1-growing-ideological-consistency/\n[29] Victoria Moreno-Gil, Xavier Ramon-Vegas, and Marcel Mauri-R\u00edos. 2022. Bring-\ning journalism back to its roots: examining fact-checking practices, methods,\nand challenges in the Mediterranean context. Profesional de la informaci\u00f3n 31, 2\n(2022), e310215. https://doi.org/10.3145/epi.2022.mar.15\n[30] Vlad Niculae, Caroline Suen, Justine Zhang, Cristian Danescu-Niculescu-Mizil,\nand Jure Leskovec. 2015. QUOTUS: The Structure of Political Media Coverage\nas Revealed by Quoting Patterns. arXiv:1504.01383 [cs.CL]\n[31] Gustav Nikopensius, Mohit Mayank, Orchid Chetia Phukan, and Rajesh Sharma.\n2023. Reinforcement Learning-based Knowledge Graph Reasoning for Explain-\nable Fact-checking. In IEEE/ACM International Conference on Advances in Social\nNetworks Analysis and Mining .\n[32] Community Notes. 2024. Note ranking algorithm. https://communitynotes.\ntwitter.com/guide/en/under-the-hood/ranking-notes#helpful-rating-mapping\nAccessed: 2024-04-01.\n[33] Jinsheng Pan, Weihong Qi, Zichen Wang, Hanjia Lyu, and Jiebo Luo. 2023. Bias\nor Diversity? Unraveling Fine-Grained Thematic Discrepancy in U.S. News\nHeadlines. arXiv:2303.15708 [cs.CL]\n[34] S. Park, J.Y. Park, J. Kang, and M. Cha. 2021. The presence of unexpected biases\nin online fact-checking. The Harvard Kennedy School Misinformation Review 2, 1\n(2021). https://doi.org/10.37016/mr-2020-53\n[35] Moritz Pilarski, Kirill Solovev, and Nicolas Pr\u00f6llochs. 2023. Community Notes\nvs. Snoping: How the Crowd Selects Fact-Checking Targets on Social Media.\narXiv:2305.09519 [cs.SI]\n[36] Nicolas Pr\u00f6llochs. 2022. Community-Based Fact-Checking on Twitter\u2019s Bird-\nwatch Platform. Proceedings of the International AAAI Conference on Web and So-\ncial Media 16, 1 (May 2022), 794\u2013805. https://doi.org/10.1609/icwsm.v16i1.19335\n[37] Lee Rainie and Janna Anderson. 2017. The Fate of Online Trust in the Next\nDecade. Pew Research Center (August 10 2017). https://www.pewresearch.org/\ninternet/2017/08/10/the-fate-of-online-trust-in-the-next-decade/\n[38] A Rao, F Morstatter, M Hu, E Chen, K Burghardt, E Ferrara, and K Lerman. 2021.\nPolitical Partisanship and Antiscience Attitudes in Online Discussions About\nCOVID-19: Twitter Content Analysis. J Med Internet Res 23, 6 (Jun 14 2021),\ne26692. https://doi.org/10.2196/26692\n[39] Mohammed Saeed, Nicolas Traub, Maelle Nicolas, Gianluca Demartini, and Paolo\nPapotti. 2022. Crowdsourced Fact-Checking at Twitter. In Proceedings of the 31st\nACM . ACM. https://doi.org/10.1145/3511808.3557279\n[40] Mattia Samory, Vartan Kesiz Abnousi, and Tanushree Mitra. 2020. Characterizing\nthe Social Media News Sphere through User Co-Sharing Practices. Proceedings\nof the International AAAI Conference on Web and Social Media 14, 1 (May 2020),\n602\u2013613. https://doi.org/10.1609/icwsm.v14i1.7327\n[41] F\u00e1tima C. Carrilho Santos. 2023. Artificial Intelligence in Automated Detection of\nDisinformation: A Thematic Analysis. Journalism and Media 4, 2 (2023), 679\u2013687.\nhttps://doi.org/10.3390/journalmedia4020043\n[42] Shakshi Sharma, Anwitaman Datta, Vigneshwaran Shankaran, and Rajesh\nSharma. 2023. Misinformation Concierge: A Proof-of-Concept with Curated\nTwitter Dataset on COVID-19 Vaccination. In CIKM .\n[43] Shakshi Sharma, Anwitaman Datta, and Rajesh Sharma. 2023. AMIR: Automated\nMisInformation Rebuttal\u2013A COVID-19 Vaccination Datasets based Recommen-\ndation System. arXiv preprint arXiv:2310.19834 (2023).\n[44] Shakshi Sharma and Rajesh Sharma. 2021. Identifying possible rumor spreaders\non twitter: A weak supervised learning approach. In 2021 International Joint\nConference on Neural Networks (IJCNN) . IEEE, 1\u20138.\n[45] Shakshi Sharma, Rajesh Sharma, and Anwitaman Datta. 2022. (Mis) leading the\nCOVID-19 Vaccination Discourse on Twitter: An Exploratory Study of Infodemic\nAround the Pandemic. IEEE Transactions on Computational Social Systems (2022).\n[46] Jieun Shin and Kjerstin Thorson. 2017. Partisan Selective Sharing: The Biased\nDiffusion of Fact-Checking Messages on Social Media: Sharing Fact-Checking\nMessages on Social Media. Journal of Communication 67 (02 2017). https:\nWho Checks the Checkers?\nExploring Source Credibility in Twitter\u2019s Community Notes\n//doi.org/10.1111/jcom.12284\n[47] L Singh, L Bode, C Budak, K Kawintiranon, C Padden, and E Vraga. 2020. Un-\nderstanding high- and low-quality URL Sharing on COVID-19 Twitter streams.\nJournal of Computational Social Science 3, 2 (2020), 343\u2013366. https://doi.org/10.\n1007/s42001-020-00093-6 Epub 2020 Nov 27.\n[48] Timo Spinde, Elisabeth Richter, Martin Wessel, Juhi Kulshrestha, and Karsten\nDonnay. 2023. What do Twitter comments tell about news article bias? Assessing\nthe impact of news article bias on its perception on Twitter. Online Social Networks\nand Media 37-38 (2023), 100264. https://doi.org/10.1016/j.osnem.2023.100264\n[49] Twitter. 2023. Community Notes. https://communitynotes.twitter.com/guide/\nen/under-the-hood/download-data Accessed: July 6, 2023.\n[50] Soroush Vosoughi, Deb Roy, and Sinan Aral. 2018. The spread of true and false\nnews online. Science 359, 6380 (2018), 1146\u20131151. https://doi.org/10.1126/science.\naap9559 arXiv:https://www.science.org/doi/pdf/10.1126/science.aap9559\n[51] Yili Wang, Jiaxuan Guo, Chengsheng Yuan, and Baozhu Li. 2022. Sentiment\nAnalysis of Twitter Data. Applied Sciences 12, 22 (2022). https://doi.org/10.3390/\napp122211775[52] Galen Cassebeer Weld, Maria Glenski, and Tim Althoff. 2021. Political Bias\nand Factualness in News Sharing Across more then 100, 000 Online Communi-\nties. ArXiv abs/2102.08537 (2021). https://api.semanticscholar.org/CorpusID:\n231942492\n[53] Stefan Wojcik, Sophie Hilgard, Nick Judd, Delia Mocanu, Stephen Ragain,\nM. B. Fallin Hunzaker, Keith Coleman, and Jay Baxter. 2022. Birdwatch: Crowd\nWisdom and Bridging Algorithms can Inform Understanding and Reduce the\nSpread of Misinformation. arXiv:2210.15723 [cs.SI]\n[54] Zhiping Xiao, Jeffrey Zhu, Yining Wang, Pei Zhou, Wen Hong Lam, Mason A.\nPorter, and Yizhou Sun. 2023. Detecting political biases of named entities and\nhashtags on Twitter. EPJ Data Science 12, 1 (2023), 20. https://doi.org/10.1140/\nepjds/s13688-023-00386-6\n[55] Hope Yen, Ali Swenson, and Amanda Seitz. 2020. AP FACT CHECK:\nTrump\u2019s claims of vote rigging are all wrong. Associated Press News (3\nDec 2020). https://apnews.com/article/election-2020-ap-fact-check-joe-biden-\ndonald-trump-technology-49a24edd6d10888dbad61689c24b05a5", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Who Checks the Checkers? Exploring Source Credibility in Twitter's Community Notes", "author": ["U Kangur", "R Chakraborty", "R Sharma"], "pub_year": "2024", "venue": "arXiv preprint arXiv:2406.12444", "abstract": "In recent years, the proliferation of misinformation on social media platforms has become a  significant concern. Initially designed for sharing information and fostering social connections"}, "filled": false, "gsrank": 8, "pub_url": "https://arxiv.org/abs/2406.12444", "author_id": ["mUt1P2gAAAAJ", "9yWBPqoAAAAJ", "GKegbo0AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:DayBpC33u-sJ:scholar.google.com/&output=cite&scirp=7&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=DayBpC33u-sJ&ei=BLWsaNaSEPnSieoPxKLpgQ0&json=", "num_citations": 7, "citedby_url": "/scholar?cites=16986442194963508237&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:DayBpC33u-sJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2406.12444"}}, {"title": "Entity Based Bias in News Articles", "year": "NA", "pdf_data": "Leipzig University\nInstitute of Computer Science\nDegree Programme Computer Science, B.Sc.\nEntity Based Bias in News Articles\nBachelor\u2019s Thesis\nLukas Felix G\u00f6hlich\n1. Referee: Prof. Dr. Martin Potthast\nSubmission date: January 31, 2022\nDeclaration\nUnless otherwise indicated in the text or references, this thesis is\nentirely the product of my own scholarly work.\nLeipzig, January 31, 2022\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nLukas Felix G\u00f6hlich\nAcknowledgements\nI would like to express my special thanks of gratitude to my supervisors Prof.\nKhalid Al Khatib and Shahbaz Syed, as well as my referee Prof. Dr. Martin\nPotthast. Computations for this work were done (in part) using resources of\nthe Leipzig University Computing Centre.\ni\nAbstract\nThe internet allows people to collect a wide range of information on every-\nday events. Still, the critical judgment of such information lays upon people\u2019s\nresponsibility. An automatic tool to classify a document\u2019s ideology (and pin-\npoint the hyperpartisan content) could protect readers from unconscious one-\nsided news consumption and aid authors to produce unbias news coverage.\nIn this thesis, we investigate how political bias manifests in a certain part of\na news article, i.e., the text given to introduce people . To do so, we leveraged a\nlinguistic pattern of interpolated text we call a \u2018descriptive statement\u2019, study-\ning to what extent such statements encode political bias. Within an existing\ncorpus of news articles labelled with political bias, we determine descriptive\nstatements, engineer features that represent bias there (according to our view\nof bias), and use these features to train a collection of machine learning models\nto classify a given article into left, center, or right .\nInspecting the effectiveness of utilizing descriptive statements for bias iden-\ntification, we compare our best-performing models to a \u2018lower bound\u2019 approach\nthat guesses the article\u2019s bias. Besides, we compare the models against two\n\u2018upper bound\u2019 approaches that utilize the content of the entiregiven article.\nThe results demonstrate that identifying bias in descriptive statements can be\nused to classify article bias (it outperforms the lower bound), suggesting that\ndescriptive statements are used to a wide extent by authors to encode bias.\nStill, the entire article supposedly contains several linguistic parts (besides de-\nscriptive statement) that encode bias, and thus, can be used for more effective\nbias classification (upper bound).\nContents\n1 Introduction 1\n1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2 Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.3 Our Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n2 Related Work 4\n2.1 Token Level Approaches . . . . . . . . . . . . . . . . . . . . . . 4\n2.2 Richer Context Approaches . . . . . . . . . . . . . . . . . . . . 5\n2.3 Article Level Classification . . . . . . . . . . . . . . . . . . . . . 6\n3 Approach 7\n3.1 Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n3.2 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n3.3 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n4 Experiments & Results 27\n4.1 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n4.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n5 Limitations, Future Work & Conclusion 36\n5.1 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n5.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n5.3 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\nBibliography 39\ni\nChapter 1\nIntroduction\n1.1 Motivation\nOpinion-freeandobjectivenewscoverageisvitalforautonomousjudgment,de-\ncisions, andtheresultingactions. Therefore, onlyunbiasinformationcanassist\nindividuals to act in their best interests, resulting in true self-determination.\nThe web allows open access to information and thus expands the ability\nof individuals to obtain information independently and shape their views on\ncurrent events [Hamborg et al., 2019]. The ubiquitous online and social media\nprogressed into a big news source with a significant impact on people\u2019s beliefs\nand behaviours. Furthermore, news coverage has evolved away from complex\nand detailed reporting towards a personal and subjective style [Blake et al.,\n2019]. This can be ascribed to the efforts of news outlets to go viral on the\nweb and social media, at least to some degree. Media channels discovered the\npotency of content that stimulates strong emotions, often induced by one-sided\ncoverage [Kiesel et al., 2019].\nThese trends suggest an increasing exposure to polarized reporting, while\nleaving critical consumption to the individual\u2019s responsibility. This is illus-\ntrated in 2016 United States presidential elections [Kiesel et al., 2019]. Un-\nderstanding of an article\u2019s agenda and ideology is indispensable to make an\nadequate judgment of the presented information. However, in our world where\nsocial media is omnipresent, constantly identifying an author\u2019s intent demands\nskill and persistence. Assisting automatic tools that help people decide what\nnews articles to consume and what to discard evolved into a pressing issue.\nAn individual\u2019s beliefs are influenced by their experiences, education, and\ncultural background. Despite an author\u2019s intention of delivering an objective\nperspective on a subject, creating an opinion-free and unbiased report is not\na trivial task, as the perception of bias is linked to ones\u2019 political views [Yano\net al., 2010]. Understanding the mechanisms by which bias is introduced is\n1\nCHAPTER 1. INTRODUCTION\nvital to detect and avoid it. Thus, it is the first step to debiasing of documents,\nmanually and automatically.\n1.2 Bias\nIn general, bias is described as a disproportionate weight that influences judg-\nment. The term encompasses various forms and concepts across many disci-\nplines [Wikipedia, 2021].\nIn this thesis, we investigate bias from the perspective of natural language\nprocessing. In a text, bias can manifest itself in the (1) lexical level i.e. word\nchoiceandpositiveandnegativeconnotations, orthe(2)semanticlevel, i.e. the\nchoice of presented or neglected information [Fan et al., 2019]. Since the term\n\u201cbias\" is used in abroad fashion, we focus on certain types of text spans with\nextraneous information merely given by an author to influence the reader\u2019s\nopinion.\nIn this thesis, we view biasas a text:\n1. affiliated with an ideology, at the lexical or semantic levels.\n2. incompatible with the context of the document\n1.3 Our Approach\nOnepossiblesourceofbiasistheselectedwaytointroduceentities. Whenmen-\ntioning named entities, authors tend to give background information alongside.\nDepending on the information itself, its relationship with the context, and its\ndegree of generality, such information may convey a sentiment towards the en-\ntity. It is the author\u2019s decision what information to include and what words to\nuse. Therefore, it is a powerful opportunity to frame the entity and introduce\nbias, consciously or unconsciously. We call such bias \"entity-based bias\".\nIn this thesis, we will explore to what extent bias is encoded in the in-\nformation presented when introducing named entities. We will focus on news\narticles and consider bias affiliated with the left-right political spectrum. The\nnamed entities investigated will be restricted to people.\nTo gain a tangible notion of what information we want to consider, we will\nneed a rigorous delimitation from the rest of the source document. Since we\nwant to work with brief information that is closely tied to an entity, we opted\nto leverage a linguistic pattern known as an appositive ; a type of relative clause\n(Figure 1.1 ) [Radford, 2019].\n2\nCHAPTER 1. INTRODUCTION\nThese allegations ,which Trump made during his campaign, turned turned out to\nbe fake.\nFigure 1.1: Example of an appositive. The appositive (cursive) is surrounded by\ncommas, it is preceded by its head (bold), and is prefixed by its relativizer (under-\nlined).\nSince we want to work with appositives that state information on named\nentities, namely people, we will constrain the appositives\u2019 heads and relativiz-\ners. We define a descriptive statement as an appositive where\n1. the head is a proper noun (in our case a person)\n2. the relativizer is a/an/who/whose/the\nThe election of Trump,acandidate who made fear and xenophobia a central part of\nhis campaign , has spurred advocates to pledge to fight for the dignity of all families.\nFigure 1.2: Example of a descriptive statement from a Truthdig article, a publisher\nlabelled as leftby BuzzFeed journalists or MediaBiasFactCheck.com\nAll eyes are on the markets as Wall Street could welcome Trump,abusinessman\nat heart, with open arms.\nFigure 1.3: ExampleofadescriptivestatementfromaFoxNewsarticle, apublisher\nlabelled as rightby BuzzFeed journalists or MediaBiasFactCheck.com\nWith these definitions established, we can proceed to describe our research\nquestion as follows: To what extent does a descriptive statement encode bias?\nTo investigate this research question, we create a corpus that comprises de-\nscriptive statements and use it to train and test a machine learning model that\npredicts article-level bias on the left-right political spectrum. The effectiveness\nof the model determines the degree of bias encoded in a descriptive statement.\nThe hypothesis that descriptive statements entail bias to an extent that allows\nthem to be leveraged to classify an article\u2019s ideological bias is tested, and the\nresults of our experiments confirm this hypothesis.\nThe thesis at hand is structured as follows; Chapter 2 gives a brief overview\nof related work in the field of bias classification. In chapter 3 we explain the\napproach to answer our research question in detail, including how we created\nand preprocessed a corpus of descriptive statements, and how we designed\nfeatures to represent bias for individual descriptive statements. Chapter 4\nconsiders the experiments we conducted using the corpus and the features.\nUltimately, in chapter 5, we then discuss these experiments and their results,\nalongside examining the limitations and conversing about future work.\n3\nChapter 2\nRelated Work\n2.1 Token Level Approaches\nPrior work in natural language processing on bias detection has primarily\nfocused on leveraging bias arising from content realization; how information\nis worded. Since semantics are subordinate to such forms of bias, they are\nindependent of outside context but dependent on linguistic attributes, such as\npolarized words.\nYano et al. [2010] compiled a corpus drawing 1100 sentences from political\nAmerican blogs. Each sentence given to five annotators to be labelled with\nthe extent of bias, the biases direction on the liberal-conservative spectrum\nand the words cuing the author\u2019s bias. The work suggests that certain tokens\nrecurrently introduce bias into the sentence and that these tokens vary de-\npending on the ideology. It is also shown that bias perception was influenced\nby annotators\u2019 political views, highlighting the complexity of compiling a well\nannotated corpus.\nRecasens et al. [2013] also investigates bias introducing tokens and contin-\nues to apply them. The work employs Wikipedia\u2019s revision history and specifi-\ncally edits aiming to eliminate bias. Comparisons between the text before and\nafter the edits were used to gain insights on the biases lexical realisation. Two\nmajor types of biases were found. Framing bias stemming from word choice\nthat conveys an attitude and epistemological bias related to the believability of\na proposition. Single word edits were used to train a model, giving all unedited\nwords from the sentence as negative examples and the edited word as a positive\nexample. The trained model could predict the bias introducing word with a\n34.35% accuracy and a 58.70% accuracy if returning three possible candidates.\nYano et al. [2010] and Recasens et al. [2013] works imply that single tokens\ncan be indicative of bias at sentence level and associated the ideology. In this\nthesis, while not fully depending on them, we also employ bias cues on the\n4\nCHAPTER 2. RELATED WORK\ntoken level.\n2.2 Richer Context Approaches\nApproaches merely considering single tokens or bag or words classifiers fail to\ntake into account compositional effects.\nIyyer et al. [2014] used logistic regression and deep learning models to\nclassify bias as liberalorconservative on the sentence and phrase level. The\ntrained RNN model outperformed logistic regression models that use bag of\nwordsand word embeddng features. The model achieved 70.2% in accuracy\non theConvote corpus. Since we plan to examine a very specific concept, the\ndescriptivestatementforpeople,wewillnothaveenoughdatatotrainanRNN.\nHowever Iyyer et al. [2014] shows that a logistic regression model with word\nembeddings still performs reasonably well, achieving 66.6% in accuracy. The\nConvote corpus was created from US Congressional floor debate transcripts.\nThe sentences were labeled by propagating down the ideology of the speaker\u2019s\nparty. This resembles the labelling of the corpus we plan to use, in which\ndescriptive statements are labeled based on the articles\u2019 outlets\u2019 ideology they\noccurred in.\nFan et al. [2019] created BASIL, a corpus of 300 news articles covering\n100 events. For each event BASILincludes the reporting of a liberal, center\nand conservative outlet. Bias spans in the articles were annotated with the\ntype of bias, falling into either lexicalorinformational bias . Fan et al. [2019]\ncalls bias concerning factual content employed merely to sway the reader\u2019s\nopinioninformational bias and bias that manifests itself in word choice and\nlinguistic attributes lexical bias .BASILshows the prevalence of informational\nbias in new articles with 73.6% falling into that category. It is also suggested\nthat informational bias occurs evenly throughout the article, while lexical bias\noccurs in the first quarter. A binary classification task was carried out on the\ntoken and sentence level, predicting whether bias occurs or not. A BERT-\nmodel achieved an F1-score of 18.71% for informational bias and 25.96% for\nlexical bias on the token level. On the sentence level the model achieved\nF1-scores of 43.27% for informational bias and 31.49% for lexical bias . This\nshows the importance of context when detecting informational bias , which is\nalso relevant to our task, dealing with information given to introduce an entity.\nIyyer et al. [2014] and Fan et al. [2019] illustrate the importance of richer\ncontext when detecting bias, leading us to work with representations of the\nentire descriptive statement in this thesis.\n5\nCHAPTER 2. RELATED WORK\n2.3 Article Level Classification\nAlthough previous work has been done in bias detection on token, phrase and\nsentence level, using these predictions in order to make assumptions on an\narticle\u2019s bias is still mostly uninvestigated.\nChen et al. [2020] shows that using low level predictions to generate second\norder features intending to predict an article\u2019s bias is promising. The paper\nexamines the performance of models trained on entire articles of the BASIL\ncorpus and concludes that the performance of such models largely depends\nupon whether the model has been trained on other articles covering the same\nevent, but perform with 55% accuracy at best. Leveraging second-order fea-\ntures improves the performance to 62%. In this thesis we use the descriptive\nstatements as second order features.\n[Kiesel et al., 2019] investigated the automated detection of hyperpartisan-\nship, an extreme left- or right-wing affiliation, in news articles. In order to do\nso, two corpora of news articles were labeled and provided as resources for a\nshared task. One corpus featured 1273 manually labeled articles and the other\ncontained 754,000 articles labeled by publisher. 42 teams submitted their ap-\nproaches. Most teams employed convolutional neural networks (CNN). This\nway Bertha von Suttner managed to achieve a 82.2% accuracy on the manually\nlabel corpus. Tintin managed to achieve the best accuracy of 70.6% on the\nby-publisher corpus by only using n-grams as features. We utilize large parts\nof the by-publisher corpus for extracting descriptive statements.\n6\nChapter 3\nApproach\nIn this chapter we will explore how we created and preprocessed a corpus of\ndescriptive statements. We will then go into what preliminary analysis we\nperformed on the corpus and how we designed our features to represent bias.\n3.1 Task\nIn order to answer our research question, we trained a classifier to infer a news\narticle\u2019s bias using descriptive statements as input. The performance of the\nclassifier indicates the level of bias encoded in descriptive statements. To be\nable to train such a classifier, we needed a corpus of descriptive statements\nand features representing bias based on our definition i.e. a representation for\nthe statement\u2019s affiliation with an ideology and the statement\u2019s relevance to\nthe context. In the following, we explain the requirements the corpus needs\nto meet, how we created it, and the intuition and technical realization of the\nfeatures.\n3.2 Data\n3.2.1 Data Source\nSince a descriptive statement is not a common concept, we could not fall\nback on an existing corpus. Thus, we were challenged to create the data set of\ndescriptivestatementsourselves. Wechosetoderivethedescriptivestatements\nfromanexistingcorpus; Data for PAN at SemEval 2019 Task 4: Hyperpartisan\nNews Detection (https://doi.org/10.5281/zenodo.1489920). The corpus had\noriginally been created to perform detection of hyperpartisanship, however,\nthe training data of the final version and all the data of the previous versions\n7\nCHAPTER 3. APPROACH\ncontain article-level bias labels on the left-right political spectrum. Since the\npreviousversionsarenotfullycleanedandhavesomeencodingerrors, weopted\nfor the training data to be our source from which we extracted the descriptive\nstatements. The source data contains 600,000 articles. 50% of the articles\nare labeled to have no bias, 25% are labeled left and 25% are labeled right.\nThe articles are labeled by their publishers\u2019 overall bias provided by BuzzFeed\njournalists or MediaBiasFactCheck.com .\n3.2.2 Descriptive Statement Extraction\nDescriptive Statement Detection Due to our strict definition of descrip-\ntive statements, we can easily translate parts of the linguistic pattern detection\ninto a string pattern matching problem. Detecting a proper noun i.e. named\nentity, especially over multiple tokens, is not a trivial task and can not be\naccomplished with simple pattern matching rules. Everything else, however,\ncan be captured using a regular expression (regex).\nFigure 3.1: Components of a descriptive statement. (2)denotes the components\nof a descriptive statement detectable using a regular expression.\nWe tested the regular expression in Figure 3.2 with a fixed head of\n\"Trump\" and retrieved 2543 descriptive statements. 26 of the 30 examined\nstatements were of the kind we expected, resulting in a precision of 86.67%.\nThe structure the regex detects is not exclusive to descriptive statements as it\noccasionally arises in other compositions. In our evaluation all false positives\nused the relativizer the. We evaluated the precision for 15 statements that\n8\nCHAPTER 3. APPROACH\nFigure 3.2: Descriptive Statement Regex - (1)and(4)capture the surrounding\ncommas. (2)forces the leading token within the commas to be one of our defined\nrelativizers. Since the chunk of text the pattern matching is carried out on, is not\nconstrained to single sentences (3)ensures that any text, but no sentence separating\ncharacter i.e. \".\", \"!\", \"?\" occurs between the surrounding commas.\nusedtheas their relativizer. The precision was only 73%. However, 19% of\nall the descriptive statements for \"Trump\" use theas their relativizer, so we\ndecided not to exclude it.\nWorse for Trump,theCNU poll shows military voters , a traditionally Republican\nconstituency, back Clinton.\nFigure 3.3: Example of a false positive descriptive statement captured by the regex\nWe also tried alternative regular expressions. Before constraining the rela-\ntivizers, we tried a regex limiting the length of a clause, in the hope of detect-\ning small pieces of information. The regex allowed for clauses of nine words or\nless. This regex had a 20% precision. Another assumption we tried, was that\ndescriptive statements occur early in the sentence. We expanded our regex\n(Figure 3.2 ) so that it only detected descriptive statements at the start of\nthe sentence. The precision of this regex was 93.33%. However, its recall per-\nformed poorly, as we were only able to retrieve 1129 for a head of \"Trump\"\ncompared to the 2058 statements our final regex retrieved.\nThe regular expression only constitutes the first stage of the detection\nmechanism. After a match is found, named entity recognition (NER) is em-\nployed to detect the bounding box of the descriptive statement\u2019s head, if\npresent. (Denoted as (1)inFigure 3.1 ). The statement\u2019s head and con-\ntent then get concatenated into a descriptive statement ( Figure 3.4 ). We\nusedSPACEY \u2019sen_core_web_trf pipe to perform the NER. The order of\nexecuting matters, since NER is an expensive operation, only performing it on\npossible candidates, qualified by the regular expression, increases the runtime\nperformance of the detection significantly.\nThe English language capitalizes proper nouns. One might be inclined to\nexploit this and substitute the NER for another pattern matching solution,\ndetecting all leading capitalized tokens. However examples like \"Vincent van\nGogh\" show what kind of pitfalls can arise.\n9\nCHAPTER 3. APPROACH\nFigure 3.4: Descriptive statement component detection - (1)denotes the head\ndetected using NER. (2)denotes the components detected using the Regular Ex-\npression.\nTarget Entity The descriptive statement\u2019s head specifies its target entity.\nHoweverentitiesarenotalwaysreferredtousingthesameexactname. \"Trump\"\nfor example is mentioned as \"Trump\", \"Donald Trump\", \"Donald J. Trump\",\n\"Donald J Trump\" or \"the Orange Man\". These references need to be grouped\nsothedescriptivestatementscanbeassociatedwiththesameentity. Addition-\nally the surname alone is often ambiguous and the entity in question can only\nbe decided based on the statement\u2019s context. To address these problems we\nemployed GENRE (Cao et al. [2021]), Facebook\u2019s generative entity retrieval\nsystemforentitydisambiguation. GENRE providesuswithaconfidence-score,\nwe later use. The score is negative and the closer it is to zero, the more confi-\ndent the model is. In order to perform entity disambiguation, GENRE needs\nthe entities bounding box within its surrounding text. For this we reused our\npreviously separately detected statement head.\nSource Article Context Since our definition of bias is dependent on the\ndescriptive statements context, we retrieved the entire article as well. How-\never, the original corpus only split the articles into paragraphs. Since we want\nto have more granular control over the context we consider, we used SPACY\u2019s\nsentenizicer to split each article into a two-dimensional list representing para-\ngraphs and sentences.\nBias Labels The bias labels in the original data are \"left\", \"center-left\",\n\"least\", \"center-right\" and \"right\". We assume that center-left and center-\nright might not be extreme enough, for us to be able to pick up on subtle cues\nwith the amount of data we have. Therefore we merged \"left-center\", \"least\"\nand \"right-center\" into the label \"center\".\nExtracted Raw Data Statistics Using this extraction pipeline we were\nable to retrieve 198,659 descriptive statements on 97,868 target entities. Con-\nsidering we had 600.000 source articles, on average around every third article\nuses a descriptive statement on a person.\nThe mean number of statements per entity is 2.03, with 2645 statements\nbeing the maximum (Donald Trump) and 1 statement being the minimum.\n10\nCHAPTER 3. APPROACH\n0 50 100 150 200 250 300 350 400\nNumber of Statements per Entity100101102103104105FrequencyNumber of Statements per Entity\nFigure 3.5: Raw Descriptive Statement Data Distribution - The x-axis represents\nthe number of descriptive statements associated with a single entity and the y-axis\nrepresents the number of entities with such a associated statement count. The lighter\nbars display buckets of size 10.\nThe standard deviation of 10.93 and Figure 3.5 imply that most entities\nonly have one or very few descriptive statements, while only some exceed 50\nstatements. This is to be expected, considering that only some entities have a\nlot of news coverage while others only appear occasionally.\n41.54% of the descriptive statements are \"center\", 37.33% are \"left\" and\n21.13% are \"right\". This is curious we would expect a similar amount of labels\non the left as on the right.\nFor multiple entities we observed recurring identical descriptive statements\nfor the same targeting the entity. For example the statement \"Trumps, who\ncampaigned on warmer ties with Putin\" occurs 96 times. We conclude that\nthis is due to news outlets working with agencies that provide them with\ninformation. This information might not always get paraphrased.\n11\nCHAPTER 3. APPROACH\nAttribute Description\ndescriptive statement the statement itself\ntarget entity the disambiguated entity the descriptive state-\nment gives information on\ntarget entity confidence score the confidence score of the GENRE model on\nthe predicted target entity\ncontext the article the descriptive statement occurred\nin, split into paragraphs and sentences\nbias the bias of the article the descriptive statement\noccurred in. (left | center | right)\nTable 3.1: Extracted Descriptive Statement Schema\n3.2.3 Preprocessing\nAs stated before and as suggested by Figure 3.5 many entities only have one\nor few associated statements. Our definition of bias is based on the affiliation\nof a descriptive statement with an ideology. In order to develop features rep-\nresenting ideology affiliation, we need a reasonably sized sample of statements,\nsuchthatsimilarstatementsmightbeprevalentwithinanideology. Ifhowever,\nthe sample size is too small, we have no representative indication of ideology.\nEntity statement count center left right\n1Donald Trump 2645 45.29% 23.52% 31.19%\n2Barack Obama 966 22.26% 48.65% 29.09%\n3Suzanne M. Levine 884 0.0% 0.11% 99.89%\n4Hillary Clinton 506 19.57% 42.69% 37.75%\n5John McCain 403 31.27% 48.39% 20.35%\n6George W. Bush 359 13.93% 77.16% 8.91%\n7Mitt Romney 308 15.91% 64.61% 19.48%\n8Bernie Sanders 306 21.57% 46.73% 31.7%\n9Steve Bannon 289 57.09% 20.42% 22.49%\n10Bill Clinton 275 15.27% 56.36% 28.36%\n11Vladimir Putin 253 56.52% 26.88% 16.6%\n12David MacKay (VC) 230 0.0%100.0% 0.0%\n13Mike Pence 201 46.27% 30.85% 22.89%\n14Aleksandr Kogan 197 100.0% 0.0% 0.0%\n15James Comey 185 76 47 62\nTable 3.2: First 15 Entities sorted by Statement Count\nConsidering Table 3.2 , two more problems become apparent. Some en-\n12\nCHAPTER 3. APPROACH\ntities\u2019 statements primarily occur in articles with the same ideology. For the\nsame argument as before, this skewed distribution prevents us from creating\nrepresentative features indicating ideology. In Table 3.2 , this is the case for\n\"Suzanne M. Levine\" (3), \"David MacKay\" (12)and \"Aleksandr Kogan\" (14).\nThere also seems to be a correlation between heavy repetition of descriptive\nstatements and a skewed label distribution. For \"Suzanne M. Levine\" all state-\nments are identical, 77.25% for \"David MacKay\" and 49.75% for \"Aleksandr\nKogan\".\nThe other problem is that the entity disambiguation is not always correct.\n\"Suzanne M. Levine\", for example, is actually \"Suzanne Frey\", an executive\nat Alphabet.\nWe conducted further processing of the data to counteract the described\nproblems. Note that, since the preprocessing steps are dependent on each\nother, they have to be carried out in the presented order.\nTarget Entity Preprocessing Some entities are not disambiguated cor-\nrectly. We used the GENRE-model\u2019s confidence score as an indicator to which\nreferences might have been mapped to the wrong entity. The confidence score\nis negative. A value closer to zero denotes a higher confidence of the model.\nFigure 3.6 suggests that a significant amount of statements has a confi-\ndence score between -0.5 and 0. To decide if -0.5 is a reasonable minimum-\nthreshold, we compare the precision of the disambiguation between -0.5 and 0\nto the precision between -1.0 and -0.5.\nWhether a disambiguation is correct has to be decided manually. Since\nsome prior knowledge about the entity is needed to make an educated judge-\nment on the disambiguation, it is not feasible to pick random statements from\nthe corpus. Instead we used the 10 entities with the most statements. For each\nscore interval and for each entity we sorted the statements chronologically. If\napplicable, we chose 5 statements evenly distributed throughout the sorted\nlists of entities for each score interval. The chronological sorting is to avoid\nthe disambiguation performing better on newer or older events. We assessed\neach disambiguation using the source article as context.\nThe precision between -0.5 and 0 was 97.78%. The precision between -1\nand -0.5 was 32.14%. Since this is a significant drop in precision, we choose\n-0.5 to be our threshold and discard all descriptive statements with a lower\nscore.\nAfter setting this threshold we are left with 75,999 statements (38.26%)\nand 27,411 entities (28%). Although a lot of statements are lost, 77.57% of the\nremoved statements belonged to entities with less than 5 associated statements\nand 94.33% of the statements removed belonged to entities with less than 30\nassociated statements. As stated before, we are not able to profit of entities\n13\nCHAPTER 3. APPROACH\n5.0\n 4.5\n 4.0\n 3.5\n 3.0\n 2.5\n 2.0\n 1.5\n 1.0\n 0.5\n 0.0\nStatement Disambiguation Score010000200003000040000500006000070000FrequencyDisambiguation Score Distribution\nFigure 3.6: GENRE-Model Confidence Score Distribution - The x-axis represents\nthe the confidence scores. The y-axis represents the frequency of scores within the\ninterval. The darker bars represent an interval of 0.1. The lighter bars represent an\ninterval of 0.5.\nwith few statements anyway.\nStatement Bias Distribution To create a representative statement distri-\nbution feature, an entity\u2019s statements should be evenly distributed across the\nbias labels to some extent. In order to ensure at least a low degree of uniform\ndistribution across the bias labels, we employed Shannon Entropy .\nGiven a discrete random variable X, with possible outcomes x1,...,xn,\nwhich occur with probability P(x1), ..., P (xn), the entropy of Xis formally\ndefined as:\nH(X) =\u2212nX\ni=1P(xi) logP(xi)\n(Wikipedia [2021])\n14\nCHAPTER 3. APPROACH\n0 50 100 150 200 250 300 350 400 450\nNumber of Statements per Entity0.00.20.40.60.81.0Entity Statement Bias EntropyEntity Statement Bias Entropy vs Entity Number of Statements\nFigure 3.7: Entity Descriptive Statement Bias Entropy - Each marker represents\nan entity. The x-axis represents the entity\u2019s the number of statements. The y-axis\nrepresents the entity\u2019s statements\u2019 bias distribution entropy. Within the shaded area\nare the entities that qualify above the threshold.\nWe can treat the distribution of bias labels for an entity\u2019s statements as a\nrandom variable X, where the possible outcomes are xcenter,xleft,xright. Then,\nin case of xcenter:\nxcenter =the number of statements with bias \u2019center\u2019\nP(xcenter ) =xcenter\nxcenter +xleft+xright\nA uniform probability, i.e. in our case an equal amount of statements\nlabeled center, left, and right, yields maximum entropy and can then only\ndecrease (Wikipedia [2021]). Thus, we can introduce an entropy threshold,\nbelow which we disregard the entity. Depending on this threshold, we can\ncontrol the allowed deviation from a uniform distribution.\nThe goal of the entropy threshold is not to force a close to uniform distribu-\ntion, but rather to eliminate entities with very skewed bias label distributions.\n15\nCHAPTER 3. APPROACH\nThe threshold should be an adequate tradeoff between the label distributions\nwe use and the amount of data we can use.\nBased on Figure 3.7 we chose an entropy of 0.639 as our threshold. Apply-\ning it excludes low entropy entities while preserving entities with reasonable\nentropies and a large number of statements. The chosen entropy threshold\nensures that no bias label is present in under 10% of the statements. After\nutilizing the threshold, we are left with 32764 statements (43.11%) for 3161\nentities (11.53%). 70.67% of the statements came from entities with under 5\nassociated statements and 90.46% came from entities with under 30 associated\nstatements.\n05101520253035404550556065707580859095100\nMinimum number of Statements per Entity050001000015000200002500030000T otal number of StatementsT otal Statements vs. Minimum number of Statements per Entity\nFigure 3.8: Minimum Statement Threshold - This figure shows how the minimum\nstatements per entity threshold influences the total number of statements. The dark\nbar represents the chosen threshold of 35. The shaded area and the bards reaching\nit, represent thresholds that result in at least 15,000 descriptive statements.\nMinimum Statement Count As previously stated, our definition of bias\nconsiders how prevalent a statement is within an ideology. The prevalence\nis not representative if the statement or similar statements only occur very\n16\nCHAPTER 3. APPROACH\nfew times. Since many entities don\u2019t have a lot of associated statements, the\nexpressiveness of their statements\u2019 bias labels might be limited. Therefore we\nwant to exclude entities with a deficient statement count. We introduce an-\nother threshold exercising a minimum statement count. This threshold should\nnot come at the cost of losing too many descriptive statements overall.\nFigure 3.8 suggests, that if we want to work with over 15,000 descriptive\nstatements overall, the threshold should be below 40. Therefore we choose\na minimum of 35 descriptive phrases per entity. We think that within 35\nstatements, the most common information has a high chance of appearing\nmultiple times.\nPreprocessed Data Statistics After these preprocessing steps, we are left\nwith 15.714 statements for 156 entities. The maximum number of statements\nper entity is 2543 (Trump) and the minimum is 35. The mean number of state-\nments per entity is 100.73. 34.57% of the descriptive statements are labeled\nas \"center\". 38.04% are labeled as \"left\" and 27.39% are labeled as \"right\".\nPreliminary Data Inspection To investigate the viability of our approach\nwe conducted some early experiments. For the entity \"Trump\", we explored\nif any co-occurrences or n-grams have a prevalence in descriptive statements\nstemming from articles labelled with a certain bias.\nco-occurrence left occurrences overall occurrences\nclimate, denier 3 3\nreality, tycoon 3 3\nstar, television 2 3\nTable 3.3: Co-OccurrencesprevalentinDescriptiveStatementsfromarticlelabelled\nasleft.\nco-occurrence right occurrences overall occurrences\nannual, growth 4 4\nblue, collar 3 3\nnet, worth 3 3\nTable 3.4: Co-OccurrencesprevalentinDescriptiveStatementsfromarticlelabelled\nasright.\nAlthough some observed prevalent co-occurrences seem to be caused by\nchance, such as \"GOP, nominee\" used in left articles 6 out of 6 times, our\n17\nCHAPTER 3. APPROACH\npreliminaryexperimentssuggestthatdescriptivestatementsexhibittendencies\nlinked to ideology.\nWe continued to examine semantically similar statements, to further inves-\ntigate if there were tendencies related to ideology affiliation among them. To\ndo this, we encoded the descriptive statements for \"Trump\" using SBERT\u2019s\nsentence embeddings [Nils Reimers, 2021] and grouped them by using agglo-\nmorotive clustering. An approach we will later also use to create a bias repre-\nsentation. (We will go into greater detail on this in section 3.3.1 ). To explore\nthese clusters we built a web-gui.\nFigure 3.9: Cluster Explorer Interface - Each numbered statement contained in a\ngray box represents a cluster. The statement itself represents all similar statements\nwithin the cluster. The diagram with the red and blue sections is the \"bias-meter\".\nRed represents left bias, blue represents right bias. The opaque bias-meter represents\nthe bias distribution within the cluster, the pale bias-meter presents the overall bias\nfor comparison. Tapping on a cluster reveals each of the statements contained within\nthe cluster.\n18\nCHAPTER 3. APPROACH\nFigure 3.10: First 11 Clusters (by size) - Although the bias distribution of some\nclusters is not very distinctive others seem indicative of bias.\n19\nCHAPTER 3. APPROACH\nFigure 3.11: a cluster were the majority of statements come from left articles.\nFigure 3.12: a cluster were the majority of statements come from right articles.\n20\nCHAPTER 3. APPROACH\n3.3 Model\nDue to the novelty of our approach, our model should merely indicate the\nviability of our features. Therefore we prefer a simple model that allows for\ninterpretability. Additionally, the limited data we have, does not allow for\ncomplex deep learning models. Thus, we opted for a basic logistic regression\nclassifier.\n3.3.1 Feature Engineering\nIn order to train a model, we must decide which information we deem rele-\nvant for the classification of an article\u2019s bias and how we can represent this\ninformation in a multi-dimensional numerical form.\nDescriptive Statement The descriptive statement itself may be a feasible\nfeature. As examined in our related work, lexical features i.e. word choice can\nbe a signal of ideology. In order to be able to pick up on lexical characteristics,\nwe created bag of words vector representation of the descriptive statements.\nWe used the following pipeline: each statement was tokenized, stop words were\nremoved and the remaining tokens were lemmatized. We used SPACY for\nthis. From these lemmatized tokens, we created vectors were each component\nis associated with one of the tokens. The value of each component represents\nthe number of occurrences of the token within the encoded statement. For\nvectorization, we used SKLearn \u2019sCountVectorizer .\nFigure 3.13: Descriptive statement lexical vector pipeline\nFurthermore, our definition of bias is also based on the affiliation of a de-\nscriptive statements\u2019 information with an ideology. We believe that, if seman-\ntically similar statements are prevalent within articles of the same ideology in\nthe training data, inputting unseen statements that also have similar seman-\ntics, is a strong indicator for the unseen source articles\u2019 biases. To represent\nsemantic similarity, we used sentence embeddings from SBERTof the descrip-\ntive statements [Nils Reimers, 2021].\n21\nCHAPTER 3. APPROACH\nTarget Entity The target entity of the descriptive statement may be impor-\ntant. How a statement frames an entity is dependent on context. This context\nmay be beyond the articles content and concern general knowledge about the\nentity. Therefore statements on separate entities might be similar, but their\neffects might differ. This influence how authors use the statements depending\non their outlet\u2019s ideology. The representation of the entity is a single numerical\nvalue from 1 to n, where nis the number of entities.\nBias Distribution The encodings of the descriptive statements themselves\nprovide information on the distributions of bias labels for similar statements\nimplicitly. However, we conceive this feature to be so important, that we en-\ngineer a feature representing the distributions of bias labels for similar state-\nments explicitly. This gives us control over what to consider similar and how\nthe distributions of bias labels should be interpreted.\nThe idea is to group training data examples by descriptive statement sim-\nilarity. For each group, the bias distribution is determined by the biases of its\nmembers. This bias distribution is propagated down to each member as their\nbias distribution for training. When inferring the bias distribution for unseen\nexamples, the group most similar to the given example is retrieved and its bias\ndistribution will be used. Firgure 3.14 illustrates this process.\nFigure 3.14: Bias Distribution Assignment - (a), (b) and(c)are groups of sim-\nilar descriptive statements. Their members (square, circle or triangle) are training\nexamples. The members\u2019 shapes represent their biases. For each group, a bias distri-\nbution is calculated from its members labels and propagated back to the members.\nTherefore, each member in a group has the same bias distribution. The diamond\nrepresents an unseen example. Its descriptive statement is most similar to the state-\nments of (c). Therefore the diamond example is assigned the same bias distribution\nas the members of (c).\nIn order to group the training examples, we employ SKLearn \u2019s implemen-\n22\nCHAPTER 3. APPROACH\ntation of agglomorotive clustering . We chose this method since it does not need\na predetermined number of clusters. Agglomerative clustering is a hierarchical\nbottom-up approach. Each example starts in its own single element cluster.\nGraduallytheclustersgetmerged(Wikipedia[2021]). Whetherclustersshould\nbe merged depends on two parameters. A distance threshold determines the\nmaximum separation above which clusters are not merged. The linkage type\ncontrols where the distance between two clusters is measured i.e. between\ntheir centroids (average point), their closest, or their furthest points. This is\nillustrated in Figure 3.15 .\nThese parameterizations can only be made with respect to the chosen met-\nric; the function representing the distance between two clusters.\nFigure 3.15: Cluster Distance Threshold - Clusters (a)and(b)get merged into\ncluster (c), as the distance between their centroids is below threshold t. In this\nexample linkage type is \"average\". In the next iteration, however, (c)and(d)will\nnot be merged as their distance exceeds t.\nWe utilize two different metrics to assess descriptive statement similarity.\nBoth metrics constitute a sub-variation of the bias distribution feature. We\nemployed a bag of words approach in order to cluster statements, and thus to\npick up on lexical cues. The descriptive statements were tokenized, stop words\nremoved and the remaining tokens were lemmatized. The similarity between\ntwo descriptive statements\u2019 bag of words was assessed using the Jaccard index .\nLetAandBbe sets. Then the Jaccard Index is\nJ(A, B) =|A\u2229B|\n|A\u222aB|\nWikipedia [2021]\nIn our case, AandBare sets of tokens, generated by the bag of words\npipeline.\n23\nCHAPTER 3. APPROACH\nAs argued before, we also want to group statements that encode the same\ninformation. Therefore we used semantic similarity as our metric. For this, we\nutilizedSBERTs sentence embeddings and calculated their cosine similarity .\nSC(A, B) =Pn\ni=1AiBiqPn\ni=1A2\niqPn\ni=1B2\ni\nIn our case, AandBare the embedded vector representations of the de-\nscriptive statements.\nFor both, the Jaccard index and cosine similarity metric, we needed to\nfind an appropriate parameterization for distance threshold and linkage type.\nThe parameterization should result in a clustering where elements are homoge-\nneous within a cluster and heterogeneous between different clusters. Choosing\nthese parameters needs to be done manually. We fixed linkage type to \"aver-\nage\", allowing us to fine-tune the distance threshold. Due to the number of\nstatements, judging the quality of a clustering is not a trivial task and needs\nto be systemically done to be adequate. Assessing whether clusters are het-\nerogeneous, forces one to compare all clusters. This is not feasible manually.\nHowever, assessing whether a single cluster is homogeneous can be done by\nonly assessing the cluster itself. Due to this, we start with a relaxed distance\nthreshold and gradually tighten the threshold until it is strict enough to form\na homogeneous outcome. This observation is done on the 50 largest clusters\nof the entity with the most statements (Trump). This should also be repre-\nsentative for all other entities, as this should not be entity-dependent. For the\nlexical metric, we chose a distance threshold of 0.8. Thus, for clusters c1and\nc2to get merged, a descriptive statement from c1must share at least 20% of\nthe tokens for all statements from c2on average. For the semantic threshold,\nwe chose 0.5. Thus, in order for clusters to get merged, the cosine similarity\nbetween their centroids must be at least 0.5.\nOther sub-variations of the feature come from the calculation of the bias\ndistribution of a cluster. One sub-variant encode the bias distribution as is. It\nis represented as a vector v, with |v|= 3, where each component virepresents a\nbias label. The value viis represent the portion of its represented label within\nthe cluster.\nE.g.v1represents the label \"center\", with\nv1=number of examples with bias label \"center\" in the cluster\nnumber of examples in the cluster\nThe other sub-variant encode the distribution as a one-hot vector, where only\nthe component representing the most frequent bias label has value 1 and all\nothers are 0. The intuition is that having more extreme values might help the\nmodel to distinguish better.\n24\nCHAPTER 3. APPROACH\nFigure 3.16: Bias Distribution Vector Sub-Variants - The distribution bias vector\nrenders the portions for each bias label. 4 out of 12 examples are labeled as \"square\",\n7 out of 12 as \"circle\" and 1 out of 12 as \"triangle\". The one-hot bias vector only\nreturns the most frequent label, resulting in the component associated with \"circle\"\nto be 1 and all others 0.\nThe third sub-variation of the feature arises from what statements are\nclustered. As said before the frame established by the descriptive statement\u2019s\ninformation, might depend on the recipient\u2019s common knowledge of the entity.\nThus, similar statements might have different effects depending on the entity\nand this, in turn, might affect the bias distribution. Therefore we have a\n\"withinentity\"distributionfeature, whereeachentities\u2019descriptivestatements\nare clustered separately and therefore, the calculated distributions are only be\nraised upon the entity itself. However, it is also possible that the influence of\nthe target entity is minimal and that the feature profits of a larger selection of\nstatements, which is why we also consider a \"cross entity\" distribution feature\nsub-variant, where the clustering is performed on all descriptive statements,\ndisregarding the target entity.\nThe distance metric ( lexical/semantic ), the bias distribution calculation\n(distribution /one-hot), and the statement separation ( within-entity ,across-\nentity) are three sub-variations. Their combinations result in eight different\nbias distribution feature variants.\nContext Fit Our definition of bias is not only based on the affiliation of a\ndescriptive statement with an ideology, but also considers the relevance of a\nstatement to its context. This is to take into account whether the information\nis tangential to its context and thus merely given to sway the reader\u2019s opinion\non the entity.\nThe context fit feature should therefore represent the relevance of the de-\nscriptive statement to a context. We had to decide how this relevance can\nbe calculated and what to consider as context. We concluded to utilize the\nsemantic similarity between the descriptive statement and the context as our\nmeasurement of relevance. Although this might seem like a crude approach,\nour intuition is that broadly related information still bear a larger similarity\nthan completely unconnected material. We use the SBERT embeddings and\n25\nCHAPTER 3. APPROACH\ntheir cosine similarity again. For the context, we agreed to create four differ-\nent variations regarding different fragments of the statements source article as\ncontext.\nWecallthefirstvariation paragraph context fit . Thisvariationmeasuresthe\nsimilarity between the statement and the paragraph the statement occurred\nin. If a paragraph represents a unit of meaning, we believe the information\nwithin a paragraph should be somewhat related to the descriptive statement.\nA close variation we call window context fit , considers the two sentences\nbefore and after the sentence containing the descriptive phrase. Similar to\nthe intuition of the paragraph similarity, we suspect the information in the\nneighborhood of the source statement to bear a level of resemblance.\ntitle context fit measures the similarity of the source article\u2019s title and the\ndescriptive statement. The idea is that the title roughly indicates the article\u2019s\ntopic and therefore relevant statements should be similar to an extent.\nFor the same argument lead context fit measures the similarity between to\nfirst three sentences and the descriptive statement. The beginning of articles\noften summarizes the content and gives background information. Therefore\nrelevant descriptive statements should have some connection to these intro-\nductory sentences.\nFeature Variations\nDescriptive StatementLexical Encoding\nSemantic Encoding\nTarget Entity -\nBias DistributionCombinable sub-variations (8 Variations):\nSimilarity Metric : (lexical / semantic)\nBias vector : (distribution / one hot)\nStatement selection : (cross-entity / within-entity)\nContext Fitparagraph context fit\nwindow context fit\ntitle context fit\nlead context fit\nTable 3.5: Extracted Features - Considering all sub-variations combinations, we\nhave a total of 15 features.\n26\nChapter 4\nExperiments & Results\nToanalyzetheperformancesofourfeatures, webrokeourextracteddescriptive\nstatement data into training and testing splits, created multiple combinations\nof our engineered features, and used them to train and test logistic regression\nmodels. We evaluated the models performances, in order to assess the extent\nto which descriptive statements encode bias.\n4.1 Experiments\n4.1.1 Data Split\nWe decided to allocate around 90% of our data to training, leaving around 10%\nto testing. We have highlighted the importance of the entity to the descriptive\nstatement multiple times throughout this thesis. Since the entity can be a\nfeature itself, we ensured that the distribution of entities within the training\nand testing is the same, i.e. that the data is stratified with respect to the enti-\nties. However, we also want the bias label distribution to be similar within the\ntraining and testing splits, not only throughout the entire data but also on the\nentity level. This means, for each entity we want the distribution of bias labels\nto be the same in the training and testing data. Additionally, to not artifi-\ncially enhance our results, we made sure, that no exact descriptive statement\noccurs in both; the training and the testing data. Also, there are no repeating\nstatements within the testing data, where possible. This leaves us with 14,742\n(93.81%) training and 972 (6.19%) testing examples. The deviation from the\n90:10 distribution is due to our multiple constraints.\n27\nCHAPTER 4. EXPERIMENTS & RESULTS\n4.1.2 Implementation\nDue to the novelty of the approach, we can not make assumptions about which\nfeatures are important and how they may perform in combination. Therefore\nwe chose to test all combinations of features, except single features. Consid-\nering we have 15 features, this results in 215\u221216 = 32 .768\u221216 = 32 .753\ncombinations. (16 combinations have only one or no features.) For the model,\nwe chose SKLearn \u2019s implementation of a logistic regression model. Since the\ncreation of our features is runtime intensive, we derived the training and test-\ning feature vectors from our data once and then only concatenated the vectors\nfor each combination resulting in the final combination feature vector for each\nmodel. For each combination, we measured the performance using the model\u2019s\naccuracy.\naccuracy =number of correct predictions\ntotal number of predictions\n4.1.3 Upper & Lower Limits\nTo compare our results, we generated three other models. As a lower limit,\nwe will use a model that guessing the bias label at random. For our upper\nlimit, we wanted to see how approaches perform, that are not constrained to\ndescriptive statements but have the same foundation otherwise. We created\ntwo logistic regression models trained on the large portions of the entire article.\nOne model was trained on semantic characteristics. We used SBERT\u2019s word\nembeddings on the article as feature vectors. However, SBERT is limited to\n512 \"word pieces\", corresponding to the first 300-400 words of the articles\n(Nils Reimers [2021]). The other model was trained on lexical cues. We used\nthe same tokenization-pipeline and token-count-vector representation as our\nlexical statement encoding feature.\n4.2 Results\nTo get a better understanding of what feature combinations are influential,\nwe grouped the feature combinations into cohorts. The cohorts are differ-\nentiated by the use of distribution and context features, but can all contain\ncombinations using entity and descriptive statements features. The distribu-\ntion+context cohort contains combinations using distribution and context fea-\ntures, the distribution cohort contains combinations using distribution features\nbut no context features, contextcontains context features but not distribution\nfeatures, and plaincontains entity and descriptive statement encoding features\nonly.\n28\nCHAPTER 4. EXPERIMENTS & RESULTS\nTo evaluate our results we will start analyzing individual features. We will\nexamine the distribution andcontextcohort individually and then move on to\nthe overall results. We will weigh up the cohorts against each other and finally,\nwe will compare our results to the lower and upper limits.\n4.2.1 Individual Features\nTable 4.1: Features ranked by Average Accuracy\nFeature Avg. Acc.\n1distribution - semantic metric, distribution bias vector, within-entity 44.58%\n2distribution - lexical metric, distribution bias vector, within-entity 44.35%\n3entity 43.74%\n4distribution - semantic metric, one-hot bias vector, within-entity 43.65%\n5distribution - lexical metric, one-hot bias vector, within-entity 43.51%\n6distribution - semantic metric, distribution bias vector, cross-entity 43.42%\n7distribution - semantic metric, one-hot bias vector, cross-entity 43.33%\n8context - paragraph 43.25%\n9statement - semantic 43.24%\n10context - window 43.23%\n11context - title 43.23%\n12context - lead 43.22%\n13statement - lexical 43.02%\n14distribution - lexical metric, one-hot bias vector, cross-entity 42.77%\n15distribution - lexical metric, distribution bias vector, cross-entity 41.81%\nTable 4.1 ranks all features by the mean accuracy of the combinations they\noccur in. The mean accuracies themselves are not deciding here, but the\norder of features should give a first feel for their compared viability, especially\nbetween feature variants. The distribution feature performs relatively well,\nas almost all of them place in the top ranks. When comparing distribution\nfeatures only differing in metric, the semantic metric always achieves a higher\nmean accuracy compared to the lexical metric . The same can be said for the\ndistribution bias vector andone-hot bias vector , aseverycombinationthatonly\ndiffers in the bias vector representation, obtains a higher mean accuracy when\nusing the distribution bias vector . All distribution variants using the within-\nentityclusteringoutperformthe cross-entity setting. The entityfeaturesplaces\nat rank 3. This might be tied to the within-entity clustering setting. On\naverage the context features perform similarly, with their scores only varying\nby around 0.01%. The semantic encoding for the statement itself outperforms\nthelexicalencoding.\n29\nCHAPTER 4. EXPERIMENTS & RESULTS\n4.2.2 Distribution Cohort\nTable 4.2: Top 5 Performing Distribution Feature Combinations\nFeature Combination Acc.\n1entity\ndistribution - semantic metric, distribution bias vector, cross-entity\ndistribution - semantic metric, distribution bias vector, within-entity\ndistribution - semantic metric, one-hot bias vector, cross-entity\ndistribution - semantic metric, one-hot bias vector, within-entity50.82%\n2entity\ndistribution - semantic metric, distribution bias vector, cross-entity\ndistribution - semantic metric, distribution bias vector, within-entity50.41%\n3entity\ndistribution - semantic metric, distribution bias vector, cross-entity\ndistribution - semantic metric, distribution bias vector, within-entity\ndistribution - semantic metric, one-hot bias vector, cross-entity50.31%\n4distribution - semantic metric, distribution bias vector, cross-entity\ndistribution - semantic metric, distribution bias vector, within-entity50.21%\n5distribution - semantic metric, distribution bias vector, cross-entity\ndistribution - semantic metric, distribution bias vector, within-entity\ndistribution - semantic metric, one-hot bias vector, cross-entity50.21%\nThe top five distribution feature combinations score 50.39% on average. Con-\nsistent with Table 4.1 , all distribution variants use the semantic metric . In\nfact, the first combination to utilize a distribution feature employing the lex-\nical metric only occurs at rank 33. This strongly indicates that the semantic\nclustering results in a more potent distribution representation. Also congruent\nwithTable 4.1 ; thedistribution bias vector is more prevalent compared to\ntheone-hotrepresentation. In Table 4.2 there is no combination with a one-\nhot bias vector representation, that does not include the same feature with\nadistribution bias vector representation, thus suggesting, that the one-hot-\nfeatures is indifferent and dispensable. In the listed combinations in Table\n4.2, thecross- andwithin-entity variations are primarily used simultaneously.\nHowever, in contrast to Table 4.1 , thecross-entity sub-variation seems to be\nfavored. Noneofthefirstfivecombinationsusethestatementasafeature. The\naverage accuracy in the distribution cohort is 43.93%, 30.76% is the minimum.\n4.2.3 Context Cohort\n30\nCHAPTER 4. EXPERIMENTS & RESULTS\nTable 4.3: Top 5 Performing Context Feature Combinations\nFeature Combination Acc.\n1entity\nstatement - semantic\nstatement - lexical\ncontext - paragraph\ncontext - window\ncontext - lead49.49%\n2entity\nstatement - semantic\nstatement - lexical\ncontext - title49.38%\n3entity\nstatement - semantic\nstatement - lexical\ncontext - title\ncontext - window49.38%\n4entity\nstatement - semantic\nstatement - lexical\ncontext - paragraph\ncontext - title\ncontext - window49.38%\n5entity\nstatement - semantic\nstatement - lexical\ncontext - paragraph\ncontext - lead49.28%\nThe top five feature combinations in the contextcohort score an accuracy of\n49.38% on average. Similar to Table 4.1 , considering Table 4.3 no order of\nimportance can be deduced from the context feature variants. However, all\nlisted combinations contain the semantic and lexical encodings of the descrip-\ntive statement. When ordered by accuracy, the first 32 combinations from the\ncontextcohort all include the semantic representation of the descriptive state-\nment itself and 76.67% of the combinations using the semantic representation\nscore above the context-median. This illustrates how important the semantic\nencoded statement feature is for the contextcohort. A possible explanation\ncould be, that the context itself can not be used for classification across the\nbias labels, as it only signals the similarity between the descriptive statement\nand the context. The semantic encoding of the statement, however, can be\nused to link similar examples to an ideology. The average accuracy in the\n31\nCHAPTER 4. EXPERIMENTS & RESULTS\ncontextcohort is 45.14%, the minimum is 36.62%.\n4.2.4 All Combinations\nTable 4.4: Top 10 Performing Feature Combinations\nFeature Combination Acc.\n1entity\ncontext - paragraph\ndistribution - semantic metric, distribution bias vector, cross-entity\ndistribution - semantic metric, distribution bias vector, within-entity\ndistribution - semantic metric, one-hot bias vector, cross-entity\ndistribution - semantic metric, one-hot bias vector, within-entity51.03%\n2entity\ncontext - title\ndistribution - semantic metric, distribution bias vector, cross-entity\ndistribution - semantic metric, distribution bias vector, within-entity\ndistribution - semantic metric, one-hot bias vector, cross-entity\ndistribution - semantic metric, one-hot bias vector, within-entity51.03%\n3entity\ncontext - title\ncontext - paragraph\ndistribution - semantic metric, distribution bias vector, cross-entity\ndistribution - semantic metric, distribution bias vector, within-entity\ndistribution - semantic metric, one-hot bias vector, cross-entity\ndistribution - semantic metric, one-hot bias vector, within-entity50.93%\n4entity\ndistribution - semantic metric, distribution bias vector, cross-entity\ndistribution - semantic metric, distribution bias vector, within-entity\ndistribution - semantic metric, one-hot bias vector, cross-entity\ndistribution - semantic metric, one-hot bias vector, within-entity50.82%\n5entity\ncontext - title\ncontext - paragraph\ncontext - window\ndistribution - semantic metric, distribution bias vector, cross-entity\ndistribution - semantic metric, distribution bias vector, within-entity\ndistribution - semantic metric, one-hot bias vector, cross-entity\ndistribution - semantic metric, one-hot bias vector, within-entity50.82%\nContinued on next page\n32\nCHAPTER 4. EXPERIMENTS & RESULTS\nTable 4.4 \u2013 continued from previous page\nFeature Combination Acc.\n6entity\ncontext - paragraph\ncontext - window\ndistribution - semantic metric, distribution bias vector, cross-entity\ndistribution - semantic metric, distribution bias vector, within-entity\ndistribution - semantic metric, one-hot bias vector, cross-entity\ndistribution - semantic metric, one-hot bias vector, within-entity50.72%\n7entity\ncontext - window\ndistribution - semantic metric, distribution bias vector, cross-entity\ndistribution - semantic metric, distribution bias vector, within-entity\ndistribution - semantic metric, one-hot bias vector, cross-entity\ndistribution - semantic metric, one-hot bias vector, within-entity50.62%\n8entity\ncontext - paragraph\ncontext - window\ncontext - lead\ndistribution - semantic metric, distribution bias vector, cross-entity\ndistribution - semantic metric, distribution bias vector, within-entity\ndistribution - semantic metric, one-hot bias vector, cross-entity\ndistribution - semantic metric, one-hot bias vector, within-entity50.62%\n9entity\ncontext - window\ndistribution - semantic metric, distribution bias vector, cross-entity\ndistribution - semantic metric, distribution bias vector, within-entity50.51%\n10entity\ncontext - paragraph\ncontext - window\ndistribution - semantic metric, distribution bias vector, cross-entity\ndistribution - semantic metric, distribution bias vector, within-entity50.51%\nTable 4.4 states that the best performing feature combinations have around\n51% accuracy. The differences between the combinations of successive ranks\nare minute, as they only vary by a single feature. This and the similarity of\nthe scores hint at robust results, as it suggests that the top combinations and\nscores are not caused by chance, but that combinations of the kind presented\ninTable 4.4 are actually more indicative of bias than others. All of the top\ncombinations contain the entity feature. None of the combinations contain an\nencoding of the descriptive statement, however, they all contain distribution\nfeatures. This suggests that the listed combinations perform better with the\nexplicitly modeled distribution feature, compared to the implicit distribution\n33\nCHAPTER 4. EXPERIMENTS & RESULTS\ngiven by the statements themselves. 9 out of the 10 highest performing feature\ncombinations are from the distribution+context cohorts. When combining dis-\ntribution and context, titleandparagraph seem to be the strongest context\nfeatures, as they occur frequently in the top five combinations. The top eight\ncombinations all feature all of the semantic distribution variations. All ten\ncombinations contain semantic distribution features utilizing the distribution\nbias vector , as seen in the distribution cohort. The overall mean accuracy is\n43.41%.\ndistribution+context\n(30600)distribution\n(2032)context\n(116)plain\n(4)0.360.380.400.420.440.460.480.50AccuracyAccuracy by Cohort\nFigure 4.1: Accuracy by Cohort - the x-axis represent the different cohorts. The\nnumber below the cohort-label displays the number of combinations within the co-\nhort. The y-axis represents the accuracy. The triangle indicates the mean accuracy\nof the cohort. The box represents the interquartile range. The horizontal line within\nthe box represents the median value. The whiskers represent the maximum and\nminimum (excluding outliers).\nFigure 4.1 compares all accuracies of the cohorts. It suggests that the\nrange of accuracies of a cohort is correlated to the number of feature combi-\nnations evaluated within the cohort. This is plausible because more combi-\nnations increase the chances for outliers. The maximums and minimums of\nthe interquartile ranges, the mean and median accuracy seem to be inversely\n34\nCHAPTER 4. EXPERIMENTS & RESULTS\nCohort Size Mean Acc. Max Acc. Min Acc.\ndistribution+context 30600 43.37% 51.03% 30.04%\ndistribution 2032 43.93% 50.82% 30.76%\ncontext 116 45.14% 49.49% 36.63%\nplain 447.69% 49.18% 45.99%\nTable 4.5: Accuracy Scores by Cohort\ncorrelated to the number of combinations. This relationship, however, can\nnot be explained, as it would suggest that more combinations perform worse\non average than fewer combinations. We assume this inverse correlation to\nbe a coincidence. Therefore, Figure 4.1 andTable 4.5 displays that the\ndistribution+context cohort performs worst on average and that plainfeature\ncombinations perform best.\nThissuggeststhatmanyfeaturecombinationsfromthe distribution+context\ncohort are low-grade, except for a few exceptions. Also, the plaincohort per-\nforms above the overall average for all its four combinations.\n4.2.5 Lower & Upper Limit\nModel Accuracy\n1Article - Lexical 80.97%\n2Article - Semantic 73.97%\n3Our Best Performing Combination 51.03%\n4Guessing Model 33.33%\nTable 4.6: Limits Accuracy Scores\nTable 4.6 displays how our best model compares to the our lower and\nupper limits. We were able to outperform a guessing baseline by around\n18%. However the semantic upper limit performed around 23% better than\nour model and the lexical upper limit surpassed our model by around 30%.\nWe will the exceptional performance of our upper limits in the next chapter.\n35\nChapter 5\nLimitations, Future Work &\nConclusion\n5.1 Limitations\nOurapproachinthisthesisreliesheavilyontheaffiliationofadescriptivestate-\nment with an ideology with respect to the statement\u2019s target entity. Therefore,\nthe availability of a suitable sized pool of statements for each considered entity\ncan be crucial to get a representative notion of affiliation for the most common\nstatements. We use the threshold of a minimum of 35 statements per entity.\nA larger threshold may lead to more meaningful representation.\nThe detection method for a descriptive statement achieved around 87%\naccuracy. However, more advanced detection mechanisms may lead to higher\naccuracy.\nAlthough most of the retrieved descriptive statements were of a general\ninformation nature, some were very specific. Specific information may not be\nrecurring and can therefore not be associated with an ideology. Statements\nlike this might have introduced some noise into the data.\nThe way our source data has been labeled holds some weaknesses. The\ndescriptive statements were labeled using their source article, which in turn,\nwas labeled by their publisher. Hence, the resulting label for a descriptive\nstatement may not be accurate.\nAs explored in the related work, the BASILcorpus contains an article for\neach ideology reporting on the same event. This ensures that the models are\nnot trained on the discussed topics, which may have a tendency to occur less or\nmore frequently depending on the ideology. This leakage might have positively\nimpacted the performances of our descriptive statement models and especially\nour upper limits, as they were trained on the entire article.\n36\nCHAPTER 5. LIMITATIONS, FUTURE WORK & CONCLUSION\n5.2 Future Work\nDue to the novelty of our approach, we constrained our scope to people and\nthe left-right political spectrum. Future work may consider more named enti-\nties, such as organizations or locations. The bias spectrum may be expanded\nto a multidimensional space, such as the Political Compass, representing the\neconomic (left-right) and social (authoritarian-libertarian) axis (Wikipedia\n[2021]). Also, a hard classification unsuitable for bias and altered to a contin-\nuous value on a spectrum.\nOur definition of a descriptive statement is very constraining, thus making\nits occurrence sparse. Employing a different strategy to detect introductory\ninformation could result in the ability to retrieve a wider range of initial infor-\nmation and therefore an extended collection of bias signals.\nFuture work may also leverage the collection of descriptive statements and\ntheir distributions for debiasing documents by exchanging bias affiliated de-\nscriptive statements against neutral ones.\n5.3 Conclusion\nThisthesisaddressespoliticalbiasinacertainpartofarticlediscourse, namely,\nthetextpartthatintroducespeople. Inparticular, wehaveproposedatangible\ndefinition for introductory information: \u201cdescriptive statement\u201d, asking the\nquestion of To what extent does a descriptive statement encode bias?\nToanswerthisquestion, wehavebuiltapipelinetoretrievesuchstatements\nand used disambiguation to group them by \u2018entity\u2019. We have performed a\nseries of preprocessing steps on our data, enabling it to be used as a corpus\nfor training a bias classification model.\nWe have engineered features that reflect our definition of bias by represent-\ning ideology affiliation and the contextual relevance of a descriptive statement.\nWe trained and tested models for predicting article-level bias for all encourag-\ning feature combinations. To have a comparison, we included a lower baseline\nmodel that guesses the article bias label. We also trained two upper limit mod-\nels; a model trained on a semantic representation of the article and a model\ntrained on a lexical representation of the article.\nTheresultsofourexperimentshavedemonstratedthatourbest-performing\nfeature combination surpasses our baseline by around 18% with an accuracy\nof 51.03%. Even the average feature combination performs around 10% bet-\nter than the baseline. The upper limits performed exceptionally well. The\nsemantic model scored an accuracy of 73.97% and the lexical model scored an\naccuracy of 80.97%. The upper limits were expected to exceed our approach,\nas they can access more information than our model.\n37\nCHAPTER 5. LIMITATIONS, FUTURE WORK & CONCLUSION\nUltimately, we observe that the classification of the article bias using de-\nscriptive statements works relatively well, especially considering the limited\ninformation a statement itself manifests. Hence, as an answer to our research\nquestion: descriptive statements encode bias to an extent that allows them to\nbe used for article-level bias detection to a certain degree.\n38\nBibliography\nJonathan S Blake et al. News in a digital age: Comparing the presentation of\nnews information over time and across media platforms . Rand Corporation,\n2019.\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. Autore-\ngressive entity retrieval. In International Conference on Learning Represen-\ntations, 2021. URL https://openreview.net/forum?id=5k8F6UU39V .\nWei-Fan Chen, Khalid Al-Khatib, Benno Stein, and Henning Wachsmuth. De-\ntecting media bias in news articles using gaussian bias distributions. arXiv\npreprint arXiv:2010.10649 , 2020.\nLisa Fan, Marshall White, Eva Sharma, Ruisi Su, Prafulla Kumar Choubey,\nRuihong Huang, and Lu Wang. In plain sight: Media bias through the lens\nof factual reporting. arXiv preprint arXiv:1909.02670 , 2019.\nFelix Hamborg, Karsten Donnay, and Bela Gipp. Automated identification of\nmedia bias in news articles: an interdisciplinary literature review. Interna-\ntional Journal on Digital Libraries , 20(4):391\u2013415, 2019.\nMohit Iyyer, Peter Enns, Jordan Boyd-Graber, and Philip Resnik. Politi-\ncal ideology detection using recursive neural networks. In Proceedings of\nthe 52nd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 1113\u20131122, 2014.\nJohannes Kiesel, Maria Mestre, Rishabh Shukla, Emmanuel Vincent, Payam\nAdineh, David Corney, Benno Stein, and Martin Potthast. Semeval-2019\ntask 4: Hyperpartisan news detection. In Proceedings of the 13th Interna-\ntional Workshop on Semantic Evaluation , pages 829\u2013839, 2019.\nNils Reimers. Input sequence length. https://www.sbert.net/examples/\napplications/computing-embeddings/README.html , 2021. Accessed:\n2021-01-03.\n39\nBIBLIOGRAPHY\nAndrew Radford. Relative clauses: Structure and variation in everyday En-\nglish, volume 161. Cambridge University Press, 2019.\nMarta Recasens, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky. Lin-\nguistic models for analyzing and detecting biased language. In Proceedings\nof the 51st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 1650\u20131659, 2013.\nWikipedia. bias. https://en.wikipedia.org/wiki/Bias , 2021. Accessed:\n2021-11-27.\nTae Yano, Philip Resnik, and Noah A Smith. Shedding (a thousand points of)\nlight on biased language. In Proceedings of the NAACL HLT 2010 Workshop\non Creating Speech and Language Data with Amazon\u2019s Mechanical Turk ,\npages 152\u2013158, 2010.\n40", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Entity Based Bias in News Articles", "author": ["LF G\u00f6hlich"], "venue": "NA", "pub_year": "NA", "abstract": "The internet allows people to collect a wide range of information on everyday events. Still, the  critical judgment of such information lays upon people\u2019s responsibility. An automatic tool to"}, "filled": false, "gsrank": 9, "pub_url": "https://downloads.webis.de/theses/papers/goehlich_2022.pdf", "author_id": [""], "url_scholarbib": "/scholar?hl=en&q=info:42_EcaHsiYMJ:scholar.google.com/&output=cite&scirp=8&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=42_EcaHsiYMJ&ei=BLWsaNaSEPnSieoPxKLpgQ0&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:42_EcaHsiYMJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://downloads.webis.de/theses/papers/goehlich_2022.pdf"}}, {"title": "Factoid: A new dataset for identifying misinformation spreaders and political bias", "year": "2022", "pdf_data": "FACTOID: A New Dataset for Identifying\nMisinformation Spreaders and Political Bias\nFlora Sakketou\u0003y, Joan Plepi\u0003y, Riccardo Cerveroz, Henri-Jacques Geiss\u0005,\nPaolo Rossoz, Lucie Fleky\nyConversational AI and Social Analytics (CAISA) Lab\nDepartment of Mathematics and Computer Science, University of Marburg, Germany\nzPattern Recognition and Human Language Technology (PRHLT) Research Center\nUniversitat Polit `ecnica de Val `encia, Spain\n\u0005Department of Computer Science, Technical University of Darmstadt\n\u0003These authors contributed equally to this work\nf\ufb02ora.sakketou, joan.plepi, lucie.\ufb02ek g@uni-marburg.de, frcerver1@upvnet, prosso@dsic g.upv.es\nhenri-jacques.geiss@stud.tu-darmstadt.de\nAbstract\nProactively identifying misinformation spreaders is an important step towards mitigating the impact of fake news on our\nsociety. In this paper, we introduce a new contemporary Reddit dataset for fake news spreader analysis, called FACTOID,\nmonitoring political discussions on Reddit since the beginning of 2020. The dataset contains over 4K users with 3.4M\nReddit posts, and includes, beyond the users\u2019 binary labels, also their \ufb01ne-grained credibility level (very low to very\nhigh) and their political bias strength (extreme right to extreme left). As far as we are aware, this is the \ufb01rst fake news\nspreader dataset that simultaneously captures both the long-term context of users\u2019 historical posts and the interactions\nbetween them. To create the \ufb01rst benchmark on our data, we provide methods for identifying misinformation spreaders\nby utilizing the social connections between the users along with their psycho-linguistic features. We show that the users\u2019\nsocial interactions can, on their own, indicate misinformation spreading, while the psycho-linguistic features are mostly\ninformative in non-neural classi\ufb01cation settings. In a qualitative analysis we observe that detecting affective mental processes\ncorrelates negatively with right-biased users, and that the openness to experience factor is lower for those who spread fake news.\nKeywords: fake news spreader detection, fake news and political bias dataset, Reddit\n1. Introduction\nAs the popularity of social media platforms continu-\nously grows, so does the dissemination of online dis-\ninformation. Many deep learning systems have been\ntherefore developed to detect false or biased news\n(Zhou and Zafarani, 2020; Zellers et al., 2019; Monti et\nal., 2019; Shu et al., 2017). While fake news detection\nis a big step to mitigate the impact of misinformation on\nour society (Figueira and Oliveira, 2017; Visentin et al.,\n2019), it is not suf\ufb01cient, since limiting the diffusion of\nfalse information and avoiding its catastrophic effects\nis extremely challenging, especially once it has been\nshared on the Web (Cheng and Chen, 2020; McKay and\nTenove, 2020). Research shows that fact corrections\nfrequently fail in reducing people\u2019s misconception of\nthe truth, and occasionally they even have a \u201cback\ufb01r-\ning\u201d effect where people\u2019s misconception is reinforced\n(Redlawsk et al., 2010; Nyhan and Rei\ufb02er, 2010; Swire\net al., 2017; Berinsky, 2017).\nIt is essential to address this issue at its origin - to ef-\n\ufb01ciently and rapidly identify accounts and users which\nare likely to propagate posts from the handles of unre-\nliable news sources. While there are numerous datasets\nfocusing on this issue at a post-level, only very few\nof those allow to approach this matter on a user level,\nsince, in most cases, fake news posts are not associated\nwith their individual authors.Moreover, existing datasets designed for identifying\nmisinformation spreaders only include binary labels for\nthe users. However, reality is not black and white,\ntherefore a credibility score associated with each user\nis more realistic. In addition, since partisan polariza-\ntion constitutes one of the primary drivers of political\nfake news sharing (Osmundsen et al., 2021), it is be-\ncoming all the more vital to explore the political bias of\nusers in combination with their misinformation spread-\ning behavior. To the best of our knowledge, there is no\nexisting dataset that combines both of these dimensions\non a user level with \ufb01ne-grained scores.\nTo this end, we introduce a dataset for distinguish-\ning the authors that have shared news from unreliable\nsources in the past, from those that share news from re-\nliable sources, covering the posting activity of the users\nbefore and after the 2020 US presidential elections. We\nuse the terms misinformation spreaders andreal news\nspreaders , respectively. Apart from the binary labels,\nwe assign a credibility score to each user based on the\nfactuality of the news sources they shared, and a polit-\nical bias score based on the level of partisanship of the\nnews sources they share.\nOur contributions can be summarized as follows:\n\u2022 We introduce FACTOID1: a user-level FAC tuality\nand p Olitical b IasDataset, that contains a set\n1https://github.com/caisa-lab/FACTOID-datasetarXiv:2205.06181v1  [cs.SI]  11 May 2022\nof 4,150 news-spreading users with 3.3M Red-\ndit posts in discussions on contemporary politi-\ncal topics, covering the time period from January\n2020 to April 2021 on individual user level.\n\u2022 Additionally, we provide \ufb01ne-grained scores\nabout the users\u2019 factuality and political bias.\n\u2022 We conduct classi\ufb01cation experiments for iden-\ntifying misinformation spreaders by utilizing the\nsocial connections between the users along with\ntheir posting history representations and psycho-\nlinguistic features.\n\u2022 The curated dataset preserves the structure of the\nthreads, facilitating the exploration of the users\u2019\nsocial activity by modeling it in a graph. We show\nthat the users\u2019 social interactions can, on their\nown, indicate misinformation spreading, when\nused in a graph attention network.\n\u2022 We conduct qualitative analysis of the impact of\nvarious psycho-linguistic features, such as affec-\ntive mental processes and openness to experience.\n2. Related Work\nRelevant Datasets. User pro\ufb01ling approaches have\nbeen investigated for various tasks, such as author pro-\n\ufb01ling (Vivitha Vijayan, 2019), bot detection (Cai et\nal., 2017; Hurtado et al., 2019; Kosmajac and Ke-\nselj, 2020), gender detection (Daelemans et al., 2019),\namong others. However, fake news spreader detec-\ntion is an under-explored research direction. There are\nsome datasets approaching this matter from different\nangles, for example, attempts have been made to an-\nalyze the users reactions to fake news (Glenski et al.,\n2018) or to analyze users who debunk fake news (V o\nand Lee, 2019). However, there are only a few publicly\navailable datasets suitable for the task.\nShu et al. (2018) constructed a dataset by assessing\nthe users\u2019 trust level on fake news. More recently, the\nPAN 2020 competition (Rangel et al., 2020) brought\nthe problem of misinformation spreaders identi\ufb01cation\nto the fore. The dataset of the competition contained\n500 users with 100 posts each, for two languages. Gi-\nachanou et al. (2020) and Mu and Aletras (2020) cre-\nated a dataset containing misinformation and real news\nspreaders by collecting users that posted articles that\nhave been debunked as fake and built their user his-\ntory based on their previous posts. We draw inspira-\ntion from the method of curation of these datasets and\nuse a similar semi-supervised method to obtain a de-\nscription of the authors and their context. However, the\nproposed dataset is distinctive in three aspects: it con-\ntains \ufb01ne-grained labels about (a) the users\u2019 credibility\nand (b) political bias, and (c) it preserves the structure\nof the threads. Additionally, while the aforementioned\ndatasets utilized Twitter as a source, we utilize Reddit\nwhich does not have a word limit on the posts, making\nthe task all the more challenging.\nApproaches to Spreader Detection. Our dataset\npreserves the structure of the threads, facilitating theexploration of the users\u2019 social activity by modeling it\nin a graph. The recent advances in graph representation\nlearning (Wu et al., 2021) in various domains provide\na promising, under-explored research direction in the\ncontext of fake news spreader detection. More speci\ufb01-\ncally, Graph Attention Networks (GAT) (Veli \u02c7ckovi \u00b4c et\nal., 2018) have achieved state-of-the-art-results in vari-\nous natural language processing tasks (Plepi and Flek,\n2021; Sawhney et al., 2021; Kacupaj et al., 2021; Ren\nand Zhang, 2020). However, this method has not been\nexplored on user graphs in the context of fake news\nspreader detection. Research has shown that users tend\nto interact with like-minded individuals (Bahns et al.,\n2017). Therefore, we wish to leverage this attribute in\norder to obtain better user representations.\nTraditional feature-based user modeling methods an-\nalyze the users\u2019 linguistic patterns in order to infer\npsycho-linguistic features (Tausczik and Pennebaker,\n2010; Girlea et al., 2016). These works extract evi-\ndence of mental processes through the Linguistic In-\nquiry and Word Count (LIWC) software in order to\ntackle the problem of identifying deceptive authors.\nCertain psycho-linguistic characteristics are assumed\nto underlie the vulnerability to fake information, there-\nfore the LIWC tool has often been used to inves-\ntigate the phenomenon of misinformation from both\ndocument-level (Zhou et al., 2020; P \u00b4erez-Rosas et al.,\n2018) and user-level perspectives (Giachanou et al.,\n2020; Cervero et al., 2021). Interestingly, this method\nhas been used in comparison and in conjunction with\ninnovative graph-based architectures (Ren and Zhang,\n2020). Therefore, we believe that leveraging these\npsycho-linguistic features and their combination to-\ngether with the users\u2019 social interactions can contribute\nin order to obtain a strong, competitive baseline.\n3. Dataset\n3.1. Terminology\nThe term misinformation in this paper is used speci\ufb01-\ncally in the context of politics as an umbrella term that\ncovers many aspects: (a) misinformation : any news\nthat is false or misleading but is not intended as such,\n(b)disinformation : any false or misleading information\nthat is spread with the speci\ufb01c intent of deception, (c)\nhyperpartisan news : news that might not be entirely\nfalse, but they are phrased in a way that satis\ufb01es a spe-\nci\ufb01c political agenda and (d) satirical news : any false\ncontent that has a humorous intent.\n3.2. Data Collection\nReddit2is an inexpensive source of high-quality data\n(Jamnik and Lane, 2017). On Reddit, registered users\ntend to submit posts with richer content than Twitter,\nthus we are able to gather enough context for each user.\nHaving enough users with rich contextual density is\nparticularly bene\ufb01cial for similarity assessment, which\n2https://www.reddit.com\nmakes it the primary choice as the source for collecting\ndisinformation spreaders and real news spreaders post\nhistories.\nThe data crawling was performed in a user-centric and\niterative fashion. To begin with, we manually compiled\na list of 65 subreddits regarding controversial political\ntopics that were commonly discussed before the elec-\ntions, such as general politics or the US presidential\nrace, the SARS-CoV-2 pandemic, women\u2019s and men\u2019s\nrights, climate change, vaccines, abortion, gun con-\ntrol, 5G in general. For each of those subreddits, the\nmost recent threads were crawled and inserted into a\ndatabase. On this data, we performed the \ufb01rst itera-\ntion of the URL domain-based disinformation and real\nnews spreader extraction to generate a list of Reddit\nuser accounts with equal amounts of users for either\nclass. We then collected the complete histories of all\nthe users in said list, thus gathering all threads in which\nthey participated in the list of political subreddits. All\nof those threads were inserted into the database from\nwhich, again, a now larger list of misinformation and\nreal news spreaders can be extracted. This process was\niterated until the dataset reached its current form.\nWe show the subreddits included in the resulting\ndataset and the corresponding number of unlabeled,\nreal and fake news posts they contain in Table 1. In\nthe parenthesis, we note the stance that each subreddit\nsupports in its description. For each topic, the subred-\ndits with a very low number of fake news posts, are\ngrouped in the rows named \u201cOther\u201d. In this table, the\ntopics are shown based on a descending number of total\nfake news posts, the same stands for the subreddits that\nbelong to them. For each topic, we opted for an equal\ndistribution of political partisanship and stances, by se-\nlecting the same number of the most popular subreddits\nfor each stance and for the same time period.\nAs we can see, the largest portion of unlabeled, fake\nand real news posts are from the subreddit r/politics\nwhich is a reddit with no speci\ufb01c political agenda for\ndiscussing the news regarding US politics. We can see\nthat the conservative party seems to be posting more\nfrequently based on the number of unlabeled posts.\nIn addition, all topics have a skewed distribution of\nstances.\n3.3. Media Domain Lists\nLikewise to the work of Baly et al. (2018), the website\nmediabiasfactcheck.com was used as the main source\nfor annotated news outlet domains. It was deemed a\nsuitable resource for the study at hand as it offers anno-\ntations for two dimensions: the factuality level and the\npolitical bias of a large proportion of high frequented\nonline news media.\nSince we also opted for a binary label for the disin-\nformation spreaders, we created a mapping for those\nlabels. To be considered a disinformation domain, the\nmediabiasfactcheck label has to be below or at Mixed\nfactuality level or labeled as satire, while the real newsSubreddit # unlabeled # real # fake\nGeneral political debate\nr/politics (no bias) 2.399.254 81.261 3.869\nr/Conservative (right) 346.042 5.165 2.784\nr/conservatives (right) 24.310 526 453\nr/Republican (right) 17.797 500 256\nr/ConservativesOnly (right) 9.431 57 62\nr/democrats (left) 11.747 338 41\nOther (mostly left) 72.135 2.355 81\nSARS-CoV-2\nr/NoNewNormal (anti) 72.411 1.941 1.387\nr/LockdownSkepticism (no bias) 62.480 1.441 275\nr/NoLockdownsNoMasks (anti) 1.887 82 61\nr/Coronavirus (no bias) 92.163 2.753 54\nOther (mostly no bias) 21.697 606 53\nWomen\u2019s and men\u2019s rights\nr/MensRights (men) 57.654 1.636 501\nr/Egalitarianism (non-speci\ufb01c) 83 4 42\nr/antifeminists (men) 1.138 44 15\nOther (mostly women) 1.399 47 11\nClimate change\nr/climateskeptics (questioning) 38.606 756 856\nr/climatechange (science) 7.858 622 153\nr/GlobalClimateChange (science) 26 2 0\nr/climate (science) 120 12 0\nVaccines\nr/DebateVaccines (no bias) 32.635 1.624 637\nr/DebateVaccine (no bias) 2.707 57 22\nr/TrueantiVaccination (anti) 3.428 48 18\nOther (mixed anti and pro) 7.255 225 16\nAbortion\nr/prolife (anti) 7.109 167 82\nr/Abortiondebate (no bias) 7.590 84 22\nOther (mostly pro) 5.228 84 4\nGuns\nr/progun (pro) 10.774 453 61\nr/Firearms (pro) 12.728 200 33\nr/GunsAreCool (pro) 4.930 233 27\nr/gunpolitics (no bias) 1.967 61 11\nr/guncontrol (anti) 1.062 206 10\nOther (mostly pro) 9.744 338 6\n5G\nr/5GDebate (no bias) 2.192 19 6\nTable 1: This table shows the names of the subreddits\nthat belong to each topic and the corresponding num-\nber of unlabeled, real and fake news posts. The rows\nnamed \u201cOther\u201d contain the subreddits with a low num-\nber of fake news posts for each topic.\ndomains have to be at least Mostly factual and between\nRight-Center andLeft-Center political bias.\nAs for the credibility of the assigned annotations, the\nmaintainers of mediabiasfactcheck.com state that they\n\u201care looking at political bias, how factual the informa-\ntion is, and links to credible, veri\ufb01able sources\u201d (medi-\nabiasfactcheck.com, 2021). In the description of their\nmethodology, they also describe that they base the la-\nbels on reviews of at least 10 headlines and 5 news sto-\nries (mediabiasfactcheck.com, 2021).\nDate Event Description\nFeb 5 Trump is acquitted on the charges of\nabuse of power and obstruction of\nCongress.\nJul 11 Mail-in votes are encouraged.\nJul 30 Donald Trump threatens to postpone the\nelection if it appears mail-in votes might\ngo against him. (We regard this as if this\nhad happened in August, since the ef-\nfects of this political event would be still\ndiscussed during that month)\nAug 11 Joe Biden chooses Senator Kamala Har-\nris (D-CA) as his running mate (event 1)\nNov 3 2020 United States elections (event 2)\nJan 6 US Capitol is attacked by supporters of\nTrump (event 3)\nFeb 24 Johnson & Johnson\u2019s vaccine candi-\ndate receives emergency use authoriza-\ntion from the FDA (event 4)\nTable 2: Major political events coinciding with the\npeaks observed in the number of fake and real news\nposts from Figure 2\nAs a further resource to extend the list of disinforma-\ntion media sources, an \u201cindex of fake-news, clickbait,\nand hate sites\u201d (Review, 2021) by the Columbia Jour-\nnalism Review3was consulted. Its curators state that it\nwas created by merging pre-existing fake news domain\nlists from various sources and then checking their ac-\ntual invalidity with the fact checking platforms Politi-\nFact and Snopes (Review, 2021). Finally, to ensure the\nquality of all annotations, we cross-matched the labels\nof the common domains by consulting both Snopes and\nMedia Bias/Fact Check.\nIn total, in this way, we aggregated 1577 disinformation\nand 571 real news domains for our ground truth and\npost-level annotations.\n3.4. Binary Annotation.\nThe users were annotated as misinformation spreaders\nandreal news spreaders based on the posted web-links\nin their history. More precisely, we \ufb01rst extracted news\nlinks from the users\u2019 posts using regular expression\nmatching. To decide whether the extracted link was\ncounted as misinformation or real news, its domain was\nmatched with the two lists of domains of online news\noutlets, each corresponding to one class. Users were\nthen labeled as misinformation spreaders if they had\nat least two detected misinformation links in their post\nhistory, while for being real news spreaders they had to\nhave no shared links from the misinformation list and\nat least one link posted from the factual news list.\n3https://www.cjr.org3.5. Fine-grained labels.\nIn addition to the binary separation of users into misin-\nformation spreaders and real news spreaders, each user\nwas annotated with the following factors by averag-\ning over a \ufb02oat mapping of the labels from mediabi-\nasfactcheck.com , for a more \ufb01ne-grained annotation.\nFactuality degree (fd). This factor represents the av-\nerage level of factuality of each author, and is also in\nthe range of [-3, +3] with each label corresponding to\ndifferent scales; very low ( svl=\u00003), low (slf=\u00002),\nmixed (smx=\u00001), mostly factual ( smf= +1 ), high\n(shf= +2 ), very high ( svh= +3 ). Similarly, the\nfactuality factor of each author is computed as follows:\nfd=P\n`s`\u0001N`P\n`N`\nwhereN`in the number of posts labeled as `2\n[vl;lf;mx;mf;hf;vh ]\nPolitical bias (pb). This factor represents the level of\npartisanship and is a number in the range of [-3, +3]\nwhere each of the labels correspond to different scales\n(s`); extreme left ( sel=\u00003), left (sl=\u00002), center left\n(scl=\u00001), least biased ( slb= 0), center right ( scr=\n+1), right (sr= +2 ), and extreme right ( ser= +3 ).\nThe political bias of each author is computed as:\npb=P\n`s`\u0001N`P\n`N`\nwhereN`in the number of posts labeled as `2\n[el;l;cl;lb;cr;r;er ]\nScience belief (sb). This factor quanti\ufb01es the level\nof belief in science and is a number in the range of\n[-1, +1] where each of the labels correspond to differ-\nent scales (s`); conspiracy theory article ( sc=\u00001),\nscience-based article ( ss= 1). Similarly, the science\nfactor of each author is computed as follows:\nsb=P\n`s`\u0001N`P\n`2fl\nwhereN`in the number of posts labeled as `2[s;c]\nSatire degree (sd). This factor represents the level of\nsatire in the fake news posts. The higher this factor is,\nthe less intentional the misinformation spreading. It is\nin the range of [0, 1] and is computed as the number\nof satire posts Nsdivided by the number of fake news\npostsNfn:\nsd=Ns\nNfn\nDiscussion. Current datasets for fake news spreaders\ndetection characterize a user as a fake news spreader\nbased on whether they posted more than nnumber of\nposts, which nbeing an arbitrary number around two\nor three. By introducing these \ufb01ne-grained labels we\npose some interesting questions to the research com-\nmunity. How many times should a user post about fake\nnews in order to be considered as a fake news spreader?\nShould it also depend on what kind of fake news post\nthey posted (e.g. a post from a pseudoscience source vs\npost from a source that has a mixed factuality report-\ning shouldn\u2019t have the same gravity). While satirical\nnews is fake, the intent is usually humorous, however\nthe dissemination of such news could be equally harm-\nful. Should users who post from these sources also be\nconsidered as fake news spreaders? Should we con-\nsider a threshold of factuality degree instead of count-\ning fake news posts to separate fake news posters and\nreal news spreaders?\n3.6. Dataset Statistics\nThe dataset comprises a total of 3.354.450 posts au-\nthored by 4,150 users with a class distribution of 74:26\nof real news and fake news spreaders respectively, col-\nlected from January 2020 to April 2021. Misinforma-\ntion spreaders had an average of 1240 posts, with this\ncount being at 654 for the real news spreaders. In to-\ntal, 2% of the posts contained links to real news media,\nwhile 0.3% pointed to domains from the misinforma-\ntion list.\nUsing the post-level annotations from Section 3.5, the\npolitical biases of the users can be looked at: 41.17% of\nthe users that have left wing political bias are misinfor-\nmation spreaders, while 58.82% of them are real news\nspreaders. 91.58% of the users that have right wing po-\nlitical bias are fake news spreaders, while only 8.41%\nof them are real news spreaders. Figure 1 depicts the\nfactuality factor over political bias of each user. While\nthere is an apparent correlation (Pearson correlation of\n-0.45) between the political bias and factuality of the\nusers, it is important to note that this effect is not an\nisolated case or a problem that rises from the process of\ncollecting our data, in fact, this phenomenon has been\nobserved by many researchers (Shrestha and Spezzano,\n2019) who show that there is indeed a high correla-\ntion between the perceived bias of a publisher and the\ntrustworthiness of news content. In addition, (Gar-\nrett and Bond, 2021) showed that US conservatives are\nuniquely susceptible to misinformation regarding the\npolitical events and generally political extremes (both\nthe left and the right) are substantially susceptible to\nconspiracy beliefs. Note that from Table 1, we can see\na higher posting activity from the right wing party com-\npared to the left wing, which leads us to the conclusion\nthat right-wing supporters might be more active in so-\ncial platforms compared to left-wing supporters.\nThe timestamps and thread structure of all stored posts\nis preserved in the dataset, in order to encourage a more\ncomprehensive analysis of the users and their posting\nbehavior. Figure 2 shows the number of fake news and\nreal news posted per month. We also provide a list of\npivotal political events4that happened during this time\n4https://en.wikipedia.org/wiki/2020 inUnited States\npolitics andgovernment,\nhttps://en.wikipedia.org/wiki/2021 intheUnited States\nExtrem e RightRight\nRight-Center Least BiasedLeft-CenterLeft\nExtrem e Left\nPolitical biasVery LowLowMixedMostly FactualHighVery HighFactuality levelReal news \nspreadersMisinform ation \nspreadersFigure 1: Factuality factor over political bias of each\nuser.\nJ a n\n 2 0\n'F e\nb  2\n0 'M a\nr  2\n0 'A p\nr  2\n0 'M a\ny  2\n0 'J u n\n 2 0\n'J u l\n 2 0\n'A u\ng  2\n0 'S e\np  2\n0 'O c\nt  2\n0 'N o\nv  2\n0 'D e\nc  2\n0 'J a n\n 2 1\n'F e\nb  2\n1 'M a\nr  2\n1 'A p\nr  2\n1 '01 0 0 02 0 0 03 0 0 0N u m b e r  o f  l a b e l e d  p o s t s\ne v e n t  1  \ne v e n t 2\ne v e n t  3  \ne v e n t  4  f a k e r e a l\nFigure 2: Number of fake news posts and real news\nposts associated with the political events from Table 2\nperiod in Table 2. We can see that these events coin-\ncide with the increase in the number of fake news and\nreal news posts. We can see an obvious increase in real\nnews right until the US elections and a sudden increase\nduring the attack on the Capitol. This is logical since\nthe elections were scheduled and discussed months be-\nfore they happened while the attack was an event that\ndeveloped over a few days. A smoother curve is ob-\nserved for the fake news, where the numbers do seem\nto \ufb02uctuate in the same manner during these events, but\nnot to the same degree.\n4. Encoding the Users\n4.1. Problem Formulation\nWe denote the user to be classi\ufb01ed as\nui2 U =fu1;u2;:::;uNg. Each user\nuiis associated with a posting history Hi=\nf(pi\n1;ti\n1);(pi\n2;ti\n2);:::; (pi\nLi;ti\nLi)gwherepi\nkis a text\nauthored by the user ui, posted at time ti\nkwhere\nti\n1< ti\n2<\u0001\u0001\u0001< ti\nLiandLiis the individual posting\nhistory length of each user ui.\nFake news spreader detection. For the following\nexperiments we utilize the binary labels introduced in\nSection 3.4. We therefore formulate the author pro\ufb01l-\ning problem as a binary classi\ufb01cation task to predict\nthe classyiof the user, where yi2fmisinformation\nspreader, real news spreader g.\nPolitical bias identi\ufb01cation. We utilize the \ufb01ne-\ngrained labels of the political bias introduced in Sec-\ntion 3.5. The left-wing supporters are the users with\npb <\u00000:5, while the right-wing supporters are those\nwithpb> 0:5. Accordingly, the identi\ufb01cation of parti-\nsanship is de\ufb01ned as a binary classi\ufb01cation task to pre-\ndict the class yiof the user, where yi2 fleft wing,\nright wingg.\n4.2. User representations\nBERT-based representations We use Sentence-\nBERT (SBERT) (Reimers and Gurevych, 2019) to ob-\ntain the embedding ei\nkof each user\u2019s individual histor-\nical postspi\nk. SBERT is a modi\ufb01cation of BERT that\nis speci\ufb01cally designed to produce semantically mean-\ningful sentence embeddings, and has achieved state-\nof-the-art performance on various challenging datasets\n(Agirre et al., 2012; Cer et al., 2017; Marelli et al.,\n2014), rendering this encoding method particularly\nsuitable for representing the users.\nWe want to encode the users\u2019 historical context Hiby\nobtaining their user representations Ei2Rdb. Lee et\nal. (2020) empirically showed that simple average sen-\ntence embeddings compare favorably to more complex\nmethods. Each user\u2019s historical encoding is averaged\nover the individual posting history length of each user\nLi, as:\nEi=1\nLiLiX\nk=1ei\nk\nUser2Vec In addition, we also adopt User2Vec (Amir\net al., 2016) to compute the initial user representation.\nEi2Rduof useruibased on their corresponding his-\ntorical postsHi, optimizing the conditional probability\nof texts given the author.\nEncoding the psycho-linguistic features In order to\nanalyze the relationship between users\u2019 tendency to\nspread fake news and their personality traits and men-\ntal processes, we use the Big Five Model and LIWC\nsoftware respectively. The two methodologies are de-\nscribed hereafter.\nThe Big Five Model (BFM) (John and Srivastava,\n1999) assumes that human personality can be summa-\nrized in \ufb01ve main aspects: (i) openness to experience ,\n(ii) conscientiousness , i.e. the interactions between ra-\ntional thought and instincts, (iii) agreeableness , or theintensity of individuals\u2019 reactions within the social con-\ntext, (iv) extraversion , and (v) emotional instability .\nAfter de\ufb01ning these basic dimensions, this approach\nargues for the existence of semantic associations be-\ntween them and speci\ufb01c sets of adjectives which are\nrecurrent in the natural language when describing in-\ndividuals\u2019 psychological traits. Accordingly, Neuman\nand Cohen (2014) derive a personality score with the\nfollowing process: for each factor, they compute the\nmean of all the cosine similarities between the embed-\nding representations5of every word in the input text\nand every benchmark adjective empirically observed as\nto be able to encode that precise personality trait; the\nhigher this average similarity, the greater the evidence\nof a given factor. Neuman and Cohen also included\n9 extra factors describing mental disorders, like para-\nnoia, and narcissism .\nThe Linguistic Inquiry and Word Count software\n(LIWC) (Pennebaker et al., 2015) applies a lexicon-\nbased method for mapping the text into 64 psycho-\nlinguistic categories de\ufb01ned to obtain evidences of\nmany mental processes underlying the natural lan-\nguage, and grouped into 2 macro-categories: (i) lin-\nguistic dimensions , i.e. function words, common verbs\nand adjectives, etc. and (ii) psychological processes\nof many kinds, including the affective, cognitive, and\nsocial type. In conclusion, the LIWC representation\nof one document consists in the set of relative fre-\nquencies for the categories, according to the number\nof words identi\ufb01ed in the text that are associated with\neach of them. Again, both psycho-linguistic encodings\nare achieved by an averaging operation over the post-\nlevel ones. In particular, it was preferred to extract the\nvalues of the LIWC features as means of the relative\nfrequencies at the post level in order to extract the av-\nerage incidence of each category within the single pub-\nlication, with the aim of avoiding that the calculations\nwere biased towards the most frequent classes within\nthe composition of the global user discourse.\n5. Methodology\n5.1. Graph construction\nSocial science argues that like-minded people tend to\ninteract more with each other (Bahns et al., 2017),\ntherefore we construct the social graph in a way that\ncaptures the users\u2019 social interactions with each other.\nWe de\ufb01ne as social interaction the replies and mentions\nin a post thread. For each thread of posts, we con-\nnect all the chain of replies to the root (i.e. the origi-\nnal post) of the conversation and all mentions/replies to\neach other. Following, these connections are translated\nto user connections in the social graph. This method\nis more clearly depicted in Figure 3. The social graph\nG= (V;E)is comprised by a set of user nodes Vand a\nset of edgesEbetween these users.\n5The word embeddings are produced by a Word2Vec ar-\nchitecture, pre-trained on the Google News Corpus.\nFigure 3: Transforming a post/reply tree in social me-\ndia into a social graph network.\n5.2. Graph Attention Network\nGraph neural networks are able to leverage the seman-\ntic and social relations between users (Wu et al., 2021).\nAs users have a different in\ufb02uence on one another, we\nneed to focus on users that have more relevant con-\nnections with higher in\ufb02uence. To model the gravity\nof the in\ufb02uences of the neighbourhood to a node, we\nuse Graph Attention Networks (GAT) (Veli \u02c7ckovi \u00b4c et\nal., 2018). GAT attends to the neighborhood of each\nuser and assigns an importance score to the connections\nthat contribute more to the detection of misinformation\nspreaders. The input to a GAT layer is a set of users em-\nbeddingsE=fE1;:::;ENgwhereN=jUj. A GAT\nlayer produces updated features, eE=ffE1;:::;gENg,\nwherefEi2Rdg. First, the GAT layer applies a shared\nlinear transformation by a weight matrix W2Rdg\u0002db.\nThen, we apply a shared self-attention mechanism to\neach nodei, using the neighbourhood N(i). The nor-\nmalized attention weight \u000bijbetween node iand neigh-\nbour nodejis computed as follows:\n\u000bij=exp(LeakyReLU (a|\nw[WEikWEj])P\nk2N(i)exp(LeakyReLU (a|\nw[WEikWEk])\n(1)\nwhere>represents the transpose and kis the concate-\nnation operation. aw2R2dg, is a trainable parameter\nvector. The attention weights \u000bijrepresent the impor-\ntance of relation from node ito nodej. To stabilize\nthe learning process, we employ a multi-head attention\n(Vaswani et al., 2017). We compute the output repre-\nsentation of a nodefEias follows:\nfEi=ReLU0\n@1\nKKX\nk=1X\nj2N(i)\u000bk\nijWkEj1\nA (2)\nwhere,Wkdenotes normalized attention weight and\nlinear transformation for k-th head.\nClassi\ufb01cation Layer The overall learned representa-\ntions for each user, are forwarded into a linear layer\nparameterized by a weight matrix Wo2Rdo\u0002dr. The\n\ufb01nal prediction is computed as:\n^y=softmax (Wo\u0000(h)): (3)Given the true label yfor a user, we use cross-entropy\nloss to calculate the loss Las follows:\nL=\u0000NX\ni=1yiln( ^yi) + (1\u0000yi)ln(1\u0000^yi): (4)\n6. Experiments\n6.1. Models used\nWe compare our graph-based model as described in\nSection 5, with a Support Vector Machine (SVM), Lo-\ngistic Regression (LogReg), and a Random Forest (Rn-\nFor) classi\ufb01er which are trained by using the following\nfeatures:\nUBERT : We use the SBERT embeddings of the docu-\nments averaged over the user\u2019s history as feature vec-\ntors, as described in Section 4.2.\nUser2Vec : To initialize the user feature vectors, we use\nUser2Vec over the vocabulary of each user during their\nhistory.\nPsycholing : We concatenate both LIWC and BFM fea-\ntures, to compute an initial feature vector for the users.\n6.2. Performance evaluation and ablation\nstudy\nTable 3 shows GAT\u2019s F1score on the Reddit dataset\nfor the fake news spreader detection task. We com-\npare the graph-based results by using different initial-\nization methods, namely UBERT, User2Vec, psycho-\nlinguistic, concatenation of User2Vec and psycho-\nlinguistic features, and random vectors. Interestingly,\nthe proposed model achieves the best performance by\nutilizing User2Vec, despite having lower dimensional-\nity than UBERT. This is mainly attributed to the fact\nthat User2Vec embeddings were obtained based based\non this dataset, while UBERT was pre-trained on a\ngeneral-use corpus. The psycho-linguistic features, on\ntheir own, perform rather poorly with GAT and con-\ncatenating them to User2Vec does not contribute to the\nperformance. However, the psycho-linguistic features\nperform comparably to UBERT in the non-neural base-\nlines, which is in line with the observations of Rashkin\net al. (2017).\nFake News Spreader Detection\nModel F1score\nGAT + User2Vec (200) 61.6%\nGAT + UBERT (768) 61.2%\nGAT + Psycholing (83) 53.6%\nGAT + User2Vec + Psycholing (283) 59.4%\nGAT + Random (200) 47.8%\nTable 3: Comparison of different user embeddings\ntechniques for the GAT model on the fake news\nspreader detection task. Reported values are the F1-\nscores over a 5-fold Cross Validation. Bold denotes the\nbest overall performance on the task.\nTable 4 shows the F1score of the baseline models\nfor both the the political bias and fake news spreader\ndetection tasks. For the political bias identi\ufb01cation\ntask, UBERT consistently obtains better results than\nUser2Vec, and achieves the best result with SVM. On\nthe other hand, for the Fake news spreader detection\ntask, we observe the reversed behavior. User2Vec\nconsistently obtains signi\ufb01cantly better results than\nUBERT, and achieves the best result with a Random\nForest classi\ufb01er.\nPolitical Bias Fake News Spreader\nModel UBERT User2Vec UBERT User2Vec\nSVM 66.2% 63.0% 53.9% 61.1%\nLogReg 64.7% 62.8% 58.6% 59.8%\nRnFor 64.9% 63.5% 49.7% 61.3%\nTable 4: Comparison of different user embeddings\ntechniques for the baseline models for both political\nbias and fake news spreaders detection. Reported val-\nues are theF1-scores over a 5-fold Cross Validation.\nBold denotes the best overall performance on the task.\nTable 5 shows the ablative results of the psycho-\nlinguistic features on the Reddit dataset for both politi-\ncal bias and fake news spreaders detection. In general,\npsycho-linguistic features show a signi\ufb01cantly higher\neffectiveness in distinguishing users on the basis of po-\nlitical bias. Detected mental processes appear to be sig-\nni\ufb01cantly more useful than personality factors: this re-\nsult is coherent with the study conducted through the\nLIWC software by Jordan et al. (2019) about the link\nbetween political ideology and language use. Most rel-\nevant mental process is the affective kind, which cor-\nrelates negatively with the target class, suggesting that\nright-biased users tend to express fewer emotions such\nas anxiety, anger and sadness in the text. As regards\nthe other task, the BFM encoding appears slightly more\neffective for identifying fake news spreaders. Indeed,\nsince personality regulates the behavior in real con-\ntexts, it is reasonable to assume it to be also in\ufb02u-\nential within virtual communities. The dominant fac-\ntor is here the openness to experience : as expected,\nin those who spread fake news, there is greater rejec-\ntion or less curiosity towards ideas outside their belief\nsystem. Also, the schizotypy disorder appears relevant,\nconsistent with previous empirical observations (Buck-\nels et al., 2018).\nWe note that the psycho-linguistic features are not\nadaptive to the tasks since they are lexicon-based,\ntherefore the embedding-based features achieve sig-\nni\ufb01cantly higher F1scores in the political bias detec-\ntion task. By comparing all results for the fake news\nspreader detection task, we observe that the GAT model\noutperforms all baselines. Therefore, the social inter-\nactions constitute a promising tool for predicting the\nbehavior of unseen users.\n7. Conclusion\nIn this study we introduce a new user-centered dataset\nfor misinformation spreader analysis, monitoring polit-Political Bias Fake News Spreader\nModel LIWC BFM Both LIWC BFM Both\nSVM 55.1% 38.8% 61.0% 56.2% 51.0% 53.9%\nLogReg 63.6 % 51.5% 63.9% 58.3 % 55.1% 58.3 %\nRnFor 56.6% 54.8 % 61.7% 55.9% 58.4% 54.8%\nTable 5: Ablation study over the psycho-linguistic fea-\ntures and their combination for both political bias and\nfake news spreaders detection. Reported values are the\naverageF1-scores over a 5-fold Cross Validation. Un-\nderlines denote the best result for the combination of\nfeatures considered, while bold denotes the best overall\nperformance on the task. \u2019Both\u2019 indicates the concate-\nnation of both representations.\nical discussions on Reddit since the beginning of 2020.\nWe create a dataset that contains over 4K users with\n3.4M Reddit posts, covering the time period before and\nafter the US presidential elections. Apart from the fake\nnews/real news distinction, the dataset contains \ufb01ne-\ngrained labels about the users\u2019 credibility level and po-\nlitical bias. As far as we are aware, this is the \ufb01rst\nfake news spreader dataset that simultaneously cap-\ntures both the long-term context of user\u2019s historical\nposts and the interactions between users. To create the\n\ufb01rst benchmark on our data, we provide methods for\nidentifying misinformation spreaders by utilizing the\nsocial connections between the users along with their\npsycho-linguistic features. In a subsequent analysis\nwe observe that social connections increase robustness\nover content features, that detecting affective mental\nprocesses correlates negatively with right-biased users,\nand that the openness to experience factor is lower for\nthose who spread fake news.\n8. Ethical Considerations and\nLimitations\nThe ability to automatically approximate personal char-\nacteristics of online users in order to improve natural\nlanguage classi\ufb01cation algorithms requires us to con-\nsider a range of ethical concerns. Researchers are ad-\nvised to take account of users\u2019 expectations (Shilton\nand Sayles, 2016; Townsend and Wallace, 2016) when\ncollecting public data such as Reddit. All user data is\nkept separately on protected servers, linked to the raw\ntext and network data only through anonymous IDs. In\naddition, any user-augmented classi\ufb01cation efforts risk\ninvoking harmful stereotyping, as the algorithm labels\npeople as misinformation spreaders. These can be em-\nphasized by the semblance of objectivity created by the\nuse of a computer algorithm (Koolen and van Cranen-\nburgh, 2017).\nAcknowledgements\nThis work has been supported by the German Federal\nMinistry of Education and Research (BMBF) as a part\nof the Junior AI Scientists program under the reference\n01-S20060. The work at the Universitat Polit `ecnica de\nVal`encia was in the framework of XAI-DisInfodemics:\neXplainable AI for disinformation and conspiracy de-\ntection during infodemics (PLEC2021-007681) funded\nby the Spanish Ministry of Science and Innovation,\nand IBERIFIER: Iberian digital media research and\nfact-checking hub (INEA/CEF/ICT/A202072381931,\nn. 2020-EU-IA-0252) funded by the European Digital\nMedia Observatory.\n9. Bibliographical References\nAgirre, E., Diab, M., Cer, D., and Gonzalez-Agirre, A.\n(2012). Semeval-2012 task 6: A pilot on semantic\ntextual similarity. In Proceedings of the First Joint\nConference on Lexical and Computational Seman-\ntics - Volume 1: Proceedings of the Main Conference\nand the Shared Task, and Volume 2: Proceedings of\nthe Sixth International Workshop on Semantic Eval-\nuation , SemEval \u201912, page 385\u2013393, USA. Associa-\ntion for Computational Linguistics.\nAmir, S., Wallace, B. C., Lyu, H., Carvalho, P., and\nSilva, M. J. (2016). Modelling context with user\nembeddings for sarcasm detection in social media.\nInProceedings of The 20th SIGNLL Conference on\nComputational Natural Language Learning , pages\n167\u2013177, Berlin, Germany, August. Association for\nComputational Linguistics.\nBahns, A., Crandall, C., Gillath, O., and Preacher, K.\n(2017). Similarity in relationships as niche construc-\ntion: Choice, stability, and in\ufb02uence within dyads\nin a free choice environment. Journal of Personality\nand Social Psychology , 11:329\u2013355, 02.\nBaly, R., Karadzhov, G., Alexandrov, D., Glass, J., and\nNakov, P. (2018). Predicting factuality of reporting\nand bias of news media sources. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing , pages 3528\u20133539, Brus-\nsels, Belgium, 10. Association for Computational\nLinguistics.\nBerinsky, A. J. (2017). Rumors and health care\nreform: Experiments in political misinformation.\nBritish Journal of Political Science , 47(2):241\u2013262.\nBuckels, E., Trapnell, P., Andjelovic, T., and Paulhus,\nD. (2018). Internet trolling and everyday sadism:\nParallel effects on pain perception and moral judg-\nment. Journal of Personality , 87, 04.\nCai, C., Li, L., and Zengi, D. (2017). Behavior en-\nhanced deep bot detection in social media. In 2017\nIEEE International Conference on Intelligence and\nSecurity Informatics (ISI) , pages 128\u2013130.\nCer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., and\nSpecia, L. (2017). Semeval-2017 task 1: Semantic\ntextual similarity multilingual and crosslingual fo-\ncused evaluation. Proceedings of the 11th Interna-\ntional Workshop on Semantic Evaluation (SemEval-\n2017) .\nCervero, R., Rosso, P., and Pasi, G. (2021). Pro\ufb01l-\ning fake news spreaders: Personality and visual in-\nformation matter. In Proc. 26th Int. Conf. on Ap-plications of Natural Language to Information Sys-\ntems, NLDB-2021 , pages 355\u2013363. Springer-Verlag,\nLNCS(12801), 06.\nCheng, Y . and Chen, Z. F. (2020). The in\ufb02uence\nof presumed fake news in\ufb02uence: Examining pub-\nlic support for corporate corrective response, media\nliteracy interventions, and governmental regulation.\nMass Communication and Society , 23(5):705\u2013729.\nDaelemans, W., Kestemont, M., Manjavacas, E., Pot-\nthast, M., Rangel, F., Rosso, P., Specht, G., Sta-\nmatatos, E., Stein, B., Tschuggnall, M., Wiegmann,\nM., and Zangerle, E. (2019). Overview of pan 2019:\nBots and gender pro\ufb01ling, celebrity pro\ufb01ling, cross-\ndomain authorship attribution and style change de-\ntection. In Fabio Crestani, et al., editors, Experimen-\ntal IR Meets Multilinguality, Multimodality, and In-\nteraction , pages 402\u2013416, Cham. Springer Interna-\ntional Publishing.\nFigueira, A. and Oliveira, L. (2017). The current\nstate of fake news: challenges and opportunities.\nProcedia Computer Science , 121:817\u2013825. CEN-\nTERIS 2017 - International Conference on ENTER-\nprise Information Systems / ProjMAN 2017 - In-\nternational Conference on Project MANagement /\nHCist 2017 - International Conference on Health and\nSocial Care Information Systems and Technologies,\nCENTERIS/ProjMAN/HCist 2017.\nGarrett, R. K. and Bond, R. M. (2021). Conserva-\ntives&#x2019; susceptibility to political mispercep-\ntions. Science Advances , 7(23):eabf1234.\nGiachanou, A., R \u00b4\u0131ssola, E. A., Ghanem, B., Crestani,\nF., and Rosso, P. (2020). The role of personal-\nity and linguistic patterns in discriminating between\nfake news spreaders and fact checkers. In Elisabeth\nM\u00b4etais, et al., editors, Natural Language Process-\ning and Information Systems , pages 181\u2013192, Cham.\nSpringer International Publishing.\nGirlea, C., Girju, R., and Amir, E. (2016). Psycholin-\nguistic features for deceptive role detection in were-\nwolf. In Proceedings of the 2016 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 417\u2013422, San Diego, California, 06. As-\nsociation for Computational Linguistics.\nGlenski, M., Weninger, T., and V olkova, S. (2018).\nIdentifying and understanding user reactions to de-\nceptive and trusted social news sources. CoRR ,\nabs/1805.12032.\nHurtado, S., Ray, P., and Marculescu, R. (2019). Bot\ndetection in reddit political discussion. In Proceed-\nings of the Fourth International Workshop on Social\nSensing , SocialSense\u201919, page 30\u201335, New York,\nNY , USA. Association for Computing Machinery.\nJamnik, M. R. and Lane, D. J. (2017). The use\nof reddit as an inexpensive source for high-quality\ndata. Practical Assessment, Research and Evalua-\ntion, 22:5.\nJohn, O. P. and Srivastava, S. (1999). The big \ufb01ve\ntrait taxonomy: History, measurement, and theoreti-\ncal perspectives. In Pervin, L.A. and John, O.P . Eds.,\nHandbook of Personality: Theory and Research, Vol.\n2, pages 102\u2013138, New York. Guilford Press.\nJordan, K. N., Sterling, J., Pennebaker, J. W., and Boyd,\nR. L. (2019). Examining long-term trends in poli-\ntics and culture through language of political lead-\ners and cultural institutions. Proceedings of the Na-\ntional Academy of Sciences , 116:3476 \u2013 3481.\nKacupaj, E., Plepi, J., Singh, K., Thakkar, H.,\nLehmann, J., and Maleshkova, M. (2021). Conver-\nsational question answering over knowledge graphs\nwith transformer and graph attention networks. In\nProceedings of the 16th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Main Volume , pages 850\u2013862, Online,\nApril. Association for Computational Linguistics.\nKoolen, C. and van Cranenburgh, A. (2017). These\nare not the stereotypes you are looking for: Bias and\nfairness in authorial gender attribution. In Proceed-\nings of the First ACL Workshop on Ethics in Natural\nLanguage Processing , pages 12\u201322, Valencia, Spain,\nApril. Association for Computational Linguistics.\nKosmajac, D. and Keselj, V . (2020). Twitter user pro-\n\ufb01ling: Bot and gender identi\ufb01cation - notebook for\nPAN at CLEF 2019. In Avi Arampatzis, et al., ed-\nitors, Experimental IR Meets Multilinguality, Multi-\nmodality, and Interaction - 11th International Con-\nference of the CLEF Association, CLEF 2020, Thes-\nsaloniki, Greece, September 22-25, 2020, Proceed-\nings, volume 12260 of Lecture Notes in Computer\nScience , pages 141\u2013153. Springer.\nLee, J. H., Collados, J. C., Anke, L. E., and Schockaert,\nS. (2020). Capturing word order in averaging based\nsentence embeddings. In ECAI .\nMarelli, M., Menini, S., Baroni, M., Bentivogli, L.,\nBernardi, R., and Zamparelli, R. (2014). A SICK\ncure for the evaluation of compositional distribu-\ntional semantic models. In Proceedings of the Ninth\nInternational Conference on Language Resources\nand Evaluation (LREC\u201914) , pages 216\u2013223, Reyk-\njavik, Iceland, May. European Language Resources\nAssociation (ELRA).\nMcKay, S. and Tenove, C. (2020). Disinformation as a\nthreat to deliberative democracy. Political Research\nQuarterly , 0(0):1065912920938143.\nmediabiasfactcheck.com. (2021). me-\ndiabiasfactcheck.com. https:\n//mediabiasfactcheck.com/\nmethodology/ . Accessed: 2021-08-10.\nMonti, F., Frasca, F., Eynard, D., Mannion, D., and\nBronstein, M. M. (2019). Fake news detection on\nsocial media using geometric deep learning. CoRR ,\nabs/1902.06673.\nMu, Y . and Aletras, N. (2020). Identifying twitter\nusers who repost unreliable news sources with lin-\nguistic information. PeerJ Comput. Sci. , 6:e325.\nNeuman, Y . and Cohen, Y . (2014). A vectorial seman-tics approach to personality assessment. Scienti\ufb01c\nreports , 4:4761, 04.\nNyhan, B. and Rei\ufb02er, J. (2010). When corrections\nfail: The persistence of political misperceptions. Po-\nlitical Behavior , 32(2):303\u2013330.\nOsmundsen, M., Bor, A., Vahlstrup, P. B., Bechmann,\nA., and Petersen, M. B. (2021). Partisan polariza-\ntion is the primary psychological motivation behind\npolitical fake news sharing on twitter. American Po-\nlitical Science Review , 115(3):999\u20131015.\nPennebaker, J. W., Boyd, R., Jordan, K., and Black-\nburn, K. (2015). The development and psychomet-\nric properties of LIWC2015 . University of Texas at\nAustin.\nP\u00b4erez-Rosas, V ., Kleinberg, B., Lefevre, A., and Mihal-\ncea, R. (2018). Automatic detection of fake news.\nInCOLING .\nPlepi, J. and Flek, L. (2021). Perceived and intended\nsarcasm detection with graph attention networks. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021 , pages 4746\u20134753.\nRangel, F. M., Giachanou, A., Ghanem, B., and Rosso,\nP. (2020). Overview of the 8th author pro\ufb01ling task\nat PAN 2020: Pro\ufb01ling fake news spreaders on twit-\nter. In Linda Cappellato, et al., editors, Working\nNotes of CLEF 2020 - Conference and Labs of the\nEvaluation Forum, Thessaloniki, Greece, September\n22-25, 2020 , volume 2696 of CEUR Workshop Pro-\nceedings . CEUR-WS.org.\nRashkin, H., Choi, E., Jang, J. Y ., V olkova, S., and\nChoi, Y . (2017). Truth of varying shades: Analyzing\nlanguage in fake news and political fact-checking.\nInProceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing , pages\n2931\u20132937, Copenhagen, Denmark, September. As-\nsociation for Computational Linguistics.\nRedlawsk, D. P., Civettini, A. J. W., and Emmerson,\nK. M. (2010). The affective tipping point: Do moti-\nvated reasoners ever \u201cget it\u201d? Political Psychology ,\n31(4):563\u2013593.\nReimers, N. and Gurevych, I. (2019). Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP) , pages\n3982\u20133992, Hong Kong, China, November. Associ-\nation for Computational Linguistics.\nRen, Y . and Zhang, J. (2020). Hgat: Hierarchical\ngraph attention network for fake news detection.\nArXiv , abs/2002.04397.\nReview, C. J. (2021). Cjr index of fake-news, click-\nbait, and hate sites. https://www.cjr.org/\nfake-beta . Accessed: 2021-08-10.\nSawhney, R., Joshi, H., Shah, R. R., and Flek,\nL. (2021). Suicide ideation detection via social\nand temporal user representations using hyperbolic\nlearning. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies , pages 2176\u20132190, Online, June. Associa-\ntion for Computational Linguistics.\nShilton, K. and Sayles, S. (2016). \u201d we aren\u2019t all going\nto be on the same page about ethics\u201d: Ethical prac-\ntices and challenges in research on digital and social\nmedia. In 2016 49th Hawaii International Confer-\nence on System Sciences (HICSS) , pages 1909\u20131918.\nIEEE.\nShrestha, A. and Spezzano, F. (2019). Online mis-\ninformation: From the deceiver to the victim. In\nProceedings of the 2019 IEEE/ACM International\nConference on Advances in Social Networks Analy-\nsis and Mining , ASONAM \u201919, page 847\u2013850, New\nYork, NY , USA. Association for Computing Ma-\nchinery.\nShu, K., Sliva, A., Wang, S., Tang, J., and Liu, H.\n(2017). Fake news detection on social media: A data\nmining perspective. CoRR , abs/1708.01967.\nShu, K., Wang, S., and Liu, H. (2018). Understanding\nuser pro\ufb01les on social media for fake news detec-\ntion. In 2018 IEEE Conference on Multimedia In-\nformation Processing and Retrieval (MIPR) , pages\n430\u2013435.\nSwire, B., Ecker, U., and Lewandowsky, S. (2017).\nThe role of familiarity in correcting inaccurate in-\nformation. Journal of Experimental Psychology:\nLearning, Memory, and Cognition , 43(12):1948\u2013\n1961, December.\nTausczik, Y . and Pennebaker, J. (2010). The psycho-\nlogical meaning of words: Liwc and computerized\ntext analysis methods. Journal of Language and So-\ncial Psychology , 29:24\u201354, 03.\nTownsend, L. and Wallace, C. (2016). Social media\nresearch: A guide to ethics. University of Aberdeen ,\n1:16.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,\nJones, L., Gomez, A. N., Kaiser, L., and Polo-\nsukhin, I. (2017). Attention is all you need. CoRR ,\nabs/1706.03762.\nVeli\u02c7ckovi \u00b4c, P., Cucurull, G., Casanova, A., Romero, A.,\nLi`o, P., and Bengio, Y . (2018). Graph attention net-\nworks.\nVisentin, M., Pizzi, G., and Pichierri, M. (2019).\nFake news, real problems for brands: The impact of\ncontent truthfulness and source credibility on con-\nsumers\u2019 behavioral intentions toward the advertised\nbrands. Journal of Interactive Marketing , 45:99\u2013\n112.\nVivitha Vijayan, S. G. (2019). A survey on author pro-\n\ufb01ling techniques. International Journal of Computer\nSciences and Engineering , 7:1065\u20131069, 3.\nV o, N. and Lee, K. (2019). Learning from fact-\ncheckers: Analysis and generation of fact-checking\nlanguage. In Proceedings of the 42nd International\nACM SIGIR Conference on Research and Devel-\nopment in Information Retrieval , SIGIR\u201919, page335\u2013344, New York, NY , USA. Association for\nComputing Machinery.\nWu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and\nYu, P. S. (2021). A comprehensive survey on graph\nneural networks. IEEE Transactions on Neural Net-\nworks and Learning Systems , 32(1):4\u201324, Jan.\nZellers, R., Holtzman, A., Rashkin, H., Bisk, Y .,\nFarhadi, A., Roesner, F., and Choi, Y . (2019).\nDefending against neural fake news. CoRR ,\nabs/1905.12616.\nZhou, X. and Zafarani, R. (2020). A survey of fake\nnews: Fundamental theories, detection methods, and\nopportunities. ACM Comput. Surv. , 53(5), Septem-\nber.\nZhou, X., Jain, A., Phoha, V . V ., and Zafarani, R.\n(2020). Fake news early detection. Digital Threats:\nResearch and Practice , 1:1 \u2013 25.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Factoid: A new dataset for identifying misinformation spreaders and political bias", "author": ["F Sakketou", "J Plepi", "R Cervero", "HJ Geiss"], "pub_year": "2022", "venue": "arXiv preprint arXiv \u2026", "abstract": "Proactively identifying misinformation spreaders is an important step towards mitigating the  impact of fake news on our society. In this paper, we introduce a new contemporary Reddit"}, "filled": false, "gsrank": 10, "pub_url": "https://arxiv.org/abs/2205.06181", "author_id": ["GOKQnfMAAAAJ", "", "", "E2hXC24AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:6aM80b35IBAJ:scholar.google.com/&output=cite&scirp=9&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=6aM80b35IBAJ&ei=BLWsaNaSEPnSieoPxKLpgQ0&json=", "num_citations": 24, "citedby_url": "/scholar?cites=1162203297516135401&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:6aM80b35IBAJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2205.06181"}}, {"title": "Predicting factuality of reporting and bias of news media sources", "year": "2018", "pdf_data": "arXiv:1810.01765v1  [cs.IR]  2 Oct 2018Predicting Factuality of Reporting and Bias of News Media So urces\nRamy Baly1, Georgi Karadzhov3, Dimitar Alexandrov3, James Glass1, Preslav Nakov2\n1MIT Computer Science and Arti\ufb01cial Intelligence Laborator y, MA, USA\n2Qatar Computing Research Institute, HBKU, Qatar;\n3So\ufb01a University, Bulgaria\n{baly, glass}@mit.edu, pnakov@qf.org.qa\n{georgi.m.karadjov, Dimityr.Alexandrov}@gmail.com\nAbstract\nWe present a study on predicting the factual-\nity of reporting and bias of news media. While\nprevious work has focused on studying the ve-\nracity of claims or documents, here we are in-\nterested in characterizing entire news media.\nThese are under-studied but arguably impor-\ntant research problems, both in their own right\nand as a prior for fact-checking systems. We\nexperiment with a large list of news websites\nand with a rich set of features derived from\n(i) a sample of articles from the target news\nmedium, ( ii) its Wikipedia page, ( iii) its Twit-\nter account, ( iv) the structure of its URL, and\n(v) information about the Web traf\ufb01c it attracts.\nThe experimental results show sizable perfor-\nmance gains over the baselines, and con\ufb01rm\nthe importance of each feature type.\n1 Introduction\nThe rise of social media has democratized con-\ntent creation and has made it easy for everybody\nto share and spread information online. On the\npositive side, this has given rise to citizen journal-\nism, thus enabling much faster dissemination of\ninformation compared to what was possible with\nnewspapers, radio, and TV . On the negative side,\nstripping traditional media from their gate-keeping\nrole has left the public unprotected against the\nspread of misinformation, which could now travel\nat breaking-news speed over the same democratic\nchannel. This has given rise to the proliferation\nof false information that is typically created ei-\nther ( a) to attract network traf\ufb01c and gain \ufb01nan-\ncially from showing online advertisements, e.g., as\nis the case of clickbait , or ( b) to affect individual\npeople\u2019s beliefs, and ultimately to in\ufb02uence major\nevents such as political elections ( V osoughi et al. ,\n2018 ). There are strong indications that false in-\nformation was weaponized at an unprecedented\nscale during the 2016 U.S. presidential campaign.\u201cFake news\u201d, which can be de\ufb01ned as \u201cfabri-\ncated information that mimics news media con-\ntent in form but not in organizational process or\nintent\u201d ( Lazer et al. ,2018 ), became the word of\nthe year in 2017, according to Collins Dictio-\nnary. \u201cFake news\u201d thrive on social media thanks\nto the mechanism of sharing, which ampli\ufb01es ef-\nfect. Moreover, it has been shown that \u201cfake news\u201d\nspread faster than real news ( V osoughi et al. ,\n2018 ). As they reach the same user several times,\nthe effect is that they are perceived as more cred-\nible, unlike old-fashioned spam that typically dies\nthe moment it reaches its recipients. Naturally,\nlimiting the sharing of \u201cfake news\u201d is a major fo-\ncus for social media such as Facebook and Twitter.\nAdditional efforts to combat \u201cfake news\u201d have\nbeen led by fact-checking organizations such as\nSnopes, FactCheck and Politifact, which manu-\nally verify claims. Unfortunately, this is inef\ufb01cient\nfor several reasons. First, manual fact-checking is\nslow and debunking false information comes too\nlate to have any signi\ufb01cant impact. At the same\ntime, automatic fact-checking lags behind in terms\nof accuracy, and it is generally not trusted by hu-\nman users. In fact, even when done by reputable\nfact-checking organizations, debunking does little\nto convince those who already believe in false in-\nformation.\nA third, and arguably more promising, way\nto \ufb01ght \u201cfake news\u201d is to focus on their source.\nWhile \u201cfake news\u201d are spreading primarily on so-\ncial media, they still need a \u201chome\u201d, i.e., a website\nwhere they would be posted. Thus, if a website is\nknown to have published non-factual information\nin the past, it is likely to do so in the future. Ver-\nifying the reliability of the source of information\nis one of the basic tools that journalists in tradi-\ntional media use to verify information. It is also\narguably an important prior for fact-checking sys-\ntems ( Popat et al. ,2017 ;Nguyen et al. ,2018 ).\nFact-checking organizations have been producing\nlists of unreliable online news sources, but these\nare incomplete and get outdated quickly. There-\nfore, there is a need to predict the factuality of re-\nporting for a given online medium automatically,\nwhich is the focus of the present work. We further\nstudy the bias of the source (left vs. right), as the\ntwo problems are inter-connected, e.g., extreme-\nleft and extreme-right websites tend to score low\nin terms of factual reporting. Our contributions\ncan be summarized as follows:\n\u2022We focus on an under-explored but arguably\nvery important problem: predicting the factu-\nality of reporting of a news medium. We fur-\nther study bias, which is also under-explored.\n\u2022We create a new dataset of news media\nsources, which has annotations for both tasks,\nand is 1-2 orders of magnitude larger than\nwhat was used in previous work. We release\nthe dataset and our code, which should facil-\nitate future research.1\n\u2022We use a variety of sources such as ( i) a\nsample of articles from the target website,\n(ii) its Wikipedia page, ( iii) its Twitter ac-\ncount, ( iv) the structure of its URL, and ( v) in-\nformation about the Web traf\ufb01c it has at-\ntracted. This combination, as well as some\nof the sources, are novel for these problems.\n\u2022We further perform an ablation study of the\nimpact of the individual (groups of) features.\nThe remainder of this paper is organized as fol-\nlows: Section 2provides an overview of related\nwork. Section 3describes our method and fea-\ntures. Section 4presents the data, the experiments,\nand the evaluation results. Finally, Section 5con-\ncludes with some directions for future work.\n2 Related Work\nJournalists, online users, and researchers are well-\naware of the proliferation of false information, and\nthus topics such as credibility and fact-checking\nare becoming increasingly important. For exam-\nple, the ACM Transactions on Information Sys-\ntems journal dedicated, in 2016, a special issue on\nTrust and Veracity of Information in Social Media\n(Papadopoulos et al. ,2016 ).\n1The data and the code are at\nhttps://github.com/ramybaly/News-Media-Reliability/There have also been some related shared tasks\nsuch as the SemEval-2017 task 8 on Rumor De-\ntection ( Derczynski et al. ,2017 ), the CLEF-2018\nlab on Automatic Identi\ufb01cation and Veri\ufb01cation\nof Claims in Political Debates ( Atanasova et al. ,\n2018 ;Barr\u00f3n-Cede\u00f1o et al. ,2018 ;Nakov et al. ,\n2018 ), and the FEVER-2018 task on Fact Extrac-\ntion and VERi\ufb01cation ( Thorne et al. ,2018 ).\nThe interested reader can learn more about\n\u201cfake news\u201d from the overview by Shu et al.\n(2017 ), which adopted a data mining perspective\nand focused on social media. Another recent sur-\nvey was run by Thorne and Vlachos (2018 ), which\ntook a fact-checking perspective on \u201cfake news\u201d\nand related problems. Yet another survey was\nperformed by Li et al. (2016 ), covering truth dis-\ncovery in general. Moreover, there were two re-\ncent articles in Science :Lazer et al. (2018 ) of-\nfered a general overview and discussion on the sci-\nence of \u201cfake news\u201d, while V osoughi et al. (2018 )\nfocused on the process of proliferation of true\nand false news online. In particular, they ana-\nlyzed 126K stories tweeted by 3M people more\nthan 4.5M times, and con\ufb01rmed that \u201cfake news\u201d\nspread much wider than true news.\nVeracity of information has been studied at dif-\nferent levels: ( i) claim-level (e.g., fact-checking ),\n(ii) article-level (e.g., \u201cfake news\u201d detection ),\n(iii) user-level (e.g., hunting for trolls ), and\n(iv) medium-level (e.g., source reliability estima-\ntion). Our primary interest here is in the latter.\n2.1 Fact-Checking\nAt the claim-level, fact-checking and ru-\nmor detection have been primarily ad-\ndressed using information extracted from\nsocial media, i.e., based on how users com-\nment on the target claim ( Canini et al. ,\n2011 ;Castillo et al. ,2011 ;Ma et al. ,2015 ,\n2016 ;Zubiaga et al. ,2016 ;Ma et al. ,2017 ;\nDungs et al. ,2018 ;Kochkina et al. ,2018 ).\nThe Web has also been used as a source of\ninformation ( Mukherjee and Weikum ,2015 ;\nPopat et al. ,2016 ,2017 ;Karadzhov et al. ,2017b ;\nMihaylova et al. ,2018 ;Baly et al. ,2018 ).\nIn both cases, the most important information\nsources are stance (does a tweet or a news article\nagree or disagree with the claim?), and source re-\nliability (do we trust the user who posted the tweet\nor the medium that published the news article?).\nOther important sources are linguistic expression,\nmeta information, and temporal dynamics.\n2.2 Stance Detection\nStance detection has been addressed as a task in\nits own right, where models have been devel-\noped based on data from the Fake News Chal-\nlenge ( Riedel et al. ,2017 ;Thorne et al. ,2017 ;\nMohtarami et al. ,2018 ;Hanselowski et al. ,2018 ),\nor from SemEval-2017 Task 8 ( Derczynski et al. ,\n2017 ;Dungs et al. ,2018 ;Zubiaga et al. ,2018 ). It\nhas also been studied for other languages such as\nArabic ( Darwish et al. ,2017b ;Baly et al. ,2018 ).\n2.3 Source Reliability Estimation\nUnlike stance detection, the problem of source\nreliability remains largely under-explored. In\nthe case of social media, it concerns modeling\nthe user2who posted a particular message/tweet,\nwhile in the case of the Web, it is about the trust-\nworthiness of the source (the URL domain, the\nmedium). The latter is our focus in this paper.\nIn previous work, the source reliability of news\nmedia has often been estimated automatically\nbased on the general stance of the target medium\nwith respect to known manually fact-checked\nclaims, without access to gold labels about\nthe overall medium-level factuality of report-\ning ( Mukherjee and Weikum ,2015 ;Popat et al. ,\n2016 ,2017 ,2018 ). The assumption is that reliable\nmedia agree with true claims and disagree with\nfalse ones, while for unreliable media it is mostly\nthe other way around. The trustworthiness of Web\nsources has also been studied from a Data Analyt-\nics perspective. For instance, Dong et al. (2015 )\nproposed that a trustworthy source is one that con-\ntains very few false facts. In this paper, we follow\na different approach by studying the source relia-\nbility as a task in its own right, using manual gold\nannotations speci\ufb01c for the task.\nNote that estimating the reliability of a source\nis important not only when fact-checking a\nclaim ( Popat et al. ,2017 ;Nguyen et al. ,2018 ),\nbut it also gives an important prior when solv-\ning article-level tasks such as \u201cfake news\u201d and\nclick-bait detection ( Brill,2001 ;Finberg et al. ,\n2002 ;Hardalov et al. ,2016 ;Karadzhov et al. ,\n2User modeling in social media and news community fo-\nrums has focused on \ufb01nding malicious users such as opinion\nmanipulation trolls , paid ( Mihaylov et al. ,2015b ) or just per-\nceived ( Mihaylov et al. ,2015a ;Mihaylov and Nakov ,2016 ;\nMihaylov et al. ,2018 ;Mihaylova et al. ,2018 ),sockpuppets\n(Maity et al. ,2017 ),Internet water army (Chen et al. ,2013 ),\nandseminar users (Darwish et al. ,2017a ).2017a ;De Sarkar et al. ,2018 ;Pan et al. ,2018 ;\nP\u00e9rez-Rosas et al. ,2018 ).\n2.4 \u201cFake News\u201d Detection\nMost work on \u201cfake news\u201d detection has relied on\nmedium-level labels, which were then assumed to\nhold for all articles from that source.\nHorne and Adali (2017 ) analyzed three small\ndatasets ranging from a couple of hundred to a few\nthousand articles from a couple of dozen sources,\ncomparing ( i) real news vs. ( ii) \u201cfake news\u201d vs.\n(iii) satire, and found that the latter two have a lot\nin common across a number of dimensions. They\ndesigned a rich set of features that analyze the text\nof a news article, modeling its complexity, style,\nand psychological characteristics. They found that\n\u201cfake news\u201d pack a lot of information in the title\n(as the focus is on users who do not read beyond\nthe title), and use shorter, simpler, and repetitive\ncontent in the body (as writing fake information\ntakes a lot of effort). Thus, they argued that the\ntitle and the body should be analyzed separately.\nIn follow-up work, Horne et al. (2018b ) created\na large-scale dataset covering 136K articles from\n92 sources from opensources.co , which they\ncharacterize based on 130 features from seven cat-\negories: structural, sentiment, engagement, topic-\ndependent, complexity, bias, and morality. We use\nthis set of features when analyzing news articles.\nIn yet another follow-up work, Horne et al.\n(2018a ) trained a classi\ufb01er to predict whether a\ngiven news article is coming from a reliable or\nfrom an unreliable (\u201c fake news \u201d or conspiracy )3\nsource. Note that they assumed that all news from\na given website would share the same reliability\nclass. Such an assumption is \ufb01ne for training (dis-\ntant supervision), but we \ufb01nd it problematic for\ntesting, where we believe manual documents-level\nlabels are needed.\nPotthast et al. (2018 ) used 1,627 articles from\nnine sources, whose factuality has been manu-\nally veri\ufb01ed by professional journalists from Buz-\nzFeed. They applied stylometric analysis, which\nwas originally designed for authorship veri\ufb01ca-\ntion, to predict factuality (fake vs. real).\nRashkin et al. (2017 ) focused on the language\nused by \u201cfake news\u201d and compared the prevalence\nof several features in articles coming from trusted\nsources vs. hoaxes vs. satire vs. propaganda.\n3We show in parentheses, the labels from\nopensources.co that are used to de\ufb01ne a category.\nHowever, their linguistic analysis and their auto-\nmatic classi\ufb01cation were at the article level and\nthey only covered eight news media sources.\nUnlike the above work, ( i)we perform classi\ufb01-\ncation at the news medium level rather than fo-\ncusing on an individual article. Thus, ( ii) we use\nreliable manually-annotated labels as opposed to\nnoisy labels resulting from projecting the cate-\ngory of a news medium to all news articles pub-\nlished by this medium (as most of the work above\ndid).4Moreover, ( iii) we use a much larger set\nof news sources, namely 1,066, which is 1-2 or-\nders of magnitude larger than what was used in\nprevious work. Furthermore, ( iv) we use a larger\nnumber of features and a wider variety of feature\ntypes compared to the above work, including fea-\ntures extracted from knowledge sources that have\nbeen largely neglected in the literature so far such\nas information from Wikipedia and the structure\nof the medium\u2019s URL.\n2.5 Media Bias Detection\nAs we mentioned above, bias was used as a\nfeature for \u201cfake news\u201d detection ( Horne et al. ,\n2018b ). It has also been the target of classi\ufb01ca-\ntion, e.g., Horne et al. (2018a ) predicted whether\nan article is biased ( political orbias) vs. unbiased.\nSimilarly, Potthast et al. (2018 ) classi\ufb01ed the bias\nin a target article as ( i) left vs. right vs. main-\nstream, or as ( ii) hyper-partisan vs. mainstream.\nFinally, Rashkin et al. (2017 ) studied propaganda,\nwhich can be seen as extreme bias. See also a\nrecent position paper ( Pitoura et al. ,2018 ) and an\noverview on bias the Web ( Baeza-Yates ,2018 ).\nUnlike the above work, we focus on bias at the\nmedium level rather than at the article level. More-\nover, we work with \ufb01ne-grained labels on an ordi-\nnal scale rather then having a binary setup (some\nwork above had three degrees of bias, while we\nhave seven).\n3 Method\nIn order to predict the factuality of reporting and\nthe bias for a given news medium, we collect in-\nformation from multiple relevant sources, which\nwe use to train a classi\ufb01er. In particular, we col-\nlect a rich set of features derived from ( i) a sample\nof articles from the target news medium, ( ii) its\n4Two notable exceptions are ( Potthast et al. ,2018 ) and\n(P\u00e9rez-Rosas et al. ,2018 ), who use news articles whose fac-\ntuality has been manually checked and annotated.Wikipedia page if it exists, ( iii) its Twitter account\nif it exists, ( iv) the structure of its URL, and ( v) in-\nformation about the Web traf\ufb01c it has attracted.\nWe describe each of these sources below.\nArticles We argue that analysis (textual, syntac-\ntic and semantic) of the content of the news arti-\ncles published by a given target medium should be\ncritical for assessing the factuality of its reporting,\nas well as of its potential bias. Towards this goal,\nwe borrow a set of 141 features that were previ-\nously proposed for detecting \u201cfake news\u201d articles\n(Horne et al. ,2018b ), as we have described above.\nThese features are used to analyze the following\narticle characteristics:\n\u2022Structure : POS tags, linguistic features\nbased on the use of speci\ufb01c words (func-\ntion words, pronouns, etc.), and fea-\ntures for clickbait title classi\ufb01cation from\n(Chakraborty et al. ,2016 );\n\u2022Sentiment : sentiment scores using lexicons\n(Recasens et al. ,2013 ;Mitchell et al. ,2013 )\nand full systems ( Hutto and Gilbert ,2014 );\n\u2022Engagement : number of shares, reactions,\nand comments on Facebook;\n\u2022Topic : lexicon features to differentiate be-\ntween science topics and personal concerns;\n\u2022Complexity : type-token ratio, readability,\nnumber of cognitive process words (identify-\ning discrepancy, insight, certainty, etc.);\n\u2022Bias: features modeling bias us-\ning lexicons ( Recasens et al. ,2013 ;\nMukherjee and Weikum ,2015 ) and sub-\njectivity as calculated using pre-trained\nclassi\ufb01ers ( Horne et al. ,2017 );\n\u2022Morality : features based on the Moral Foun-\ndation Theory ( Graham et al. ,2009 ) and lex-\nicons ( Lin et al. ,2017 )\nFurther details are available in ( Horne et al. ,\n2018b ). For each target medium, we retrieve some\narticles, then we calculate these features separately\nfor the title and for the body of each article, and\n\ufb01nally we average the values of the 141 features\nover the set of retrieved articles.\nWikipedia We further leverage Wikipedia as an\nadditional source of information that can help pre-\ndict the factuality of reporting and the bias of a\ntarget medium. For example, the absence of a\nWikipedia page may indicate that a website is not\ncredible. Also, the content of the page might ex-\nplicitly mention that a certain website is satirical,\nleft-wing, or has some property related to our task.\nAccordingly, we extract the following features:\n\u2022Has Page: indicates whether the target\nmedium has a Wikipedia page;\n\u2022Vector representation for each of the follow-\ning segments of the Wikipedia page, when-\never applicable: Content ,Infobox ,Summary ,\nCategories , and Table of Contents . We gen-\nerate these representations by averaging the\nword embeddings (pretrained word2vec em-\nbeddings) of the corresponding words.\nTwitter Given the proliferation of social media,\nmost news media have Twitter accounts, which\nthey use to reach out to more users online. The\ninformation that can be extracted from a news\nmedium\u2019s Twitter pro\ufb01le can be valuable for our\ntasks. In particular, we use the following features:\n\u2022Has Account: Whether the medium has a\nTwitter account. We check this based on the\ntop results for a search against Google, re-\nstricting the domain to twitter.com . The\nidea is that media that publish unreliable in-\nformation might have no Twitter accounts.\n\u2022Veri\ufb01ed: Whether the account is veri\ufb01ed by\nTwitter. The assumption is that \u201cfake news\u201d\nmedia would be less likely to have their Twit-\nter account veri\ufb01ed. They might be interested\nin pushing their content to users via Twitter,\nbut they would also be cautious about reveal-\ning who they are (which is required by Twit-\nter to get them veri\ufb01ed).\n\u2022Created: The year the account was created.\nThe idea is that accounts that have been active\nover a longer period of time are more likely\nto belong to established media.\n\u2022Has Location: Whether the account provides\ninformation about its location. The idea is\nthat established media are likely to have this\npublic, while \u201cfake news\u201d media may want to\nhide it.\u2022URL Match: Whether the account includes a\nURL to the medium, and whether it matches\nthe URL we started the search with. Estab-\nlished media are interested in attracting traf-\n\ufb01c to their website, while fake media might\nnot. Moreover, some fake accounts mimic\ngenuine media, but have a slightly different\ndomain, e.g., .com.co instead of.com .\n\u2022Counts : Statistics about the number of\nfriends, statuses, and favorites. Established\nmedia might have higher values for these.\n\u2022Description: A vector representation gener-\nated by averaging the Google News embed-\ndings ( Mikolov et al. ,2013 ) of all words of\nthe pro\ufb01le description paragraph. These short\ndescriptions might contain an open declara-\ntion of partisanship, i.e., left or right polit-\nical ideology (bias). This could also help\npredict factuality as extreme partisanship of-\nten implies low factuality. In contrast, \u201cfake\nnews\u201d media might just leave this description\nempty, while high-quality media would want\nto give some information about who they are.\nURL We also collect additional information\nfrom the website\u2019s URL using character-based\nmodeling and hand-crafted features. URL features\nare commonly used in phishing website detection\nsystems to identify malicious URLs that aim to\nmislead users ( Ma et al. ,2009 ). As we want to\npredict a website\u2019s factuality, using URL features\nis justi\ufb01ed by the fact that low-quality websites\nsometimes try to mimic popular news media by us-\ning a URL that looks similar to the credible source.\nWe use the following URL-related features:\n\u2022Character-based: Used to model the URL by\nrepresenting it in the form of a one-hot vec-\ntor of character n-grams, where n\u2208[2,5].\nNote that these features are not used in the \ufb01-\nnal system as they could not outperform the\nbaseline (when used in isolation).\n\u2022Orthographic: These features are very ef-\nfective for detecting phishing websites, as\nmalicious URLs tend to make excessive use\nof special characters and sections, and ulti-\nmately end up being longer. For this work,\nwe use the length of the URL, the num-\nber of sections and the excessive use of spe-\ncial characters such as digits, hyphens and\nName URL Factuality Twitter Handle Wikipedia page\nAssociated Press http://apnews.com\u22c6Very High @apnews ~/wiki/Associated_Press\nNBC News http://www.nbcnews.com/ High @nbcnews ~/wiki/NBC_News\nRussia Insider http://russia-insider.com Mixed @russiainsider ~/wiki/Russia_Insider\nPatriots V oice http://patriotsvoice.info/ Low @pegidaukgroup N/A\nTable 1 : Examples of media with various factuality scores. (\u22c6In our experiments, we treat Very High asHigh .)\nName URL Bias Twitter Handle Wikipedia page\nLoser.com http://loser.com Extreme Left @Loser_dot_com ~/Loser.com\nDie Hard Democrat http://dieharddemocrat.com/ Left @democratdiehard N/A\nDemocracy 21 http://www.democracy21.org/ Center-Left @fredwertheimer ~/Democracy_21\nFederal Times http://www.federaltimes.com/ Center @federaltimes ~/Federal_Times\nGulf News http://gulfnews.com/ Center-Right @gulf_news ~/Gulf_News\nFox News http://www.foxnews.com/ Right @foxnews ~/Fox_News\nFreedom Outpost http://freedomoutpost.com/ Extreme Right @FreedomOutpost N/A\nTable 2 : Examples of media with various bias scores.\ndashes. In particular, we identify whether the\nURL contains digits, dashes or underscores\nas individual symbols, which were found to\nbe useful as features for detecting phishing\nURLs ( Basnet et al. ,2014 ). We also check\nwhether the URL contains short (less than\nthree symbols) or long sections (more than\nten symbols), as a high number of such sec-\ntions could indicate an irregular URL.\n\u2022Credibility: Model the website\u2019s URL\ncredibility by analyzing whether it ( i) uses\nhttps:// , (ii) resides on a blog-hosting\nplatform such as blogger.com , and\n(iii) uses a special top-level domain,\ne.g.,.gov is for governmental websites,\nwhich are generally credible and unbiased,\nwhereas.co is often used to mimic .com .\nWeb Traf\ufb01c Analyzing the web traf\ufb01c to the\nwebsite of the medium might be useful for de-\ntecting phishy websites that come and disappear\nin certain patterns. Here, we only use the recip-\nrocal value of the website\u2019s Alexa Rank ,5which is\na global ranking for over 30 million websites in\nterms of the traf\ufb01c they receive.\nWe evaluate the above features in Section 4,\nboth individually and as groups, in order to deter-\nmine which ones are important to predict factual-\nity and bias, and also to identify the ones that are\nworth further investigation in future work.\n5http://www.alexa.com/4 Experiments and Evaluation\n4.1 Data\nWe use information about news media listed on the\nMedia Bias/Fact Check (MBFC) website,6which\ncontains manual annotations and analysis of the\nfactuality of reporting and/or bias for over 2,000\nnews websites. Our dataset includes 1,066 web-\nsites for which both bias and factuality labels were\nexplicitly provided, or could be easily inferred\n(e.g., satire is of low factuality).\nWe model factuality on a 3-point scale ( Low,\nMixed , and High ),7and bias on a 7-point scale\n(Extreme-Left ,Left,Center-Left ,Center ,Center-\nRight ,Right , and Extreme-Right ).\nSome examples from our dataset are presented\nin Table 1for factuality of reporting, and in Ta-\nble2for bias. In both tables, we show the names\nof the media, as well as their corresponding Twit-\nter handles and Wikipedia pages, which we found\nautomatically. Overall, 64% of the websites in our\ndataset have Wikipedia pages, and 94% have Twit-\nter accounts. In cases of \u201cfake news\u201d sites that\ntry to mimic real ones, e.g., ABCnews.com.co\nis a fake version of ABCnews.com , it is possible\nthat our Twitter extractor returns the handle for the\nreal medium. This is where the URL Match feature\ncomes handy (see above).\nTable 3provides detailed statistics about the\ndataset. Note that we have 1-2 orders of magni-\ntude more media sources than what has been used\n6https://mediabiasfactcheck.com\n7MBFC also uses Very High as a label, but due to its very\nsmall size, we merged it with High .\nin previous studies, as we already mentioned in\nSection 2above.\nFactuality Bias\nLow 256 Extreme-Left 21\nMixed 268 Left 168\nHigh 542 Center-Left 209\nCenter 263\nCenter-Right 92\nRight 157\nExtreme-Right 156\nTable 3 : Label distribution (counts) in our dataset.\nIn order to compute the article-related features, we\ndid the following: ( i) we crawled 10\u2013100 articles\nper website (a total of 94,814), ( ii) we computed\na feature vector for each article, and ( iii) we aver-\naged the feature vectors for the articles from the\nsame website to obtain the \ufb01nal vector of article-\nrelated features.\n4.2 Experimental Setup\nWe used the above features in a Support Vec-\ntor Machine (SVM) classi\ufb01er, training a separate\nmodel for factuality and for bias. We report re-\nsults for 5-fold cross-validation. We tuned the\nSVM hyper-parameters, i.e., the cost C, the ker-\nnel type, and the kernel width \u03b3, using an internal\ncross-validation on the training set and optimiz-\ning macro-averaged F1. Generally, the RBF ker-\nnel performed better than the linear kernel.\nWe report accuracy and macro-averaged F1\nscore. We also report Mean Average Error (MAE),\nwhich is relevant given the ordinal nature of\nboth the factuality and the bias classes, and also\nMAEM, which is a variant of MAE that is more\nrobust to class imbalance. See ( Baccianella et al. ,\n2009 ;Rosenthal et al. ,2017 ) for more details\nabout MAEMvs. MAE.\n4.3 Results and Discussion\nWe present in Table 4the results of using features\nfrom the different sources proposed in Section 3.\nWe start by describing the contribution of each\nfeature type towards factuality and bias.\nWe can see that the textual features extracted\nfrom the A RTICLES yielded the best performance\non factuality. They also perform well on bias, be-\ning the only type that beats the baseline on MAE.\nThese results indicate the importance of analyzing\nthe contents of the target website. They also show\nthat using the titles only is not enough, and that thearticle bodies contain important information that\nshould not be ignored.\nOverall, the W IKIPEDIA features are less use-\nful for factuality, and perform reasonably well for\nbias. The best features from this family are those\nabout the page content , which includes a general\ndescription of the medium, its history, ideology\nand other information that can be potentially help-\nful. Interestingly, the has page feature alone yields\nsizable improvement over the baseline, especially\nfor factuality. This makes sense given that trust-\nworthy websites are more likely to have Wikipedia\npages; yet, this feature does not help much for pre-\ndicting political bias.\nThe T WITTER features perform moderately for\nfactuality and poorly for bias. This is not sur-\nprising, as we normally may not be able to tell\nmuch about the political ideology of a website just\nby looking at its Twitter pro\ufb01le (not its tweets!)\nunless something is mentioned in its description ,\nwhich turns out to perform better than the rest of\nthe features from this family. We can see that the\nhas twitter feature is less effective than has wiki\nfor factuality, which makes sense given that Twit-\nter is less regulated than Wikipedia. Note that the\ncounts features yield reasonable performance, in-\ndicating that information about activity (e.g., num-\nber of statuses) and social connectivity (e.g., num-\nber of followers) is useful. Overall, the T WITTER\nfeatures seem to complement each other, as their\nunion yields the best performance on factuality.\nThe URL features are better used for factual-\nity rather than bias prediction. This is mainly due\nto the nature of these features, which are aimed\nat detecting phishing websites, as we mentioned\nin Section 3. Overall, this feature family yields\nslight improvements, suggesting that it can be use-\nful when used together with other features.\nFinally, the Alexa rank does not improve over\nthe baseline, which suggests that more sophisti-\ncated T RAFFIC -related features might be needed.\n4.4 Ablation Study\nFinally, we performed an ablation study in order\nto evaluate the impact of removing one family of\nfeatures at a time, as compared to the F ULL sys-\ntem, which uses all the features. We can see in\nTables 5and6that the F ULL system achieved the\nbest results for factuality, and the best macro-F 1\nfor bias, suggesting that the different types of fea-\ntures are largely complementary and capture dif-\nSource Feature Dim. Factuality Bias\nMacro-F 1Acc. MAE MAEMMacro-F 1Acc. MAE MAEM\nMajority Baseline 22.47 50.84 0.73 1.00 5.65 24.67 1.39 1.71\nTraf\ufb01c Alexa rank 1 22.46 50.75 0.73 1.00 7.76 25.70 1.38 1.71\nURL URL structure 12 39.30 53.28 0.68 0.81 13.50 23.64 1.65 2.06\nTwittercreated at. 1 30.72 52.91 0.69 0.92 5.65 24.67 1.39 1.71\nhas account 1 30.72 52.91 0.69 0.92 5.65 24.67 1.39 1.71\nveri\ufb01ed 1 30.72 52.91 0.69 0.92 5.65 24.67 1.39 1.71\nhas location 1 36.73 52.72 0.69 0.82 9.44 24.86 1.54 1.85\nURL match 2 39.98 54.60 0.66 0.72 10.16 25.61 1.51 1.97\ndescription 300 44.79 51.41 0.65 0.70 19.08 25.33 1.73 2.04\ncounts 5 46.88 57.22 0.57 0.66 18.34 24.86 1.62 2.01\nTwitter \u2013 All 308 48.23 54.78 0.59 0.64 21.38 27.77 1.58 1.83\nWikipediahas page 1 43.53 59.10 0.57 0.63 14.33 26.83 1.63 2.14\ntable of content 300 43.95 51.04 0.60 0.65 15.10 22.96 1.86 2.25\ncategories 300 46.36 53.70 0.65 0.61 25.64 32.16 1.70 2.10\ninformation box 300 46.39 51.14 0.71 0.65 19.79 26.85 1.68 1.99\nsummary 300 51.88 58.91 0.54 0.52 30.02 37.43 1.47 1.98\ncontent 300 55.29 62.10 0.51 0.50 30.92 38.61 1.51 2.01\nWikipedia \u2013 All 301 55.52 62.29 0.50 0.49 28.66 35.93 1.51 2.00\nArticlestitle 141 53.20 59.57 0.51 0.58 30.91 37.52 1.29 1.53\nbody 141 58.02 64.35 0.43 0.51 36.63 41.74 1.15 1.43\nTable 4 : Results for factuality and bias prediction. Bold values indicate the best-performing feature type\nin its family of features, while underlined values indicate the best-performing feature type overall.\nferent aspects that are all important for making a\ngood classi\ufb01cation decision.\nFor factuality, excluding the W IKIPEDIA fea-\ntures yielded the biggest drop in performance.\nThis suggests that they provide information that\nmay not be available in other sources, includ-\ning the A RTICLES , which achieved better results\nalone. On the other hand, excluding the T RAFFIC\nfeature had no effect on the model\u2019s performance.\nFor bias, we experimented with classi\ufb01cation\non both a 7-point and a 3-point scale.8Sim-\nilarly to factuality, the results in Table 6indi-\ncate that W IKIPEDIA offers complementary infor-\nmation that is critical for bias prediction, while\nTRAFFIC makes virtually no difference.\n5 Conclusion and Future Work\nWe have presented a study on predicting factual-\nity of reporting and bias of news media, focus-\ning on characterizing them as a whole. These\nare under-studied, but arguably important research\nproblems, both in their own right and as a prior for\nfact-checking systems.\n8We performed the following mapping:\n{Extreme-Right, Right }\u2192Right, { Extreme-Left, Left }\u2192Left,\nand { Center, Right-Center, Left-Center }\u2192CenterWe have created a new dataset of news media\nsources that has annotations for both tasks and is\n1-2 orders of magnitude larger than what was used\nin previous work. We are releasing the dataset and\nour code, which should facilitate future research.\nWe have experimented with a rich set of features\nderived from the contents of ( i) a sample of articles\nfrom the target news medium, ( ii) its Wikipedia\npage, ( iii) its Twitter account, ( iv) the structure of\nits URL, and ( v) information about the Web traf\ufb01c\nit has attracted. This combination, as well as some\nof the types of features, are novel for this problem.\nOur evaluation results have shown that most of\nthese features have a notable impact on perfor-\nmance, with the articles from the target website,\nits Wikipedia page, and its Twitter account being\nthe most important (in this order). We further per-\nformed an ablation study of the impact of the indi-\nvidual types of features for both tasks, which could\ngive general directions for future research.\nIn future work, we plan to address the task as\nordinal regression, and further to model the inter-\ndependencies between factuality and bias in a joint\nmodel. We are also interested in characterizing\nthe factuality of reporting for media in other lan-\nguages. Finally, we want to go beyond left vs.\nFeatures Macro-F 1Acc. MAE MAEM\nMAJORITY BASELINE 22.47 50.84 0.73 1.00\nFULL 59.91 65.48 0.41 0.44\nFULL W /OTRAFFIC 59.90 65.39 0.41 0.43\nFULL W /OTWITTER 59.52 65.10 0.41 0.47\nFULL W /OURL 57.23 63.32 0.44 0.49\nFULL W /OARTICLES 56.15 63.13 0.46 0.51\nFULL W /OWIKIPEDIA 55.93 63.23 0.44 0.52\nTable 5 : Ablation study for the contribution of each feature type fo r predicting the factuality of reporting.\nFeatures 7-Way Bias 3-Way Bias\nMacro-F 1Acc. MAE MAEMMacro-F 1Acc. MAE MAEM\nMAJORITY BASELINE 5.65 24.67 1.39 1.71 22.61 51.33 0.49 0.67\nFULL 37.50 39.87 1.25 1.55 61.31 68.86 0.39 0.53\nFULL W /OTRAFFIC 37.49 39.84 1.25 1.55 61.30 68.86 0.38 0.53\nFULL W /OTWITTER 36.88 39.49 1.20 1.38 63.27 69.89 0.38 0.50\nFULL W /OURL 36.60 39.68 1.24 1.48 60.93 68.11 0.40 0.53\nFULL W /OWIKIPEDIA 34.75 37.62 1.33 1.58 59.92 66.89 0.41 0.54\nFULL W /OARTICLES 29.95 36.96 1.40 1.85 53.67 62.48 0.47 0.62\nTable 6 : Ablation study for the contribution of each feature type fo r predicting media bias.\nright bias that is typical of the Western world and\nto model other kinds of biases that are more rele-\nvant for other regions, e.g., islamist vs. secular is\none such example for the Muslim World.\nAcknowledgments\nThis research was carried out in collaboration be-\ntween the MIT Computer Science and Arti\ufb01cial\nIntelligence Laboratory (CSAIL) and the Qatar\nComputing Research Institute (QCRI), HBKU.\nWe would like to thank Israa Jaradat, Kritika\nMishra, Ishita Chopra, Laila El-Beheiry, Tanya\nShastri, and Hamdy Mubarak for helping us with\nthe data extraction, cleansing, and preparation.\nFinally, we thank the anonymous reviewers for\ntheir constructive comments, which have helped\nus improve this paper.\nReferences\nPepa Atanasova, Llu\u00eds M\u00e0rquez, Alberto Barr\u00f3n-\nCede\u00f1o, Tamer Elsayed, Reem Suwaileh, Wajdi Za-\nghouani, Spas Kyuchukov, Giovanni Da San Mar-\ntino, and Preslav Nakov. 2018. Overview of the\nCLEF-2018 CheckThat! lab on automatic identi-\n\ufb01cation and veri\ufb01cation of political claims, task 1:\nCheck-worthiness. In CLEF 2018 Working Notes.\nWorking Notes of CLEF 2018 - Conference and Labsof the Evaluation Forum , CEUR Workshop Proceed-\nings, Avignon, France. CEUR-WS.org.\nStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-\ntiani. 2009. Evaluation measures for ordinal regres-\nsion. In Proceedings of the 9th IEEE International\nConference on Intelligent Systems Design and Ap-\nplications , ISDA \u201909, pages 283\u2013287, Pisa, Italy.\nRicardo Baeza-Yates. 2018. Bias on the web. Com-\nmun. ACM , 61(6):54\u201361.\nRamy Baly, Mitra Mohtarami, James Glass, Llu\u00eds\nM\u00e0rquez, Alessandro Moschitti, and Preslav Nakov.\n2018. Integrating stance detection and fact checking\nin a uni\ufb01ed corpus. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies , NAACL-HLT \u201918, pages 21\u2013\n27, New Orleans, LA, USA.\nAlberto Barr\u00f3n-Cede\u00f1o, Tamer Elsayed, Reem\nSuwaileh, Llu\u00eds M\u00e0rquez, Pepa Atanasova, Wajdi\nZaghouani, Spas Kyuchukov, Giovanni Da San Mar-\ntino, and Preslav Nakov. 2018. Overview of the\nCLEF-2018 CheckThat! lab on automatic identi\ufb01-\ncation and veri\ufb01cation of political claims, task 2:\nFactuality. In CLEF 2018 Working Notes. Working\nNotes of CLEF 2018 - Conference and Labs of the\nEvaluation Forum , CEUR Workshop Proceedings,\nAvignon, France. CEUR-WS.org.\nRam B Basnet, Andrew H Sung, and Quingzhong Liu.\n2014. Learning to detect phishing URLs. Interna-\ntional Journal of Research in Engineering and Tech-\nnology , 3(6):11\u201324.\nAnn M Brill. 2001. Online journalists embrace new\nmarketing function. Newspaper Research Journal ,\n22(2):28.\nKevin R. Canini, Bongwon Suh, and Peter L. Pirolli.\n2011. Finding credible information sources in so-\ncial networks based on content and social structure.\nInProceedings of the IEEE International Confer-\nence on Privacy, Security, Risk, and Trust, and the\nIEEE International Conference on Social Comput-\ning, SocialCom/PASSAT \u201911, pages 1\u20138, Boston,\nMA, USA.\nCarlos Castillo, Marcelo Mendoza, and Barbara\nPoblete. 2011. Information credibility on Twitter. In\nProceedings of the 20th International Conference on\nWorld Wide Web , WWW \u201911, pages 675\u2013684, Hy-\nderabad, India.\nAbhijnan Chakraborty, Bhargavi Paranjape, Kakarla\nKakarla, and Niloy Ganguly. 2016. Stop clickbait:\nDetecting and preventing clickbaits in online news\nmedia. In Proceedings of the 2016 IEEE/ACM In-\nternational Conference on Advances in Social Net-\nworks Analysis and Mining , ASONAM \u201916, pages\n9\u201316, San Francisco, CA, USA.\nCheng Chen, Kui Wu, Venkatesh Srinivasan, and\nXudong Zhang. 2013. Battling the Internet Water\nArmy: detection of hidden paid posters. In Proceed-\nings of the 2013 IEEE/ACM International Confer-\nence on Advances in Social Networks Analysis and\nMining , ASONAM \u201913, pages 116\u2013120, Niagara,\nCanada.\nKareem Darwish, Dimitar Alexandrov, Preslav Nakov,\nand Yelena Mejova. 2017a. Seminar users in\nthe Arabic Twitter sphere. In Proceedings of the\n9th International Conference on Social Informatics ,\nSocInfo \u201917, pages 91\u2013108, Oxford, UK.\nKareem Darwish, Walid Magdy, and Tahar Zanouda.\n2017b. Improved stance prediction in a user similar-\nity feature space. In Proceedings of the Conference\non Advances in Social Networks Analysis and Min-\ning, ASONAM \u201917, pages 145\u2013148, Sydney, Aus-\ntralia.\nSohan De Sarkar, Fan Yang, and Arjun Mukherjee.\n2018. Attending sentences to detect satirical fake\nnews. In Proceedings of the 27th International\nConference on Computational Linguistics , COL-\nING \u201918, pages 3371\u20133380, Santa Fe, NM, USA.\nLeon Derczynski, Kalina Bontcheva, Maria Liakata,\nRob Procter, Geraldine Wong Sak Hoi, and Arkaitz\nZubiaga. 2017. SemEval-2017 Task 8: Ru-\nmourEval: Determining rumour veracity and sup-\nport for rumours. In Proceedings of the 11th In-\nternational Workshop on Semantic Evaluation , Se-\nmEval \u201917, pages 60\u201367, Vancouver, Canada.Xin Luna Dong, Evgeniy Gabrilovich, Kevin Murphy,\nVan Dang, Wilko Horn, Camillo Lugaresi, Shao-\nhua Sun, and Wei Zhang. 2015. Knowledge-based\ntrust: Estimating the trustworthiness of web sources.\nProc. VLDB Endow. , 8(9):938\u2013949.\nSebastian Dungs, Ahmet Aker, Norbert Fuhr, and\nKalina Bontcheva. 2018. Can rumour stance alone\npredict veracity? In Proceedings of the 27th In-\nternational Conference on Computational Linguis-\ntics, COLING \u201918, pages 3360\u20133370, Santa Fe, NM,\nUSA.\nHoward Finberg, Martha L Stone, and Diane Lynch.\n2002. Digital journalism credibility study. Online\nNews Association. Retrieved November , 3:2003.\nJesse Graham, Jonathan Haidt, and Brian A Nosek.\n2009. Liberals and conservatives rely on different\nsets of moral foundations. Journal of personality\nand social psychology , 96(5):1029.\nAndreas Hanselowski, Avinesh PVS, Benjamin\nSchiller, Felix Caspelherr, Debanjan Chaudhuri,\nChristian M. Meyer, and Iryna Gurevych. 2018. A\nretrospective analysis of the fake news challenge\nstance-detection task. In Proceedings of the 27th\nInternational Conference on Computational Lin-\nguistics , COLING \u201918, pages 1859\u20131874, Santa Fe,\nNM, USA.\nMomchil Hardalov, Ivan Koychev, and Preslav Nakov.\n2016. In search of credible news. In Proceedings\nof the 17th International Conference on Arti\ufb01cial In-\ntelligence: Methodology, Systems, and Applications ,\nAIMSA \u201916, pages 172\u2013180, Varna, Bulgaria.\nBenjamin Horne and Sibel Adali. 2017. This just in:\nFake news packs a lot in title, uses simpler, repetitive\ncontent in text body, more similar to satire than real\nnews. CoRR , abs/1703.09398.\nBenjamin Horne, Sibel Adali, and Sujoy Sikdar. 2017.\nIdentifying the social signals that drive online dis-\ncussions: A case study of Reddit communities. In\nProceedings of the 26th IEEE International Confer-\nence on Computer Communication and Networks ,\nICCCN \u201917, pages 1\u20139, Vancouver, Canada.\nBenjamin D. Horne, William Dron, Sara Khedr, and\nSibel Adali. 2018a. Assessing the news landscape:\nA multi-module toolkit for evaluating the credibility\nof news. In Proceedings of the The Web Conference ,\nWWW \u201918, pages 235\u2013238, Lyon, France.\nBenjamin D. Horne, Sara Khedr, and Sibel Adali.\n2018b. Sampling the news producers: A large news\nand feature data set for the study of the complex\nmedia landscape. In Proceedings of the Twelfth In-\nternational Conference on Web and Social Media ,\nICWSM \u201918, pages 518\u2013527, Stanford, CA, USA.\nClayton Hutto and Eric Gilbert. 2014. V ADER: A par-\nsimonious rule-based model for sentiment analysis\nof social media text. In Proceedings of the 8th Inter-\nnational Conference on Weblogs and Social Media ,\nICWSM \u201914, Ann Arbor, MI, USA.\nGeorgi Karadzhov, Pepa Gencheva, Preslav Nakov, and\nIvan Koychev. 2017a. We built a fake news & click-\nbait \ufb01lter: What happened next will blow your mind!\nInProceedings of the International Conference on\nRecent Advances in Natural Language Processing ,\nRANLP \u201917, pages 334\u2013343, Varna, Bulgaria.\nGeorgi Karadzhov, Preslav Nakov, Llu\u00eds M\u00e0rquez, Al-\nberto Barr\u00f3n-Cede\u00f1o, and Ivan Koychev. 2017b.\nFully automated fact checking using external\nsources. In Proceedings of the International Confer-\nence on Recent Advances in Natural Language Pro-\ncessing , RANLP \u201917, pages 344\u2013353, Varna, Bul-\ngaria.\nElena Kochkina, Maria Liakata, and Arkaitz Zubi-\naga. 2018. All-in-one: Multi-task learning for ru-\nmour veri\ufb01cation. In Proceedings of the 27th In-\nternational Conference on Computational Linguis-\ntics, COLING \u201918, pages 3402\u20133413, Santa Fe, NM,\nUSA.\nDavid M.J. Lazer, Matthew A. Baum, Yochai Ben-\nkler, Adam J. Berinsky, Kelly M. Greenhill, Filippo\nMenczer, Miriam J. Metzger, Brendan Nyhan, Gor-\ndon Pennycook, David Rothschild, Michael Schud-\nson, Steven A. Sloman, Cass R. Sunstein, Emily A.\nThorson, Duncan J. Watts, and Jonathan L. Zit-\ntrain. 2018. The science of fake news. Science ,\n359(6380):1094\u20131096.\nYaliang Li, Jing Gao, Chuishi Meng, Qi Li, Lu Su,\nBo Zhao, Wei Fan, and Jiawei Han. 2016. A sur-\nvey on truth discovery. SIGKDD Explor. Newsl. ,\n17(2):1\u201316.\nYing Lin, Joe Hoover, Morteza Dehghani, Marlon\nMooijman, and Heng Ji. 2017. Acquiring back-\nground knowledge to improve moral value predic-\ntion. arXiv preprint arXiv:1709.05467 .\nJing Ma, Wei Gao, Prasenjit Mitra, Sejeong Kwon,\nBernard J. Jansen, Kam-Fai Wong, and Meeyoung\nCha. 2016. Detecting rumors from microblogs with\nrecurrent neural networks. In Proceedings of the\n25th International Joint Conference on Arti\ufb01cial In-\ntelligence , IJCAI \u201916, pages 3818\u20133824, New York,\nNY , USA.\nJing Ma, Wei Gao, Zhongyu Wei, Yueming Lu, and\nKam-Fai Wong. 2015. Detect rumors using time se-\nries of social context information on microblogging\nwebsites. In Proceedings of the 24th ACM Inter-\nnational on Conference on Information and Knowl-\nedge Management , CIKM \u201915, pages 1751\u20131754,\nMelbourne, Australia.\nJing Ma, Wei Gao, and Kam-Fai Wong. 2017. De-\ntect rumors in microblog posts using propagation\nstructure via kernel learning. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics , ACL \u201917, pages 708\u2013717, Van-\ncouver, Canada.Justin Ma, Lawrence K. Saul, Stefan Savage, and Ge-\noffrey M. V oelker. 2009. Identifying suspicious\nURLs: An application of large-scale online learn-\ning. In Proceedings of the 26th Annual International\nConference on Machine Learning , ICML \u201909, pages\n681\u2013688, Montreal, Canada.\nSuman Kalyan Maity, Aishik Chakraborty, Pawan\nGoyal, and Animesh Mukherjee. 2017. Detection of\nsockpuppets in social media. In Proceedings of the\nACM Conference on Computer Supported Coopera-\ntive Work and Social Computing , CSCW \u201917, pages\n243\u2013246, Portland, OR, USA.\nTodor Mihaylov, Georgi Georgiev, and Preslav Nakov.\n2015a. Finding opinion manipulation trolls in news\ncommunity forums. In Proceedings of the Nine-\nteenth Conference on Computational Natural Lan-\nguage Learning , CoNLL \u201915, pages 310\u2013314, Bei-\njing, China.\nTodor Mihaylov, Ivan Koychev, Georgi Georgiev, and\nPreslav Nakov. 2015b. Exposing paid opinion ma-\nnipulation trolls. In Proceedings of the International\nConference Recent Advances in Natural Language\nProcessing , RANLP \u201915, pages 443\u2013450, Hissar,\nBulgaria.\nTodor Mihaylov, Tsvetomila Mihaylova, Preslav\nNakov, Llu\u00eds M\u00e0rquez, Georgi Georgiev, and Ivan\nKoychev. 2018. The dark side of news community\nforums: Opinion manipulation trolls. Internet Re-\nsearch .\nTodor Mihaylov and Preslav Nakov. 2016. Hunting for\ntroll comments in news community forums. In Pro-\nceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics , ACL \u201916, pages\n399\u2013405, Berlin, Germany.\nTsvetomila Mihaylova, Preslav Nakov, Llu\u00eds M\u00e0rquez,\nAlberto Barr\u00f3n-Cede\u00f1o, Mitra Mohtarami, Georgi\nKaradjov, and James Glass. 2018. Fact checking in\ncommunity forums. In Proceedings of the Thirty-\nSecond AAAI Conference on Arti\ufb01cial Intelligence ,\nAAAI \u201918, pages 879\u2013886, New Orleans, LA, USA.\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.\n2013. Linguistic regularities in continuous space\nword representations. In Proceedings of the 2013\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , NAACL-HLT \u201913, pages\n746\u2013751, Atlanta, GA, USA.\nLewis Mitchell, Morgan R Frank, Kameron Decker\nHarris, Peter Sheridan Dodds, and Christopher M\nDanforth. 2013. The geography of happiness:\nConnecting Twitter sentiment and expression, de-\nmographics, and objective characteristics of place.\nPloS one , 8(5):e64417.\nMitra Mohtarami, Ramy Baly, James Glass, Preslav\nNakov, Llu\u00eds M\u00e0rquez, and Alessandro Moschitti.\n2018. Automatic stance detection using end-to-\nend memory networks. In Proceedings of the 16th\nAnnual Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies , NAACL-HLT \u201918,\npages 767\u2013776, New Orleans, LA, USA.\nSubhabrata Mukherjee and Gerhard Weikum. 2015.\nLeveraging joint interactions for credibility analy-\nsis in news communities. In Proceedings of the\n24th ACM International on Conference on Informa-\ntion and Knowledge Management , CIKM \u201915, pages\n353\u2013362, Melbourne, Australia.\nPreslav Nakov, Alberto Barr\u00f3n-Cede\u00f1o, Tamer El-\nsayed, Reem Suwaileh, Llu\u00eds M\u00e0rquez, Wajdi Za-\nghouani, Pepa Atanasova, Spas Kyuchukov, and\nGiovanni Da San Martino. 2018. Overview of the\nCLEF-2018 CheckThat! lab on automatic identi\ufb01-\ncation and veri\ufb01cation of political claims. In Pro-\nceedings of the Ninth International Conference of\nthe CLEF Association: Experimental IR Meets Mul-\ntilinguality, Multimodality, and Interaction , Lecture\nNotes in Computer Science, pages 372\u2013387, Avi-\ngnon, France. Springer.\nAn T. Nguyen, Aditya Kharosekar, Matthew Lease,\nand Byron C. Wallace. 2018. An interpretable joint\ngraphical model for fact-checking from crowds. In\nProceedings of the Thirty-Second AAAI Conference\non Arti\ufb01cial Intelligence , AAAI \u201918, New Orleans,\nLA, USA.\nJeff Z. Pan, Siyana Pavlova, Chenxi Li, Ningxi Li,\nYangmei Li, and Jinshuo Liu. 2018. Content based\nfake news detection using knowledge graphs. In\nProceedings of the International Semantic Web Con-\nference , ISWC \u201918, Monterey, CA, USA.\nSymeon Papadopoulos, Kalina Bontcheva, Eva Jaho,\nMihai Lupu, and Carlos Castillo. 2016. Overview of\nthe special issue on trust and veracity of information\nin social media. ACM Trans. Inf. Syst. , 34(3):14:1\u2013\n14:5.\nVer\u00f3nica P\u00e9rez-Rosas, Bennett Kleinberg, Alexandra\nLefevre, and Rada Mihalcea. 2018. Automatic de-\ntection of fake news. In Proceedings of the 27th In-\nternational Conference on Computational Linguis-\ntics, COLING \u201918, pages 3391\u20133401, Santa Fe, NM,\nUSA.\nEvaggelia Pitoura, Panayiotis Tsaparas, Giorgos\nFlouris, Irini Fundulaki, Panagiotis Papadakos,\nSerge Abiteboul, and Gerhard Weikum. 2018. On\nmeasuring bias in online information. SIGMOD\nRec., 46(4):16\u201321.\nKashyap Popat, Subhabrata Mukherjee, Jannik Str\u00f6t-\ngen, and Gerhard Weikum. 2016. Credibility assess-\nment of textual claims on the web. In Proceedings\nof the 25th ACM International on Conference on In-\nformation and Knowledge Management , CIKM \u201916,\npages 2173\u20132178, Indianapolis, IN, USA.\nKashyap Popat, Subhabrata Mukherjee, Jannik Str\u00f6t-\ngen, and Gerhard Weikum. 2017. Where the truthlies: Explaining the credibility of emerging claims\non the Web and social media. In Proceedings of the\n26th International Conference on World Wide Web\nCompanion , WWW \u201917, pages 1003\u20131012, Perth,\nAustralia.\nKashyap Popat, Subhabrata Mukherjee, Jannik Str\u00f6t-\ngen, and Gerhard Weikum. 2018. CredEye: A cred-\nibility lens for analyzing and explaining misinforma-\ntion. In Proceedings of The Web Conference 2018 ,\nWWW \u201918, pages 155\u2013158, Lyon, France.\nMartin Potthast, Johannes Kiesel, Kevin Reinartz,\nJanek Bevendorff, and Benno Stein. 2018. A stylo-\nmetric inquiry into hyperpartisan and fake news. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics , ACL \u201918,\npages 231\u2013240, Melbourne, Australia.\nHannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana\nV olkova, and Yejin Choi. 2017. Truth of varying\nshades: Analyzing language in fake news and polit-\nical fact-checking. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language\nProcessing , EMNLP \u201917, pages 2931\u20132937, Copen-\nhagen, Denmark.\nMarta Recasens, Cristian Danescu-Niculescu-Mizil,\nand Dan Jurafsky. 2013. Linguistic models for ana-\nlyzing and detecting biased language. In Proceed-\nings of the 51st Annual Meeting of the Associa-\ntion for Computational Linguistics , ACL \u201913, pages\n1650\u20131659, So\ufb01a, Bulgaria.\nBenjamin Riedel, Isabelle Augenstein, Georgios P Sp-\nithourakis, and Sebastian Riedel. 2017. A simple but\ntough-to-beat baseline for the Fake News Challenge\nstance detection task. ArXiv:1707.03264 .\nSara Rosenthal, Noura Farra, and Preslav Nakov. 2017.\nSemEval-2017 task 4: Sentiment analysis in Twitter.\nInProceedings of the 11th International Workshop\non Semantic Evaluation , SemEval \u201917, pages 502\u2013\n518, Vancouver, Canada.\nKai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and\nHuan Liu. 2017. Fake news detection on social me-\ndia: A data mining perspective. SIGKDD Explor.\nNewsl. , 19(1):22\u201336.\nJames Thorne, Mingjie Chen, Giorgos Myrianthous,\nJiashu Pu, Xiaoxuan Wang, and Andreas Vlachos.\n2017. Fake news stance detection using stacked en-\nsemble of classi\ufb01ers. In Proceedings of the EMNLP\nWorkshop on Natural Language Processing meets\nJournalism , pages 80\u201383, Copenhagen, Denmark.\nJames Thorne and Andreas Vlachos. 2018. Automated\nfact checking: Task formulations, methods and fu-\nture directions. In Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics ,\nCOLING \u201918, pages 3346\u20133359, Santa Fe, NM,\nUSA.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERi\ufb01cation. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , NAACL-HLT \u201918, pages\n809\u2013819, New Orleans, LA, USA.\nSoroush V osoughi, Deb Roy, and Sinan Aral. 2018.\nThe spread of true and false news online. Science ,\n359(6380):1146\u20131151.\nArkaitz Zubiaga, Elena Kochkina, Maria Liakata, Rob\nProcter, Michal Lukasik, Kalina Bontcheva, Trevor\nCohn, and Isabelle Augenstein. 2018. Discourse-\naware rumour stance classi\ufb01cation in social media\nusing sequential classi\ufb01ers. Inf. Process. Manage. ,\n54(2):273\u2013290.\nArkaitz Zubiaga, Maria Liakata, Rob Procter, Geral-\ndine Wong Sak Hoi, and Peter Tolmie. 2016.\nAnalysing how people orient to and spread rumours\nin social media by looking at conversational threads.\nPLoS ONE , 11(3):1\u201329.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Predicting factuality of reporting and bias of news media sources", "author": ["R Baly", "G Karadzhov", "D Alexandrov", "J Glass"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present a study on predicting the factuality of reporting and bias of news media. While  previous work has focused on studying the veracity of claims or documents, here we are"}, "filled": false, "gsrank": 11, "pub_url": "https://arxiv.org/abs/1810.01765", "author_id": ["zJuI3D8AAAAJ", "hK7sqzAAAAAJ", "", "pfGI-KcAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:naKo39FLnNIJ:scholar.google.com/&output=cite&scirp=10&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D10%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=naKo39FLnNIJ&ei=BbWsaMyPOeHUieoP9LKZ6AI&json=", "num_citations": 333, "citedby_url": "/scholar?cites=15176088209104347805&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:naKo39FLnNIJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/1810.01765"}}, {"title": "Safari: Cross-lingual bias and factuality detection in news media and news articles", "year": "2024", "pdf_data": "Findings of the Association for Computational Linguistics: EMNLP 2024 , pages 12217\u201312231\nNovember 12-16, 2024 \u00a92024 Association for Computational Linguistics\nSAFARI : Cross-lingual Bias and Factuality Detection\nin News Media and News Articles\nDilshod Azizov1\u2217, Zain Muhammad Mujahid1\u2217, Hilal AlQuabeh1,\nPreslav Nakov1and Shangsong Liang1,2\u2020\n1Mohamed bin Zayed University of Artificial Intelligence, UAE\n2Sun Yat-sen University, China\n{dilshod.azizov, zain.mujahid, hilal.alquabeh\nand preslav.nakov}@mbzuai.ac.ae ,liangshangsong@gmail.com\nAbstract\nIn an era where information is quickly shared\nacross many cultural and language contexts,\nthe neutrality and integrity of news media are\nessential. Ensuring that the content of the me-\ndia remains objective and factual is crucial to\nmaintaining public trust. With this in mind,\nwe introduce SAFARI (CroSs-lingual Bi As and\nFactuality Detection in News Medi Aand News\nARtIcles), a novel corpus of news media and\narticles for predicting political bias and the fac-\ntuality of the reporting in a cross-lingual setup.\nTo our knowledge, this corpus is unprecedented\nin its collection and introduces a dataset for po-\nlitical bias and factuality for three tasks: (i)\nmedia-level ,(ii) article-level , and (iii) joint\nmodeling at the article-level . At the media and\narticle levels, we evaluate the cross-lingual abil-\nity of the models; however, in joint modeling,\nwe evaluate on English data. Our frameworks\nset a new benchmark in the cross-lingual eval-\nuation of political bias and factuality. This is\nachieved via the use of various Multilingual\nPre-trained Language Models (MPLMs) and\nLarge Language Models (LLMs) coupled with\nensemble learning methods.\n1 Introduction\nThe integrity and objectivity of the news media\nare crucial in an age where information is rapidly\ndisseminated across diverse cultural and linguistic\nlandscapes (Fenton, 2009). As observed V osoughi\net al. (2018), misleading information or \u201cfake news,\u201d\nspreads six times faster than the truth and reaches\na much larger audience. This underscores the need\nfor comprehensive data to assess political bias and\nfactuality in news media and articles, particularly\nin a cross-lingual context, which remains a signif-\nicant challenge (Nakov et al., 2024). Thus, we\nintroduce a novel corpus SAFARI specifically de-\nsigned for the cross-lingual analysis of political\n*Equal contribution.\n\u2020Corresponding author.bias and factuality in news media and articles. Our\nwork in developing this corpus is motivated by the\nabsence of cross-lingual resources for detecting\npolitical bias and factuality in media and articles\nanalysis. To address this issue, we offer a dataset\nfor ten languages: (i) at media-level political bias,\nwe have slightly less than 2k, and for factuality\nmarginally over 2.6k media, (ii) at article-level we\ncollect around 190k for political bias and around\n190k of articles for factuality, and (iii) for joint\nmodeling at article-level we have moderately less\nthan 100k of English articles.\nFurthermore, the methodology behind our study\nincorporates the use of MPLMs and LLMs to assess\ndataset tasks. Our approach enables MPLMs and\nLLMs to provide an evaluation of political bias and\nfactuality across languages at the source and article\nlevels and in joint modeling. Moreover, MPLMs\nand LLMs (using zero-shot learning) are coupled\nwith ensemble learning methods for evaluation.\nOur contributions are as follows:\n\u2022We introduce a data construction pipeline that\ndelivers a large-scale corpus for cross-lingual\nevaluation of political bias and factuality, ad-\ndressing both the media and the article levels.\nAlso, we present an English-only dataset for\nthe joint modeling assessment at the article-\nlevel.\n\u2022We evaluate and compare distant supervision\nvs.expert-annotated data at the article-level\nonly for political bias.\n\u2022We employ MPLMs for analysis at the media-\nlevel, article-level, and in joint modeling lever-\naging ensemble learning, using hard and soft\nvotings.\n\u2022We implement LLMs using zero-shot learn-\ning with Mistral 7B(Jiang et al., 2023) and\nLLaMA2 7B(Touvron et al., 2023) utilizing\nan ensemble approach based on hard voting.12217\nIn Section 2, we provide a review of previous\nand recent studies that focus on political bias, factu-\nality, and joint modeling analysis. In Section 3, the\ndata collection process and the subsequent exam-\nination are elaborated. In Section 4, the research\ntasks are defined and the statistics of the dataset\nare introduced, together with the frameworks and\ntechniques. Section 5 delineates our analysis and\nthe results obtained using multiple MPLMs with\nensemble learning. Section 6 explores our inves-\ntigation of zero-shot learning for our tasks using\nLLMs and nuances of distant supervision vs.expert\nannotated data. Finally, Section 7 summarizes our\nfindings and suggests potential future directions.\n2 Related Work\nDatasets Predicting political bias and factuality\nin news media requires large-scale databases, with\nprevious efforts like those of (F\u00e4rber et al., 2020;\nCremisini et al., 2019; Zubiaga et al., 2016; Ham-\nborg et al., 2019; Kiesel et al., 2019a; Lim et al.,\n2020, 2018; Vargas et al., 2023) relied on crowd-\nsourcing to gather data. However, these databases\nare relatively small and focus mainly on English,\nwith annotations at the level of the media, article,\nand sentence (Baly et al., 2020a, 2018; Cremisini\net al., 2019; Hamborg et al., 2019; Kiesel et al.,\n2019a). In contrast, our corpus, which emphasizes\ndiverse languages, offers a larger dataset to predict\npolitical bias and factuality at the media and article\nlevels. In the following, we explore the methods\nand datasets coupled to use political bias and factu-\nality and their joint analysis.\nPolitical Bias Understanding political bias is a\nnuanced exploration with varying definitions, in-\ncluding uneven coverage or favoritism (Stevenson\net al., 1973) and systematic preferences for candi-\ndates or ideas (Waldman and Devitt, 1998). Guo\net al. (2022) employed pre-trained BERT (Devlin\net al., 2019) models to detect linguistic political\nbias in news articles. Groeling (2013) expanded\nthe concept of media bias, considering dimensions\nsuch as selection and presentation of political bias\ninfluenced by the choice of newsmakers (Smith\net al., 2001; Hassell et al., 2020). The study by Fan\net al. (2019) used annotated media from Budak et al.\n(2016), analyzing 300 NYT, FOX, and HPO arti-\ncles for bias, similar to our distant supervision ap-\nproach to capture diverse ideological perspectives.\nResearch on selection political bias requires huge\ndatabases, with studies using commercial (Soroka,2012; Padgett et al., 2019; Gilens and Hertzman,\n2000; Boykoff and Boykoff, 2004) and public\ndatasets (Boudemagh and Moise, 2017; Kwak and\nAn, 2014) using multi-source approaches (Kwak\nand An, 2016; Weaver and Bimber, 2008). Var-\nious methods measure the political bias of news\nmedia, including linking news outlets with politi-\ncians, analyzing shared audiences (Groseclose and\nMilyo, 2005; Gentzkow and Shapiro, 2010), and\nidentifying the intricate linguistic techniques used\nto shape readers\u2019 opinions and emotions (Sajwani\net al., 2024). Alternately, political bias is assessed\nthrough Twitter interactions (An et al., 2011, 2012;\nStefanov et al., 2020). Predictions extend to polit-\nical bias at the media, article and sentence levels,\noften using distant supervision with small datasets\nonly in English (Kulkarni et al., 2018; Potthast\net al., 2018; Kiesel et al., 2019b; Baly et al., 2020a;\nDa San Martino et al., 2023; Barr\u00f3n-Cede\u00f1o et al.,\n2023a,b; Azizov et al., 2023; Chen et al., 2018; Fan\net al., 2019; Spinde et al., 2022).\nFactuality Veracity of information is exam-\nined at various levels: claim-level (e.g., \u201cfact-\nchecking\u201d), article-level (e.g., \u201cfake news\u201d de-\ntection), user-level (e.g., identifying trolls), and\nmedium-level (e.g., source reliability estimation).\nClaim-level efforts focus on fact-checking and\nrumor detection using social media interactions\n(Castillo et al., 2011; Canini et al., 2011; Ma et al.,\n2015; Ma et al., 2016, 2017; Kochkina et al., 2018;\nDungs et al., 2018; Lim et al., 2020; Nguyen et al.,\n2020; Hardalov et al., 2022; Nakov et al., 2023),\nfocusing on the stance and reliability of the source.\nEarly work estimated source reliability based on\na medium\u2019s stance towards true/false claims using\nan English dataset (Mukherjee and Weikum, 2015;\nDong et al., 2015; Popat et al., 2016, 2017; Popat\net al., 2018). Recent approaches, such as Baly\net al. (2020c), used gold labels and various English\ninformation sources, which are relatively small\ncompared to our work. Mehta et al. (2022) and\nPanayotov et al. (2022) used graph-based frame-\nworks to profile news media outlets, focusing on\nrelationships and audience overlap. LLMs (e.g.,\nChatGPT) are used for the estimation of source\nreliability, as demonstrated by Yang and Menczer\n(2023), correlated with human expert ratings, and\nMehta and Goldwasser (2023) introduced a frame-\nwork that combined graph-based models, LLMs\nand human expertise for the profile of news me-\ndia, effectively identifying fake news with minimal12218\nLanguage Political Bias Factuality\nLeft Left-Center Least Biased Right-Center Right Total Very High High Mostly factual Mixed Low Very Low Total\nEnglish 259 567 637 279 134 1,876 67 1,529 166 425 202 119 2,508\nGerman - 9 5 6 1 21 1 8 8 3 1 2 23\nHindi 3 8 - 4 - 15 - 3 5 6 1 - 15\nFrench 2 4 2 2 - 10 2 5 2 2 1 2 14\nSpanish 1 3 2 3 - 9 - 7 2 3 - - 12\nHebrew 1 2 1 2 2 8 - 5 - 6 - - 11\nJapanese - 2 3 2 - 7 - 7 - 1 1 - 9\nItalian - 2 2 1 1 6 - 5 1 1 2 - 8\nArabic - 3 1 1 1 6 - 3 - 3 1 - 7\nRussian - - - 2 - 2 - - - 2 2 2 6\nTotal 1,960 2,613\nTable 1: Media-level dataset statistics.\nPolitical Bias Factuality\nLanguage Left Center Right Total Very High High Mixed Low Very Low Total\nEnglish 51,076 52,939 34,801 138,816 8,661 56,656 13,838 12,937 4,095 96,187\nSpanish 3,168 4,281 1,720 9,169 - 2,000 6,168 - - 8,168\nFrench 1,680 4,102 2,243 8,025 - 16,191 17,091 2,243 - 35,525\nGerman 1,200 2,840 1,020 5,060 130 8,140 - 100 - 8,370\nItalian - - 5,672 5,672 - - 5,672 - - 5,672\nBulgarian - 4,860 - 4,860 - - 4,860 - - 4,860\nHindi 2,890 - - 2,890 2,890 - - - - 2,890\nPersian - - 2,833 2,833 - - 2,833 - - 2,833\nPolish - - 5,000 5,000 10,000 - 6,168 - - 16,168\nRussian - - 3,980 3,980 - - 3,980 - 862 4,842\nTotal 186,305 189,347\nTable 2: Article-level dataset statistics.\nhuman input. Burdisso et al. (2024) employed rein-\nforcement learning to estimate the reliability of the\nmedia, correlated with journalist scores to predict\nreliability labels.\nJoint Modeling Joint modeling of factuality and\npolitical bias remains underexplored, with an at-\ntempt by (Baly et al., 2019) using a small English\ndataset using multi-task ordinal regression. Under-\nstanding the relationship between factuality and\npolitical bias in the news media, especially when\noutlets exhibit different behaviors on these aspects,\npresents a significant challenge.\n3 Dataset Construction\nOur methodology encompasses two levels of data\ncollection: media-level andarticle-level, both us-\ning the distant supervision technique (Mintz et al.,\n2009) for article collection. We use a two-step cri-\nterion: (i) We exclusively used sources expertly\nannotated by Media Bias/Fact Check1. (ii) We se-\nlect active media outlets. In media-level, we gather\nsources from Media Bias/Fact Check (MBFC) and\ncollect up to 30 front-page articles from each web-\n1www.mediabiasfactcheck.comsite, labeled according to their sources. Similarly,\ninarticle-level, we apply distant supervision by as-\nsigning labels to articles from media annotated by\nMBFC, and collect expert-annotated data for polit-\nical bias from AllSides2to compare performance\nwith distant supervision data. In addition, during\nthe data scraping process, we specifically targeted\nsections that focused on political, economic, and\nsocial issues. With this in mind, we used the EBK-\nmeans (Bholowalia and Kumar, 2014) clustering\nto analyze our entire dataset and identified 15 clus-\nters. Furthermore, we validate our choice with the\nsilhouette score, confirming the quality and separa-\ntion of the clusters. The percentage distribution of\ndata points was calculated across the clusters and\nvisualized in Figure 1.\n3.1 Media-level\nMedia Collection Figure 2 (Appendix A) shows\nour pipeline for media-level data collection, and\nthe following are our steps: (i) At this stage, we\ncompile a set of media sources from MBFC. After\nmanually evaluating the availability of each source\nthrough their links, we extracted the details of each\n2www.allsides.com12219\nVery High High Mixed Low Very Low Total\nCenter 8,661 29,869 - - - 38,530\nLeft - 26,787 2,587 - - 29,374\nRight - - 11,251 12,937 4,095 28,283\nTotal 96,187\nTable 3: Joint modeling dataset statistics.\nFigure 1: Topics distribution in our dataset.\nsource as JSON-formatted lines from the HTML\ncode. (ii) In the article link parsing stage, front-\npage article links from these media sources were\nparsed according to specific criteria. Only links\nthat were internal to the domain and have more than\n65 characters in length, excluding links from the\nmenu button. (iii) In the article text collection stage,\nthe previously selected article links were used to\nretrieve the title and full text of the articles. We use\nscript code and manually test to ensure effective\ntext extraction. (iv) Finally, the post-processing\nstage involved formatting the collected data in the\nrequired JSON format.\n3.2 Article-level\nArticles Collection As illustrated in Figure 2\n(Appendix A) for the data at the article-level we\nobtained the medium with the respective label from\nthe data at the media-level. Subsequently, the se-\nlection of the media for parsing involved manually\nselecting the available sources with minimum 100\narticles in their archive to have sufficient data to\nbase our predictions on. Afterthat, it required to\ndistinct structure of each website and analysis of\ntheir HTML code using a browser code inspector to\nidentify relevant tags for efficient parsing. The arti-\ncles parsing function facilitates this process in four\nstages: (i) initially retrieving the complete code\nfrom the archive page of the article, (ii) analyzing\nthis code to extract a list of articles (including ti-Set Media-level (A) Media-level (B) Article-level (A) Article-level (B) Joint modeling\nTrain 1,704 2,354 83,180 57,433 57,433\nDevelopment 86 77 10,000 10,000 10,000\nTest (Eng) 86 77 28,180 28,754 28,754\nTest (Multi) 84 105 47,489 54,527\nTest (Eng-EA) 17,456\nTable 4: Train/development/test sets distribution over\nmedia-level, article-level and joint modeling datasets.\ntles and links), (iii) making a secondary request to\ngather the full text of each article, and (iv) finally\ncompiling these data into a JSON format.\nAllsides Data obtained from AllSides were col-\nlected from the entire archive using a strategy simi-\nlar to that used for the article-level.\n3.2.1 Joint Modeling\nTo gather data on political bias and factuality at\nthe article-level, for joint modeling, we utilized the\nmethod from the article-level as shown in Figure 2\n(Appendix A), however, we combined the labels:\npolitical bias and factuality.\n3.3 Data Curation\nWe applied the same curation method for media-\nlevel, article-level, and joint modeling. As shown\nin Figure 2 (Appendix A), the curation process\ninvolved evaluating the dataset according to the\nlength of the article. Longer articles were typically\nfound in sources considered more factual, while\nno similar trend was observed for political bias.\nTo reduce the impact of very short or excessively\nlong texts, which might be less relevant or contain\nmixed content (e.g., advertising), we focused on\narticles between 500 and 1,500 words. This range\nwas chosen because the average article length in our\ndataset is 1,000 words. Although this approach may\nnot entirely eliminate bias, it helps to ensure more\ninformative representation and reduces potential\nbias across languages.\nWhen we obtain media articles, we first remove\nduplicate content. We also meticulously removed\nHTML artifacts, such as tags, scripts, and CSS\nelements, to ensure that only actual textual content\nwas retained. Alongside the advertisements, non-\nrelevant elements such as navigation menus and\nfooters were manually filtered out.\n4 SAFARI Benchmark\n4.1 Poltical Bias and Factuality\n4.1.1 Media-level\n(A) Given the news article(s) of a news outlet\n(e.g., www.bloomberg.com ), predict the overall po-12220\nHard Voting Soft Voting\nModel MAE F1 A P R MAE F1 A P R\nEnglish\nmBERT Base 0.183 82.43 82.37 83.99 82.37 0.050 80.87 80.77 81.22 80.77\nXLM-R Base 0.215 79.80 79.71 80.79 79.71 0.128 81.59 81.46 81.22 81.46\nmDeBERTaV3 Base 0.149 83.77 83.75 83.95 83.75 0.145 81.98 81.94 80.10 81.94\nDistilmBERT Base 0.176 83.64 83.78 87.23 87.78 0.125 84.46 84.37 84.19 84.37\nmBART Large 0.126 84.83 84.88 84.07 84.88 0.125 84.93 84.89 85.08 84.89\nEnsemble 0.125 84.95 84.91 85.02 84.91 0.120 84.96 84.92 84.95 84.92\nMultilingual\nmBERT Base 1.052 26.64 37.50 25.52 37.50 1.052 25.74 37.50 24.25 37.50\nXLM-R Base 1.062 26.54 36.45 25.77 36.45 1.104 23.58 32.29 22.03 32.29\nmDeBERTaV3 Base 1.052 29.05 36.45 33.44 36.45 1.010 32.12 39.58 40.26 39.58\nDistilmBERT Base 1.302 22.98 26.04 21.82 26.04 1.364 20.29 22.91 19.46 22.91\nmBART large 1.063 27.45 33.33 37.72 33.33 1.062 27.02 33.32 37.17 33.32\nEnsemble 1.117 27.44 37.62 27.14 37.62 1.118 26.88 36.63 25.52 36.63\nTable 5: Analysis of political bias using hard and soft\nvotings for each framework and ensemble at media-\nlevel (A). Bold values indicate the best scores for each\ncategory.\nlitical bias of that news outlet as: LEFT -,LEFT -\nCENTER ,CENTER -,RIGHT -CENTER OR RIGHT -\nLEANING .\n(B) Given the news article(s) of a news outlet\n(e.g., www.bloomberg.com ), predict the overall fac-\ntual reporting of that news outlet as: VERY HIGH ,\nHIGH ,MOSTLY FACTUAL ,MIXED ,LOW OR VERY\nLOW.\n4.1.2 Article-level\n(A) Given an article, classify its political bias as:\nLEFT ,CENTER ,OR RIGHT .\n(B) Given an article, classify its factual reporting\nas: VERY HIGH ,HIGH ,MIXED ,LOW ,OR VERY\nLOW.\n4.1.3 Joint Modeling\nGiven an article, classify its political bias and\nfactual reporting jointly as: CENTER -VERY HIGH ,\nCENTER -HIGH ,LEFT -HIGH ,LEFT -MIXED ,RIGHT -\nMIXED ,RIGHT -LOW AND RIGHT -VERY LOW .\nImportant In joint modeling of political bias\nand factuality, specific bias labels are strongly cor-\nrelated with certain factuality levels (Baly et al.,\n2019). For example, a \u201ccenter\u201d bias typically cor-\nresponds to \u201cvery high\u201d or \u201chigh\u201d factuality. The\nexpert-annotated data we collected from MBFC\nreflect this correlation, as it does not include un-\ncommon combinations (e.g., left-low or right-high)\nshown in Table 3. This absence aligns with the\nsource\u2019s correlation and annotation guidelines.\n4.2 Dataset Statistics\n4.2.1 Media-level\nTable 1 presents the total amount of media and\nits distribution across languages for each label.\nFor both sets, we have the same train/val/test sets.\nWhen data were acquired, as shown in Table 4, theHard Voting Soft Voting\nModel MAE F1 A P R MAE F1 A P R\nEnglish\nmBERT Base 0.132 83.20 83.19 83.23 83.19 0.090 82.93 82.59 82.50 82.59\nXLM-R Base 0.223 80.79 80.94 80.22 80.94 0.532 62.84 70.12 70.75 70.12\nmDeBERTaV3 Base 0.188 81.82 81.99 81.03 81.82 0.207 81.22 81.01 81.63 81.01\nDistilmBERT Base 0.110 81.56 81.60 81.14 81.60 0.519 60.38 67.85 56.15 67.85\nmBART Large 0.049 82.39 82.38 82.41 82.38 0.415 71.28 64.15 82.15 64.15\nEnsemble 0.142 81.49 81.59 81.15 81.59 0.143 81.83 81.36 81.48 86.36\nMultilingual\nmBERT Base 1.183 29.60 27.50 36.60 27.50 0.980 30.25 35.57 31.01 35.57\nXLM-R Base 1.006 29.76 39.71 30.88 39.71 1.490 15.00 25.00 19.24 25.00\nmDeBERTaV3 Base 1.054 24.78 30.37 38.52 30.37 1.230 21.34 27.88 37.35 27.88\nDistilmBERT Base 1.090 28.84 39.85 32.47 39.85 1.394 12.65 23.07 13.24 23.07\nmBART Large 1.386 25.45 22.73 35.70 22.73 1.240 27.00 29.80 29.91 29.80\nEnsemble 0.872 38.44 50.00 44.18 50.00 0.854 40.76 50.01 42.38 50.01\nTable 6: Analysis of factuality using hard and soft vot-\nings for each framework and in ensemble at media-level\n(B).\ndataset was segmented into training, development,\nand testing sets. There is a single combined train-\ning and validation set, exclusively in English. For\ntesting, there are two distinct sets: the first is in\nEnglish, while the second is multilingual for both\npolitical bias and factuality.\n4.2.2 Article-level\nATable 2 presents the total number of articles\nwith their political bias and distribution between\nlanguages for each label. Furthermore, Table 4\nillustrates that we have a single set of training\nand validation articles, both exclusively in English,\ncompiled using distant supervision. In addition,\nthere are three testing sets: the first comprises En-\nglish articles collected through distant supervision\n(DS), the second is an English test set assembled\nfrom AllSides, annotated by experts (EA), and the\nthird is a multilingual test set of articles.\nBAs shown in Table 4, for the factuality of re-\nporting of news articles, we have only one train and\nvalidation sets of articles in English. Our test sets\ncomprise two distinct types: English and multilin-\ngual.\n4.2.3 Joint Modeling\nTable 3 presents the total number of articles and\ntheir distribution by label. Furthermore, Table 4\nillustrates the distribution of data in train/validation\nand test sets, which are only given in English.\nNote: We carefully split the dataset into\ntrain/development/test sets to avoid data leakage.\nEach split is unique and ensures that no media or\narticles previously exposed to the model are in-\ncluded in the other sets. The splits were performed\nusing a stratified sampling approach to maintain\nthe distribution of classes across all sets. The test\nsets are unique and exclude articles from sources\npreviously exposed to the model. Moreover, the12221\nPolitical Bias Factuality\nModel MAE F1 A P R MAE F1 A P R\nEnglish-DS\nmBERT Base 0.168 81.46 81.49 81.46 81.50 0.188 81.18 81.22 81.23 81.22\nXLM-R Base 0.130 81.33 81.37 81.35 81.37 0.160 81.54 81.57 81.55 81.57\nmDeBERTaV3 Base 0.131 81.38 81.41 81.39 81.41 0.163 81.45 81.41 81.63 81.41\nDistilmBERT Base 0.162 81.23 81.27 81.25 81.27 0.162 81.49 81.52 81.49 81.52\nHard V oting 0.122 82.06 82.02 82.20 82.02 0.158 81.73 81.70 81.82 81.70\nSoft V oting 0.112 82.62 82.59 82.70 82.59 0.157 81.88 81.87 81.91 81.87\nMultilingual\nmBERT Base 0.630 61.60 61.26 67.41 61.26 0.492 66.54 66.75 68.31 66.75\nXLM-R Base 0.601 62.99 62.53 68.17 62.53 0.479 67.22 67.67 70.18 67.67\nmDeBERTaV3 Base 0.609 62.85 62.42 66.68 62.42 0.480 66.07 67.59 74.08 67.59\nDistilmBERT Base 0.627 62.45 61.92 69.32 61.92 0.498 65.73 65.80 65.76 65.80\nHard V oting 0.590 63.09 63.57 66.97 63.57 0.497 65.89 65.81 65.99 65.81\nSoft V oting 0.696 63.02 63.46 68.91 63.46 0.494 66.26 66.14 67.06 66.14\nEnglish-EA\nmBERT Base 0.223 67.33 67.38 68.70 67.38\nXLM-R Base 0.228 66.86 66.95 68.36 66.95\nmDeBERTaV3 Base 0.229 67.52 67.32 68.96 67.32\nDistilmBERT Base 0.233 66.47 66.54 67.80 66.54\nHard V oting 0.200 69.44 69.46 69.39 69.46\nSoft V oting 0.192 70.01 69.97 71.73 69.97\nTable 7: Analysis of political bias and factuality using\nframeworks independently and ensembles using hard\nvoting and soft voting at article-level. DS - distant su-\npervision. EA - Expert annotated data from AllSides.\nEnglish and multilingual test samples are unique\nand have no connection between them, as they orig-\ninate from different news outlets and languages.\nThis separation ensures an unbiased evaluation of\nthe model performance across different languages\nand contexts.\n4.3 Cross-lingual Assessment\nThe dataset predominantly consists of data in En-\nglish with labels for both tasks; however, dataset\nlacks labeled articles and media in some other lan-\nguages. To address this challenge, we employ the\ncross-lingual assessment.\nAt the media-level, we employ five MPLMs:\nmBERT BaseBase (Devlin et al., 2019), XLM-R Base\n(Conneau et al., 2019), DistilmBERT Base(Sanh\net al., 2019), mDeBERTaV3 Base(He et al., 2021),\nand mBART Large(Liu et al., 2020). However, at\nthe article-level and in joint modeling, we used the\nsame MPLMs with the exception of mBART.\nIn a previous study Baly et al. (2020a) to detect\npolitical bias at the article level, adversarial media\nadaptation and specially adapted triplet loss were\nused. Furthermore, to predict political bias and\nfactuality at media-level Baly et al. (2018) utilized\na comprehensive set of features extracted from var-\nious sources: articles, Wikipedia page, Twitter ac-\ncount, URL structure and web traffic data from\ntarget media and in joint modeling. Baly et al.\n(2019) investigates the detection of trustworthiness\nand political ideology in news outlets using a multi-\ntask ordinal regression framework, establishing a\nconnection between political bias and low trust-\nworthiness. This research shows that joint mod-Model MAE F1 A P R\nmBERT Base 0.146 81.50 81.17 81.39 80.69\nXLM-R Base 0.147 81.35 82.82 81.23 80.44\nmDeBERTaV3 Base 0.145 82.03 81.46 81.46 80.85\nDistilmBERT Base 0.149 81.01 82.50 83.15 80.06\nHard V oting 0.146 83.57 83.13 82.07 80.70\nSoft V oting 0.145 83.81 83.50 83.29 80.97\nTable 8: Analysis of politcal bias and factuality jointly\nusing each model independently and in ensemble using\nhard and soft votings.\neling significantly exceeds isolated methods. In\nour study, we use a traditional ensemble learning\nmethod (Freund and Schapire, 1997) in the analysis\nat the article and media levels, using hard and soft\nvotings for performance optimization; the architec-\nture is shown in Figure 3 (Appendix A).\nAt the media-level , we integrate the predictions\nfrom individual articles into their media sources\nusing both hard and soft voting methods, along with\ncombining the models in an ensemble approach.\nAt the article-level , we collate multiple model\npredictions and individual models for classification\nof articles.\nThe elaboration of the hard voting is in Equation\n1, and the soft voting is in Equation 2.\nLetPibe the predicted label political bias or fac-\ntuality of the i-th article. The aggregated political\nbias and factuality Pmcan be calculated as follows:\nPm=mode (P1, P2, . . . , P n). (1)\nFor soft voting, let Pi,jbe the predicted probabil-\nity of the i-th article belonging to the j-th political\nbias class or factuality. The aggregated political\nbias and factuality Pmcan be calculated as follows:\nPm= arg max\nj/parenleft\uf8ecigg\n1\nnn/summationdisplay\ni=1Pi,j/parenright\uf8ecigg\n. (2)\nForjoint modeling , our study uses One Hot En-\ncoding (OHE) (Bishop, 2006) to accommodate\nmulti-class labels (e.g., left, center, right; very high,\nhigh, mixed, low, very low) within our loss func-\ntion. Our dataset comprises various classes repre-\nsenting different political biases and the factuality\nof the reporting. To effectively train our model,\nthese classes are transformed into binary format,\nresulting in a label array such as [0,1,0]for po-\nlitical bias and [0,0,1,0,0]for factuality. This\nrepresentation ensures an optimal interpretation by\nthe model. Using OHE, we facilitate the model\u2019s\nability to handle and learn from the multi-faceted12222\nPolitical Bias Factuality\nModel MAE F1 A P R MAE F1 A P R\nEnglish\nMistral 1.247 30.21 33.67 37.19 33.67 1.242 11.12 21.07 19.04 21.07\nLLaMA2 1.134 22.12 34.70 40.34 34.70 1.601 19.10 26.21 18.75 26.21\nEnsemble 1.160 27.97 32.00 27.18 32.00 1.581 15.14 20.93 17.79 20.93\nMultilingual\nMistral 1.564 18.72 22.88 17.54 22.88 1.003 7.99 20.03 28.77 20.03\nLLaMA2 1.560 4.14 13.68 36.20 13.68 1.676 20.62 26.06 18.97 26.06\nEnsemble 1.484 16.71 22.22 22.93 22.22 1.076 25.73 30.81 25.00 25.73\nTable 9: Analysis of political bias and factuality of\nreporting using hard voting for each framework and\nensemble of models at media-level.\nnature of our data. We use the tokenizer\u2019s function\nin our pipeline, whose primary function is to con-\nvert textual data into embeddings, a critical step\nin preparing the data for model training. However,\nthe tokenizer does not directly participate in the\ntransformation of the label space. The conversion\nof label formats is handled by a separate function\nin our data pre-processing pipeline.\nThis task can be formulated as follows: Given\nthe one-hot encoded vectors for political bias yP\nand factuality yF, and the features xof an article,\nthe joint prediction can be modeled as shown in\nEquation 3:\n\u02c6y=softmax (Wx+b), (3)\nwhere \u02c6yis the predicted probability distribution\nover the joint classes of political bias and factuality,\nWis the weight matrix and bis the vector.\nThe loss function for training the model is de-\nfined as the sum of the cross-entropy losses for\npolitical bias and factuality, as expressed in Equa-\ntion 4:\nL=\u2212\uf8eb\n\uf8ed/summationdisplay\njyP,jlog \u02c6yP,j+/summationdisplay\nkyF,klog \u02c6yF,k\uf8f6\n\uf8f8,\n(4)\nwhere yP,jandyF,kare the true labels of polit-\nical bias and factuality, respectively, and \u02c6yP,jand\n\u02c6yF,kare the predicted probabilities.\nEvaluation Measures We evaluate our frame-\nworks using the following measures: Mean Abso-\nlute Error (MAE), F1 Score (F1), Accuracy (A),\nPrecision (P), and Recall (R). We report MAE\ngiven the ordinal nature of both the factual and\npolitical bias classes (Baly et al., 2018, 2020b).\nFurthermore, we provide Weighted Average for F1,\nPrecision and Recall due to class imbalance. Addi-\ntionally, we evaluated the stability of our MPLMs\nby averaging the results over 3-5 independent runsPolitical Bias Factuality\nModel MAE F1 A P R MAE F1 A P R\nEnglish-DS\nMistral 0.732 45.06 48.70 56.02 48.70 1.637 13.99 21.30 15.15 21.30\nLLaMA2 0.748 46.56 48.92 55.50 48.92 1.233 16.85 24.56 15.30 24.56\nEnsemble 0.747 46.84 48.33 49.98 48.33 1.287 20.72 27.54 18.24 27.54\nMultilingual\nMistral 0.880 40.62 42.26 45.22 42.26 1.744 10.46 19.31 14.87 19.31\nLLaMA2 0.835 38.98 42.16 42.66 42.16 1.581 16.19 23.19 14.96 23.19\nEnsemble 0.841 43.30 44.41 44.89 44.41 1.630 13.03 20.94 11.54 20.94\nEnglish-EA\nMistral 0.838 40.05 41.53 43.50 41.53\nLLaMA2 0.809 36.64 41.57 41.31 41.57\nEnsemble 0.817 39.67 41.63 42.28 41.63\nTable 10: Analysis of political bias and factuality using\nframeworks independently and ensembles using hard\nvoting at article-level.\nusing various seeds by computing the standard de-\nviation.\n5 Experimental Setup & Results\n5.1 Experimental Setup\nThe experimental setup for all tasks involved con-\nsistent hyper-parameters across various MPLMs,\nwith minor task-specific adjustments. More details\ncan be seen in Appendix A.\n5.2 Results\nMedia-level In our analysis that includes the de-\ntection of political bias and factuality in various\nmodels, we observe a notable performance in En-\nglish and multilingual contexts. For the detection\nof political bias, as illustrated in Table 5, the en-\nsemble of models shines in the English set with\nhigher scores, while mDeBERTaV3 excels in the\ndata of multilingual political bias using soft vot-\ning. In contrast, DistilmBERT performs poorly in\nmultilingual bias detection. When we analyze the\nfactuality, as shown in Table 6, mBERT emerges as\nthe best performer in the English dataset using hard\nvoting, but XLM-R and DistilmBERT lag behind.\nIn the multilingual context, soft voting outperforms\nothers.\nArticle-level Analyzing political bias and factu-\nality in English distant supervision and expert an-\nnotated sets, and multilingual set, the performance\nof various models and ensemble methods employ-\ning hard and soft voting reveals promising results,\nas shown in Table 7. In the English-DS context\nfor both political bias and factuality, soft voting\nemerges as the most effective classifier, outperform-\ning all individual models with the highest scores\nin all evaluation measures. For the multilingual\ntest set, hard voting shows a slight advantage over12223\nModel MAE F1 A P R\nMistral 0.351 29.68 29.68 12.12 29.68\nLLaMA2 0.340 31.84 31.84 27.88 31.84\nEnsmeble 0.317 23.62 36.48 18.15 36.48\nTable 11: Analysis of political bias and factuality jointly\nusing each model independently and in ensemble using\nhard voting.\nother methods in detecting political bias. In con-\ntrast, XLM-R leads in the factuality assessment. In\nthe English-EA dataset, only political bias is eval-\nuated, and soft voting ensemble of models is the\nmost effective.\nJoint Modeling In analyzing the joint perfor-\nmance of political bias and factuality in multiple\nmodels and ensemble methods, we observe the dis-\ntinction. According to the results in Table 8, the\nensemble of models using soft voting clearly out-\nperforms all other individual classifiers. However,\na hard voting ensemble of models, slightly behind\nsoft voting, while still showing good performance,\nespecially in precision, where it almost matches\nsoft voting. Among the individual models, mDe-\nBERTaV3 is the most efficient in this joint task.\nSummary In summary, our study reveals that\nemploying the soft voting ensemble method is ef-\nfective across all tasks, albeit with nuances. This\neffectiveness comes in part from soft voting by\naveraging scores, leading to performance variabil-\nity depending on the balance of weak and strong\nmodels. This was particularly evident in the multi-\nlingual test sets for article-level political bias and\nfactuality, as well as in the multilingual test set\nfor media-level bias and the English test set for\nmedia-level factuality. Furthermore, given the time\nand cost constraints associated with human annota-\ntions, the use of distant supervision data is a help-\nful approach3(more details can be seen in Sub-\nsection 6.2). We observed that specific MPLMs,\nsuch as mBERT and XLM-R, excelled in differ-\nent tasks. The media-level dataset includes up to\n30 articles per media outlet, ensuring comprehen-\n3We conducted a manual analysis of a total of 500 articles\nfrom 124 media outlets and 1000 articles from 219 media\noutlets, randomly selected from AllSides. We cross-referenced\nthese articles with Media Bias/Fact Check labels. Interestingly,\n471 (94.2%) and 945 (94.5%) of the articles aligned perfectly\nwith their respective outlet label, demonstrating the reliability\nof the DS data for our tasks and strengthening our assumption.\nFurthermore, these articles were chosen to ensure a diverse\nrepresentation of the dataset, covering various media sources\nand biases.sive training, although this results in a predomi-\nnance of English data (around 95%). This pre-\ndominance aids in transferring the model\u2019s predic-\ntive capabilities to other languages, but leads to\nlower performance compared to the article-level\ndataset, which is larger and offers more data for\ntraining. Furthermore, the performance discrep-\nancy between the English and multilingual con-\nfigurations, as shown in Tables 5 and 6, can be\nattributed to several factors. Despite using a multi-\nlingual pre-trained model, fine-tuning on English\ndata does not generalize well to other languages\ndue to differences in vocabulary, syntax, grammar,\nand cultural contexts. Additionally, the model may\noverfit to English-specific patterns due to inten-\nsive English training and insufficient exposure to\ndiverse linguistic datasets during fine-tuning.\n6 Discussion\nIn this section, our analysis focuses on the latest\nLLMs, specifically Mistral 7Band LLaMA2 7B, ex-\namining their capabilities in zero-shot learning cou-\npled with ensemble using hard voting.\nFurthermore, we explore why models tested on\ndistant supervision data exhibit higher performance\nlevels compared to those tested on expert-annotated\ndata, specifically regarding the detection of politi-\ncal bias at the article-level in the English language.\n6.1 Overall Observation\nA notable challenge in our study is managing text\nlength, which poses complexities for LLMs. To\nmitigate this, we use BART (Lewis et al., 2019) for\nthe summarization of English texts and mT5 (Xue\net al., 2021) for the processing of multilingual con-\ntent with a minimum text length of 128 and a max-\nimum of 412. Our objective was to eliminate pars-\ning artifacts, reduce the input length required by\nLLMs, enhance data quality, and accelerate infer-\nence time. Subsequently, the pre-processed texts\nwere converted into task-specific prompts as out-\nlined in Section 4 and fed into LLMs.\nBased on our observation of the results in Ta-\nbles 9 and 10, LLMs in zero-shot learning settings\nrecognize political bias more effectively compared\nto factuality. Furthermore, due to the less fine-\ngrained labels for political bias at article-level com-\npared to factuality, LLMs easier predict political\nbias when there are fewer classes. In general, the\nperformance of Mistral, LLaMA2, and their en-\nsemble varies based on the tasks. Table 11 focuses12224\non joint modeling, where LLaMA2 outperforms\nMistral, and hard voting stands out for its overall\naccuracy.\n6.2 Distant Supervision vs. Expert Annotation\nTwo primary factors explain the performance differ-\nence between the models evaluated in EA vs.DS.\nFirst, the models were trained and evolved only on\nEnglish data obtained via DS that differ in quality\nand detail from EA. Second, expert-annotated data,\nwhich are considered gold labels, are more accurate\nand have more detailed annotations. This complex-\nity is a significant barrier for the models because,\nin their training and development phases, they have\nnot been exposed to such data, making it difficult\nfor them to appropriately identify and adjust to the\nnuances present in the expert-annotated test set.\n7 Conclusion & Future Work\nIn this article, we introduce SAFARI , a new large-\nscale corpus for cross-lingual evaluation at the me-\ndia and article levels, specifically designed for the\ndetection of political bias and factuality of report-\ning, along with our data construction pipeline. Fur-\nthermore, we present an exclusive English dataset\nfor joint modeling at the article-level. We also\ncompare the performance of distant supervision\nvs.human-annotated data for political bias at the\narticle-level. Moreover, our corpus is evaluated\nusing MPLMs, and we implement hard and soft\nensemble learning voting for all tasks. Lastly, we\nexperimented with LLMs using hard voting.\nIn future work, our aim is to gather a larger mul-\ntilingual corpus and conduct a more fine-grained\nanalysis of political bias and factuality. Acknowl-\nedging that the U.S.-centric left/center/right politi-\ncal spectrum is not universally applicable, we plan\nto model biases that are more relevant to different\nregions and cultures. We also intend to collabo-\nrate with experts, seek alternative data sources, and\nexpand the date ranges of news outlets to reduce\ndata imbalance and create a larger and more diverse\ndataset. Furthermore, we plan to perform a multi-\nmodal analysis of political bias and factuality in\nnews media and articles. We will also deepen our\nerror analysis, breaking it down by language to im-\nprove performance. Additionally, we will conduct\nexperiments to study cross-lingual abilities in de-\ntail, focusing on discrepancies in factuality and po-\nlitical bias for articles on the same topic across dif-\nferent languages, and stratify results based on topicdistribution. Finally, we plan to investigate politi-\ncal bias and factuality using fine-tuned LLMs, po-\ntentially leveraging techniques such as LoRA (Hu\net al., 2021) and QLoRA (Dettmers et al., 2023).\nLimitations\nWe created a corpus for diverse languages, increas-\ning the accessibility of NLP research in cross-\nlingual studies. However, we were only able to\ncover ten languages at the article and media lev-\nels, each. For some languages, we had only one\nor two labels assigned for both tasks due to the\nunavailability of annotated sources and articles in\nother languages. Additionally, for joint modeling,\nwe intended to conduct a cross-lingual evaluation;\nhowever, we faced limitations in identifying suffi-\ncient media sources in other languages for an effec-\ntive evaluation, primarily due to the challenge of\nfinding comprehensive sources that encompass the\nnecessary labels. Moreover, we find it problematic\nto use these data for news sites in some other coun-\ntries. Furthermore, due to limited computational\nresources, we were unable to fully fine-tune our\nLLMs (e.g., Mistral and LLaMA2).\nEthical Statement & Bias\nThe dataset was compiled with a firm commitment\nto comply with legal and ethical standards. This\ninvolved a careful review of the terms of use of all\nwebsites and ensuring that data collection processes\nrespect these terms. The compilation focused exclu-\nsively on publicly available data, without bypassing\naccess control measures such as paywalls or sub-\nscription models. The data collection methods used\nwere transparent and deliberately designed to min-\nimize any potential adverse impact on the source\nwebsites. Including limiting the frequency of ac-\ncess to avoid any strain on their resources. The\nnews articles are not publicly available; only the\nURLs of the media and the recipe scraping with\nlabels are provided to support research while pre-\nserving the confidentiality of the source.\nUsers should consider inherent biases in the me-\ndia sources and annotations when interpreting the\nresults. We include a diverse range of media outlets\nto minimize potential bias. This dataset can exhibit\ncertain label biases due to restricted domain cov-\nerage. However, we diligently worked to mitigate\nany detrimental biases by manual data assessment.12225\nReferences\nJisun An, Meeyoung Cha, Krishna Gummadi, Jon\nCrowcroft, and Daniele Quercia. 2012. Visualiz-\ning media bias through Twitter. In AAAI ICWSM ,\nvolume 6.\nJisun An, Meeyoung Cha, P. Krishna Gummadi, and\nJon Crowcroft. 2011. Media landscape in Twitter: A\nworld of new conventions and political diversity. In\nAAAI ICWSM .\nDilshod Azizov, S Liang, and P Nakov. 2023. Frank at\ncheckthat! 2023: Detecting the political bias of news\narticles and news media. Working Notes of CLEF .\nRamy Baly, Giovanni Da San Martino, James Glass,\nand Preslav Nakov. 2020a. We can detect your bias:\nPredicting the political ideology of news articles. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) ,\npages 4982\u20134991, Online. Association for Computa-\ntional Linguistics.\nRamy Baly, Giovanni Da San Martino, James Glass,\nand Preslav Nakov. 2020b. We can detect your bias:\nPredicting the political ideology of news articles. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) ,\npages 4982\u20134991.\nRamy Baly, Georgi Karadzhov, Dimitar Alexandrov,\nJames Glass, and Preslav Nakov. 2018. Predict-\ning factuality of reporting and bias of news media\nsources. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing ,\npages 3528\u20133539, Brussels, Belgium. Association\nfor Computational Linguistics.\nRamy Baly, Georgi Karadzhov, Jisun An, Haewoon\nKwak, Yoan Dinkov, Ahmed Ali, James Glass, and\nPreslav Nakov. 2020c. What was written vs. who\nread it: News media profiling using text analysis\nand social media context. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics , pages 3364\u20133374, Online. Association\nfor Computational Linguistics.\nRamy Baly, Georgi Karadzhov, Abdelrhman Saleh,\nJames Glass, and Preslav Nakov. 2019. Multi-task\nordinal regression for jointly predicting the trustwor-\nthiness and the leading political ideology of news\nmedia. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers) , pages\n2109\u20132116, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAlberto Barr\u00f3n-Cede\u00f1o, Firoj Alam, Tommaso Caselli,\nGiovanni Da San Martino, Tamer Elsayed, An-\ndrea Galassi, Fatima Haouari, Federico Ruggeri, Ju-\nlia Maria Stru\u00df, Rabindra Nath Nandi, et al. 2023a.\nThe clef-2023 checkthat! lab: Checkworthiness,\nsubjectivity, political bias, factuality, and authority.InEuropean Conference on Information Retrieval ,\npages 506\u2013517. Springer.\nAlberto Barr\u00f3n-Cede\u00f1o, Firoj Alam, Andrea Galassi,\nGiovanni Da San Martino, Preslav Nakov, Tamer\nElsayed, Dilshod Azizov, Tommaso Caselli, Gullal S\nCheema, Fatima Haouari, et al. 2023b. Overview\nof the clef\u20132023 checkthat! lab on checkworthiness,\nsubjectivity, political bias, factuality, and authority\nof news articles and their source. In International\nConference of the Cross-Language Evaluation Forum\nfor European Languages , pages 251\u2013275. Springer.\nPurnima Bholowalia and Arvind Kumar. 2014. Ebk-\nmeans: A clustering technique based on elbow\nmethod and k-means in wsn. International Journal\nof Computer Applications , 105(9).\nChristopher M Bishop. 2006. Pattern recognition and\nmachine learning. Springer google schola , 2:645\u2013\n678.\nEmina Boudemagh and Izabela Moise. 2017. News\nmedia coverage of refugees in 2016: a GDELT case\nstudy. In AAAI ICWSM .\nMaxwell T Boykoff and Jules M Boykoff. 2004. Bal-\nance as bias: Global warming and the us prestige\npress. Global environmental change , 14(2):125\u2013136.\nCeren Budak, Sharad Goel, and Justin M Rao. 2016.\nFair and balanced? quantifying media bias through\ncrowdsourced content analysis. Public Opinion\nQuarterly , 80(S1):250\u2013271.\nSergio Burdisso, Dairazalia S\u00e1nchez-Cort\u00e9s, Esa\u00fa\nVillatoro-Tello, and Petr Motlicek. 2024. Reliability\nestimation of news media sources: Birds of a feather\nflock together. arXiv preprint arXiv:2404.09565 .\nCanini et al. 2011. Finding credible information sources\nin social networks based on content and social struc-\nture. In IEEE SocialCom/PASSAT .\nCarlos Castillo, Marcelo Mendoza, and Barbara Poblete.\n2011. Information credibility on twitter. In Proceed-\nings of the 20th International Conference on World\nWide Web, WWW 2011, Hyderabad, India, March 28\n- April 1, 2011 , pages 675\u2013684. ACM.\nWei-Fan Chen, Henning Wachsmuth, Khalid Al Khatib,\nand Benno Stein. 2018. Learning to flip the bias of\nnews headlines. In Proceedings of the 11th Interna-\ntional conference on natural language generation ,\npages 79\u201388.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116 .\nAndres Cremisini, Daniela Aguilar, and Mark A Fin-\nlayson. 2019. A challenging dataset for bias detec-\ntion: the case of the crisis in the ukraine. In Social,12226\nCultural, and Behavioral Modeling: 12th Interna-\ntional Conference, SBP-BRiMS 2019, Washington,\nDC, USA, July 9\u201312, 2019, Proceedings 12 , pages\n173\u2013183. Springer.\nGiovanni Da San Martino, Firoj Alam, Maram Hasanain,\nRabindra Nath Nandi, Dilshod Azizov, and Preslav\nNakov. 2023. Overview of the CLEF-2023 Check-\nThat! lab task 3 on political bias of news arti-\ncles and news media. In Working Notes of CLEF\n2023\u2013Conference and Labs of the Evaluation Forum ,\nCLEF \u20192023, Thessaloniki, Greece.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers) , pages\n4171\u20134186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nXin Luna Dong, Evgeniy Gabrilovich, Kevin Murphy,\nVan Dang, Wilko Horn, Camillo Lugaresi, Shao-\nhua Sun, and Wei Zhang. 2015. Knowledge-based\ntrust: Estimating the trustworthiness of web sources.\nVLDB Endow. , 8(9):938\u2013949.\nSebastian Dungs, Ahmet Aker, Norbert Fuhr, and Kalina\nBontcheva. 2018. Can rumour stance alone pre-\ndict veracity? In Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics ,\npages 3360\u20133370, Santa Fe, New Mexico, USA. As-\nsociation for Computational Linguistics.\nLisa Fan, Marshall White, Eva Sharma, Ruisi Su, Pra-\nfulla Kumar Choubey, Ruihong Huang, and Lu Wang.\n2019. In plain sight: Media bias through the lens of\nfactual reporting. arXiv preprint arXiv:1909.02670 .\nMichael F\u00e4rber, Victoria Burkard, Adam Jatowt, and\nSora Lim. 2020. A multidimensional dataset based\non crowdsourcing for analyzing and detecting news\nbias. In Proceedings of the 29th ACM International\nConference on Information & Knowledge Manage-\nment , CIKM \u201920, page 3007\u20133014, New York, NY ,\nUSA. Association for Computing Machinery.\nNatalie Fenton. 2009. News in the digital age. In The\nRoutledge companion to news and journalism , pages\n557\u2013567. Routledge.\nYoav Freund and Robert E Schapire. 1997. A decision-\ntheoretic generalization of on-line learning and an\napplication to boosting. Journal of computer and\nsystem sciences , 55(1):119\u2013139.\nMatthew Gentzkow and Jesse M. Shapiro. 2010. What\ndrives media slant? evidence from U.S. daily news-\npapers. Econometrica , 78(1):35\u201371.Martin Gilens and Craig Hertzman. 2000. Corporate\nownership and news bias: Newspaper coverage of\nthe 1996 telecommunications act. The Journal of\nPolitics , 62(2):369\u2013386.\nTim Groeling. 2013. Media bias by the numbers: Chal-\nlenges and opportunities in the empirical study of\npartisan news. Annual Review of Political Science ,\n16.\nTim Groseclose and Jeffrey Milyo. 2005. A measure\nof media bias. The Quarterly Journal of Economics ,\n120(4):1191\u20131237.\nXiaobo Guo, Weicheng Ma, and Soroush V osoughi.\n2022. Measuring media bias via masked language\nmodeling. In Proceedings of the International AAAI\nConference on Web and Social Media , volume 16,\npages 1404\u20131408.\nFelix Hamborg, Anastasia Zhukova, and Bela Gipp.\n2019. Automated identification of media bias by\nword choice and labeling in news articles. In 2019\nACM/IEEE Joint Conference on Digital Libraries\n(JCDL) , pages 196\u2013205. IEEE.\nMomchil Hardalov, Anton Chernyavskiy, Ivan Koychev,\nDmitry Ilvovsky, and Preslav Nakov. 2022. Crowd-\nChecked: Detecting previously fact-checked claims\nin social media. In Proceedings of the 2nd Confer-\nence of the Asia-Pacific Chapter of the Association\nfor Computational Linguistics and the 12th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing , AACL_IJCNLP \u201922, online.\nHans JG Hassell, John B Holbein, and Matthew R Miles.\n2020. There is no liberal media bias in which news\nstories political journalists choose to cover. Science\nadvances , 6(14):eaay9344.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning. arXiv preprint arXiv:2111.09543 .\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021. Lora: Low-rank adap-\ntation of large language models. arXiv preprint\narXiv:2106.09685 .\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, et al. 2023. Mistral\n7b.arXiv preprint arXiv:2310.06825 .\nJohannes Kiesel, Maria Mestre, Rishabh Shukla, Em-\nmanuel Vincent, Payam Adineh, David Corney,\nBenno Stein, and Martin Potthast. 2019a. Semeval-\n2019 task 4: Hyperpartisan news detection. In Pro-\nceedings of the 13th International Workshop on Se-\nmantic Evaluation , pages 829\u2013839.12227\nJohannes Kiesel, Maria Mestre, Rishabh Shukla, Em-\nmanuel Vincent, Payam Adineh, David Corney,\nBenno Stein, and Martin Potthast. 2019b. SemEval-\n2019 task 4: Hyperpartisan news detection. In\nProceedings of the 13th International Workshop on\nSemantic Evaluation , pages 829\u2013839, Minneapolis,\nMinnesota, USA. Association for Computational Lin-\nguistics.\nElena Kochkina, Maria Liakata, and Arkaitz Zubiaga.\n2018. All-in-one: Multi-task learning for rumour\nverification. In Proceedings of the 27th International\nConference on Computational Linguistics , pages\n3402\u20133413, Santa Fe, New Mexico, USA. Associa-\ntion for Computational Linguistics.\nVivek Kulkarni, Junting Ye, Steve Skiena, and\nWilliam Yang Wang. 2018. Multi-view models for\npolitical ideology detection of news articles. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 3518\u2013\n3527, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nHaewoon Kwak and Jisun An. 2014. A first look at\nglobal news coverage of disasters by using the gdelt\ndataset. In SocInfo , pages 300\u2013308.\nHaewoon Kwak and Jisun An. 2016. Two tales\nof the world: Comparison of widely used world\nnews datasets GDELT and EventRegistry. In AAAI\nICWSM .\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-\nnoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension.\narXiv preprint arXiv:1910.13461 .\nSora Lim, Adam Jatowt, Michael F\u00e4rber, and Masatoshi\nYoshikawa. 2020. Annotating and analyzing biased\nsentences in news articles using crowdsourcing. In\nProceedings of the Twelfth Language Resources and\nEvaluation Conference , pages 1478\u20131484.\nSora Lim, Adam Jatowt, and Masatoshi Yoshikawa.\n2018. Understanding characteristics of biased sen-\ntences in news articles. In CIKM workshops , pages\n121\u2013128.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726\u2013742.\nJing Ma, Wei Gao, Prasenjit Mitra, Sejeong Kwon,\nBernard J. Jansen, Kam-Fai Wong, and Meeyoung\nCha. 2016. Detecting rumors from microblogs with\nrecurrent neural networks. In Proceedings of the\nTwenty-Fifth International Joint Conference on Artifi-\ncial Intelligence, IJCAI 2016, New York, NY, USA, 9-\n15 July 2016 , pages 3818\u20133824. IJCAI/AAAI Press.Jing Ma, Wei Gao, and Kam-Fai Wong. 2017. Detect\nrumors in microblog posts using propagation struc-\nture via kernel learning. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 708\u2013717,\nVancouver, Canada. Association for Computational\nLinguistics.\nMa et al. 2015. Detect rumors using time series of social\ncontext information on microblogging websites. In\nCIKM , pages 1751\u20131754.\nNikhil Mehta and Dan Goldwasser. 2023. An interactive\nframework for profiling news media sources. arXiv\npreprint arXiv:2309.07384 .\nNikhil Mehta, Mar\u00eda Leonor Pacheco, and Dan Gold-\nwasser. 2022. Tackling fake news detection by con-\ntinually improving social context representations us-\ning graph neural networks. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) , pages\n1363\u20131380.\nMike Mintz, Steven Bills, Rion Snow, and Dan Juraf-\nsky. 2009. Distant supervision for relation extraction\nwithout labeled data. In Proceedings of the Joint Con-\nference of the 47th Annual Meeting of the ACL and\nthe 4th International Joint Conference on Natural\nLanguage Processing of the AFNLP , pages 1003\u2013\n1011.\nSubhabrata Mukherjee and Gerhard Weikum. 2015.\nLeveraging joint interactions for credibility analy-\nsis in news communities. In Proceedings of the 24th\nACM International Conference on Information and\nKnowledge Management, CIKM 2015, Melbourne,\nVIC, Australia, October 19 - 23, 2015 , pages 353\u2013\n362. ACM.\nPreslav Nakov, Firoj Alam, Giovanni Da San Martino,\nMaram Hasanain, Rabindra Nath Nandi, Dilshod Az-\nizov, and Panayot Panayotov. 2023. Overview of the\nCLEF-2023 CheckThat! lab task 4 on factuality of\nreporting of news media. In Working Notes of CLEF\n2023\u2013Conference and Labs of the Evaluation Forum ,\nCLEF \u20192023, Thessaloniki, Greece.\nPreslav Nakov, Jisun An, Haewoon Kwak, Muham-\nmad Arslan Manzoor, Zain Muhammad Mujahid, and\nHusrev Sencar. 2024. A survey on predicting the fac-\ntuality and the bias of news media. In Findings of the\nAssociation for Computational Linguistics ACL 2024 ,\npages 15947\u201315962, Bangkok, Thailand and virtual\nmeeting. Association for Computational Linguistics.\nVan-Hoang Nguyen, Kazunari Sugiyama, Preslav\nNakov, and Min-Yen Kan. 2020. FANG: leveraging\nsocial context for fake news detection using graph\nrepresentation. In CIKM \u201920: The 29th ACM Inter-\nnational Conference on Information and Knowledge\nManagement, Virtual Event, Ireland, October 19-23,\n2020 , pages 1165\u20131174. ACM.12228\nJeremy Padgett, Johanna L Dunaway, and Joshua P Darr.\n2019. As seen on tv? how gatekeeping makes the US\nhouse seem more extreme. Journal of Communica-\ntion, 69(6):696\u2013719.\nPanayot Panayotov, Utsav Shukla, Husrev Taha Sen-\ncar, Mohamed Nabeel, and Preslav Nakov. 2022.\nGREENER: Graph neural networks for news media\nprofiling. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing ,\nEMNLP \u201922, Abu Dhabi, UAE.\nKashyap Popat, Subhabrata Mukherjee, Jannik Str\u00f6tgen,\nand Gerhard Weikum. 2016. Credibility assessment\nof textual claims on the web. In CIKM .\nKashyap Popat, Subhabrata Mukherjee, Jannik Str\u00f6tgen,\nand Gerhard Weikum. 2017. Where the truth lies:\nExplaining the credibility of emerging claims on the\nWeb and social media. In WWW Companion , pages\n1003\u20131012.\nPopat et al. 2018. CredEye: A credibility lens for ana-\nlyzing and explaining misinformation. In The Web .\nMartin Potthast, Johannes Kiesel, Kevin Reinartz, Janek\nBevendorff, and Benno Stein. 2018. A stylometric\ninquiry into hyperpartisan and fake news. In Proceed-\nings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) ,\npages 231\u2013240, Melbourne, Australia. Association\nfor Computational Linguistics.\nAhmed Sajwani, Alaa El Setohy, Ali Mekky, Diana Tur-\nmakhan, Lara Hassan, Mohamed El Zeftawy, Omar\nEl Herraoui, Osama Mohammed Afzal, Qisheng\nLiao, Tarek Mahmoud, Zain Muhammad Mujahid,\nMuhammad Umar Salman, Muhammad Arslan Man-\nzoor, Massa Baali, Jakub Piskorski, Nicolas Ste-\nfanovitch, Giovanni Da San Martino, and Preslav\nNakov. 2024. FRAPPE: FRAming, Persuasion, and\nPropaganda Explorer. In Proceedings of the 18th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics: System Demon-\nstrations , pages 207\u2013213, St. Julians, Malta. Associ-\nation for Computational Linguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108 .\nJackie Smith, John D McCarthy, Clark McPhail, and\nBoguslaw Augustyn. 2001. From protest to agenda\nbuilding: Description bias in media coverage of\nprotest events in Washington, DC. Social Forces ,\n79(4):1397\u20131423.\nStuart N Soroka. 2012. The gatekeeping function: Dis-\ntributions of information in media and the real world.\nThe Journal of Politics , 74(2):514\u2013528.\nTimo Spinde, Manuel Plank, Jan-David Krieger, Terry\nRuas, Bela Gipp, and Akiko Aizawa. 2022. Neural\nmedia bias detection using distant supervision with\nBABE\u2013bias annotations by experts. arXiv preprint\narXiv:2209.14557 .Peter Stefanov, Kareem Darwish, Atanas Atanasov, and\nPreslav Nakov. 2020. Predicting the topical stance\nand political leaning of media using tweets. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics , pages 527\u2013537,\nOnline. Association for Computational Linguistics.\nStevenson et al. 1973. Untwisting the news twisters: A\nreplication of Efron\u2019s study. Journalism Quarterly ,\n50(2):211\u2013219.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971 .\nFrancielle Vargas, Kokil Jaidka, Thiago Pardo, and Fab-\nr\u00edcio Benevenuto. 2023. Predicting sentence-level\nfactuality of news and bias of media outlets. In Pro-\nceedings of the 14th International Conference on\nRecent Advances in Natural Language Processing ,\npages 1197\u20131206, Varna, Bulgaria. INCOMA Ltd.,\nShoumen, Bulgaria.\nSoroush V osoughi, Deb Roy, and Sinan Aral. 2018.\nThe spread of true and false news online. science ,\n359(6380):1146\u20131151.\nPaul Waldman and James Devitt. 1998. Newspaper pho-\ntographs and the 1996 presidential election: The ques-\ntion of bias. Journal Mass Commun Q , 75(2):302\u2013\n311.\nDavid A Weaver and Bruce Bimber. 2008. Finding\nnews stories: a comparison of searches using Lexis-\nNexis and Google News. Journal Mass Commun Q ,\n85(3):515\u2013530.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies , pages 483\u2013498, On-\nline. Association for Computational Linguistics.\nKai-Cheng Yang and Filippo Menczer. 2023. Large lan-\nguage models can rate news outlet credibility. arXiv\npreprint arXiv:2304.00228 .\nArkaitz Zubiaga, Maria Liakata, Rob Procter, Geraldine\nWong Sak Hoi, and Peter Tolmie. 2016. Analysing\nhow people orient to and spread rumours in social\nmedia by looking at conversational threads. PLoS\nONE , 11(3):1\u201329.12229\nAppendix\nA Data Statement for SAFARI\nA.1 General Information\nDataset title SAFARI\nDataset version 1.0 (November 2023)\nData statement version 1.0 (October 2023)\nData collection period Media-level data were\ncollected from July 2023 to September 2023. The\narticle-level and joint modeling data were collected\nfrom September 2023 to November 2023. Articles\nspan from September 2012 to November 2023.\nA.2 Executive Summary SAFARI is a cross-\nlingual corpus focusing on ten languages at the\nmedia-level: English, German, Hindi, French,\nSpanish, Hebrew, Japanese, Italian, Arabic, and\nRussian. At the article-level, it includes English,\nFrench, Polish, German, Spanish, Italian, Bulgar-\nian, Hindi, Persian, and Russian. Media Bias/Fact\nCheck provided expert annotations for media-level\ndata. Article-level data were collected from web\narchives of media outlets and supplemented with\nexpert-annotated data from AllSides. Joint model-\ning used the article-level data collection approach.\nGranularity The granularity of the analysis dif-\nfers: the article-level uses a 3-point scale for politi-\ncal bias, while the media-level uses a 5-point scale.\nMedia annotated as left-center and right-center\nwere excluded to maintain distinct categories.\nDifference in Media Counts Factuality annota-\ntions (2.6k) and political bias annotations (2k) dif-\nfer due to the exclusion of sources labeled as \u201cQues-\ntionable Source,\u201d \u201cConspiracy-Pseudoscience,\u201d or\n\u201cSatire\u201d in political bias, resulting in fewer total\nannotations.\nA.3 Documentation for Source Datasets The\nSAFARI corpus was meticulously compiled for an\nin-depth analysis of political bias and factuality at\nthe media and article levels and for joint modeling.\nAtmedia-level , data was obtained from MBFC and\nannotated by experts. At article-level , data was\ncollected directly from sources listed in the MBFC,\nwith expert-annotated bias evaluations from All-\nSides. The joint modeling approach incorporated\nbias and factuality labels.A.4 Language Variety The SAFARI corpus in-\ncludes data in ten languages at both the media and\narticle levels, but joint modeling includes only En-\nglish.\nLanguage Differences Data collection began at\nthe media-level, followed by the article-level. Me-\ndia outlets with fewer than 100 articles were ex-\ncluded from the article-level dataset but retained in\nthe media-level dataset, ensuring representation\nwhile maintaining a robust article-level dataset.\nSubstitutions ensured at least 10 languages per task\nfor cross-lingual analysis. Furthermore, MBFC an-\nnotations included the country of origin, which was\nmanually verified before obtaining articles in the\ncorresponding languages.\nA.5 Experimental Setup\nHyper-parameters Thelearning rate was stan-\ndardized to 2e-5 for all models: mBERT, XLM-R,\nDistilmBERT, mDeBERTaV3, and mBART. Batch\nsize varied: 100 for mBERT and DistilmBERT,\n80 for XLM-R, and 90 for mDeBERTaV3 and\nmBART. Weight decay andmaximum sequence\nlength were uniformly set at 0.01 and 512, respec-\ntively. During training, the model was validated\nevery 100 steps and saved every 15,000 steps , with\na limit of three checkpoints to manage storage.\nEpoch Configuration Models were trained for 5\nepochs at the media-level and 3 epochs at the article\nlevel and joint modeling data to prevent overfitting\nand enhance performance.\nHardware Our models were executed on\nNVIDIA RTX A6000 (48GB) GPU.\nA.6 Library Selection We used Requests for\nretrieving page code and BeautifulSoup (bs4)\nfor searching HTML elements. These libraries\nwere chosen for their functionality and ease of use,\nwith bs4\u2019s exception handling capabilities proving\nuseful for parsing large datasets.\nA.7 Cross-referencing AllSides with Media\nBias/Fact Check Misaligned articles in cross-\nreferenced subsets covered diverse topics. In a sub-\nset of 1000 articles, 7 out of 55 misaligned articles\nfocused on \u201cTrump,\u201d 4 on \u201cFinance\u201d and \u201cEcon-\nomy,\u201d and the rest on various topics like Elections,\nCriminal Justice, Environment, etc. In a subset of\n500 articles, only 29 were misaligned, covering\ndiverse topics.12230\nFigure 2: Data construction pipeline.\nFigure 3: Architectures of hard voting (A) and soft voting (B) ensembles.12231", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Safari: Cross-lingual bias and factuality detection in news media and news articles", "author": ["D Azizov", "Z Mujahid", "H AlQuabeh"], "pub_year": "2024", "venue": "Findings of the \u2026", "abstract": "In an era where information is quickly shared across many cultural and language contexts,  the neutrality and integrity of news media are essential. Ensuring that media content remains"}, "filled": false, "gsrank": 13, "pub_url": "https://aclanthology.org/2024.findings-emnlp.712/", "author_id": ["MUao89cAAAAJ", "5fYb6pYAAAAJ", "_vbkrqMAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:28jRZvDSPWMJ:scholar.google.com/&output=cite&scirp=12&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D10%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=28jRZvDSPWMJ&ei=BbWsaMyPOeHUieoP9LKZ6AI&json=", "num_citations": 5, "citedby_url": "/scholar?cites=7151103713293224155&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:28jRZvDSPWMJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://aclanthology.org/2024.findings-emnlp.712.pdf"}}]