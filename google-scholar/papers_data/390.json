[{"title": "Googly eyes and yard signs: Deconstructing one professor's successful rebuffing of a right-wing attack on an academic institution", "year": "2020", "pdf_data": "Univ ersity of Nebr aska - Lincoln Univ ersity of Nebr aska - Lincoln \nDigitalCommons@Univ ersity of Nebr aska - Lincoln DigitalCommons@Univ ersity of Nebr aska - Lincoln \nDepar tment of T eaching, Learning, and T eacher \nEducation: F aculty Publications Depar tment of T eaching, Learning, and T eacher \nEducation \n2020 \nGoogly E yes and Y ard signs: Deconstructing One Pr ofessor \u2019s Googly E yes and Y ard signs: Deconstructing One Pr ofessor \u2019s \nSuccessful Rebuffing of a Right-wing A ttack on an Academic Successful Rebuffing of a Right-wing A ttack on an Academic \nInstitution Institution \nTher esa Catalano \nUniv ersity of Nebr aska-Lincoln , tcatalano2@unl.edu \nAri K ohen \nUniv ersity of Nebr aska-Lincoln , akohen2@unl.edu \nFollow this and additional works at: https:/ /digitalcommons.unl.edu/teachlearnfacpub \n Part of the American P olitics Commons , Communication T echnology and New Media Commons , \nCritical and Cultur al Studies Commons , Curriculum and Instruction Commons , Other Public Affairs, Public \nPolicy and Public Administr ation Commons , Social Influence and P olitical Communication Commons , and \nthe Teacher E ducation and Pr ofessional De velopment Commons \nCatalano, Ther esa and K ohen, Ari, \"Googly E yes and Y ard signs: Deconstructing One Pr ofessor \u2019s \nSuccessful Rebuffing of a Right-wing A ttack on an Academic Institution \" (2020). Depar tment of T eaching, \nLearning, and T eacher E ducation: F aculty Publications . 369. \nhttps:/ /digitalcommons.unl.edu/teachlearnfacpub/369 \nThis Ar ticle is br ought t o you for fr ee and open access b y the Depar tment of T eaching, Learning, and T eacher \nEducation at DigitalCommons@Univ ersity of Nebr aska - Lincoln. It has been accepted for inclusion in Depar tment \nof Teaching, Learning, and T eacher E ducation: F aculty Publications b y an authoriz ed administr ator of \nDigitalCommons@Univ ersity of Nebr aska - Lincoln. \n1   \nGoogly eyes and yard signs: \nDeconstructing one professor\u2019s  \nsuccessful rebuffing of a right-wing  \nattack on an academic institution \nTheresa Catalano \nAri Kohen \nUniversity of Nebraska\u2013Lincoln, USA \nCorresponding author \u2014  Theresa Catalano, Department of Teaching, Learning and Teacher Education, \nUniversity of Nebraska\u2013Lincoln, 118 Henzlik Hall, Lincoln, NE 68588, USA. Email  tcatalano2@unl.edu   \nAbstract \nRight-wing populism is on the rise worldwide, and political attacks against univer -\nsities have increased in the United States since the election of Donald Trump. In \n2017, an incident occurred at the University of Nebraska\u2013Lincoln which resulted in \naccusations of hostility toward conservative students. Just over a year later, political \nforces again attempted to denigrate the university\u2019s reputation, but this time they \ndid not succeed. This (multimodal) positive discourse analysis/ generative critique \ncombines collaborative auto-ethnography to describe the way these events were rep -\nresented in the media, deconstructing a professor\u2019s methods of countering a right-\nwing attack on an academic institution. Findings demonstrate the use of multiple \nstrategies such as controlling the narrative through social media savvy; using lin -\nguistic strategies such as refutation of strawman fallacies, syntax, deixis and emo -\ntional appeal; and use of image. \nKeywords: Anti-intellectualism, collaborative auto-ethnography, generative cri -\ntique, positive discourse analysis, right-wing populism digitalcommons.unl.edu\nPublished in Discourse & Society (2019) \nDOI: 10.1177/0957926519880037 \nCopyright \u00a9 2019 Theresa Catalano & Ari Kohen; published by SAGE Publications. Used by \npermission.  \n  \ncatalano & kohen in discourse & society  (2019)     2\nIntroduction \nOn 28 August 2017, a University of Nebraska\u2013Lincoln (UNL) under -\ngraduate student set up a recruitment table for Turning Point USA \n(a right-wing conservative organization which features a Professor \nWatchlist) on the campus. Members of the UNL community, including \na graduate student/teaching assistant and faculty member in the Eng -\nlish Department, protested nearby. Part of the protest (which included \nprofanity and namecalling) was recorded on video and released to so -\ncial media. Social media coverage then led to harsh public criticism \n(including from conservative state senators) that publicly accused the \nuniversity of restricting free speech and being an unsafe environment \nfor conservative students. Initially, the graduate student/teaching as -\nsistant was put on leave for her own \u2018safety\u2019, but after pressure from \nthree state senators, she was removed from her position as a teacher, \nwhich resulted in UNL being censured by the American Association \nof University Professors (AAUP), a status it currently still holds (for \nmore on this, see Fucci and Catalano, 2019). \nJust over a year after these events, right-wing politicians from Ne -\nbraska tried again to advance the narrative that UNL is hostile to con -\nservatives. This time, they targeted UNL professor Ari Kohen for liking \na Facebook post which showed a photograph of a vandalized campaign \nyard sign (googly eyes and references to flatulence, for example, F ar-\ntenberry, odor had been added) for Congressman Jeffrey Fortenberry. \nBelow is the photo that Kohen liked:  \nSee original photo here: https://www.rollingstone.com/politics/politics-news/\nfortenberry-googly-eyesvandalism- professor-751618/     \n\ncatalano & kohen in discourse & society  (2019)      3\nDays after liking the photo on his cell phone screen, Kohen spoke \nwith Dr William \u2018Reyn\u2019 Archer III, Chief of Staff for Congressman \nFortenberry. The phone call lasted 53 minutes and was recorded by \nKohen1 who made selected excerpts available to the press and online. \nArcher accused Kohen of \u2018liking vandalism\u2019 and threatened to use his \noffice to make that claim public, essentially putting Kohen and, by ex -\ntension, the university, in hot water. However, this time, Kohen, then \npresident of UNL\u2019s chapter of the AAUP, tenured professor of political \nscience (and human rights, heroism, and restorative justice scholar), \nand widely read blogger, was ready. Through the use of a number of \neffective strategies, Kohen successfully rebuffed the politician and his \nstaff, resulting in no negative consequences for the university, but a \ndamaged reputation for the congressman and his staffer due to a com -\nplaint he filed with the Office of Congressional Ethics and the resul -\ntant negative press. This article incorporates (multimodal) positive \ndiscourse analysis (PDA)/generative critique with the aim of docu -\nmenting successful academic pushback against anti-intellectualism \nand right-wing populism (RWP), using Kohen\u2019s situation as an exam -\nple. We also employ collaborative auto-ethnography (CAE), using dis -\ncussions between Kohen and the first author (Catalano) to comple -\nment and provide context and insider perspectives on why Kohen did \nwhat he did. In order to give some context for understanding our anal -\nysis, the next section provides a brief overview of RWP and its link to \nanti-intellectualism. \nAnti-intellectualism and RWP \nAccording to Zakaria (1997: 23\u201324), \u2018illiberal democracies\u2019 (e.g. right-\nwing populist governments), which increasingly limit the freedoms of \nthe people they represent, are increasing around the world (Wodak, \n2019: 199). RWP is a \u2018hybrid political ideology that rejects the post-\nwar political consensus and usually, though not always, combines lais -\nsez-faire liberalism and anti-elitism with other, often profoundly dif -\nferent and contradictory ideologies\u2019 (Wodak and Krzy\u017canowski, 2017: \n1. This is not the first time Archer has been in trouble after having his conversations re -\ncorded. In 2000, he resigned from his then job as Texas health commissioner after hav -\ning his recorded conversation with former employee Demetria Montgomery released to \nthe public. In the conversation, Archer made several racist comments (Vertuno, 2000). \ncatalano & kohen in discourse & society  (2019)     4\n475). RWP is called \u2018populism\u2019 because it appeals to a homogenized \ncommon man or woman, and it grows out of \u2018public pessimism, anx -\niety, and disaffection\u2019 of \u2018the people\u2019 (Betz, 1994: 41). In this \u2018post -\nshame\u2019 era in which RWP reigns and far-right ideologies have become \nnormalized, antielitist (along with anti-pluralist/exclusionary) rhet -\noric supports the \u2018shamelessness, humiliation of other participants, \ndefamation, lies and ad hominem attacks\u2019 of \u2018powerful politicians that \nfrequently resonate as \u201cauthentic\u201d with the core followers of these pol -\niticians, their parties or governments\u2019 (Wodak, 2019: 197). \n \u2018Intellectuals have been ridiculed and chastised since ancient times\u2019 \n(Siniver, 2016: 631) and scholars such as Hofstadter (1963) have ar -\ngued that \u2018anti-intellectualism is part of the fabric of American soci -\nety, a product of evangelism, primitivism, business activism and egal -\nitarianism\u2019 (p. 22). It is also an important element of RWP (Wodak, \n2019: 198) and is \u2018a pervasive social phenomenon which transcends \ntemporal and spatial boundaries\u2019 (Siniver, 2016: 630). Anti-intellec -\ntualism (and anti-elitism in general) is based on the construction of \nthe dichotomy of \u2018real\u2019 and \u2018true\u2019 people versus the \u2018elites\u2019 or \u2018the es -\ntablishment\u2019 who are distinct from the common people (Wodak, 2017: \n553). Who exactly are considered the \u2018elites\u2019 varies from country to \ncountry. In the United States, the rich are not necessarily part of the \n\u2018elites\u2019 (since everybody could possibly become rich), but intellectu -\nals (including scholars and teachers) are seen as dangerous, along \nwith journalists and the politically powerful (Wodak, 2017: 555\u2013557). \nThe way RWP connects to anti-intellectualism is through a political \nrhetoric which aims to divide the people into two camps, \u2018the people\u2019 \nagainst \u2018the establishment\u2019 . Since intellectuals (by being a source of \nknowledge and information but also generally tending to be progres -\nsive rather than conservative, politically) fit into \u2018the establishment\u2019, \nanti-intellectual discourses seek to foster public distrust in universi -\nties as a whole. \nOne strategy of the right-wing in the United States has been to pro -\nvoke situations that can be used to demonstrate that universities are \nagainst conservatives (Scott, 2017: 5). This has been accomplished \nby creating traps for university professors to fall in (e.g. a professor \nmakes a comment publicly or in class that demonstrates his or her \nleft-leaning political views which is filmed by a conservative student \nand then shared to viral effect on social media; cf. Fucci and Catalano, \n2019). Conservative politicians and groups weigh in on the situation, \ncatalano & kohen in discourse & society  (2019)      5\nand the actions of one professor are then highly publicized and extrap -\nolated to represent the university as a whole. This provides supposed \nevidence that universities are hostile to conservatives. The \u2018googly \neyes\u2019 events fit neatly into this description of right-wing \u2018traps\u2019 de -\nscribed above. \nRWP, social media and free speech \nRWP and social media \nRWP, along with other types of political and social movements, has \nbenefited from the use of social media. Social media increase the po -\ntential for hate speech just by its wide distribution around the globe \nand also can \u2018create a sense of community without the constraining \ninfluence of communities in real space\u2019 and thus increase the possi -\nbility that a \u2018nameless, faceless audience member seeking support for \nhis violent plans can find it online\u2019 (Lidsky, 2011:163). According to \nKrzy\u017canowski and Tucker (2018), \nthe growth of the political use of social media has, accord -\ningly, also been viewed as one of the central factors in not \nonly the further celebrification of politics (cf. Donald Trump) \nbut also in the ensuing \u2013 and currently still ongoing \u2013 re-\nemergence and success of (right-wing) populist politics in \nEurope and the USA in the second decade of 2000s (p. 14). \nNumerous studies have reported the use of social media to express \nand spread false information and repeat nativist and nationalist pop -\nulist propaganda (e.g. Wodak, 2015). In this right-wing discourse, \u2018the \nmechanism of \u201cscapegoating\u201d (singling out a group for negative treat -\nment on the basis of collective responsibility) constitutes an important \nfeature\u2019 (Wodak, 2017: 553) and this type of discourse \u2018always com -\nbines and integrates form and content, targets specific audiences, and \nadapts to specific contexts\u2019 . Hence, social media (or social networking \nsites (SNSs)) provide the perfect space for these discourses to occur. \nBesides providing a space for right-wing discourses to thrive, re -\nsearch has shown how social media can be used as a form of activ -\nism or resistance. For example, Mortensen (2011) studied the 2009 \ncatalano & kohen in discourse & society  (2019)     6\nuprising in Iran highlighting the way in which social media empow -\nered people to share information and make it more difficult to keep \nviolent acts by governments hidden and at the same time recogniz -\ning the limits of social media (especially because it is not available to \neveryone) and its potential to be used as a government tool to moni -\ntor and control citizens. Kelsey and Bennett (2014) described the way \nthat Twitter was used to mobilize resistance against the state and to \nshow support for an individual who previously lacked power, whereas \nCustodio (2014) explored offline dimensions of online actions protest -\ning human rights violations before the 2016 Summer Olympics. This \nstudy builds on this research exploring the role of social media in re -\nsistance from an auto-ethnographic angle. \nFreedom of speech \nFreedom of speech is \u2018the right to express one\u2019s ideas, however true or \nfalse they may be\u2019 (Scott, 2017), as enshrined in the First Amendment. \nThis should not be confused with academic freedom, which protects \nfaculty rights to engage in intellectual discussions or debates without \nfear of censorship or retaliation: \nThe First Amendment generally, and freedom of expression \nin particular, are not absolute concepts, and that is why they \nare at once so difficult to administer and so essential to a free \nsociety and an educated citizenry. (Hudson, 2018, last para.) \nBecause of the important intellectual work of discussion and de -\nbate, \u2018public universities are particularly rich grounds for conflict over \nmatters of speech\u2019 (Hudson, 2018, second para.). However, this does \nnot mean that political propaganda in the classroom or discrimination \nagainst students because of political beliefs is protected. Nonetheless, \norganizations such as the AAUP strongly contend that \u2018where ques -\ntions arise concerning the propriety of conduct of a faculty member, \nthe matter should be referred to appropriate faculty bodies at the fac -\nulty member\u2019s institution\u2019 (Tiede, 2017, sixth para.), something that \ndid not happen in the case at UNL described above (Fucci and Cata -\nlano, 2019). \nIn recent years, the Supreme Court (under Chief Justice John G \nRoberts Jr.) \u2018has been more likely to embrace free speech arguments \ncatalano & kohen in discourse & society  (2019)      7\nconcerning conservative speech than liberal speech\u2019, which is in con -\ntrast to earlier time periods (Liptak, 2018, eighth para.). Moreover, \nspeech has become a \u2018weapon\u2019 of the Right (Scott, 2017: 3). One exam -\nple of this can be seen in Fucci and Catalano (2019), in which politi -\ncians accused faculty of being hostile to conservatives and arguing that \npolitical actions of professors are not protected by free speech. This is \ntrue also of this article, in which conservative politicians argue that \na professor\u2019s \u2018likes\u2019 on Facebook constitute negative actions against a \nconservative politician and the criminal act of vandalism itself. \nSocial media/SNSs and university professors \nSNSs, like Facebook, constitute a space in which university professors \n(and teachers in general) can interact with the general public, which \nsometimes includes their students or colleagues. This \u2018contributes to \na blurring of boundaries between professional and personal perso -\nnas\u2019 (Sugimoto et al., 2015: 1) and the creation of novel issues related \nto the communication of ideas that could be considered offensive or \nwrong. In addition, \nfaculty profiles on SNSs or other types of social media \nthrough which they communicate, such as blogs, may list the \ninstitution with which they are affiliated, together with rank \nor status. Questions then arise whether or not the faculty \nmember is enacting a professional or personal role on these \nsites and through these media. (Sugimoto et al., 2015: 8) \nLikes, comments and shares constitute various degrees of political \nengagement on social media, and in many cases, \u2018controversies over \nfaculty activities have led to threats of physical violence against indi -\nvidual faculty members or the institution\u2019 (Tiede, 2017, fifth para.).  \nThere are many examples of inappropriate conduct/communication \non social media from faculty members (e.g. Blackford, 2011; Kingkade, \n2013; Miller, 2010), and \u2018public reaction to these messages suggests \nthat new expectations have developed around the public presentation \nof faculty members in the online space\u2019 (Sugimoto et al., 2015: 9). Dif -\nferent universities have taken different actions in response to these \nsituations varying from public apologies to suspension and censure, \nand it is clear that standards related to social media participation of \ncatalano & kohen in discourse & society  (2019)     8\nfaculty are not yet fully developed. As such, organizations such as the \nAAUP have recommended faculty and students \u2018engage in open dia -\nlogue about norms of behavior; expressing the expectations and val -\nues of both groups\u2019 (Sugimoto et al., 2015:10). They also argue that \n\u2018social media policies at institutions of higher education are therefore \nnot so much for the individuals associated with the institution, as they \nare for the institution itself\u2019 (Sugimoto et al., 2015: 10). In the case of \nKohen, who merely \u2018liked\u2019 a humorous photograph on Facebook, and \ndid so as a private citizen, not as a faculty employee, the First Amend -\nment does clearly protect his actions (Stein, 2013). And, unlike the \nevents of the previous year described in Fucci and Catalano (2019), \nuniversity administrators did not act on Kohen\u2019s situation in any way, \nsince it did not involve the university (other than his being employed \nthere). However, social media situations are not always this clear, and \nit is important to note that much work still needs to be done in clari -\nfying the First Amendment and its relation to university faculty\u2019s use \nof social media as it connects to free speech. \nMethod \nMultimodal PDA/generative critique \nIn order to examine the way in which the discourse of Kohen (as rep -\nresented in the media) is constructive as opposed to de-constructive, \nwe employ a multimodal form of PDA (Martin, 2004) or generative \ncritique which examines social change that is happening and is \u2018ori -\nented to equality and heterogeneous well-being\u2019 (Haraway, 1997: 95, \nas cited in Macgilchrist, 2016: 273). Although the analysis will look \nat the ways in which power is enacted by powerful groups (e.g. local \npoliticians), our main focus is on the way that Kohen used language \n(and multimodal modes of communication such as Twitter and image) \nto resist the powers attempting to threaten him (and the university). \nPDA was born out of critique of critical discourse analysis (CDA, now \noften referred to as critical discourse studies or CDS) calling for it to \n\u2018focus on community, taking into account how people get together and \nmake room for themselves in the world \u2013 in ways that redistribute \npower . . .\u2019 (Martin, 2004: 6). Within the field of CDA/CDS, scholars \ncatalano & kohen in discourse & society  (2019)      9\nsuch as Wodak disagree that PDA is anything different from CDA/\nCDS, arguing that being critical is not about being positive or nega -\ntive, it is about questioning the extant social order and aiming for pos -\nitive change (personal communication, 11 March 2018). In addition, \nMacgilchrist (2016: 273) re-frames PDA in terms of post-foundational \nthinking which has the potential to address \u2018unequal power relations \nthrough (fine-grained) analysis of hope-giving, reparative discourse\u2019 \n(Macgilchrist, 2016: 262). Hence, for our study, we use \u2018PDA\u2019 to em -\nphasize our focus on documenting positive change and resistance to \nabuses of power as it happens.  \nFinally, we heed the call for the voices of the \u2018oppressed\u2019 to be \nheard and for a comparison of the findings of the analyst with what \nthe members of the target community think and say. As such, we in -\ncorporate CAE, which we explain in the next section. \nCAE \nAs mentioned above, scholars in the field of CDS have continued to \npush for the addition of ethnographic approaches in response to its \nproblem of disconnect between the researcher, producer and read -\ners of texts. Ethnographic approaches represent a distinct way to an -\nalyze language in culture (among other things) and assist in explor -\ning the \u2018beliefs, values, and desires\u2019 of participants (Chouliaraki and \nFairclough, 1999: 62). Various scholars have been combining CDS and \nethnographic approaches for a number of years (e.g. Chouliaraki and \nFairclough, 1999; Rogers, 2002; Wodak, 1996, 2009) In fact, in the \n2011 special issue of Critical Discourse Studies , authors address the \ncombination of CDS with ethnographic studies from a range of topics, \nand Krzy\u017canowski\u2019s (2011) introduction to the issue recognizes the vi -\ntal link between ethnography and CDS in providing a more in-depth \nanalysis of societal issues. Machin and Mayr (2012) also advocate for \nadding an ethnographic dimension to the analysis of newspaper dis -\ncourse, which can mean interviewing editors and journalists about \ntheir choices. In addition, Baroni and Mayr (2017) and Mayr (2018) \nhave used ethnography to show how disenfranchised people resist and \nmake positive change in their communities. \nIn the case of our article, we employ CAE in order to include Kohen\u2019s \nperspective in the analysis, which he was also involved in writing. CAE \ncatalano & kohen in discourse & society  (2019)     10\nis autobiographical and engages multiple authors who are also the \nparticipants (Chang et al., 2016). For this article, both authors en -\ngage in collaboration through discussions and question/answer ses -\nsions regarding the accuracy of the analysis and Kohen\u2019s thoughts on \nthe strategies behind his successful rebuffing of right-wing politi -\ncians. These discussions then inform the analysis, making it nuanced \nand more accurate. This is because Kohen understands better than \nanyone how he successfully combatted this anti-intellectual attack, \nand also Catalano has the background in CDS needed to help artic -\nulate linguistic and visual elements behind the discourse. In dialog -\ning together, we engage in the self-reflexive examination of our own \nassumptions and perspectives and use the researcher\u2019s personal ex -\nperiences as primary data (Chang et al., 2016) along with the media \ndiscourse in which his experiences are represented. Because the re -\nsearchers in this study are also the participants, institutional review \nboard approval was not necessary. \nData collection \nIn addition to collaborative auto-ethnographic discussions of Kohen\u2019s \nexperiences related to his liking a post on Facebook, we examined 18 \narticles (including three videos and original tweet threads from Ko -\nhen) which represented the related events. All articles fit the follow -\ning criteria: \n1. Local/regional or national news report about the \u2018googly eyes\u2019 \nevents. \n2. Must be 500 words or more. \n3. Must contain multimodal data and not just text or just image. \nArticles were found using the search term \u2018Ari Kohen\u2019 + \u2018googly \neyes\u2019 or \u2018Fartenberry\u2019 . All articles were published between the dates \nof 1 November and 6 November 2018 and were found in local/regional \nor national news reports of various political tendencies (e.g. Omaha \nWorld Herald (right-center), Lincoln Journal Star (neutral), Washing -\nton Post (left-center) and MSNBC (left)).2 \n2. Media bias was determined using mediabiasfactcheck.com \ncatalano & kohen in discourse & society  (2019)      11\nData analysis \nArticles (including tweets, but not image and video) were compiled \ninto one .txt file and uploaded to MAXQDA,3 which was used to check \nword frequencies and thematically code data (Saldana, 2015) using \npre-determined thematic codes based on a first reading and also add -\ning in-vivo coding when new themes were determined. Themes were \nselected based on strategies used by Kohen to combat his accusers. In \naddition to the verbal file, images and videos were also collected into \none file. The authors employed techniques from (multimodal) CDS \nfor the analysis of visual and verbal data. The following three themes \nemerged after analysis: Controlling the Narrative, Attending to Lan -\nguage and Image (see Findings section below). As part of this multi -\nlayered analysis, the authors came together after the initial draft was \nwritten up (Catalano wrote first draft and Kohen revised and made \nchanges) to discuss the analysis. During this collaborative discussion, \nwhich occurred on 26 February 2019 (and in subsequent follow-up \nemails), Catalano asked Kohen a series of questions based on the ini -\ntial analysis. Kohen answered the open-ended questions, but this also \nled to a wider discussion. This discussion was recorded with the Quick \nVoice Pro app on Catalano\u2019s cell phone. Salient findings from this dis -\ncussion (such as Kohen\u2019s personal perspective on the analysis) are in -\ncluded verbatim (or in summary form) in the findings section. \nFindings \nControlling the narrative \nThe first venue where the incident was represented was in Kohen\u2019s \ninterview with Chris Dunker at the Lincoln Journal Star . Kohen\u2019s side \nof the story was featured first, and he utilizes many strategies we will \npoint out later, but his first (multimedia) strategy was to post a link \nto the article on his Twitter page, along with a series of tweets about \nthe events that also included a link to his phone conversation with \n3. MAXQDA (www.maxqda.com) is a software program that facilitates qualitative, quantita -\ntive and mixed-methods analysis. \ncatalano & kohen in discourse & society  (2019)     12\nArcher on YouTube. He then copied all his tweets into one Facebook \npost and published that as well. \nKohen was thoughtful about his use of social media when he pub -\nlished his tweets (and Facebook post) related to the phone call with \nWilliam \u2018Reyn\u2019 Archer III (herein referred to as \u2018Archer\u2019), knowing \nthat his large number of influential followers would share them and \nspread news of the incident widely. Hence, he harnessed the power of \nsocial media (i.e. Twitter and Facebook) by getting his version of the \nevents out before anyone had a chance to hear Archer\u2019s perspective. \nWhile this might seem like a simple strategy and common sense, it of -\nten does not happen (cf. Fucci and Catalano, 2019) because the sub -\njects of these attacks are often blindsided by their escalation. \nKohen remarked in our conversations that had the incident with \nTurning Point not occurred the year before, he would not have known \n\u2018in any sort of firsthand way what a kind of machine there was to gin \nup controversy\u2019 . But, having actually seen how the narrative played \nout, and became \u2018like a snowball going downhill\u2019, this got him think -\ning about \u2018how getting a story out impacts how it will be perceived\u2019 . \nIn addition, because of the Turning Point events of the previous year, \nhe was well aware that \u2018the way the story comes out from the jump, \nthat\u2019s how the story is going to go\u2019, and he says that knowing what \ntheir intention was and what they wanted to do with this information \n\u2018really allowed me to think about how to put my narrative out and, in \neffect, undercut what they wanted to do\u2019 . \nCognitive linguists such as George Lakoff have long held that \u2018mes -\nsaging is about thinking, not just language\u2019 (Lakoff and Wehling, 2012: \n3). That is, in order to get language right, you have to understand the \nthoughts the language provokes. One important lesson from this field \nis that it is important not just to tell the truth and give the facts, but \nhow this is framed can make a difference. \u2018It is impossible to commu -\nnicate without activating frames, and so which frame is activated is \nof crucial importance\u2019 (Lakoff, 2006: 10), and the first frame we are \nexposed to is what is activated. Hence, when we say \u2018Don\u2019t think of an \nelephant\u2019, the first thing that happens in our brains is that we imag -\nine an elephant in our heads (Lakoff, 2014). In the case of the \u2018googly \neyes\u2019 events, it was important for Kohen to activate frames related to \nArcher\u2019s inappropriate actions first, so that this would be what people \nremembered and thought of when reading about the events. This was \ncatalano & kohen in discourse & society  (2019)      13\naccomplished successfully by his initial interview with Chris Dunker \nfrom the Lincoln Journal Star and then linking the Dunker article to \nhis first tweets about the events (and his Facebook posts, including a \nlink to the phone conversation), as well as in subsequent interviews. \nKohen commented in his conversation with Catalano that he recog -\nnized that it was very important to reach journalists first. Because Ko -\nhen is a seasoned blogger who used to do weekly podcasts, he already \nhad a microphone and the necessary equipment to record the phone \nconversation when he spoke to Archer. In addition, he used his Twit -\nter account in which he has a large (and growing) number of followers \n\u2013 many of whom are university professors, journalists and even pol -\niticians \u2013 to release information about the hour-long phone call with \nArcher before Archer had a chance to reach out to the press. More -\nover, before he tweeted extensively, he filed a formal complaint with \nthe Office of Congressional Ethics and consulted with the UNL AAUP \nchapter. Kohen also noted in our discussion that he was aware that \nthe first tweet in the thread \u2018had to grab people, and it did\u2019, and also \n\u2018it was massively multiplied out into the world and eventually seen by \nmore than 300,000 people\u2019 . He also said that he knew that it was not \nonly important to get the message out, but to get it to the right people. \nThe initial tweet in Kohen\u2019s thread reached more than 300,000 peo -\nple; a tweet farther down in that thread, which contained a link to an \nexcerpt from the recorded call, reached 72,000 people. Of those, 1700 \nclicked through to hear the audio on YouTube. However, of those lis -\nteners, many included journalists such as Chris Hayes who ended up \nfeaturing it on his popular MSNBC show. Hence, Kohen emphasizes \nthat \u2018framing matters, as does hitting the right audience!\u2019  \nAttending to language \nObviously, when controlling the narrative, the language used to talk \nabout the events is key. In this section, we analyze his tweets and con -\nversation with Archer that point to the use of a number of linguistic \nstrategies used to get his story across and rebuff Archer\u2019s accusations. \nAgain, in alignment with Lakoff\u2019s (2014) work regarding frame ac -\ntivation, Kohen presented Archer\u2019s negative actions clearly first (so \nthat these would be remembered by readers/ viewers), followed by a \nsimple statement and justification of his own actions, and a repetition \ncatalano & kohen in discourse & society  (2019)     14\nof Archer\u2019s actions. Below, we illustrate how this was done in several \nof Kohen\u2019s tweets:4 \nExample (1) \nLast Friday, I received a threatening phone call from Congressman \nJeff Fortenberry\u2019s Chief of Staff, Reyn Archer. \n924 Retweets 1,580 Likes \n(Gettys, 2018) \nExample (2) \nBecause I liked a silly photo on Facebook, Archer \u2013 who is politi -\ncally powerful and very wellconnected \u2013 contacted my employer and \nthen threatened to use his office to make public that I like vandalism, \nwhich is clearly untrue. \n14 replies 104 retweets 545 likes \n(Weiner, 2018) \nSeveral elements are worth pointing out in the above tweets. First, \nKohen uses honorifics (official titles that suggest a degree of respect, \ncf. Machin and Mayr, 2012) and functionalization (when social ac -\ntors are referred to in terms of an occupation or role; Van Leeuwen, \n2008) with the statement Congressman Jeff Fortenberry\u2019s Chief of \nStaff, Reyn Archer . Alternatively, Kohen could have just said he re -\nceived a threatening call from Reyn Archer. However, it was important \n4. Tweets found in articles in the corpus are cited as such. Tweets not cited in other arti -\ncles (and hence taken directly from Kohen\u2019s Twitter thread) are cited as Kohen (2018). \n\ncatalano & kohen in discourse & society  (2019)      15\nthat he identified his connection to Fortenberry while conveying the \npower and authority that Archer had via his relationship to Forten -\nberry. In the second tweet, Kohen forefronts and simplifies his own \nactions (through syntax, aka placing the clause Because I liked a silly \nphoto on Facebook first) and then names Archer and highlights his \npower and influence with the clause who is politically powerful and \nvery-well connected. Through the possessive his combined with office , \nhe uses deixis to employ relational identification (Van Leeuwen, 2013) \ndrawing attention to Archer\u2019s role as a public official (or his connec -\ntion to that of Fortenberry). He then ends the tweet by negating the \naccusation of liking vandalism (e.g. which is clearly untrue ). Example \n(2) demonstrates a textbook case of exactly the way in which Lakoff  \nrecommends handling right-wing attacks (American Freedom Radio, \n2016). That is, first state what actually happened (not the distorted \nversion presented by the opposition), then state what the accusers \nare saying or doing, then negate the accusations. Often, people start \ntheir defense by negating the accusations against them. However, ac -\ncording to what we know about the brain, this just activates negative \nframes since we are repeating the negative accusations first, and they \nwill stay activated. \nIt is also worth reiterating here the large number of likes and \nretweets that each of his tweets received (see beneath each tweet, and \nalso, this number continues to grow), because this demonstrates the \npower of social media in spreading the narrative that Kohen wanted \npeople to hear first. Within seconds, thousands of people were able to \nread this and follow the situation, and within 24 hours, news of the \nevents had spread to national news sources. By the weekend, the inci -\ndent was featured on John Oliver\u2019s HBO show (see https://www.you -\ntube.com/watch?v=JMHjVSDuqvMandt=80s  ). \nBesides these initial statements of the situation and his justification \nfor his actions, Kohen made a point to explicitly state the implications \nof Archer\u2019s threats, again emphasizing Archer\u2019s power and influence \n(and political affiliation) with using his platform and his connections \nwithin right-wing media outlets . Moreover, use of the metaphor troll \nstorm appeals to the emotions of readers by conjuring images of natu -\nral disaster and also because troll storm is frequently used in the con -\ntext of neo-Nazi discourse: \ncatalano & kohen in discourse & society  (2019)     16\nExample (3) \nThe implication was that he would work to create a right-wing troll \nstorm, using his platform and his connections within right-wing me -\ndia outlets. \n2 replies 76 retweets 441 likes \n(Kohen, 2018b) \nHe then frames the incident thematically, as opposed to episodically \n(Iyengar, 1994), pointing out (below) past consequences of similar ac -\ntions at the university (and hence intertextually connecting the two \nincidents) as opposed to just dealing with it as one isolated event: \nExample (4) \nIn the past, such efforts have directly resulted in weeks of threaten -\ning letters, voicemails, and email messages to faculty members who \nfound themselves publicly called out in this way (including several of \nmy colleagues at UNL). These have included death threats. \n1 reply 77 retweets 462 likes \n(Campbell, 2018) \nIn example (5), Kohen highlights his First Amendment rights, but \nthen repeats the word threat , and with the word Shabbat , he highlights \nhis status as an observant Jew (Shabbat, or Sabbath, is the weekly Jew -\nish day of rest): \nExample (5) \n\ncatalano & kohen in discourse & society  (2019)      17\nThis is obviously a violation of my First Amendment right to free \nspeech. But more than that, Archer\u2019s threat to name me publicly as \nsome sort of evil liberal professor happened on Friday afternoon, \nright before Shabbat. \n2 replies 82 retweets 551 likes \n(Kohen, 2018d) \nThrough Kohen\u2019s tweets and the re-tweeting and sharing of them in \nnumerous articles, our word frequency analysis in MAXQDA revealed \n100 tokens of this word (or versions of the word such as threatened , \nor threaten ) in a corpus of 8278 words total. Only function words (i.e. \nsmall words such as the or a and words that are not nouns, verbs, ad -\njectives or adverbs) and the words Kohen, Archer and Fortenberry \nwere used  more frequently. This tells us that Kohen\u2019s message of the \nnegative (and inappropriate) actions of Archer was dominant in the \nmedia discourse. In the next example, Kohen connects his Jewish back -\nground to the massacre of Jews at a synagogue in Pittsburgh that took \nplace immediately after Archer\u2019s threatening call. \nIn this way, he links RWP to real life effects, again providing nec -\nessary context for the situation and more evidence as to why threats \nlike those by Archer can lead to violent consequences: \nExample (6) \nLess than 15 hours later, a heavily armed man walked into a syna -\ngogue in Pittsburgh in murdered 11 Jews because of a conspiracy \ntheory, shared publicly by prominent right-wing politicians, that Jews \nare responsible for bringing undesired immigrants to the US. \n2 replies 70 retweets 470 likes \n(Kohen, 2018a) \nKohen\u2019s discourse in example (6) was necessary, because the First \nAmendment actually protects Archer from being responsible for any \nconsequences of his actions (such as what would happen if he made \n\ncatalano & kohen in discourse & society  (2019)     18\ngood on his threats to publish Kohen\u2019s \u2018liking vandalism\u2019 on right-wing \nplatforms and caused a troll storm). This is the case of Terry Jones \nwho was not held responsible for the violent responses of his audi -\nence when he leveraged social media to spread his anti-Islamic speech \naround the world (Lidsky, 2011: 151). Such incidents illustrate the \u2018in -\ncendiary capacity of social media\u2019 and the \u2018mismatch between existing \ndoctrinal categories and new types of dangerous speech\u2019, particularly \nin the way that offensive speech in one location can become deadly \nwhen transmitted to another (Lidsky, 2011: 147, 150). In example (7) \nbelow, Kohen connects the violence of Pittsburgh to his own commu -\nnity. He does this through use of deixis ( me, my family, my commu -\nnity) which show relational identification (Van Leeuwen, 2013) and \npoint to his personal and kinship relations as well as his social rela -\ntion to the events of Pittsburgh through his Jewish identity. In addi -\ntion, because of the violent actions that occurred in Pittsburgh, the \nhistory of violence against Jews, and the way that his use of deixis \nlinks this to him and his family and community, it helps him to make \nan emotional appeal to his audience (and connects to his earlier com -\nment about a troll storm ). Adding to this, he repeats the negative ac -\ntions of Archer and not his own actions which Archer wanted to high -\nlight and negatively portray: \nExample (7) \nThis horrible act of violence could very easily have happened to me, \nmy family, and my community; *real* violence easily could have been \nthe result of the actions Archer threatened to take against me. \n3 replies 63 retweets 474 likes \n(Kohen, 2018c) \nKohen chose to draw on the connection between Lincoln, Nebraska \nand its bestknown antisemitic citizens, Gary (Gerhard) Lauck, a major \ndistributor of neo-Nazi literature around the world (Vaughan, 2017), \nand Daniel Kleve, a white supremacist student on UNL\u2019s campus who \nattended the Unite the Right rally in Charlottesville and displayed \n\ncatalano & kohen in discourse & society  (2019)      19\nneo-Nazi banners around the state (Hayden, 2018). In the next two \ntweets, Kohen reemphasizes the power of Archer and Fortenberry, \nusing it, and their negative actions (e.g. threat), to justify his filing a \ncomplaint with the Office of Congressional Ethics: \nExample (8) \nArcher and Fortenberry know perfectly well that I don\u2019t \u2018like vandal -\nism\u2019. But they wanted to intimidate me, with both the power of their \noffice and with a specific threat to unleash a mob against me. And \nthat\u2019s why I\u2019ve filed a report with the Office of Congressional Ethics. \n19 replies 154 retweets 1,265 likes \n(Campbell, 2018) \nIn this final tweet below, Kohen again uses functionalization to fo -\ncus readers on the profession of Fortenberry with the term elected of -\nficials , reminding them of the job he should be doing, instead of po -\nlicing a private citizen\u2019s Facebook page. Moreover, he underscores the \nimbalance of power once again, by making this reference to Forten -\nberry\u2019s role as politician. At the same time, through use of deixis, and \nthe \u2018inclusive\u2019 We, he connects himself to his readers, and to all Ne -\nbraskans who Fortenberry serves, and his use of reprehensible gar -\nbage also appeals to readers on a strong emotional level: \nExample (9) \nWe deserve a lot better from our elected officials than this \nreprehensible garbage. \n41 replies 116 retweets 1,329 likes \n(Kohen, 2018e) \n\ncatalano & kohen in discourse & society  (2019)     20\nA final linguistic strategy worth pointing out is the way Kohen re -\nfutes Archer\u2019s \u2018strawman fallacy\u2019 which attempts to distort his actions. \nStrawman fallacies are common in RWP discourse (Wodak, 2019) and \nfunction by taking a claim someone has made (in this case, Kohen\u2019s \n\u2018like\u2019 on Facebook), distorting it (e.g. saying he is liking vandalism) \nand then degrading or refuting it (e.g. claiming that this is not behav -\nior suitable to a university professor and denigrating the university\u2019s \nreputation in the process). Even though this is not the original claim/\naction, the person is then discredited in the process. Through the way \nthat Kohen repetitively and explicitly refutes this distortion of his ac -\ntions (and shared his conversation about it widely), he is successful \nin countering this argument before it catches on and people forget or \nconfuse his original actions with the distorted actions that Archer at -\ntempts to accuse him of. A good example of this can be seen in this ex -\ncerpt from his phone conversation with Archer (Gettys, 2018) (bolded \nsections are the authors\u2019, which indicate areas of focus in the analysis): \nExample (10) \nKohen:  . . . You\u2019re going through and seeing what I have liked . . . \nArcher:  What you\u2019re liking . . . \nKohen:  On my private Facebook page. \nArcher:  I know but what you\u2019re liking is vandalism. \nKohen:  No it\u2019s not. \nArcher:  Yes it is . . . \nKohen:  I\u2019m liking a photo. \nArcher:  You\u2019re liking what the photo represents is vandalism. If \nsomeone were liking something with Blackface, would that \nbe acceptable? If someone were liking, you see, you\u2019ve got \nto understand, these are icons, they\u2019re not, and they\u2019re rep -\nresentative of behavior. We know all this through our social \nlearning. \nKohen:  So your argument is anything that I like on Facebook repre -\nsents an endorsement, by me, of the thing \u2013 not the post \n\u2013 but the thing that is happening in the world? \nIn example (10), Kohen makes explicit that the representation is not \nthe thing itself (e.g. Rene Magritte\u2019s \u2018Ceci n\u2019est pas une pipe\u2019), as well \nas the multiple meaning potentials of the sign, and then shares this \nconversation widely through Twitter and also through multiple media \ncatalano & kohen in discourse & society  (2019)      21\nsources. Hence, he makes the valid argument that the photograph rep -\nresented a \u2018sophomoric\u2019 and \u2018silly\u2019 (Morton, 2018) photo, and nothing \nmore. Kohen first points out the sinister nature of the fact that Archer \nis going through and seeing what I\u2019ve liked , highlighting that this is his \nprivate Facebook page. By using the term private , he calls attention to \nthe fact that his action of \u2018liking\u2019 happened in his own personal space, \navoiding any connection being made to his professional space as a uni -\nversity professor. He essentially reiterates that even though it was a \nreal campaign yard sign that was vandalized, it was also a photograph \nof that sign and there was no way for viewers to know whether this \nwas just a photo someone made up or the real thing. \nOf the linguistic strategies pointed out in this section, Kohen said \nhe was for the most part unaware that he was doing them. However, \nas an early adopter of the Internet and digital native, he was con -\nscious of his social media strategies mentioned in the previous sec -\ntion. And, because he knows \u2018what the network can do\u2019, he \u2018had an in -\ntuitive sense of how to get that story to go\u2019 . Kohen also said, \u2018There\u2019s \nsomething about having built up a little bit of cachet on that site\u2019, in \nthat people know he is not an alarmist or conspiracy theorist and \u2018not \njust someone who is out there to just burn people down\u2019 . \nIn terms of his strategy of highlighting Archer\u2019s threatening ac -\ntions first, he recognized the importance of showing the \u2018dispropor -\ntionate reaction\u2019 of Fortenberry\u2019s staff linguistically. Because of the \nTurning Point events of the previous year, he knew that pretending \nhe did not do something or being quiet about it did not serve any -\nbody. He added, \u2018You know, when you do something, you can say you \ndid it\u2019 . In this case, the reaction was disproportionate and wrong, so \nit grabbed people\u2019s attention, \u2018because anybody who saw the banner, \nwhether they were conservative or liberal, thought it was funny\u2019 . So, \nKohen felt that this was an easy strategy to come up with because his \nso-called offense was \u2018not an event at all in anybody\u2019s eyes\u2019 but he also \ndid not want to put himself in the position of defending vandalism, so \nhe knew if he highlighted what each person actually did and refuted \nthe strawman fallacy (although he did not call it that), he could get \nhis message across more effectively. \nFinally, as regards the emotional appeal of the words/expressions \nhe chose, Kohen said he was very aware that \ncatalano & kohen in discourse & society  (2019)     22\nemotion moves tweets far more than any sort of intellec -\ntual agreement; when people feel a certain way as they read \nsomething \u2013 usually outrage when it comes to Twitter \u2013 \nthey\u2019re more likely to react to it and to share it with others. \nIf they simply agree with an argument, they\u2019re more likely to \n\u2018like\u2019 it and move on. \nThus, emotional appeal appears to be an intentional strategy he used \nto increase coverage of the story, which added to his control of the \nnarrative, as mentioned earlier. \nImage \nA final element of the media discourse about the \u2018googly eyes\u2019 events \nworth examining is visual communication. Non-verbal elements in the \ndata included images from Kohen\u2019s tweets and from the 18 articles (in -\ncluding three videos) examined. In Kohen\u2019s tweets (which are repro -\nduced in numerous articles), what is interesting to note is the profile \nphotograph that accompanies Kohen\u2019s messages, shown in Photo 1.  \nIn this photo, Kohen is shown sitting at what looks like a table of \npanelists, behind an iconic photo of the White House where it is writ -\nten The White House, Washington . Although the photo is small and it \nis hard to see him clearly, he is shown in a demand image (when the \nsubject gazes directly at the viewer, and in essence asks something of \nthem) as opposed to an offer image in which he is looking off camera \nand in which viewers become voyeurs of the scene (Machin, 2007: \n113). He is dressed in a suit and tie, and it is clear from the photo that \nhe is important, given how he is dressed and where he is located. This \nmight seem like nothing out of the ordinary, but Twitter profile pho -\ntos can be very different ranging from classic headshots to icons that \nrepresent their political views. In the case of the \u2018googly eyes\u2019 events, \nKohen\u2019s photo (albeit it is very small in most of the articles) worked \npositively for him because it represented him in a way that showed \nhis professionalism and status. It also helped his case because his \ntweets were the first publicly released information about the events, \nand hence the tweets were reproduced in full in many of the me -\ndia sources we examined, and therefore this positive image of Kohen \ngained a lot of traction and publicity. We tabulated visual representa -\ntions across articles and determined whether these representations \ncatalano & kohen in discourse & society  (2019)      23\nwere overall positive, negative or neutral based on metonymic asso -\nciations related to setting, poses, facial expressions (e.g. smile = pos -\nitive, frown = negative) and distance, angle, and gaze, and whether \nthe facial expressions occurred in a demand or offer image (Kress \nand van Leeuwen, 1996; Van Leeuwen, 2008). From this analysis, it \nis clear that Kohen had the most positive representations (13) while \nArcher had the least (2). In contrast, Fortenberry had the most visual \nrepresentations overall (41 to Kohen\u2019s 16), 34 of which were negative \n(and consisted mostly of the yard sign photos in which his real eyes \nwere replaced with googly ones). The two photos (other than the yard \nsigns) that represented Fortenberry negatively, were both offer im -\nages, one (from Roll Call ; Photo 2) showing him frowning and not gaz -\ning at the camera while the second ( Newsweek ; Photo 3) photo con -\ntained an upward camera angle (often representing reverence and \nsymbolic power; Van Leeuwen, 2008). However, it was an offer im -\nage that featured him frowning as microphones were being pushed \nin his face by journalists. \nPhoto 1. Kohen\u2019s Twitter profile picture. Source: Kohen (2018 a-f). \nNot surprisingly, the more positive the representation of Forten -\nberry, the more rightleaning the news source. Those that included of -\nficial government photos or images of Fortenberry smiling and look -\ning at or speaking to the camera included the Sandhills Express (based \nin Norfolk, a micropolitan area surrounded by rural areas which usu -\nally votes Republican), 1011now (based in Grand Island and Kear -\nney, more non-urban areas), and the Omaha World Herald (right-cen -\nter). Oddly, The Washington Post (left-center) also had one demand \n\ncatalano & kohen in discourse & society  (2019)     24\nimage of Fortenberry smiling, but since it was a Twitter photo, it had \nto be included with the tweet they published so it is possible that is \nwhy they incorporated such a positive image. Naturally, because more \nleft-leaning media sources published articles/videos about the \u2018googly \neyes\u2019 events than right-leaning (which is largely due to Kohen\u2019s con -\ntrolling the narrative), it is not surprising that there were more pos -\nitive representations of Kohen than Fortenberry or Archer. However, \nit is surprising that even in right-leaning news sources, negative rep -\nresentations of Kohen were not present at all. \nPhoto 2. Rep. Jeff Fortenberry, R-Neb., participates in the news conference on a bill \nto repeal certain provisions on the Affordable Care Act in 2012 (credit: Bill Clark/CQ \nRoll Call file photo). Source: Kopp (2018). \nPhoto 3. US Rep. Jeff Fortenberry (R-NE) walks through the Capitol Building on 15 \nOctober 2013, in Washington, DC. An associate professor who \u2018liked\u2019 a photo of a \nFortenberry defaced campaign ad circulating on social media was allegedly threat -\nened by Fortenberry\u2019s chief of staff (credit: Andrew Burton/Getty Images). Source: \nPerez (2018).  \nAll in all, the images (namely, the 28 images of the vandalized yard \nsign which were widely shared) worked to forefront the ridiculousness \n\ncatalano & kohen in discourse & society  (2019)      25\nof the whole incident and also connect to the powerful person behind \nthe accusations against Kohen (e.g. Fortenberry, who was represented \n34 times negatively (largely through the yard sign), 1 neutral and 6 \npositive). They also painted a positive picture of Kohen, whose only \nactions in his visual representations (16 total \u2013 13 positive and 3 neu -\ntral) were smiling, teaching and talking. Archer, on the contrary, was \nshown in only two offer images (both negative) in which he is speak -\ning into a microphone (he appears to be testifying, but the photo cap -\ntion says only AP file photo ; Photo 4) while his eyebrows are raised \nand his lips are pursed as if contesting something someone is saying \n(see Dunker, 2018 for the original photo). \nKohen said he did not have control over the images that were pub -\nlished, but as a savvy social media user, he is always careful about \nwhat photos he posts, knowing that they could be re-purposed at any \npoint in time by someone else. As such, he did have control of the \navailable images that people could find on the Internet of himself, \nagain demonstrating the importance of his social media strategies in \ngetting a positive impression of his side of the story across first. He \nalso had some control over the fact that the yard sign was repeatedly \nshown in article after article (through his sharing of the tweets and \nFacebook posts), which put forth a ridiculous impression of Forten -\nberry. This is due to the silliness of the photo and also because of \nFortenberry\u2019s inability to find it funny and to take a joke. \nPhoto 4. Archer, AP file photo. Source: Dunker (2018).  \n\ncatalano & kohen in discourse & society  (2019)     26\nConclusion \nThis multi-layered multimodal PDA/generative critique employed CAE \nto analyze 18 articles (including tweets, videos and call recordings em -\nbedded in the articles) that represented the \u2018googly eyes\u2019 events. The \ngoal of our article was to demonstrate constructive strategies used \nby Kohen to combat the right-wing attack by William \u2018Reyn\u2019 Archer \nIII (acting as a proxy for Congressman Jeffrey Fortenberry). Findings \nrevealed the use of multimodal/multimedia strategies to control the \nnarrative (e.g. social media, framing), linguistic strategies such as use \nof syntax (i.e. clause placement), functionalization, and deixis, refu -\ntation of strawman fallacies (e.g. highlighting multiple meaning po -\ntentials of the sign), lexical items/tropes with emotional appeal (e.g. \nreprehensible garbage, troll storm ) and use of image (e.g. available \nphotos of Kohen, sharing of yard sign photo). \nIn conversations with Kohen, a richer understanding of the data \nwas gained, adding nuance and depth to the way the events were \nrepresented in the media and Kohen\u2019s successful strategies in fight -\ning back. From discussions with Kohen, we learned that his biggest \nasset was his social media savvy and also his knowledge of framing, \nwhich helped him know how to arrange the threads so that what he \nwanted people to focus on came out first. This included the image of \nthe yard sign, as well as his tweets. He then knew how to get his mes -\nsage out to the most people and also to the right people (e.g. other \npolitical scientists and journalists with large readerships who would \nshare his post widely), and he knew how to get it to focus on the neg -\native actions of Fortenberry\u2019s staff as well as their implications. We \nalso learned from Kohen that he has spent many years building up ca -\nchet on SNS, and he has done this through interacting with people in \npositive ways (e.g. engaging with his followers by having discussions \nwith them and \u2018liking\u2019 things they might write to him) This network \nhe built up through his own positive and supportive actions/words on \nthe sites then came to his aid when needed. Kohen\u2019s style of engaging \nwith followers is in contrast to many politicians whose posts are of -\nten onesided, meaning they are simply used to broadcast rather than \nto engage. That is, they post a link to a statement their staff has writ -\nten or they send out a couple of sentences they want their followers \nto read; they usually do not engage with the people who comment on \ntheir posts or comment on the posts of others. \ncatalano & kohen in discourse & society  (2019)      27\nIn summary, we have shown the multimodal way in which Kohen \nwas able to save his own reputation and the university\u2019s by combin -\ning a variety of strategies to explicitly reveal the nefarious intentions \nof Archer and Fortenberry in their attack on him. In doing so, we also \nprovide a useful model for how this can be done for other academics \nworldwide who find themselves in similar anti-intellectual quanda -\nries backed by right-wing agendas for which our article can provide \nnot only inspiration and hope in desperate times, but concrete ways \nto handle such situations successfully. \nDeclaration of conflicting interests \u2014 The author(s) declared no potential conflicts \nof interest with respect to the research, authorship and/or publication of this article. \nFunding \u2014 The author(s) received no financial support for the research, authorship \nand/or publication of this article. \nReferences \nAmerican Freedom Radio (2016) Exclusive: How Trump won and how all can win-\nwin now! Dr. George Lakoff on the Carol Rosin show [video file], 9 December. \nAvailable at: https://www.youtube.com/watch?v=drP9WRcaCO4   (accessed 1 \nFebruary 2019). \nBaroni A and Mayr A (2017) \u2018Shared photography\u2019: (photo)journalism and \npolitical mobilisation in Rio de Janeiro\u2019s favelas. Journalism Practice 11(2\u20133): \n285\u2013301. \nBetz HG (1994) Radical Right-Wing Populism in Western Europe . New York: St. \nMartins\u2019 Press. \nBlackford LB (2011) Berea College professor apologizes for racially charged tweet. \nLexington Herald Leader . Available at: https://www.kentucky.com/news/local/\neducation/article44142468.html   (accessed 30 January 2019). \nCampbell A (2018) Congressman Jeff Fortenberry\u2019s chief of staff threatens \nprofessor for liking Facebook post. Huffington Post , 1 November. Available at: \nhttps://www.huffingtonpost.com/entry/jeff-fortenberry-nebraska-professor-\nthreats_us_5bdb343ee4b01abe6a1c4ca8   (accessed 20 February 2019). \nChang H, Ngunjiri F and Hernandez KAC (2016) Collaborative Autoethnography . \nLondon: Routledge. \nChouliaraki L and Fairclough N (eds) (1999) Discourse in Late Modernity: \nRethinking Critical Discourse Analysis . Edinburgh: Edinburgh University Press. \nCustodio L (2014): Offline dimensions of online favela youth reactions to human \nrights violations before the 2016 Olympics in Rio de Janeiro. In: Wood N \n(ed.) Brazil in Twenty-First Century Popular Media: Culture, Politics, and \nNationalism on the World Stage . Baltimore, MD: Lexington Books, pp. 139\u2013156. \ncatalano & kohen in discourse & society  (2019)     28\nDunker C (2018) Fortenberry complains about Facebook reaction to vandalized \nsign; UNL professor says he\u2019s bullying. Lincoln Journal Star . Available at: \nhttps://journalstar.com/news/local/education/fortenberry-complains-about-\nfacebook-reaction-to-vandalized-sign-unl-professor/article_ddfa4b2a-ffcf-\n594d-9417-502f0f01cb3d.html   (accessed 30 January 2019). \nFucci T and Catalano T (2019) Missing the (turning) point: The erosion of \ndemocracy at an American university. Journal of Language and Politics 18(3): \n346\u2013370. \nGettys T (2018) Lawmaker\u2019s top aide threatens public university professor \nfor liking Facebook joke mocking his boss. The New Civil Rights Movement , \n1 November. Available at: https://www.thenewcivilrightsmovement.\ncom/2018/11/lawmakers-top-aide-threatens-public-university-professor-for-\nliking-facebook-joke-mocking-his-boss/   (accessed 1 February 2019). \nHaraway DJ (1997) Modest_witness@second_millennium.femaleman_meets_\noncomouse . New York: Routledge. \nHayden ME (2018) Nebraska White supremacist who praises violence poses \nunique challenges to campus free speech. Newsweek , 13 February. Available at: \nhttps://www.newsweek.com/nebraska-white-supremacist-unique-challenges-\ncampus-free-speech-804442   (accessed 30 January 2019). \nHofstadter R (1963) Anti-Intellectualism in American Life (3. Print.), vol. 713. New \nYork: Vintage. \nHudson DL (2018) Free speech on public college campuses overview. Freedom \nForum Institute. Available at: https://www.freedomforuminstitute.org/first-\namendment-center/topics/freedom-of-speech-2/free-speech-on-public-college-\ncampuses-overview/   (accessed 30 January 2019). \nIyengar S (1994) Is Anyone Responsible? How Television Frames Political Issues . \nChicago, IL: University of Chicago Press. \nKelsey D and Bennet L (2014) Discipline and resistance on social media: \nDiscourse, power and context in the Paul Chambers \u2018Twitter Joke Trial\u2019 . \nDiscourse, Context, Media 3: 37\u201345. \nKingkade T (2013) Geoffrey Miller censured by University of New Mexico for \nlying about fatshaming tweet. Huffington Post . Available at: http://www.\nhuffingtonpost.com/2013/08/07/geoffrey-miller-censured-unm_n_3716605.\nhtml   (accessed 30 January 2019). \nKohen A (2018a) Less than 15 hours later, a heavily armed man walked into a \nsynagogue in Pittsburgh in murdered 11 Jews because of a conspiracy theory, \nshared publicly by prominent right-wing politicians, that Jews are responsible \nfor bringing undesired immigrants to the US, 1 November. Available at: https://\ntwitter.com/kohenari/status/1057989081156730881   (accessed 30 January \n2019). \nKohen A (2018b) The implication was that he would work to create a right-\nwing troll storm, using his platform and his connections within right-wing \nmedia outlets, 1 November. Available at: https://twitter.com/kohenari/\nstatus/1057989081156730881   (accessed 30 January 2019). \ncatalano & kohen in discourse & society  (2019)      29\nKohen A (2018c) This horrible act of violence could very easily have happened \nto me, my family, and my community; *real* violence easily could have been \nthe result of the actions Archer threatened to take against me, 1 November. \nAvailable at: https://twitter.com/kohenari/status/1057989081156730881   \n(accessed 30 January 2019). \nKohen A (2018d) This is obviously a violation of my First Amendment right \nto free speech. But more than that, Archer\u2019s threat to name me publicly \nas some sort of evil liberal professor happened on Friday afternoon, right \nbefore Shabbat, 1 November. Available at: https://twitter.com/kohenari/\nstatus/1057989081156730881   (accessed 20 January 2019). \nKohen A (2018e) We deserve a lot better from our elected officials than this \nreprehensible garbage, 1 November. Available at: https://twitter.com/\nkohenari/status/1057989081156730881   (accessed 30 January 2019). \nKohen A (2018f) You can read about what happened in this article in today\u2019s \nLincoln Journal- Star. Archer tells the reporter that he didn\u2019t threaten me, \n1 November. Available at: https://journalstar.com/news/local/education/\nfortenberry-complains-about-facebook-reaction-to-vandalized-sign-unl-\nprofessor/article_ddfa4b2a-ffcf-594d-9417-502f0f01cb3d.html?mode=nowapp   \n(accessed 30 January 2019). \nKopp E (2018) Feud over professor\u2019s Facebook \u2018like\u2019 prompts complain against \nFortenberry Chief of Staff. Roll Call . Available at: https://www.rollcall.\ncom/news/politics/jeff-fortenberry-chief-staff-facebook-like    (accessed 3 \nSeptember 2019). \nKress G and van Leeuwen T (1996) Reading Images: The Grammar of Visual \nDesign . London: Routledge. \nKrzy\u017canowski M (2011) Ethnography and critical discourse analysis: Towards a \nproblem-oriented research dialogue. Critical Discourse Studies 8(4): 231\u2013238. \nKrzy\u017canowski M and Tucker JA (2018) Re/constructing politics through social and \nonline media. Journal of Language and Politics 17(2): 141\u2013154. \nLakoff G (2006) Thinking Points: Communicating Our American Values and Vision . \nLondon: Macmillan. \nLakoff G (2014) The All New Don\u2019t Think of an Elephant! Know Your Values and \nFrame the Debate . Chelsea, VT: Chelsea Green Publishing. \nLakoff G and Wehling E (2012) The Little Blue Book: The Essential Guide to \nThinking and Talking Democratic . New York: Simon & Schuster. \nLidsky LB (2011) Incendiary speech and social media. Texas Tech Law Review 44: \n147. \nLiptak A (2018) How conservatives weaponized the First Amendment. The New \nYork Times . Available at: https://www.nytimes.com/2018/06/30/us/politics/\nfirst-amendment-conservatives-supreme-court.html   (accessed 19 February \n2019). \nMacgilchrist F (2016) Fissures in the discourse-scape: Critique, rationality and \nvalidity in postfoundational approaches to CDS. Discourse and Society 27(3): \n262\u2013277. \ncatalano & kohen in discourse & society  (2019)     30\nMachin D (2007) Introduction to Multimodal Analysis . New York: Bloomsbury \nPublishing. \nMachin D and Mayr A (2012) How to Do Critical Discourse Analysis: A Multimodal \nIntroduction . London: SAGE. \nMartin JR (2004) Positive discourse analysis: Solidarity and change. Revista \nCanaria de Estudios Ingleses 49(1): 179\u2013202. \nMayr A (2018) Rioting and disorderly behavior as political media practice on the \nstress of L.A. during the riots of 1992. In: Kelsey D and Hart C (eds) Discourses \nof Disorder: Riots, Strikes and Protests in the Media . London: Edinburgh \nUniversity Press. \nMiller MH (2010) East Stroudsburg U. suspends professor for Facebook posts. \nThe Chronicle of Higher Education . Available at: http://chronicle.com/blogs/\nwiredcampus/east-stroudsburgu-suspends-professor-for-facebook-posts   \n(accessed 1 February 2019). \nMortensen M (2011) When citizen photojournalism sets the news agenda: Neda \nAgha Soltan as a Web 2.0 icon of post-election unrest in Iran. Global Media and \nCommunication 7(1): 4\u201316. \nMorton J (2018) UNL professor files ethics complaint, alleging Fortenberry\u2019s \nchief of staff threatened him over Facebook post. Kearney Hub . Available \nat: https://www.kearneyhub.com/news/state/unl-professor-files-ethics-\ncomplaint-alleging-fortenberry-s-chief-of/article_297fd1ba-debb-11e8-bdcd-\n175f53473992.html   (accessed 20 February 2019). \nPerez M (2018) Professor claims he was threatened by Congressman\u2019s Chief \nof Staff for \u2018liking\u2019 vandalized \u2018Fartenberry\u2019 sign on Facebook. Newsweek . \nAvailable at: https://www.newsweek.com/congressman-chief-staff-allegedly-\nthreatened-professor-vandalized-fartenberry-1197840   (accessed 3 September \n2019). \nRogers R (2002) Through the eyes of the institution: A critical discourse analysis \nof decision making in two special education meetings. Anthropology and \nEducation Quarterly 33(2): 213\u2013237. \nSaldana J (2015) The Coding Manual for Qualitative Researchers . London; New \nYork: SAGE. \nScott JW (2017) On Free Speech and Academic Freedom. AAUP Journal of \nAcademic Freedom 8: 1\u201313. \nSiniver A (2016) Anti-intellectualism and Israeli politics. British Journal of Middle \nEastern Studies 43(4): 630\u2013643. \nStein B (2013) Court rules that Facebook likes are free speech. ACLU . Available at: \nhttps://www.aclu.org/blog/free-speech/court-rules-facebook-likes-are-free-\nspeech   (accessed 1 February 2019). \nSugimoto C, Hank C, Bowman T, et al. (2015) Friend or faculty: Social networking \nsites, dual relationships, and context collapse in higher education. First \nMonday 20(3). Available at: https://firstmonday.org/article/view/5387/4409   \nTiede HJ (2017) Exhuming McCarthy: Meet me at the book burning. American \nAssociation of University Professors . Available at: https://www.aaup.org/\ncomment/3979#.W2ONPBpKg_U   \ncatalano & kohen in discourse & society  (2019)      31\nVan Leeuwen T (2008) Discourse and Practice: New Tools for Critical Discourse \nAnalysis . New York: Oxford University Press. \nVan Leeuwen T (2013) The representation of social actors. In: Caldas-Coultard \nCR and Coulthard M (eds) Texts and Practices: Readings in Critical Discourse \nAnalysis . London: Routledge, pp. 41\u201379. \nVaughan C (2017) The farm belt fuhrer: The making of a neo-Nazi. The Guardian , \n6 July. Available at: https://www.theguardian.com/world/2017/jul/06/neo-\nnazi-gerhard-lauck-nebraska-antisemitism   (accessed 30 January 2019). \nVertuno J (2000) Texas health official on paid leave. The Washington Post , \n19 October. Available at: http://www.washingtonpost.com/wp-srv/\naponline/20001019/aponline194937_000.htm   (accessed 1 February 2019). \nWeiner S (2018) Rep. Jeff Fortenberry\u2019s chief of staff threatened a professor over \nliking a defaced sign on Facebook. Splinter News , 1 November. Available at: \nhttps://splinternews.com/repjeff-fortenberrys-chief-of-staff-threatened-a-\nprof-1830167147   (accessed 20 February 2019). \nWodak R (1996) Disorders of Discourse . London: Longman. \nWodak R (2009) The Discourse of Politics in Action: Politics as Unusual . London: \nPalgrave Macmillan. \nWodak R (2015) The Politics of Fear: What Right-wing Populist Discourses Mean . \nLondon: SAGE. \nWodak R (2017) The \u2018establishment\u2019, the \u2018elites\u2019, and the \u2019people\u2019 . Who\u2019s who? \nJournal of Language and Politics 16(4): 551\u2013565. \nWodak R (2019) Entering the \u2018post-shame era\u2019: The rise of illiberal democracy, \npopulism and neoauthoritarianism in Europe. Global Discourse 9(1): 195\u2013213. \nWodak R and Krzy\u017canowski M (2017) Right-wing populism in Europe and USA. \nJournal of Language and Politics 16(4): 471\u2013484. \nZakaria F (1997) The rise of illiberal democracy. Foreign Affairs 76(6): 22\u201343. \nAbout the Authors \nTheresa Catalano  is Associate Professor of Second Language Education/Ap -\nplied Linguistics at the University of Nebraska\u2013Lincoln. Her research focuses \non language and migration, language education and (multimodal) critical \ndiscourse studies. Her recent book Talking about Global Migration was pub -\nlished in 2016 by Multilingual Matters. She teaches graduate/undergradu -\nate courses on multimodal discourse analysis, linguistics, intercultural com -\nmunication and (dual) language education. \nAri Kohen  is Associate Professor of Political Science and Schlesinger Pro -\nfessor of Social Justice at the University of Nebraska\u2013Lincoln. His most re -\ncent book, Untangling Heroism: Classical Philosophy and the Concept of the \ncatalano & kohen in discourse & society  (2019)     32\nHero was published by Routledge in 2014. His first book, In Defense of Hu -\nman Rights: A Non-Religious Grounding in a Pluralistic World , also from \nRoutledge, was published in 2007. In addition to a dozen articles on human \nrights, heroism and restorative justice, Kohen is also co-editor of a new se -\nries of edited volumes on contemporary research and teaching on the Holo -\ncaust from the University of Nebraska Press.  ", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Googly eyes and yard signs: Deconstructing one professor's successful rebuffing of a right-wing attack on an academic institution", "author": ["T Catalano", "A Kohen"], "pub_year": "2020", "venue": "Discourse & Society", "abstract": "Right-wing populism is on the rise worldwide, and political attacks against universities have  increased in the United States since the election of Donald Trump. In 2017, an incident"}, "filled": false, "gsrank": 612, "pub_url": "https://journals.sagepub.com/doi/abs/10.1177/0957926519880037", "author_id": ["r1eFvtEAAAAJ", "Ab_FnnkAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:TcJOJqq32rcJ:scholar.google.com/&output=cite&scirp=611&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D610%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=TcJOJqq32rcJ&ei=dbWsaILdNb_SieoPzJnloAQ&json=", "num_citations": 1, "citedby_url": "/scholar?cites=13248103195278754381&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:TcJOJqq32rcJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1376&context=teachlearnfacpub"}}, {"title": "Polls, clickbait, and commemorative $2 bills: problematic political advertising on news and media websites around the 2020 US elections", "year": "2021", "pdf_data": "This paper appears at the 2021 Internet Measurement Conference (IMC).\nPolls, Clickbait, and Commemorative $2 Bills:\nProblematic Political Advertising on News and Media Websites\nAround the 2020 U.S. Elections\nEric Zeng, Miranda Wei, Theo Gregersen, Tadayoshi Kohno, Franziska Roesner\nPaul G. Allen School of Computer Science & Engineering\nUniversity of Washington\nSeattle, WA, USA\n{ericzeng,weimf,theoag,yoshi,franzi}@cs.washington.edu\nABSTRACT\nOnline advertising can be used to mislead, deceive, and manipulate\nInternet users, and political advertising is no exception. In this pa-\nper, we present a measurement study of online advertising around\nthe 2020 United States elections, with a focus on identifying dark\npatterns and other potentially problematic content in political adver-\ntising. We scraped ad content on 745 news and media websites from\nsix geographic locations in the U.S. from September 2020 to January\n2021, collecting 1.4 million ads. We perform a systematic qualitative\nanalysis of political content in these ads, as well as a quantitative\nanalysis of the distribution of political ads on different types of\nwebsites. Our findings reveal the widespread use of problematic\ntactics in political ads, such as bait-and-switch ads formatted as\nopinion polls to entice users to click, the use of political controversy\nby content farms for clickbait, and the more frequent occurrence\nof political ads on highly partisan news websites. We make policy\nrecommendations for online political advertising, including greater\nscrutiny of non-official political ads and comprehensive standards\nacross advertising platforms.\nCCS CONCEPTS\n\u2022Information systems \u2192Online advertising ;\u2022Social and\nprofessional topics \u2192Computing / technology policy ;\u2022Se-\ncurity and privacy \u2192Human and societal aspects of security\nand privacy .\n1 INTRODUCTION\nThe 2020 United States general elections were one of the most\nimportant and contentious elections in recent history. Issues facing\nthe U.S. included the COVID-19 pandemic and ensuing economic\ncrisis, controversy surrounding President Donald Trump\u2019s first\nterm, and renewed movement for racial justice following the murder\nof George Floyd and other police violence. During this election\nseason, online political advertising was more prominent than ever:\ncampaigns turned to online ads as the pandemic reduced in-person\nevents and canvassing [ 89], and spent record sums advertising on\nGoogle and Facebook [ 69]. The misuse of online ads in non-political\ncontexts is a well-known problem, ranging from distasteful clickbait\nads to outright scams and malware [ 47,58,95\u201397]. In this paper, we\ninvestigate misleading and manipulative tactics in online political\nadvertising, for purposes such as collecting email addresses and\ndriving traffic to political content websites.\nWe take a broad view of what constitutes a \u201cpolitical\u201d ad in our\nwork, considering any ad with political content, whether or notthe ad was placed by an official political campaign committee. In\nour investigation, we ask: Who ran political ads during this period?\nWhat was the content of these ads, and do they use problematic\ntechniques? Did the number of political ads on different types of\nwebsites differ?\nTo answer these questions, we conducted measurements of on-\nline advertising before, during, and after the Nov. 3rd elections. We\ncollected a daily crawler-based sample of ads from 745 online news\nand media websites from September 2020 to January 2021, provid-\ning insight into the ads people saw while reading news during this\nperiod. We continued collecting data through several post-election\ndevelopments: contested vote counting in multiple states, the Geor-\ngia U.S. Senate runoff election on January 5, and attack on the U.S.\nCapitol on January 6. Our crawlers collected data from six locations\nwith varying political contestation: Atlanta, GA; Miami, FL; Raleigh,\nNC; Phoenix, AZ; Salt Lake City, UT; and Seattle, WA.\nUsing a combination of qualitative and quantitative techniques,\nwe analyze the political ads in our dataset, including identifying\nexamples of misleading and manipulative techniques, the distribu-\ntion of political ads across websites of different political biases, and\npolitical affiliations and organization types of the advertisers.\nScope. Our crawler-based dataset provides a complementary per-\nspective to the political ad archives from Google and Facebook.\nThough our dataset is not as complete as the political ad archives,\nand partially overlaps Google\u2019s, our dataset encompasses allads\non the pages we crawled \u2014 including non-political ads, political-\nthemed ads were not officially classified as political and thus do not\nappear in Google\u2019s archive, and ads served via ad networks outside\nof Google Ads. Additionally, we capture the URL of the website that\neach ad appeared on, allowing us to measure contextual targeting\nof political ads on news and media websites.\nContributions. First, we characterize the quantity and content of\nonline advertising longitudinally during the 2020 U.S. Presidential\nElection and shortly thereafter, and at scale.\n\u2022We observe differences in the number of political ads in\ndifferent geographical locations.\n\u2022We observe shifts in the quantity of political ads through the\nelection, and the effects of political ad bans.\n\u2022We characterize the topics of all online advertisements that\nwe collected during this time period.\nThrough our qualitative analysis, we observed several problem-\natic types of online political advertising, such as:\nEric Zeng, Miranda Wei, Theo Gregersen, Tadayoshi Kohno, Franziska Roesner\n\u2022The use of misleading and manipulative patterns in political\nads. For example, ads that purport to be political polls, but use\ninflammatory framing, and appear to be used for gathering\nemail addresses.\n\u2022Political topics in clickbait and native advertising. These ads\nimitate the look of links to news articles, but link to external\nsites. Headlines often imply controversy about candidates,\nand may fuel disinformation.\nWe also find that problematic political ads are more common on\npartisan and low-quality news sites.\n\u2022More partisan websites have more political ads, on both ends\nof the political spectrum.\n\u2022Problematic categories of ads, such as political products and\npolls, appear more frequently on right-leaning sites.\nWe discuss the potential harms from the problematic political ads\nwe observed, and we make recommendations for platform policies,\ngovernment regulation, and future research. We also release our\nfull dataset of ads and metadata.\n2 BACKGROUND AND RELATED WORK\n2.1 The 2020-21 U.S. Elections and Ads\nBetween September 2020 and January 2021, the U.S. held a presi-\ndential election, congressional elections, and numerous state and\nlocal elections. In the presidential election, Joe Biden, a Democrat,\nand his running mate, Kamala Harris, ran against Donald Trump,\nthe incumbent Republican president, and his running mate, Mike\nPence [8]. We provide more historical background in Appendix A.\nBefore the election, tech companies faced mounting pressure to\naddress concerns about political advertising spreading misinforma-\ntion and causing other harms. Some companies had already banned\npolitical ads (Pinterest in 2018 [ 31], Twitter in 2019 [ 17]), at least in\npart due to revelations that Russian organizations had purchased\npolitical ads during the 2016 presidential election [ 41]. Google and\nFacebook allowed political ads in 2020, but implemented several\nshort-term bans. Our dataset of display ads was likely impacted by\nGoogle\u2019s bans from Nov. 4 through Dec. 10 [ 25,78], and again after\nthe storming of the Capitol between Jan. 14 and Feb. 24 [26].\nStill, political ads around the 2020-21 elections set new records\nfor ad spending, with overall spending in the billions. On Facebook\nand Google alone, the Trump campaign spent $276 million and the\nBiden campaign spent $213 million [69].\n2.2 Online Political and Problematic Ads\nPrior work studies the online ad ecosystem from various perspec-\ntives. In the computer security and privacy community, researchers\nhave often studied the privacy implications of online ads and the\ntracking enabling them (e.g., [ 9,45,59,71,75,90]). In this work,\nwe focus on the content of ads and contextual targeting that may\ncause different ads to appear on different types of sites, rather than\non the underlying privacy-invasive mechanisms.\nRecent work in computer science identifies types of problematic\ncontent in ads (e.g., clickbait, distasteful ads, misleading content,\nmanipulative techniques) [ 96,97], and types explicitly malicious ads\n(e.g., spreading malware) [ 47,58,67,93,95]. Online ads play a role\nin spreading mis/disinformation (e.g., during the 2016 and 2018 U.S.elections) [ 14,21,79,80] as well as in monetizing mis/disinforma-\ntion websites [ 15,27,40,60]. Other work has shown that ads (e.g., on\nFacebook) may be targeted in discriminatory ways [ 2,43]. Studies of\nmisleading and manipulative patterns (often called \u201cdark patterns\u201d)\nbeyond ads also inform our work (e.g., [ 51,57]), particularly a recent\nstudy of such patterns in political campaign emails [52].\nSignificant work in other fields (e.g., political science and mar-\nketing) also studies political ads. Kim et al. identified political ads\non Facebook purchased by \u201csuspicious\u201d groups, including Russian\ngroups known for spreading disinformation [ 41]. Stromer-Galley\net al. [ 85] studied U.S. political ads on Facebook in 2016 and 2020,\nwhile Ballard et al. [ 7] characterized political campaign web display\nads during the 2012 U.S. elections. Other work considered deceptive\npolitical advertising, (not necessarily online) including deceptively\nformatted \u201cnative\u201d ads (e.g., [ 18,55]). Van Steenburg provides a\nsystematic literature review of political advertising research and\nproposes a research agenda, identifying the study of the impact of\ntechnology (i.e., the internet) as one key theme and area for future\nwork (but does not discuss the manipulative patterns or non-official\npolitical ads that we see in our dataset) [84].\nOur work considers ads appearing on websites rather than social\nmedia, and we capture all ads (not only those marked as political\nads). Prior work has found that Facebook\u2019s ad archives are incom-\nplete and use a limited definition of \u201cpolitical\u201d [20, 21, 81]. Indeed,\nwe found many ads that contained political themes but were not\nplaced by an official campaign.\n3 METHODOLOGY\nIn this section, we describe our methodology for measuring ads\nthroughout the 2020 U.S. elections. In summary, we selected a group\nof popular mainstream and alternative news websites and scraped\nads from these sites using crawlers in different locations. We col-\nlected 1.4 million ads in total from September 2020 to January 2021.\nWe analyzed the content of our ads dataset using a combination\nof natural language processing, to automate tasks like identifying\nwhich ads were political, and manual qualitative analysis tech-\nniques, to provide greater context such as the party affiliation of\nthe advertiser. See Figure 1 for a summary of our analysis pipeline.\n3.1 Ad Crawling\n3.1.1 Seed Websites. To collect ads, we crawled news and me-\ndia websites that spanned the political spectrum and information\necosystem. We identified 6,144 mainstream news websites in the\nTranco Top 1 million [ 44], using categories provided by the Alexa\nWeb Information Service [ 4]. These mainstream sites included na-\ntional newspapers, local newspapers, TV stations, and online digital\nmedia. We also compiled a list of 1,344 websites which we refer to\nas \u201cmisinformation websites\u201d. Websites in this list were identified as\n\u201cfake news\u201d, alternative news, mis/disinformation, highly partisan,\npropaganda, or conspiracy websites by fact checkers (Politifact [ 83],\nSnopes [42], Media Bias/Fact Check [54], and others [23, 36, 61]).\nTo ensure that our crawlers could complete the crawl list in one\nday, we truncated the list to 745 sites by picking all sites with a\nranking higher than 5,000 (411 sites), and then sampling from the\nremaining tail (334 sites) by choosing 1 site per bucket of 10,000\nsite rank, to ensure that lower ranked sites were represented. In\nPolls, Clickbait, and Commemorative $2 Bills:\nProblematic Political Advertising on News and Media Websites Around the 2020 U.S. Elections\nFigure 1: Overview of our analysis methodology. We used NLP techniques to preprocess and organize our dataset, and then\nconducted manual content analyses to explore political ads in greater detail, and to validate automated outputs. Blue boxes\nrepresent data, green boxes represent automated processes, and red boxes represent manual and qualitative analyses.\nSite Bias # Sites Examples\nMainstream News and Media Websites\nLeft 63 jezebel.com, salon.com\nLean Left 57 miamiherald.com, theatlantic.com\nCenter 46 npr.org, realclearpolitics.com\nLean Right 18 foxnews.com, nypost.com\nRight 44 dailysurge.com, thefederalist.com\nUncategorized 376 adweek.com, nbc.com\nNews Websites Labeled as Misinformation\nLeft 13 alternet.org, dailykos.com\nLean Left 6 greenpeace.org, iflscience.com\nCenter 1 rferl.org\nLean right 11 rt.com, newsmax.com\nRight 60 breitbart.com, infowars.com\nUncategorized 50 globalresearch.ca, vaxxter.com\nTable 1: Summary of our seed sites, by misinformation label\nand political bias (sources in Section 3.1.1).\nTable 1, we show the number of sites in our crawl list by misinfor-\nmation label and political bias. The political bias of websites were\naggregated from Media Bias/Fact Check [54] and AllSides [3].\n3.1.2 Crawler Implementation. We built a web crawler to scrape\nads based on Puppeteer [ 28], a Chromium-based browser automa-\ntion library. Each crawler node crawls the seed list once per day,\ncrawling 6 domains in parallel in random order. For each seed do-\nmain, the crawler loads the root page and detects ads using CSS\nselectors from EasyList [ 19], a filter list used by ad blockers. Ele-\nments smaller than 10 pixels in width or height (like tracking pixels)\nwere ignored. The crawler scrolls to each ad, takes a screenshot,\nand collects the HTML content. Then, the crawler clicks the ad,\nand collects the URL and content of the landing page. Because ads\nmay differ on site homepage vs. subpages, for each seed domain,\nthe crawler also visits and collects ads from an article on the site.\nTo minimize behavioral ad targeting, we crawled each seed do-\nmain using a clean browser profile (similar to prior work [ 96]). For\neach domain we visited, we ran separate browser instances inside\na new Docker container, so that no tracking cookies or other state\npersisted across domains (though fingerprinting may be possible).3.1.3 Crawler Nodes and Locations. We crawled ads using 4 nodes\nfrom geographical locations where we predicted the political land-\nscape could result in different ads.\n\u2022Sep. 25, 2020 \u2013 Nov. 12, 2020 : We first crawled from two cities\nin states predicted to be contested (Miami, FL; Raleigh, NC)\nand two uncompetitive (Seattle, WA; Salt Lake City, UT).\n\u2022Nov. 13, 2020 \u2013 Dec. 8, 2020 : Due to contested election results,\nwe switched two crawlers to Phoenix, AZ and Atlanta, GA.\nThe other two crawlers alternated between the 4 previous\nlocations (Seattle, Salt Lake City, Miami, Raleigh).\n\u2022Dec. 9, 2020 \u2013 Jan. 19, 2021 : After the presidential election\nwas resolved, we crawled from Atlanta, GA and Seattle, WA\nto observe the Georgia special election. Due to the Capitol\ninsurrection, we continued crawling for 2 weeks.\nTo simulate crawling from these locations, we tunneled our\ntraffic through the Mullvad VPN service. Mullvad\u2019s VPN servers\nran on rented servers in local data centers (100TB, Tzulo, and M247).\nWe verified that the VPN servers were located in the advertised\nlocations using commercial IP geolocation services.\nIn sum, we ran 312 daily crawls, on 4 machines, using Chromium\n88.0.4298.0, on a Debian 9 Docker image. The hardware was: Intel\nCore i7-4790 3.6GHz 32GB RAM, Intel Core i7-7740X 4.3 GHz 64GB\nRAM, and Intel Core i5-6600 3.30GHz, 16GB RAM (2x).\n3.1.4 Data Collection Errors. No data was collected globally from\n10/23\u201310/27 (VPN subscription lapsed), nor 12/16\u201312/29 and 1/15\u2013\n1/19 in Seattle (VPN server outage). Some individual crawls also\nsporadically failed. In total, 33 of 312 daily crawl jobs failed.\n3.2 Preprocessing Ad Content\n3.2.1 Extracting Text from Ads. To enable large-scale analysis of\nthe content of our dataset, we extracted the text of each ad. For\nads where 100% of the visual content is contained in an image,\nwe used the Google Cloud Vision API to perform optical character\nrecognition (OCR). We extracted text from 877,727 image ads (62.6%)\nusing this method. For native ads (i.e., sponsored content headlines),\nthe text is contained in the HTML markup, so we automatically\nextracted the text from these ads using JavaScript. We extracted\ntext from 524,518 native ads (37.4%) using this method.\nEric Zeng, Miranda Wei, Theo Gregersen, Tadayoshi Kohno, Franziska Roesner\n3.2.2 Ad Deduplication. Many ads in our dataset appeared multi-\nple times, some appearing tens of thousands of times. To reduce\nredundancy during qualitative coding and the runtime of machine\nlearning tasks, we de-duplicated ads using the extracted text. We\ngrouped our dataset by the domain of the landing page of the ad,\nand for each group, we used MinHash-Locality Sensitive Hashing1\n(LSH) to identify ads with a Jaccard similarity >0.5. We maintained\na mapping of unique ads to their duplicates, which we used later\nto propagate qualitative labels for unique ads to their duplicates,\nenabling analysis of the whole dataset. After deduplication, we\nobtained a subset of 169,751 unique ads.\n3.3 Analyzing Ad Content with Topic Modeling\nTo help us broadly understand the content of the ads in our dataset,\nwe used topic modeling to automatically create groups of semanti-\ncally similar ads, allowing us to qualitatively analyze those groups.\nWe experimented with several topic modeling and text clustering\nalgorithms, and selected Gibbs-Sampling Dirichlet Mixture Model\n(GSDMM) [ 94], which performed best on our dataset (see our ex-\nperimental methodology in Appendix B). Second, we automatically\ngenerated qualitative descriptions of each ad cluster, by using c-tf-\nidf to extract the most significant words from the text cluster [ 33].\nWe applied GSDMM & c-tf-idf to describe the topics in our overall\nads dataset (Sec. 4.3) and political product ads (Sec. 4.7).\n3.4 Analyzing Political Ads In-Depth\nOur main focus is the content of political ads in our dataset. We de-\nfined a political ad broadly: any ad with political content, whether or\nnot the advertiser was a political campaign. This includes ads with\nincidental political content, such as ads for products incorporating\nelection imagery or ads promoting political news articles.\nOur analysis of political ads consisted of three phases. First, we\nused machine learning to automatically identify political ads in our\noverall ads dataset. Second, we manually labeled the attributes of\neach political ad, such as the purpose of the ad, and the advertiser\u2019s\npolitical affiliation. Lastly, we performed quantitative analyses of\nthe labeled political ad data.\n3.4.1 Political Ads Classifier. To analyze political ads, we first\nneeded to isolate political ads from the overall ads dataset. We\nimplemented a binary text classifier based on the BERT language\nmodel, to classify our ads as political or non-political.\nWe started by generated a training set of political and non-\npolitical ads by labeling a random sample of ads in our dataset,\nobtaining 646 political ads and 1,937 non-political ads. We supple-\nmented this data by crawling 1,000 political ads from the Google\npolitical ad archive [ 30] to balance the classes. We implemented\nthe classifier by fine-tuning the DistilBERT model [ 76] for a binary\nclassification task. We trained our model with a 52.5% / 22.5% / 25%\nTrain / Validation / Test split. Our model achieved an accuracy of\n95.5%, and an \ud835\udc391score of 0.9. We ran the classifier on our dedupli-\ncated dataset (169,751 unique ads) and it classified 8,836 unique ads\nas political (5.2%).\n1We used the MinHash LSH implementation from the datasketch Python library:\nhttp://ekzhu.com/datasketch/lsh.html.3.4.2 Qualitative Analysis of Political Ads. Next, we we qualita-\ntively coded the 8,836 unique political ads in our dataset to build\na systematic categorization of the ads\u2019 content and characteris-\ntics [ 74]. Prior work in computer science and political science has\nalso analyzed ad content using qualitative coding [ 85,96]. We de-\nscribe the development of our qualitative codebook and coding\nmethods in detail in Appendix C.\nCodebook Summary. We describe the high level categories of\nour codebook; a full list of subcodes is presented in Table 2, and a\nfull set of definitions in Appendix C. We identified three mutually\nexclusive categories at the top level. (1) Campaigns and Advo-\ncacy ads explicitly addressed a political candidate, election, policy,\nor call to action. We further coded the Election Level ,Ad Purpose ,\nPolitical Affiliation , and Organization Type . We coded Election Level\nbased on the level of government, and Purpose based on the desired\naction in the ad. We coded Organization Type by first identifying\nthe advertiser, using \u201cPaid for By...\u201d labels and the landing page\ncontent, and then looking up the legal registration of the adver-\ntiser. We coded Affiliation if the advertiser was officially associated\nwith a political party, or indicated alignment with words such as\n\u201cconservative\u201d. We were able to attribute an organization type and\nadvertiser affiliation for 96.5% of the campaigns and advocacy ads.\n(2) Political News and Media ads promoted political news arti-\ncles, videos, news sources, or events. We further demarcated two\nsubcategories. Sponsored Articles / Direct Links to Articles included\nads which promoted a specific article or piece of content. News\nOutlets, Programs, Events, and Related Media contained all other\ntypes of political news and media. (3) Political Products ads cen-\ntered on selling a product or service by using political imagery or\ncontent. We labeled political product ads as either Political Memora-\nbilia,Nonpolitical Products Using Political Topics , orPolitical Services .\nAds were labeled as (4) Malformed/Not Political if the classifier\nidentified the ad as political, but the content was occluded, incor-\nrectly cropped, or contained multiple ads, in a way that made it\nimpossible to analyze the ad. False positives (ads incorrectly labeled\nas political by the classifier) were also given this label.\n3.5 Ethics\nOur data collection method had two types of impacts on the web.\nFirst, our crawler visited web pages and scraped their content. We\nbelieve this had a minimal impact: all sites we visited were public-\nfacing content websites, contained no user data, and were visited\nby our crawlers no more than 4 times per day.\nSecond, our crawler clicked on ads to scrape the landing page\nof the ads. By clicking on the ads, we may cause the advertiser\nto be charged for the clickthrough (unless our click is detected as\nillegitimate), which is paid to the website and various middlemen.\nWe determined that clicking on ads was necessary because it\nwas the only way for us to obtain the content and URL of the\nlanding page for each ad. Many ads obscure their landing page\nthrough nested iframes and redirect chains. This data was needed\nfor automatically determining the identity of the advertiser and for\nmanually investigating the landing pages during qualitative coding\n(when the ad itself did not have sufficient context).\nIt is difficult to estimate the costs incurred to advertisers as a\nresult of our crawls, but we believe the amount was low enough to\nPolls, Clickbait, and Commemorative $2 Bills:\nProblematic Political Advertising on News and Media Websites Around the 2020 U.S. Elections\nbe inconsequential. We cannot precisely determine the cost because\nthe bid for each ad is not visible, and we do not know if advertisers\npay using a cost-per-impression model or cost-per click model. For\nadvertisers who pay based on impressions, we estimate the amount\ncharged to be $3.00 per thousand impressions [ 87]. If all advertisers\npaid by impression, we estimate the total cost to alladvertisers to be\napproximately $4,200. For the average advertiser, the mean number\nof ads we crawled was 63, and the median was 3, resulting in a mean\ncost of $0.19, and median cost of $0.009. If advertisers instead paid\nper click, we estimate a cost of approximately $0.60 per click [ 39]: in\nthis case, the the mean advertiser would have been charged $37.80,\nand the median would have paid $1.80. The outlier advertisers\nin our dataset who received the most clicks were predominantly\nintermediary entities, such as Zergnet (36k ads), mysearches.net\n(26k ads), and comparisons.org (9k ads). These intermediaries place\nads on other websites on behalf of advertisers on their platform,\nmeaning that costs incurred for these intermediaries were spread\namong many individual sub-advertisers.\nStepping back, as we discuss further in Section 5, because of the\ndistributed nature of the web ad ecosystem and the complex incen-\ntives of different stakeholders, we believe it is critical that external\naudits investigate the content and practices in this ecosystem, as\nwe do in this study. Towards that end, we believe that the (small)\ncosts of our study were justified. It is only through the process of\nclicking on ads, and evaluating the resulting landing pages, that can\none fully understand the impact to users if they were to click on\nthe ads. This is akin to the observation that malware websites may\nbe linked from ads, potentially requiring search engine companies\naiming to develop lists of known malware sites to engineer their\ncrawlers to click on ads [ 63]. Moreover, similar methodologies have\nbeen used in prior works studying ads [67, 93].\n3.6 Limitations\nOur crawling methodology provided an incomplete sample of polit-\nical advertising on the web. Our crawlers only visited a finite set of\nnews and media websites, excluding other places that political ads\nappear, e.g., Facebook. Because we only visited each site once, we\nonly saw a fraction of all ad campaigns running at that time. Our\ncrawlers also only see political ad campaigns that were served to\nthem \u2014 ongoing political ad campaigns may not have been shown\nto the crawler e.g. because of targeting parameters. We may have\nfailed to load landing pages for ads because of detection and ex-\nclusion of our crawler by ad platforms. Due to VPN outages and\ncrawler bugs, some days are missing from the data (Sec. 3.1.4).\nWe relied on categorizations from the fact checkers AllSides [ 3]\nand Media Bias/Fact Check [ 54] to identify the political bias of our\ninput websites. 42% of our input sites had a rating: some uncatego-\nrized sites were non-political news websites (e.g., espn.com), while\nothers may not have been popular enough to be rated.\nOur automated content analyses were based on text extracted\nwith OCR and did not use visual context from images. Some ads\ncontained text artifacts, which negatively impacted downstream\nanalyses. Based on the sample we labeled, we estimate that 18%\nads in our dataset were malformed, i.e., impossible to read the\nad\u2019s content. This was typically caused by modal dialogs (such asnewsletter signup prompts) occluding the ad, which are difficult to\nautomatically and consistently dismiss.\nFor the majority of ads, our data did not allow us to identify\nthe ad networks involved in serving the ads. Though our crawler\ncollected the HTML content of each ad (including iframes), this\nalone was rarely sufficient to identify ad networks.\nDespite the above limitations, our dataset presents a unique and\nlarge-scale snapshot of political (and other) web ads surrounding\nthe 2020 U.S. election. These include ads that do not appear in\nGoogle\u2019s (or others\u2019) political ad transparency reports. To support\nfuture research and auditing of this ecosystem, we will release our\nfull dataset along with the publication of this paper, including ad\nand landing page screenshots, OCR data, and our qualitative labels.\n4 RESULTS\nIn this section, we present an analysis of the ads in our dataset. We\nbegin by providing an overview of the dataset as a whole, including:\nHow many ads appear overall, and how many of these are political\nads of different types (Sec. 4.1)? How did the number of ads (political\nand non-political) change over time and location (Sec. 4.2)? Overall,\nwhat ad topics were common (Sec. 4.3)?\nThen, we dive more deeply into our analysis of political ads. We\ninvestigate and characterize the sites political advertising appeared\non (Sec. 4.4), advertisers running official campaign and advocacy\nads (Sec. 4.5), misleading/manipulative campaign ads (Sec. 4.6), and\npolitical product ads (Sec. 4.7) and news and media ads (Sec. 4.8).\n4.1 Dataset Overview\nBetween September 26, 2020 and January 19, 2021, we collected\n1,402,245 ads (169,751 unique ads) from 6 locations: Atlanta, Miami,\nPhoenix, Raleigh, Salt Lake City, and Seattle. Our political ad classi-\nfier and qualitative coding, detected 67,501 ads (8,836 unique) with\npolitical content, or 3.9% of the overall dataset. During our qualita-\ntive analysis of political ads, we removed 11,558 false positives and\nmalformed ads (3,201 unique), resulting in 55,943 political ads. In\nTab. 2, we show the number of political ads, across our qualitative\ncategories. About a third of ads were from political campaigns and\nadvocacy groups; over half advertised political news and media,\nand the remainder political products.\n4.2 Longitudinal and Location Analysis\n4.2.1 Ads Overall. We show the quantity of ads collected by loca-\ntion in Fig. 2a. The number of ads per day stayed relatively stable in\neach location: consistently around 5,000 ads per day. The stability\nin ad counts indicates that changes in demand for ad space before\nand after the election had little impact on websites\u2019 ad inventory.\nWe collected about 1,000 fewer ads per crawler day in Atlanta\nthan other locations. We do not know if this was due to differ-\nences in location-based targeting or an artifact of our crawling (e.g.,\nlimitations of the Atlanta VPN provider).\n4.2.2 Political Ads. The amount of political ads over time and\nlocations is visualized in Fig. 2b. Leading up to the presidential\nelection on Nov. 3, 2020, the number of ads per day in each location\nincreases from less than 250 to peaks of 450. After election day,\nthe number of political ads seen by crawlers sharply decreases, to\nbelow 200 ads/day. This decrease could be a natural consequence\nEric Zeng, Miranda Wei, Theo Gregersen, Tadayoshi Kohno, Franziska Roesner\n(a) The number of ads collected in each crawler location. We collected a relatively constant number of ads for each location.\n(b) The number of political ads, classified as political by our text classifier, collected in each crawler location. The number of political ads was\nhigher prior to the elections in November and January, were lower in the period after the elections.\nFigure 2: Longitudinal graphs showing the number of total ads and political ads, collected in six locations from Sept. 2020 to\nJan. 2021. Salient U.S. political events, as well as ad bans implemented by Google, are superimposed for context. Gaps from mid-\nNov. to mid-Dec. are because we scheduled crawls on nonconsecutive days. Other gaps are due to VPN outages (see Sec. 3.1.4).\nof less political attention following election day; it likely was also\ndue to Google\u2019s first ad ban, from Nov. 4 to Dec. 10. We believe\nGoogle\u2019s ad bans help contextualize our results, given Google\u2019s\nlarge presence in web ads \u2014 but because we did not determine the\nad networks used by each ad, we cannot prove a causal connection.\nDuring Google\u2019s first ban, we collected 18,079 political ads. 76%\nof these ads were political news ads and political product ads. In the\n4,274 campaign and advocacy ads during this period, 82% were from\nnonprofits and unregistered groups, such as Daily Kos, UnitedVoice,\nJudicial Watch, and ACLU. The remaining 18% (783 ads) were from\nregistered committees, some from candidates in special elections\n(e.g., Luke Letlow, Raphael Warnock), but others from PAC groups\nspecifically referencing the contested Presidential election. For ex-\nample, an ad from the Democratic-affiliated Progressive Turnout\nProject PAC reads: \u201cDEMAND TRUMP PEACEFULLY TRANSFER\nPOWER \u2013 SIGN NOW\u201d.\nGoogle lifted their political ad ban on Dec. 11. At this time, we\nonly collected data from Seattle and Atlanta, and observed a rise\nin the number of political ads per day in Atlanta until the Georgia\nrun-off election on Jan. 5, 2021, but no corresponding rise in Seattle.\nThe increase in Atlanta came almost entirely from Republican-\naffiliated committees \u2014 Democratic-affiliated advertisers seem to\nhave bought very little online advertising for this election (Fig. 3).\nFollowing the Georgia election, we again observed a sharp drop\nin ads per day from the Atlanta crawler, matching the Seattle\ncrawler at less than 200 political ads per day.\nThough we observe that the volume of political advertising gen-\nerally fell after elections, Google\u2019s ban on political advertising did\nnot stop all political ads \u2014 other platforms in the display ad ecosys-\ntem still served political advertising.\nFigure 3: Campaign ads observed in Atlanta in Dec 2020\u2013Jan\n2021, prior to the Georgia special elections. Almost all ads\nduring this time period were run by Republican groups.\n4.3 Topics of Ads in Overall Dataset\nTo provide context before diving into political ads (Sec. 4.4-4.8),\nwe present results from a topic model of the entire dataset. Tab. 3\ndisplays the 10 largest topics in the data, each with a manually\nassigned topic description, the top c-TF-IDF terms, and the number\nof ads assigned to the topic.\nThe largest topic regarded \u201centerprise\u201d ads, e.g., a Salesforce ad to\n\u201cempower your partners to accelerate channel growth with external\napps.\u201d The second largest topic included \u201ctabloid\u201d ads, e.g., \u201cthe un-\ntold truth of Arnold Schwarzenegger, \u201d as well as many clickbait and\nnative advertisements. The model\u2019s fourth largest topic, \u201cpolitics\u201d,\ncontained 71,240 ads: a 64.8% overlap with the 55,943 political ads\nidentified by our classifier and qualitative coding.\nThese topics give us a sense of the context within which politi-\ncal ads were embedded. Like the web ad content studied in prior\nPolls, Clickbait, and Commemorative $2 Bills:\nProblematic Political Advertising on News and Media Websites Around the 2020 U.S. Elections\nAd Categories Count %\nPolitical News and Media 29,409 52%\nSponsored Articles 25,103 45%\nNews Outlets, Programs, Events 4,306 7%\nCampaigns and Advocacy 22,012 39%\nLevel of Election\nPresidential 5,264 9%\nFederal 5,058 9%\nState/Local (including initiatives/referenda) 2,320 4%\nNo Specific Election 2,150 4%\nNone 7,220 13%\nPurpose of Ad (not mutually exclusive)\nPromote Candidate or Policy 10,923 20%\nPoll, Petition, or Survey 7,602 14%\nVoter Information 4,145 7%\nAttack Opposition 3,612 6%\nFundraise 2,513 4%\nAdvertiser Affiliation\nDemocratic Party 5,108 9%\nRight/Conservative 5,000 9%\nRepublican Party 4,626 8%\nNonpartisan 4,628 8%\nLiberal/Progressive 1,673 3%\nUnknown 781 1%\nIndependent 172 <1%\nCentrist 24 <1%\nAdvertiser Organization Type\nRegistered Political Committee 12,131 22%\nNews Organization 4,249 8%\nNonprofit 2,736 5%\nBusiness 931 2%\nUnregistered Group 913 2%\nUnknown 781 1%\nGovernment Agency 241 <1%\nPolling Organization 30 <1%\nPolitical Products 4,522 8%\nPolitical Memorabilia 3,186 6%\nNonpolitical Products Using Political Topics 1,258 2%\nPolitical Services 78 <1%\nPolitical Ads Subtotal 55,943 100%\nPolitical Ads - False Positives/Malformed 11,558\nNon-Political Ads Subtotal 1,347,810\nTotal 1,402,245\nTable 2: Summary of the types of ads in our dataset.\nwork [ 96,97], political ads were surrounded by ordinary or legit-\nimate ads for products and services, as well as low-quality and\npotentially problematic ads.\n4.4 Distribution of Political Ads On Sites\nNext, we examine how political ads were distributed across sites\nby political bias, misinformation label, and popularity.\nPolitical Bias of Site. Overall, we find that political ads appeared\nmore frequently on sites with stronger partisan bias. Fig. 4 shows\nthe fraction of ads that were political across websites\u2019 political\nbiases for mainstream and misinformation sites.Topic c-Tf-IDF Terms Ads %\nenterprise cloud, data, business, software,\nmarketing93,475 6.7\ntabloid look, photo, star, upbeat,\ncelebrity, celeb, truth90,596 6.5\nhealth fungus, trick, fat, try, cbd, dog,\ndoctor, knee, tinnitus73,240 5.2\npolitics vote, trump, biden, president,\nelection, yes, sure71,240 5.1\nsponsored\nsearchsearch, senior, yahoo, living,\ncar, might, visa70,613 5.0\nentertainment stream, original, music, watch,\nlisten, tv, film50,248 3.6\nshopping\n(goods)boot, shipping, jewelry,\nnewchic, mattress, rug49,457 3.5\nshopping\n(deals/sales)friday, black, deal, sale, cyber,\nreview, monday45,022 3.2\nshopping\n(cars/tech)suv, luxury, phone, common-\nsearch, deal, net, auto44,179 3.2\nloans loan, mortgage, payment, rate,\napr, fix, nml43,629 3.1\nTable 3: Top Topics in the Overall Ad Dataset.\nThe percentages we calculate are the number of ads normalized\nby the total number of ads collected from sites for each level of bias.\nThe number of ads collected from sites in each bias level varies, but\nno group of sites had overwhelmingly more ads. From Left to Right,\nthe number of ads collected per site in each group were: 1,888, 1,950,\n2,618, 2,092, and 2,172, and 1,676 had unknown bias.\nTwo-sample Pearson Chi-squared tests indicate a significant as-\nsociation between the political bias of the site and the percent-\nage of ads that were political, for both mainstream news sites\n(\ud835\udf122(5,\ud835\udc41=1150676)=25393.62,\ud835\udc5d<.0001 ) and misinformation\nsites (\ud835\udf122(5,\ud835\udc41=206559)=8041.43,\ud835\udc5d<.0001 ). Pairwise com-\nparisons using Pearson Chi-squared tests, corrected with Holm\u2019s\nsequential Bonferroni procedure, indicate that all pairs of website\nbiases were significantly different ( \ud835\udc5d<.0001).\nOn mainstream news sites, conservative sites had more political\nads than others; 9% and 10.3% of ads on right-leaning and right\nsites were political, but only 6.9% and 4.4% of ads on left and left-\nleaning sites. On misinformation sites, 26% of ads on left sites were\npolitical, substantially more than right leaning sites. In 4 of the 7\nleft misinformation sites (AlterNet, Daily Kos, Occupy Democrats,\nRaw Story) over 19% of ads were political.\nWe also find that political advertisers tend to target sites match-\ning their political affiliation: Democratic and liberal groups ran\nthe majority of their ads on left-of-center sites, and likewise for\nRepublican and conservative groups on right-of-center sites (Fig. 5).\nIn particular, ads for Democratic political candidates and progres-\nsive nonprofits and causes ran substantially more on 2 of 7 Left\nmisinformation sites (Daily Kos and Occupy Democrats).\nTwo-sample Pearson Chi-squared tests indicate a significant as-\nsociation between the political bias of the site and the number of ads\nbased on the advertiser\u2019s political affiliation, for both mainstream\nnews sites ( \ud835\udf122(25,\ud835\udc41=1,150,676)=22575.49,\ud835\udc5d<.0001 ) and\nmisinformation sites ( \ud835\udf122(20,\ud835\udc41=206,559)=22168.50,\ud835\udc5d<.0001 ).\nPairwise comparisons using Pearson Chi-squared tests, corrected\nEric Zeng, Miranda Wei, Theo Gregersen, Tadayoshi Kohno, Franziska Roesner\nFigure 4: The percentage of ads, out of all ads on those sites,\nthat were political, by sites\u2019 political bias and misinforma-\ntion label. Higher percentages of ads on partisan sites were\npolitical, compared to centrist/uncategorized sites.\nthe Holm-Bonferroni method, indicate that all pairs of website bi-\nases were significantly different ( \ud835\udc5d<.0001 ) except for the (Lean\nLeft, Uncategorized) Misinformation Sites.\nSite Popularity. We found little relationship between site pop-\nularity and the number of political ads on it (Fig. 6). While sites\nhosting many political ads tended to be popular politics sites (e.g.,\ndailykos.com, mediaite.com), some popular sites (e.g., nytimes.com,\ncnn.com) ran <100 political ads. A linear mixed model analysis of\nvariance indicates no statistically significant effect of site rank on\nthe number of political ads ( \ud835\udc39(1,744)=0.805,\ud835\udc5b.\ud835\udc60.).\nAt a high level, we find that political ads are seen more on web-\nsites that are political and partisan in nature. We hypothesize that\nthis is either due to contextual targeting (political groups advertis-\ning to co-partisans), and/or because neutral news websites choose\nto block political advertising on their sites to appear of impartiality.\n4.5 Advertisers of Campaign Ads\nNext, we analyze the advertisers who ran campaign and advocacy\nads: their organization type, their affiliations, and how many they\nran. Fig. 7 shows these ads by organization type and affiliation.\nRegistered Committees. Most campaign ads (12,131, 55.1%) were\npurchased by registered committees (FEC or state PACs). These ads\nwere roughly evenly split between Republican- and Democratic-\naffiliated committees, including official candidate committees, like\nBiden for President, as well as Hybrid PACs and party-affiliated\nSuper PACs, such as the Progressive Turnout Project and the Trump\nMake America Great Again Committee. These also include candi-\ndate committees for other state, local, and federal offices.\nNonprofits. We observed campaign ads from nonpartisan non-\nprofits, e.g., AARP (259 ads, 1.2%), ACLU (256 ads, 1.2%), as well\nas explicitly conservative ones, e.g., Judicial Watch (504 ads, 2.3%),\nPro-Life Alliance (471 ads, 2.1%). Few explicitly liberal nonprofits\nran ads under our categorization system. However, some may con-\nsider self-described nonpartisan organizations as liberal, e.g., issue\norganizations like the ACLU, or voting rights groups like vote.org.\nNews Organizations. Some news organizations ran explicitly po-\nlitical ads to promote candidates or policies \u2014 these were mostly\nconservative-leaning organizations. The top advertisers in this\nFigure 5: The percentage of ads observed on websites from\nadvertisers of different political affiliations, by the political\nbias and misinformation label of the website. Advertisers\ntended to run ads on websites aligned with their politics.\nFigure 6: The total number of political ads observed on each\nsite, by the site\u2019s Tranco rank. Though the largest outliers in\nterms of political ads tend to be popular sites, many popular\nsites show few if any political ads.\ngroup are not well-known, e.g., ConservativeBuzz (1,199 ads, 5.4%),\nUnitedVoice.com (800 ads, 3.6%), and rightwing.org (393 ads, 1.8%).\nConservativeBuzz does not have a website, despite claiming to be a\nPolls, Clickbait, and Commemorative $2 Bills:\nProblematic Political Advertising on News and Media Websites Around the 2020 U.S. Elections\nFigure 7: Campaign and advocacy ads by organization type of the advertiser, color-coded by the political affiliation of the\nadvertiser. Ads from registered committees dominated, roughly evenly divided between Democratic and Republican ads, but\nads from news organizations and nonprofits were more heavily conservative and nonpartisan respectively.\nnews source on their landing page; UnitedVoice and rightwing.org\nare ranked 248,997 and 539,506 on the Tranco Top 1m.\nOther advertisers in this category are more well-known, e.g.,\nDaily Kos, a liberal blog (690 ads, 3.1%, site rank 3,218); Human\nEvents, a conservative newspaper (390 ads, 1.8%, rank 19,311); News-\nmax, a conservative news network (117 ads, 0.5%, rank 2,441).\nUnregistered Groups. Unregistered groups ran a small number\nof ads. The top advertiser was \u201cGone2Shit\u201d, a campaign from the\nmarketing firm MullenLowe, which ran 228 ads for a humorous\nvoter turnout campaign. The U.S. Concealed Carry Association ran\n162 ads. Beyond these top two, a number of \u201castroturfing\u201d groups or\nother industry interest groups ran ads, such as \u201cA Healthy Future\u201d\n(lobbying against price controls on Rx drugs), \u201cClean Fuel Washing-\nton\u201d, and \u201cTexans for Affordable Rx\u201d (a front for the Pharmaceutical\nCare Management Association, based on investigating their web-\nsite). Other top ads came from unregistered, left-leaning groups,\nsuch as \u201cProgress North\u201d and \u201cOpportunity Wisconsin\u201d, which de-\nscribe themselves as grassroots movements. We also saw a small\nnumber of groups consisting of coalitions of registered nonprof-\nits, who collectively fund an ad campaign, such as \u201cNo Surprises:\nPeople Against Unfair Medical Bills\u201d and \u201cvotewith.us\u201d.\nBusinesses and Government Agencies. Some businesses, e.g., Levi\u2019s,\nAbsolut Vodka, ran political ads: mostly nonpartisan ads for voter\nregistration. State/local election boards also ran voter information\nads, e.g. the NYC Board of Elections.\n4.6 Misleading Political Polls\nFocusing now on the content of ads in our campaign and advocacy\ncategory, rather than the advertisers, we highlight the use of polls,\npetitions, and surveys, many of which appear to contain misleading\ncontent, and manipulate users into providing their email addresses.\nThe purpose of many online political petitions and polls are to\nallow political actors to harvest personal details like email addresses,\nso that they can solicit donations, canvas, or advertise to those\npeople in the future [ 66]. This phenomenon is present in our dataset.\nIn a few cases (30 ads), ads we labeled as polls or petitions linked\nto nonpartisan public opinion polling firms such as YouGov and\nCiviqs, but most ads were from political groups, and had landing\npages asking people to provide their email addresses.\nFigure 8: The political affiliation and organization types of\npoll/petition advertisers. These ads were primarily run by\nunaffiliated conservative advertisers, mostly news organiza-\ntions and nonprofits.\nWe observe that poll and petition ads are more common from\npolitically conservative advertisers. In Fig. 8, we visualize the num-\nber of poll ads by the political affiliation of their advertisers. Non-\naffiliated conservative groups (mostly news organizations and non-\nprofits) ran the highest number of poll and petition ads (3,960\nads, 52% of total), followed by Republican party committees (1,389,\n18.2%). Democratic committees ran fewer poll ads than their Re-\npublican counterparts (1,027 ads, 13.5%), while non-partisans and\nnonaffiliated liberals rarely use poll ads (458 ads, 6%; 53 ads, 0.6%).\nPoll ads also made up a greater proportion of ads on right-leaning\nwebsites than other sites: 2.2% on Right and 1.1% on right-leaning\nwebsites were polls and petitions, compared to 1.1% on Left, 0.2%\non left-leaning, and 0.2% on center sites.\nNext, we describe several topics and manipulative tactics used\nby poll ads, which differ across political affiliations.\nDemocratic-Affiliated Groups. Most poll or petition ads from\nDemocratic-affiliated groups were for highly partisan issue-based\npetitions, e.g., \u201cStand with Obama: Demand Congress Pass a Vote-\nby-Mail Option\u201d, \u201cOfficial Petition: Demand Amy Coney Barrett\nResign - Add Your Name\u201d. However, some petitions used even more\ncontrived scenarios, such as posing as a \u201cthank you card\u201d for im-\nportant politicians (Fig. 9a). These ads were run by affiliated PACs\nrather than party or candidate committees, such as the National\nDemocratic Training Committee (290 ads), Progressive Turnout\nProject (282 ads), and Democratic Strategy Institute (215 ads).\nEric Zeng, Miranda Wei, Theo Gregersen, Tadayoshi Kohno, Franziska Roesner\n(a)\n (b)\n (c)\n (d)\nFigure 9: Examples of political ads purporting to be polls, including from: a Democratic-aligned PAC (a), the Trump campaign\n(b), a conservative news organization/email harvesting scheme (c), and a Republican-aligned PAC (d).\nRepublican-Affiliated Groups. The Trump campaign ran 906 ads\nwith positive and neutral polls promoting President Trump and\n479 ads with polls that attacked their opponent (e.g., Fig. 9b). Other\nRepublican committees, such as the NRCC, used the LockerDome ad\nplatform to run generic-looking polls not clearly labeled as political\n(e.g., Fig. 9d). Moreover, Lockerdome was also used by unaffiliated\nadvertisers, e.g., \u201cAll Sears MD\u201d, rawconservativeopinions.com, to\nrun nearly identical-looking ads that were used to sell political\nproducts; this homogenization makes it difficult for users to discern\nthe nature of such ads. We also found 5 Lockerdome ads from the\n\u201cKeep America Great Committee,\u201d whose operators turned out to\nbe using it to commit fraud and pocket donations [50].\nConservative News Organizations. The largest subgroup of adver-\ntisers that used polls were right-leaning news organizations, such\nas such as ConservativeBuzz, UnitedVoice, and rightwing.org. Some\npolls use neutral language, e.g., \u201cWho Won the First Presidential\nDebate?\u201d, while others used more provocative language, e.g., \u201cDo\nIllegal Immigrants Deserve Unemployment Benefits?\u201d (Fig. 9c).\nJournalistic investigations have found that advertisers like Con-\nservativeBuzz purport to be conservative news organizations but\nare actually run by Republican-linked digital marketing firms. Ap-\npearing as news, many of their stories are plagiarized and/or serve\na political agenda. Their misleading poll ads are an entry point for\nharvesting email addresses for their mailing lists. They profit from\nthese mailing lists by sending ads to their subscribers, including\nads from political campaigns [6, 49].\nOur data backs up these findings. We inspected poll ads from\nConservativeBuzz, UnitedVoice, and rightwing.org, who comprise\n55% of poll ads from Right/Conservative advertisers, and 29% of\npoll ads overall. The landing pages of their ads often asked for an\nemail address to submit poll responses (Appendix E). We looked\nup these advertisers in the Archive of Political Emails to see the\ncontent of the emails that they send to subscribers2. We found that\ntheir emails often contained a mix of spam for various products\n(Subject: \u201cThis Toxic Vegetable Is The #1 Danger In Your Diet\u201d),\nbiased or inaccurate political news (Subject: \u201cFauci-Obama-Wuhan\nConnection Exposed in This Bombshell Report\u201d), or a combination\nof the two (Subject: \u201cURGENT \u2013 Think Trump Won? You need to\nsee this...\u201d, selling a Trump mug).\n2https://politicalemails.org/4.7 Political Product Ads\nWe now consider ads in our dataset that used political content to\nsell products, divided into three categories.\n4.7.1 Ads for Memorabilia. We observed 3,186 ads for political\nmemorabilia, including clothing with slogans, collectibles, and nov-\nelty items. These ads were placed by commercial businesses \u2013 none\nwere affiliated with political parties. Our GSDMM model produced\n45 topics for political memorabilia ads; Tab. 4 shows the top seven.\nWe observe that the majority of memorabilia ads are targeted\ntowards conservative consumers. 2,175 advertisements (68.3% of\nmemorabilia ads) contained \u201cDonald\u201d and/or \u201cTrump\u201d. Seven of the\ntop ten topics are directly related to Trump, selling items such as\nspecial edition $2 bills (Fig. 10a), electric lighters, garden gnomes,\nand trading cards.\nSome memorabilia ads targeting conservatives used potentially\nmisleading practices. While some ads clearly advertised themselves\nas products, others disguised the memorabilia as \u201cfree\u201d items, but\nrequires payment to cover shipping and handling. Many ads did\nnot clearly disclose the name of the advertiser. Some straddled the\nline between product ads and clickbait by making claims that the\nproduct \u201cangered Democrats\u201d or would \u201cmelt snowflakes.\u201d We also\nobserved many collectible bills and coins, advertised as \"Legal U.S.\nTender\", by sellers such as Patriot Depot, making dramatic claims\nlike \"Trump Supporters Get a Free $1000 Bill.\"\nWe observed far fewer ads for left-leaning consumers; the first\ntopic containing left-leaning products was the 15th largest at 71\nads. Ads targeting liberals include a pin for \u201cflaming feminists\u201d or\na deck of cards themed around the 2020 Senate Impeachment Trial\nof former President Trump (Fig. 10b).\n4.7.2 Ads Using Political Context To Sell Something Else. We ob-\nserved 1,258 ads that leveraged the political climate for their own\nmarketing. Some of these ads were from legitimate companies,\nsuch as Capitol One advertising their alliance with the Black Eco-\nnomic Alliance to close opportunity gaps, or the Wall Street Jour-\nnal promoting their market insight tools. However, many others\nwere from relatively unknown advertisers peddling get-quick-rich\nschemes, like stocks that would \u201csoar\u201d from Biden winning the\nelection (Fig. 10c) or election-proof security in buying gold.\nOur GSDMM model found 29 topics for ads categorized as non-\npolitical products using political context. Tab. 5 details the largest 7\ntopics. The most prominent political contexts used for these topics\nwere Congress (e.g., legislation related to the product) and the 2020\nPolls, Clickbait, and Commemorative $2 Bills:\nProblematic Political Advertising on News and Media Websites Around the 2020 U.S. Elections\nTopic Weighted c-TF-IDF Terms Ads\nTrump wristbands and\nlightersAmerica, charger, USB, butane,\nrequire, vote, include643\n\u201cfree\u201d Trump flags dems, hate, give, foxworthynews,\naway, claim, flag300\nTrump electric lighters\nand garden decospark, instantly, generate, one,\nclick, open, light, garden253\n$2 bills and \u201ccurrency\u201d legal, tender, authentic, official,\nDonald, USA, make186\nIsrael support pins Israel, request, pin, Jew, fellow-\nship, Christian172\nTrump camo hats,\nbracelets, and coolerscamo, gray, anywhere, discreet,\ngo, sale, way, bracelet156\nTrump coins and bills left, gold, coin, Democrat, upset,\nhat, supporter, value133\nTable 4: Top Topics in Political Memorabilia Ads\nTopic (Context) Weighted c-TF-IDF Terms Ads\nHearing devices (congress\naction)hearing, aidion, slash, price,\nhealth, hear, act, sign, Trump266\nRetirement finance\n(congress action)sucker, punch, law, pension, even,\nrob, retire, IRA205\nInvesting (election-time) former, presidential, Stansberry,\ncongressional, veteran123\nSeniors\u2019 mortgage\n(congress action)amount, reverse, senior, Steve, cal-\nculate, tap, age97\nBanking (racial justice) JPMorgan, Chase, advance, co,\nracial, important, equality66\nPortfolio finance\n(election-time)inauguration, money, Jan, wonder,\noxford, communique63\nDating sites (for\nRepublicans)Republican, single, date, woman,\nwait, profile, view54\nTable 5: Top Topics in Ads About Nonpolitical Products Us-\ning Political Context\nelection. Finance related topics in particular often cited market\nuncertainty around the election, e.g., referencing how a certain out-\ncome might affect stocks and promoting their product as a hedge\nor chance to capitalize. Notably, three of the top four topics tar-\ngeted older audiences: \u201chearing devices,\u201d \u201cretirement finance,\u201d and\n\u201cseniors\u2019 mortgage.\u201d\n4.7.3 Where did political product ads appear? We find that politi-\ncal product ads appeared much more frequently on right-of-center\nwebsites (Fig. 11). This finding aligns with the qualitative content\nthat we observed in these ads \u2014 a large amount of Trump memora-\nbilia, and \u201cscare\u201d headlines about the election outcome. Two-sample\nPearson Chi-Squared tests indicate a statistically significant asso-\nciation between the political bias of the site and the number of\npolitical product ads observed, both for mainstream news sites\n(\ud835\udf122(10,\ud835\udc41=1,150,676)=4871.97,\ud835\udc5d<.0001 ) and misinformation\nsites (\ud835\udf122(8,\ud835\udc41=206,559)=414.75,\ud835\udc5d<.0001 ). Pairwise compar-\nisons using Pearson Chi-squared tests, corrected with the Holm-\nBonferroni method, indicate that all pairs of website biases were\nsignificantly different ( \ud835\udc5d<.0001), except for the following pairs on\nmisinformation sites: (Lean Left, Lean Right), (Lean Left, Left), and\n(Lean Left, Uncategorized).\n(a)\n (b)\n(c)\nFigure 10: Examples of political product ads, including those\nselling memorabilia (a-b) and those using the political con-\ntext to sell something else (c).\nFigure 11: The percentage of ads observed that were for po-\nlitical products, by the political bias of the site. Right sites\nmore frequently hosted ads for political products, both on\nmisinformation and mainstream sites, and both for memo-\nrabilia or nonpolitical products using political contexts.\n4.8 Political News and Media Ads\nWe observed 29,409 ads that were related to political news and me-\ndia content. At 52.0% of all political ads, this was the most populous\ncategory and accounted for more than either of the other two cate-\ngories. Unlike the product ads primarily selling goods or services,\nthese ads advertised information or information-related services.\nWe categorize these news and media ads into two groups: those that\nadvertised specific political news articles, and those that advertised\npolitical outlets, events, or related media. Article ads contained a\nrange of sensationalized, vacuous, or otherwise misleading content,\nespecially with \u201cclickbait-y\u201d language that enticed people to click.\n4.8.1 Sponsored Content / Direct Article Links. Overall, we find\nthat most political news and media ads were sponsored content\nor links to articles (25,103 ads, 85.4%). Some of these ads reported\nsubstantive content, e.g., linking to a review of a documentary:\n\u201c\u2018All In: The Fight for Democracy\u2019 Tackles the Myth of Widespread\nVoter Fraud.\u201d Others were clickbait only using political themes for\nattention, e.g., \u201cTech Guru Makes Massive 2020 Election Prediction. \u201d\nEric Zeng, Miranda Wei, Theo Gregersen, Tadayoshi Kohno, Franziska Roesner\nFigure 12: Number of ads including first and last names of\nthe 2020 presidential and VP candidates.\nMisleading Ads and Headlines. Given that our ads were primarily\nscraped from news and media websites, many appeared as native\nads that blend into the other content, albeit with an inconspicuous\n\u201cSponsored content\u201d or similar label. Further, the headline shown\nin a political article ad did not always align with the actual content\non the clickthrough page. For example, the ad shown in Fig. 13a\nlinks (via a Zergnet aggregation page) to an article3that recounts\nVanessa Trump\u2019s life before marrying Donald Trump Jr., instead\nof after, as the title suggests. Many Zergnet ads with headlines\nimplying controversy were unsubstantiated by the linked article.\nAds Mentioning Top Politicians. Overall, Trump and Biden were\nreferenced in ads much more often than Pence and Harris (Fig. 12).\nWithin political news and media ads, \u201cTrump\u201d is referenced in ads\n2.5x more than \u201cBiden\u201d (11,956 ads vs. 4,691, or 40.7% vs. 16.0%),\neven even after the election. Eight of the top ten ads mentioning\nTrump actually involve his family: e.g., \u201cTrump\u2019s Bizarre Comment\nAbout Son Barron is Turning Heads\u201d (1,377 ads, 4.7%), or \u201cEric\nTrump Deletes Tweet After Savage Reminder About His Father\u201d\n(415 ads, 1.4%). The top 10 ads mentioning Biden imply scandals\nwith his wife, e.g., Fig. 13b (1,267 ads, 4.3%), and his health, e.g., \u201cEx-\nWhite House Physician Makes Bold Claim About Biden\u2019s Health\u201d\n(423 ads, 1.4%).\nLooking at the VP candidates, Pence is referenced in ads fre-\nquently during the run up to the election and immediately follow-\ning the insurrection at the Capital, while a spike in the mentions of\nHarris occurs in late November and early December. Some of the\ntop 10 ads mentioning Pence connect him to high-profile events, in-\ncluding the VP debate (\u201cThe Pence Quote from the VP Debate That\nHas People Talking,\u201d 143 ads, 0.5%) and the U.S. Capitol storming\n(Fig. 13c). Some of the top 10 ads mentioning Harris highlight her\nex (\u201cWhy Kamala Harris\u2019 Ex Doesn\u2019t Think She Should Be Biden\u2019s\nVP,\u201d 246 ads, 0.8%) as well as her gender (\u201cWomen\u2019s Groups Are\nAlready Reacting Strongly to Kamala,\u201d 51 ads, 0.2%).\nFrequent Re-Appearances of Sponsored Content. Out of 25,103\npolitical article ads, we counted only 2,313 unique ads, meaning\nthat many political article ads were shown to our crawler multiple\ntimes. On average, a single (unique) political article ad appeared\nto our crawlers 9.9 times, compared to 9.3 times for campaign\n3https://www.thelist.com/161249/the-stunning-transformation-of-vanessa-trump/\n(a)\n (b)\n (c)\nFigure 13: Political news and media articles. \u00a9Zergnet\nads and 5.1 times for product ads. The frequent re-appearance of\npolitical article ads is likely an artifact of content farms\u2019 practice of\nproducing high quantities of low-quality articles solely for revenue\nfrom clicks [ 12]. 79.4% of all political news articles were run by\nZergnet, which accounted for 19,690 ads and only 1,388 unique\nads. Other top ad platforms for political news articles were Taboola\n(10.0%), Revcontent (5.7%), and Content.ad (1.8%).\n4.8.2 Political Outlets, Programs, Events, and Related Media. A\nsmall portion of political ads, just 4,306 (7%), advertised a polit-\nical news outlet, event, or other media content. This includes ads\nrun by well-known news organizations, e.g., Fox News, The Wall\nStreet Journal, The Washington Post, that advertised their orga-\nnizations at large, as well as highlighting specific events, such as\nCBS\u2019s coverage of the \u201cAssault on the Capitol\u201d (Appendix E), or\nspecial programs about the presidential election. Ads were also run\nby less-well known news organizations advertising themselves or\ntheir events, e.g., The Daily Caller, a right-wing news and opinion\nsite, or advocacy groups and nonprofits, e.g., Faith and Freedom\nCoalition (Appendix E), a conservative 501(c)(4). We also observed\nads about books, podcasts, movies, and more.\n4.8.3 Where did political news and media ads appear? Political\nnews and media ads appeared more often on right-of-center sites,\ncompared to center and left-of-center sites (Fig. 14). Two-sample\nPearson Chi-Squared tests indicate a statistically significant as-\nsociation between the political bias of the site and the number\nof political news and media ads, both for mainstream news sites\n(\ud835\udf122(10,\ud835\udc41=1,150,676)=16729.34,\ud835\udc5d<.0001 ) and misinforma-\ntion sites (\ud835\udf122(8,\ud835\udc41=206,559)=3985.43,\ud835\udc5d<.0001 ). Pairwise\ncomparisons using Pearson Chi-squared tests, corrected with the\nHolm-Bonferroni method, indicate that all pairs of website biases\nwere significantly different ( \ud835\udc5d<.0001 ). Nearly 5% of ads on both\nRight and Lean-Right sites are sponsored content, but only 3.9%,\n2.2%, and 0.8% on Left, Lean Left, and Center sites.\n5 DISCUSSION\n5.1 Concerns About Problematic Political Ads\nOur investigation adds to a growing body of work studying poten-\ntially problematic content in online ads, political and otherwise\n(see Sec. 2). Here, we discuss further the potential harms from the\nproblematic political ads we found.\nManipulative Polls. The most common manipulative pattern we\nobserved in our political ads was the poll-style ad. We view these ads\nPolls, Clickbait, and Commemorative $2 Bills:\nProblematic Political Advertising on News and Media Websites Around the 2020 U.S. Elections\nFigure 14: The number of political news ads observed per\nsite, by the political bias of the site. Right sites more fre-\nquently host political news ads than others.\nas problematic for two reasons. First, they manipulate people into\nclicking on ads by appealing to political motivations with (seem-\ningly) clickable user interface elements. Second, once users click,\nthey often ask users to provide personal information for further ma-\nnipulation, e.g., to put them on manipulative email newsletters [ 52].\nPolitical Clickbait. We observed attention-grabbing news and\nmedia ads that were not official political ads and thus do not ap-\npear in political ad transparency libraries. However, these ads are\nmisleading: they are often designed to looks like real news arti-\ncles, but the political controversies they imply (e.g., \u201cViral Video\nExposes Something Fishy in Biden\u2019s Speeches,\u201d Figs. 13a-13c) are\nnot usually substantiated by the underlying articles. Though we\nbelieve these ads\u2019 goal is to entice clicks for ad revenue, we worry\nthat the provocative political \u201cheadlines\u201d contribute to a climate\nof hyper-partisan political communication and muddy the infor-\nmation ecosystem to which voters are exposed. We argue that this\ntype of political-adjacent advertising requires additional scrutiny\nfrom ad platforms and the public.\nExploitative Product Ads. Most ads aiming to make money through\nthe sales of products and services are legitimate, identifiable as ads,\nand meet expectations of appropriateness [ 97]. However, we iden-\ntified product ads that we would consider exploitative, e.g., that\npromise \u201cfree\u201d products that turn out to not to be. Though such\nads are not unique to political contexts, we observed many that\nleverage political controversy to attract potential buyers.\nMisleading Political Organizations. Online ads (particularly na-\ntive ads) have been criticized for being potentially hard to identify\nas ads, and thus regulated to require disclosure [ 11,24]. We observe\nthat these issues are compounded in a political context, where the\nadvertiser\u2019s identity \u2014 e.g., political leaning, official (or not) political\norganization \u2014 is (or should be) key to a user\u2019s assessment of the ad.\nBeing mistaken for a legitimate, official political organization can\nbenefit problematic advertisers (e.g., exploitative product sellers or\nthe fraudulent \u201cKeep America Great Committee\u201d [50]).\nPartisan Ad Targeting. We observed more political ads, and more\nof the problematic ads that we discussed above, on more partisan\nwebsites, particularly right-leaning sites, as well as on low-quality\nand misinformation sites. Ad targeting in itself is not problematic,\nand naturally, political advertisers would wish to reach people withpartisan alignments most likely to click on a given ad. However,\nwe raise two concerns: first, the continued polarization of U.S. po-\nlitical discourse, reinforced by online ads; second, the risk that\nmore vulnerable people are targeted with more manipulative and\nexploitative political ads.\n5.2 Recommendations and Future Work\nRecommendations for Ad Platforms and Policymakers. Political\nads are already strongly regulated due to its sensitivity. We argue\nthat ad platforms (which make and enforce ad policies) and poli-\ncymakers (e.g., the FTC or FEC) should also consider the potential\nharms from ads not currently violating of existing policies. Many of\nthe problematic ads that we saw were notofficial political ads but\nleveraged political themes and could have political ramifications\n(e.g., spreading misinformation via clickbait headlines). Ad plat-\nforms and regulators should consider these ads alongside official\npolitical ads in transparency and regulation efforts.\nIt is worth noting that there were types of problematic political\nads that we did notobserve. In a preliminary qualitative analysis,\nwe did not find ads providing false voter information, e.g., incor-\nrect election dates, polling places, or voting methods. While that\ndoes not mean they did not exist, it nevertheless suggests that ad\nplatforms are regulating the most egregiously harmful ads.\nThe extreme decentralization of the online ad ecosystem poses\nadditional challenges for ad moderation. Though Google period-\nically banned political ads during our data collection, we contin-\nued to see political ads, including problematic political ads, placed\nby other ad platforms. Thus, we call for more comprehensive ad\nmoderation standards (and perhaps regulation) across advertising\nplatforms \u2014 while recognizing the complex financial and political in-\ncentives that may hamper the clear-cut adoption of regulation [ 34].\nFuture Research. Future research should continue to audit ad\ncontent and targeting. While our study has focused on web ads\nappearing on news and media websites, the online ad ecosystem is\nlarge and requires analysis with different data collection and anal-\nysis methods. Future work should (continue to) consider political\nand other ads across various platforms \u2014 social media, mobile web\nand apps \u2014 and sites. Moreover, we focused on U.S. political ads,\nbut future research should also critically study the role of online\nads in non-U.S. political contexts or around other historical events.\nFuture work should also directly study people who view these\nads, to better understand the actual impact of potentially problem-\natic ads and for different user populations.\nTo enable other researchers to further analyze our collected\nads, our dataset and codebook are available at: https://badads.cs.\nwashington.edu/political.\n6 CONCLUSION\nWe collected ads from 745 news and media sites around the time\nof the 2020 U.S. elections, including 55,943 political ads, which we\nanalyzed using quantitative and qualitative methods. We identified\nthe use of manipulative techniques and misleading content in both\nofficial and non-official political-themed ads, and we highlight the\nneed for greater scrutiny by ad platforms and regulators, as well as\nfurther external study and auditing of the online ad ecosystem.\nEric Zeng, Miranda Wei, Theo Gregersen, Tadayoshi Kohno, Franziska Roesner\nACKNOWLEDGEMENTS\nWe thank our shepherd, Zakir Durumeric, as well as our anonymous\nreviewers, for helping improve this paper. We thank Kentrell Owens\nand Sudheesh Singanamalla for providing feedback on an earlier\ndraft. This work was supported in part by the National Science\nFoundation under grant CNS-2041894, and by the UW Center for\nan Informed Public and the John S. and James L. Knight Foundation.\nREFERENCES\n[1]Albalawi, R., Yeap, T. H., and Benyoucef, M. Using Topic Modeling Methods\nfor Short-Text Data: A Comparative Analysis. Frontiers in Artificial Intelligence\n(2020).\n[2]Ali, M., Sapiezynski, P., Bogen, M., Korolova, A., Mislove, A., and Rieke, A.\nDiscrimination through Optimization: How Facebook\u2019s Ad Delivery Can Lead to\nBiased Outcomes. Proc. ACM Hum.-Comput. Interact. 3 , CSCW (Nov. 2019).\n[3]AllSides . Balanced news via media bias ratings for an unbiased news perspective.\nhttps://www.allsides.com/unbiased-balanced-news.\n[4]Amazon . Alexa Web Information Service API. https://awis.alexa.com/.\n[5]Arthur, D., and Vassilvitskii, S. k-means++: The Advantages of Careful\nSeeding. In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on\nDiscrete Algorithms (USA, 2007), SODA \u201907, Society for Industrial and Applied\nMathematics, p. 1027\u20131035.\n[6]Baker, S. How GOP-linked PR firms use Google\u2019s ad platform to harvest email\naddresses. Engadget, November 2019. https://www.engadget.com/2019-11-11-\ngoogle-political-ads-polls-email-collection.html.\n[7]Ballard, A. O., Hillygus, D. S., and Konitzer, T. Campaigning Online: Web\nDisplay Ads in the 2012 Presidential Campaign. PS: Political Science & Politics 49 ,\n3 (2016), 414\u2013419.\n[8]Ballotpedia . Presidential election, 2020. https://ballotpedia.org/Presidential_\nelection,_2020.\n[9]Bashir, M. A., Arshad, S., Robertson, W., and Wilson, C. Tracing Informa-\ntion Flows between Ad Exchanges Using Retargeted Ads. In Proceedings of the\n25th USENIX Conference on Security Symposium (USA, 2016), SEC\u201916, USENIX\nAssociation, p. 481\u2013496.\n[10] Blei, D. M., Ng, A. Y., and Jordan, M. I. Latent Dirichlet Allocation. Journal of\nMachine Learning Research 3 (2003), 993\u20131022.\n[11] Campbell, C., and Grimm, P. E. The Challenges Native Advertising Poses: Ex-\nploring Potential Federal Trade Commission Responses and Identifying Research\nNeeds. Journal of Public Policy & Marketing 38 , 1 (2019), 110\u2013123.\n[12] Carr, A. Low quality websites: Content farms: What is a content farm?, 2021.\nhttps://researchguides.austincc.edu/contentfarms.\n[13] Cillizza, C. Why the delayed election results prove the system is actually\nworking, Nov. 2020. https://www.cnn.com/2020/11/04/politics/donald-trump-\njoe-biden-2020-election-results/index.html.\n[14] Crain, M., and Nadler, A. Political Manipulation and Internet Advertising\nInfrastructure. Journal of Information Policy 9 (2019), 370\u2013410.\n[15] Crovitz, L. G. How Amazon, Geico and Walmart fund propaganda. The New\nYork Times, Jan. 2020. https://www.nytimes.com/2020/01/21/opinion/fake-news-\nrussia-ads.html.\n[16] Curiskis, S. A., Drake, B., Osborn, T. R., and Kennedy, P. J. An evaluation of\ndocument clustering and topic modelling in two online social networks: Twitter\nand Reddit. Information Processing & Management 57 , 2 (2020), 102034.\n[17] Dorsey, J. We\u2019ve made the decision to stop all political advertising on Twitter\nglobally, 2019.\n[18] Dykhne, I. Persuasive or Deceptive - Native Advertising in Political Campaigns.\nSouthern California Law Review 91 (2018), 339.\n[19] Easylist Filter List Project . Easylist. https://easylist.to.\n[20] Edelson, L., Lauinger, T., and McCoy, D. A Security Analysis of the Facebook\nAd Library. In IEEE Symposium on Security and Privacy (Oakland) (2020).\n[21] Edelson, L., Sakhuja, S., Dey, R., and McCoy, D. An Analysis of United States\nOnline Political Advertising Transparency. arXiv:1902.04385, Feb. 2019.\n[22] Evelyn, K. Capitol attack: the five people who died, 2021. https:\n//www.theguardian.com/us-news/2021/jan/08/capitol-attack-police-officer-\nfive-deaths.\n[23] FactCheck.org . Misinformation directory. https://www.factcheck.org/2017/07/\nwebsites-post-fake-satirical-stories/.\n[24] Federal Trade Commission . .com Disclosures: How to Make Effective\nDisclosures in Digital Advertising, Mar. 2013. https://www.ftc.gov/system/files/\ndocuments/plain-language/bus41-dot-com-disclosures-information-about-\nonline-advertising.pdf.\n[25] Fischer, S. Scoop: Google to block election ads after election day,\n2021. https://www.axios.com/google-to-block-election-ads-after-election-day-\n4b60650d-b5c2-4fb4-a856-70e30e19af17.html.[26] Fischer, S. Scoop: Google to lift political ad ban put in place following capi-\ntol siege, 2021. https://www.axios.com/capitol-siege-google-political-ad-ban-\n5000245d-35d6-4448-b7b2-daa7ccfe816a.html.\n[27] Global Disinformation Index . The Quarter Billion Dollar Question: How is\nDisinformation Gaming Ad Tech?, Sept. 2019. https://disinformationindex.org/\nwp-content/uploads/2019/09/GDI_Ad-tech_Report_Screen_AW16.pdf.\n[28] Google . Puppeteer. https://developers.google.com/web/tools/puppeteer/.\n[29] Google . Adwords API - Verticals. Google Developers, 2021. https://developers.\ngoogle.com/adwords/api/docs/appendix/verticals.\n[30] Google . Political advertising in the United States. https://transparencyreport.\ngoogle.com/political-ads/region/US, 2021. Google Transparency Report.\n[31] Graham, M. Pinterest says it will no longer allow ads on elections-related\ncontent, employees get time off to vote, 2020.\n[32] Grootendorst, M. BERTopic: Leveraging BERT and c-TF-IDF to create easily\ninterpretable topics, 2020.\n[33] Grootendorst, M. Creating a class-based TF-IDF with Scikit-Learn.\nhttps://towardsdatascience.com/creating-a-class-based-tf-idf-with-scikit-\nlearn-caea7b15b858, October 2020. Towards Data Science.\n[34] Haenschen, K., and Wolf, J. Disclaiming responsibility: How platforms dead-\nlocked the Federal Election Commission\u2019s efforts to regulate digital political\nadvertising. Telecommunications Policy 43 , 8 (2019), 101824.\n[35] Hao, S., Chen, L., Zhou, S., Liu, W., and Zheng, Y. Multi-layer multi-view topic\nmodel for classifying advertising video. Pattern Recognition 68 (2017), 66\u201381.\n[36] Herbert, C. The Fake News Codex. http://www.fakenewscodex.com, December\n2018.\n[37] Hoffman, M., Bach, F., and Blei, D. Online Learning for Latent Dirichlet\nAllocation. In Advances in Neural Information Processing Systems (2010), J. Lafferty,\nC. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, Eds., vol. 23, Curran\nAssociates, Inc.\n[38] Hubert, L., and Arabie, P. Comparing partitions. Journal of Classification 2\n(dec 1985), 193\u2013218.\n[39] Irvine, M. Google ads benchmarks for your industry. https://www.wordstream.\ncom/blog/ws/2016/02/29/google-adwords-industry-benchmarks, 2021. Word-\nstream: Online Advertising Made Easy.\n[40] Joshua Gillin . The more outrageous, the better: How clickbait ads make money\nfor fake news sites. https://www.politifact.com/punditfact/article/2017/oct/04/\nmore-outrageous-better-how-clickbait-ads-make-mone/, October 2017.\n[41] Kim, Y. M., Hsu, J., Neiman, D., Kou, C., Bankston, L., Kim, S. Y., Heinrich, R.,\nBaragwanath, R., and Raskutti, G. The Stealth Media? Groups and Targets\nbehind Divisive Issue Campaigns on Facebook. Political Communication 35 , 4\n(2018), 515\u2013541.\n[42] Kim LaCapria . Snopes\u2019 field guide to fake news sites and hoax purveyors. Snopes,\nJanuary 2016. https://www.snopes.com/news/2016/01/14/fake-news-sites/.\n[43] Kingsley, S., Wang, C., Mikhalenko, A., Sinha, P., and Kulkarni, C. Auditing\nDigital Platforms for Discrimination in Economic Opportunity Advertising. ArXiv\n2008.09656 (2020).\n[44] Le Pochat, V., Van Goethem, T., Tajalizadehkhoob, S., Korczy\u0144ski, M., and\nJoosen, W. Tranco: A Research-Oriented Top Sites Ranking Hardened Against\nManipulation. In 26th Network and Distributed System Security Symposium (NDSS)\n(2019).\n[45] L\u00e9cuyer, M., Spahn, R., Spiliopolous, Y., Chaintreau, A., Geambasu, R., and\nHsu, D. Sunlight: Fine-grained Targeting Detection at Scale with Statistical\nConfidence. In ACM Conference on Computer and Communications Security (CCS)\n(2015).\n[46] Lemire, J., Miller, Z., and Weissert, W. Biden defeats Trump for White House,\nsays \u2018time to heal\u2019, 2021. https://apnews.com/article/joe-biden-wins-white-\nhouse-ap-fd58df73aa677acb74fce2a69adb71f9.\n[47] Li, Z., Zhang, K., Xie, Y., Yu, F., and Wang, X. Knowing Your Enemy: Un-\nderstanding and Detecting Malicious Web Advertising. In ACM Conference on\nComputer and Communications Security (CCS) (2012).\n[48] Looper, E., and Bird, S. NLTK: The Natural Language Toolkit. ArXiv\nhttps://arxiv.org/abs/cs/0205028 (2002).\n[49] Markay, L. Pro-trump \u2018news sites\u2019 are harvesting your emails to sell to the cam-\npaign. https://www.thedailybeast.com/pro-trump-news-sites-are-harvesting-\nyour-emails-to-sell-to-the-campaign, August 2020. The Daily Beast.\n[50] Markay, L. Pro-Trump scam PAC operator hit with wire fraud charge. Axios,\nApril 2021. https://www.axios.com/pro-trump-scam-pac-operator-wire-fraud-\ncharge-d888876f-680c-4b47-83d8-d72da709ce8b.html.\n[51] Mathur, A., Narayanan, A., and Chetty, M. Endorsements on Social Media:\nAn Empirical Study of Affiliate Marketing Disclosures on YouTube and Pinterest.\nProceedings of the ACM on Human-Computer Interaction (CSCW) 2 (Nov. 2018).\n[52] Mathur, A., Wang, A., Schwemmer, C., Hamin, M., Stewart, B. M., and\nNarayanan, A. Manipulative tactics are the norm in political emails: Evidence\nfrom 100K emails from the 2020 U.S. election cycle. https://electionemails2020.org,\n2020.\n[53] McHugh, M. L. Interrater reliability: the kappa statistic. Biochem Med (Zagreb)\n22, 3 (October 2012), 276\u201382.\nPolls, Clickbait, and Commemorative $2 Bills:\nProblematic Political Advertising on News and Media Websites Around the 2020 U.S. Elections\n[54] Media Bias/Fact Check Team . Media Bias/Fact Check: The Most Comprehen-\nsive Media Bias Resource. https://mediabiasfactcheck.com/fake-news/.\n[55] Misra, S. Deceptive political advertising: Some new dimensions. Journal of Legal,\nEthical and Regulatory Issues 18 (2015), 71\u201380.\n[56] Muller, M. Ways of Knowing . Springer, 2014, ch. Curiosity, Creativity, and\nSurprise as Analytic Tools: Grounded Theory Method.\n[57] Narayanan, A., Mathur, A., Chetty, M., and Kshirsagar, M. Dark patterns:\nPast, present, and future. Communications of the ACM 63 , 9 (Aug. 2020), 42\u201347.\n[58] Nelms, T., Perdisci, R., Antonakakis, M., and Ahamad, M. Towards Measuring\nand Mitigating Social Engineering Software Download Attacks. In Proceedings of\nthe 25th USENIX Conference on Security Symposium (USA, 2016), SEC\u201916, USENIX\nAssociation, p. 773\u2013789.\n[59] Nikiforakis, N., Kapravelos, A., Joosen, W., Kr\u00fcgel, C., Piessens, F., and\nVigna, G. Cookieless Monster: Exploring the Ecosystem of Web-Based Device\nFingerprinting. IEEE Symposium on Security and Privacy (2013), 541\u2013555.\n[60] Ohlheiser, A. This is how Facebook\u2019s fake-news writers make money. Wash-\nington Post, Nov. 2016. https://www.washingtonpost.com/news/the-intersect/\nwp/2016/11/18/this-is-how-the-internets-fake-news-writers-make-money/.\n[61] OpenSources Contributors . Opensources. https://github.com/\nOpenSourcesGroup/opensources, April 2017.\n[62] Pedregosa, F., Varoqaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,\nO., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J.,\nPassos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.\nScikit-learn: Machine learning in Python. Journal of Machine Learning Research\n12(2011), 2825\u20132830.\n[63] Provos, N., Mavrommatis, P., Rajab, M. A., and Monrose, F. All your iframes\npoint to us. In Proceedings of the 17th Conference on Security Symposium (USA,\n2008), SS\u201908, USENIX Association, p. 1\u201315.\n[64] Qi, P., Zhang, Y., Zhang, Y., Bolton, J., and Manning, C. D. Stanza: A Python\nNatural Language Processing Toolkit for Many Human Languages. Association\nfor Computational Linguistics System Demonstrations (2020).\n[65] Qiang, J., Qian, Z., Li, Y., Yuan, Y., and Wu, X. Short Text Topic\nModeling Techniques, Applications and Performance: A Survey. ArXiv\nhttps://arxiv.org/abs/1904.07695 (2019).\n[66] Rakoczy, C. The sneaky reason you should never sign petitions or answer\nsurveys online. Mic.com, May 2017. https://www.mic.com/articles/175333/want-\nto-save-money-heres-the-surprising-reason-why-you-should-never-sign-an-\nonline-petition.\n[67] Rastogi, V., Shao, R., Chen, Y., Pan, X., Zou, S., and Riley, R. Are these Ads Safe:\nDetecting Hidden Attacks through the Mobile App-Web Interfaces. In Network\nand Distributed System Security Symposium (NDSS) (2016).\n[68] \u0158eh\u016f\u0159ek, R., and Sojka, P. Software Framework for Topic Modelling with\nLarge Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for\nNLP Frameworks (Valletta, Malta, May 2010), ELRA, pp. 45\u201350. http://is.muni.cz/\npublication/884893/en.\n[69] Ridout, T. N., Fowler, E. F., and Franz, M. M. Spending Fast and Furious:\nPolitical Advertising in 2020. The Forum 18 , 4 (2021). https://www.degruyter.\ncom/document/doi/10.1515/for-2020-2109/html.\n[70] R\u00f6der, M., Both, A., and Hinneburg, A. Exploring the Space of Topic Coherence\nMeasures. In Proceedings of the Eighth ACM International Conference on Web\nSearch and Data Mining (New York, NY, USA, 2015), WSDM \u201915, Association for\nComputing Machinery, p. 399\u2013408.\n[71] Roesner, F., Kohno, T., and Wetherall, D. Detecting and Defending Against\nThird-Party Tracking on the Web. In USENIX Symposium on Networked Systems\nDesign and Implementation (NSDI) (2012).\n[72] Romano, S., Vinh, N. X., Bailey, J., and Verspoor, K. Adjusting for Chance\nClustering Comparison Measures, 2015.\n[73] Rosenberg, A., and Hirschberg, J. V-measure: A conditional entropy-based\nexternal cluster evaluation measure. In Proceedings of the 2007 Joint Conference\non Empirical Methods in Natural Language Processing and Computational Nat-\nural Language Learning (EMNLP-CoNLL) (Prague, Czech Republic, June 2007),\nAssociation for Computational Linguistics, pp. 410\u2013420.\n[74] Saldana, J. The Coding Manual for Qualitative Researchers . SAGE Publications,\n2012.\n[75] Sanchez-Rola, I., Dell\u2019Amico, M., Balzarotti, D., Vervier, P.-A., and Bilge,\nL.Journey to the Center of the Cookie Ecosystem: Unraveling Actors\u2019 Roles and\nRelationships. In IEEE Symposium on Security and Privacy (2021).\n[76] Sanh, V., Debut, L., Chaumond, J., and Wolf, T. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter. ArXiv https://arxiv.org/abs/1910.01108v4\n(2020).\n[77] Schmidt, M. S., and Broadwater, L. Officers\u2019 Injuries, Including Concussions,\nShow Scope of Violence at Capitol Riot, 2021. https://www.nytimes.com/2021/\n02/11/us/politics/capitol-riot-police-officer-injuries.html.\n[78] Schneider, E. Google will end political ad ban this week, 2021. https://www.\npolitico.com/news/2020/12/09/google-end-political-ad-ban-443882.\n[79] Silverman, C., and Alexander, L. How Teens In The Balkans\nAre Duping Trump Supporters With Fake News, Nov. 2016. https://www.buzzfeednews.com/article/craigsilverman/how-macedonia-became-a-\nglobal-hub-for-pro-trump-misinfo#.tcR3aZa5r.\n[80] Smith, A., and Banic, V. Fake News: How a Partying Macedo-\nnian Teen Earns Thousands Publishing Lies. NBC News, Decem-\nber 2016. https://www.nbcnews.com/news/world/fake-news-how-partying-\nmacedonian-teen-earns-thousands-publishing-lies-n692451.\n[81] Sosnovik, V., and Goga, O. Understanding the Complexity of Detecting Political\nAds. In The Web Conference (WWW) (2021).\n[82] Spring, M. \u2019Stop the steal\u2019: The deep roots of Trump\u2019s \u2019voter fraud\u2019 strategy,\n2021. https://www.bbc.com/news/blogs-trending-55009950.\n[83] staff, P. Politifact\u2019s guide to fake news websites and what they peddle. Politi-\nfact, April 2017. https://www.politifact.com/article/2017/apr/20/politifacts-guide-\nfake-news-websites-and-what-they/.\n[84] Steenburg, E. V. Areas of research in political advertising: a review and research\nagenda. International Journal of Advertising 34 , 2 (2015), 195\u2013231.\n[85] Stromer-Galley, J., Hemsley, J., Rossini, P., McKernan, B., McCracken, N.,\nBolden, S., Korsunska, A., Gupta, S., Kachhadia, J., Zhang, W., and Zhang,\nF.The Illuminating Project: Helping Journalists Cover Social Media in the\nPresidential Campaign, 2020. https://illuminating.ischool.syr.edu/.\n[86] Surian, D., Nguyen, D. Q., Kennedy, G., Johnson, M., Coiera, E., and Dunn,\nA. G. Characterizing Twitter Discussions About HPV Vaccines Using Topic\nModeling and Community Detection\". J Med Internet Res 18 , 8 (Aug 2016), e232.\n[87] Team, A. Google Display Ads CPM, CPC, & CTR Benchmarks in Q1 2018. https:\n//blog.adstage.io/google-display-ads-cpm-cpc-ctr-benchmarks-in-q1-2018, 2018.\nAdStage.\n[88] Team, B. V. J. Capitol riots: A visual guide to the storming of Congress, 2021.\nhttps://www.bbc.com/news/world-us-canada-55575260.\n[89] Trish, B. From recording videos in a closet to Zoom meditating, 2020\u2019s political\ncampaigns adjust to the pandemic. https://theconversation.com/from-recording-\nvideos-in-a-closet-to-zoom-meditating-2020s-political-campaigns-adjust-to-\nthe-pandemic-145788, 2020. The Conversation.\n[90] Venkatadri, G., Andreou, A., Liu, Y., Mislove, A., Gummadi, K. P., Loiseau, P.,\nand Goga, O. Privacy Risks with Facebook\u2019s PII-Based Targeting: Auditing a\nData Broker\u2019s Advertising Interface. In IEEE Symposium on Security and Privacy\n(2018).\n[91] Vinh, N. X., Epps, J., and Bailey, J. Information Theoretic Measures for Cluster-\nings Comparison: Variants, Properties, Normalization and Correction for Chance.\nJournal of Machine Learning Research 11 , 95 (2010), 2837\u20132854.\n[92] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,\nRault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P.,\nMa, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q.,\nand Rush, A. M. HuggingFace\u2019s Transformers: State-of-the-art Natural Language\nProcessing, 2020.\n[93] Xing, X., Meng, W., Lee, B., Weinsberg, U., Sheth, A., Perdisci, R., and Lee, W.\nUnderstanding Malvertising Through Ad-Injecting Browser Extensions. In 24th\nInternational Conference on World Wide Web (WWW) (2015).\n[94] Yin, J., and Wang, J. A Dirichlet Multinomial Mixture Model-Based Approach\nfor Short Text Clustering. In Proceedings of the 20th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining (New York, NY, USA, 2014),\nKDD \u201914, Association for Computing Machinery, p. 233\u2013242.\n[95] Zarras, A., Kapravelos, A., Stringhini, G., Holz, T., Kruegel, C., and Vigna,\nG.The dark alleys of Madison Avenue: Understanding malicious advertisements.\nInACM Internet Measurement Conference (2014).\n[96] Zeng, E., Kohno, T., and Roesner, F. Bad News: Clickbait and Deceptive Ads on\nNews and Misinformation Websites. In Workshop on Technology and Consumer\nProtection (2020).\n[97] Zeng, E., Kohno, T., and Roesner, F. What Makes a \u201cBad\u201d Ad? User Perceptions\nof Problematic Online Advertising. In Proceedings of the 2021 CHI Conference\non Human Factors in Computing Systems (New York, NY, USA, 2021), CHI \u201921,\nAssociation for Computing Machinery.\nA HISTORICAL BACKGROUND\nElection day was November 3, 2020, but the results of the election\nwere significantly delayed due to the COVID-19 pandemic as states\ncontinued to receive mail-in votes and count ballots in subsequent\ndays [ 13]. During this time, Trump and his campaign maintained\nthat there was widespread voter fraud [ 82]. Most major news outlets\ndeclared the results \u2014 that Biden had obtained enough electoral\nvotes to defeat Trump \u2014 on November 7 [ 46]. Sparked by a speech\nfrom Donald Trump on January 6, 2021 in which he continued\nto falsely claim that he had won the election, thousands of his\nsupporters marched to the U.S. Capitol complex, where Congress\nEric Zeng, Miranda Wei, Theo Gregersen, Tadayoshi Kohno, Franziska Roesner\nhad assembled to certify the electoral result [ 88]. The storming\nof the Capitol resulted in over 140 injuries [ 77] and 5 deaths [ 22].\nThe certification was completed the next day and President Biden\u2019s\ninauguration was held on January 20, 2021.\nOn November 3, elections were also held for seats in the Senate\nand House of Representatives. In state and local politics, elections\nwere held for 13 governorships in 11 states and 2 territories, as well\nas for state legislative chambers, attorneys generals, state supreme\ncourt seats, and various referendums and ballot measures. In the\nstate of Georgia, no Senate candidates received a majority of the\nvote during the first round, leading to a run-off election on January\n5, 2021.\nB TEXT CLUSTERING EXPERIMENTS\nTo qualitatively categorize the overall dataset, we used topic mod-\neling and text clustering algorithms to group ads with similar con-\ntent, and then created qualitative descriptions for each grouping\nvia term frequency evaluation and manual labeling. The short-\ncontent, low-context nature of many of ads in the dataset most\nclosely aligns with short-text topic modeling problems [ 1,16,65],\nhowever prior work on topic modeling advertisement text specif-\nically is minimal [ 35]. As such, we pursued several diverse ap-\nproaches to the NLP pipeline. For tokenization and lemmatization,\nwe experimented with three pre-processing models: NLTK [ 48],\nStanford NLP Group\u2019s Stanza [ 64], and DistilBERT [ 76]. Our pre-\nprocessing filtered on NLTK\u2019s english stopword corpus4along\nwith several OCR artifacts such as \"sponsoredsponsored.\" For topic\ngeneration, we experimented with several models and techniques:\nLatent Dirichlet Allocation (LDA) [ 10,37], Gibbs-Sampling Dirichlet\nMixture Model (GSDMM)5[94], DistilBERT + K-means Cluster-\ning [ 5,76], and BERTopic [ 32]. We tested two implementations of\nLDA: Scikit-learn [ 62] and Gensim [ 68] with both Stanza and NLTK\npre-processing. The selection of LDA parameter values to evaluate\nwas based on results from Hoffman et al. [ 37]. The GSDMM model\nwas tested on parameter values following suggestions from Yin and\nWang [ 94] for both Stanza and GSDMM pre-processing as well. The\nDistilBERT model and DistilBERT pre-processing, implemented via\nHuggingface [ 92], were used to generate feature vectors for use\nby K-means clustering via Sklearn [ 62], which was tested on topic\ncount. Lastly, BERTopic was tested on topic count as well.\nTo establish an approximate baseline for topic cardinality tun-\ning and evaluation on the full deduplicated dataset, we manually\nlabeled 2,583 unique randomly-sampled advertisements from the\ndataset (1.52% of the deduplicated ads), using a list of verticals that\nGoogle Adwords provides to publishers for targeting purposes [ 29]\n(e.g. \u201c/Shopping\u201d, \u201c/Shopping/Apparel\u201d, \u201c/Shopping/Apparel/Men\u2019s\nClothing\u201d). For each ad we used the most descriptive label, but later\ncollapsed the hierarchies to the second level to form larger groups.\nThis process produced 171 unique label groups in the sample, which\nserved as reference for topic count selection and as test data for\nevaluation. After generating topics for the full deduplicated dataset,\nthe subset of ads corresponding to those labeled manually were iso-\nlated for similarity evaluation, assuming that a good model would\nroughly place ads in the same product sector in the same group.\n4https://www.nltk.org/book/ch02.html\n5https://github.com/rwalk/gsdmmModel ARI AMI H C \ud835\udc36\ud835\udc63\nBERT+K-means 0.0119 0.0337 0.3243 0.3119 0.5333\nBERTopic 0.0109 0.1411 0.3424 0.4524 0.5590\nLDA 0.2616 0.2306 0.5343 0.4696 0.4198\nGSDMM 0.4743 0.4438 0.5297 0.6328 0.5457\nTable 6: Best Performance by Model on Full Deduplicated\nDataset\nModel Preprocessor \ud835\udefc \ud835\udefd K n_iters\nFull Dedupli-\ncated DatasetStanza 0.1 0.05 180 40\nPolitical Memo-\nrabiliaNLTK 0.1 0.1 75 40\nNonpolitical\nProducts Using\nPolitical TopicsNLTK 0.1 0.1 30 40\nTable 7: Selected GSDMM Model Parameters by Data Subset\nModel Topics\nFull Deduplicated Dataset 180\nPolitical Memorabilia 45\nNonpolitical Products Using Political Topics 29\nTable 8: Selected GSDMM Model Topic Count by Data Subset\nTo evaluate similarity to our training clusters we used Adjusted\nRand Index ( \ud835\udc34\ud835\udc45\ud835\udc3c) [38] and Adjusted Mutual Index ( \ud835\udc34\ud835\udc40\ud835\udc3c ) [91] met-\nrics implemented via Scikit-learn, accounting for possible imbal-\nanced or balanced cluster sizes [ 72]. For evaluating intra-topic\nsimilarity, we measured Homogeneity ( \ud835\udc3b) and for inter-topic sim-\nilarity, we measured Completeness ( \ud835\udc36) [73], both via Scikit-learn.\nAs a general measure of topic quality, we recorded \ud835\udc36\ud835\udc63coherence\nvia Gensim, based on R\u00f6der et al [70].\nTable 6 details the best performances by model during tuning and\ntesting. GSDMM performed the best (likely because it is designed\nspecifically for short text documents), with an \ud835\udc34\ud835\udc45\ud835\udc3c=0.4743,\ud835\udc34\ud835\udc40\ud835\udc3c=\n0.4438 ,\ud835\udc3b=0.5297 ,\ud835\udc36=0.6328 , and\ud835\udc36\ud835\udc63Coherence =0.5457 , and\nthus was selected. These values are comparable to other GSDMM\nresults on Twitter data [ 16,65,86]. We ran the model on the top\nparameters 8 more times and selected the best iteration for use in\nour final results. The final GSDMM model produced 180 clusters\non the full deduplicated dataset.\nLabels for topics were designated after reviewing random sam-\nples of ads from within the topic and incorporating term results\nfrom c-TF-IDF, which utilizes a modified term frequency - inverse\ndocument frequency (TF-IDF) algorithm to select important terms\nfrom a given topic cluster [33].\nBased on the performance of GSDMM on the overall dataset,\nwe further used GSDMM for topic modeling on the political ad\nsubsets of \"political memorabilia\" and \"nonpolitical products using\npolitical topics. \" To evaluate performance in the absence of a ground\ntruth, we measured \ud835\udc36\ud835\udc63coherence. For both subsets, we tuned pa-\nrameters of topic count, alpha, and beta. After identifying the best\nPolls, Clickbait, and Commemorative $2 Bills:\nProblematic Political Advertising on News and Media Websites Around the 2020 U.S. Elections\nperforming parameters, we ran the models 10 additional times each\nbefore selecting the best iteration. The top \"political memorabilia\"\nmodel achieved a \ud835\udc36\ud835\udc63coherence of 0.7109 with 45 topics, and the\ntop \"nonpolitical products using political topics\" model achieved\na\ud835\udc36\ud835\udc63coherence of 0.6777 with 29 topics. As before, we manually\nlabeled the largest topics after reviewing random samples of ads\nfrom within the topic. However, due to the smaller topic sizes in\nthe political subsets as compared to the full dataset, we weighted\nads by their duplicate counts when generating c-TF-IDF results (e.g.\nan ad with 10 duplicates would have its text weighted 10x).\nTable 7 contains the GSDMM parameters used in our selected\nGSDMM models by dataset subset, and table 8 details the topic\ncount by the end of each model\u2019s runtime. For all three models,\ntopic labels were scaled up from the deduplicated subsets to the\nfull dataset.\nC QUALITATIVE CODEBOOK\nC.1 Methodology\nWe generated a qualitative codebook using grounded theory [ 56],\nan approach for generating themes categories via observation of the\nground-level data. First, three researchers conducted a preliminary\nanalysis of around 100 political ads each, creating open codes de-\nscribing the characteristics of ads. We met to discuss and organized\nthem into axial codes (i.e., multiple choice categories for different\nconcepts) that best addressed our research questions.\nUsing these codes, three researchers coded the 8,836 ads, meeting\nmultiple times during the process to iteratively refine the codebook\nbased on new data. To assess the consistency of the coding, all\ncoders coded a random subset of 200 ads, and we calculated Fleiss\u2019\n\ud835\udf05(a statistical measure of intercoder agreement, \ud835\udf05=0indicates\nzero,\ud835\udf05=1.0indicates perfect) on this subset. We achieved an\naverage\ud835\udf05=0.771across our 10 categories ( \ud835\udf0e=0.09), indicating\nmoderate-strong agreement [53].\nSupplementing our qualitative codes, one researcher also labeled\neach campaign-related ad with the advertisers\u2019 name and legal\nclassification (e.g., 501(c)(4) nonprofit), using information such as\nthe \u201cpaid for\u201d box in the ad, or the organization\u2019s website.\nC.2 Codebook Contents\nOur codeboook included three mutually exclusive high-level themes:\n(1) campaigns and advocacy ads ,(2) political product ads , and\n(3) political news and media ads . To account for technical er-\nrors in crawling and classification, ads were classified as Mal-\nformed/not political if the extracted text and/or image content\nwas incomplete or non-political, e.g., if screenshots failed to capture\nthe whole ad, pop-ups or other material covered the ad, multiple\nads were captured, incorrect model classification.\nC.3 Campaigns and Advocacy Ads\nWe define campaign and advocacy ads as those that explicitly ad-\ndressed or promoted a political candidate, election, policy, or call to\naction. Within this category, we further define the level of election,\nthe purpose of the ad, and advertiser-related information.\nC.3.1 Level of Election. Election level refers to candidate\u2019s juris-\ndiction, e.g., Senate elections were classified as federal. Specificcodes of election level are: presidential, federal, state / local, no\nspecific election, none. These codes are mutually exclusive. Note\nthat \"state / local\" encompasses ballot initiatives and referenda as\nwell as candidates.\nC.3.2 Purpose of Ad. Ad purpose is mutually inclusive, meaning\none campaign and advocacy ad can be assigned multiple purposes,\ne.g. voter information coupled with promoting a candidate. We\ncoded for five purposes: promote candidate or policy; poll, petition,\nor survey; voter information; attack opposition; fundraise.\nC.3.3 Advertiser Affiliation and Organization Type. To facilitate\ninsights into the advertisers, we identified their political affiliation\nand type of organization (both mutually exclusive). First, we labeled\neach advertiser by name, using information from the ad content\nand/or the landing page (e.g., disclosures that say \u201cPaid for By...\u201d).\nThen, for each advertiser, we investigated their legal organization\nstatus, based on criteria developed by Kim et al. [ 41]. Organizations\nlisted on the Federal Election Commission website, or state elections\nboards were labeled as Registered Committees. 501(c)(3), 501(c)(4),\nand 501(c)(6) tax-exempt nonprofits, and legitimate foreign non-\nprofits that were visible in the Propublica Nonprofit Explorer or\nGuidestar were labeled as Nonprofit organizations. Advertisers\nwhose websites\u2019 home pages were news front pages were labeled\nas news organizations (regardless of their legitimacy). Elections\nboards, state Secretaries of State, or any other state or local gov-\nernment institutions were labeled as Government Agencies. Adver-\ntisers who ran poll ads, and were listed FiveThirtyEight\u2019s Pollster\nRatings were labeled as poll organizations. Ads from corporations\nand other commercial ventures were listed as businesses. Any ads\nwhere the advertiser was not identifiable was listed as unknown.\nWe also attempted to determine the political affiliation of the\nadvertiser. We coded affiliations as Democratic party, Republican\nparty, or independent if the advertiser was officially associated\nwith those political parties (local or national branches), or a candi-\ndate running under that party\u2019s ticket. Codes of right/conservative,\nliberal/progressive, and centrist apply to advertisers not officially\nassociated with a party, but that explicitly indicate their political\nalignment with words like \"conservative\" or \"progressive\", either in\nthe ad itself or on their websites. Nonpartisan affiliation refers to\nexplicitly nonpartisan advertisers or nonpartisan election positions,\ne.g. some local sheriff offices.\nC.4 Political Product Ads\nWe define political products ads as those centered on selling a prod-\nuct or service, using political imagery or content. This is further\ndelineated into three mutually exclusive subcategories: political\nmemorabilia, nonpolitical products using political topics, and polit-\nical services.\nC.4.1 Political Memorabilia. Political memorabilia includes all ads\nmarketing products with some form of political design, e.g. 2nd-\namendment-themed apparel, keepsakes such as election trading\ncards, and merchandise such as Trump flags. This encompasses\nproducts sold for profit and those marketed as free or giveaways.\nC.4.2 Nonpolitical Products Using Political Topics. We coded ads\nas nonpolitical products using political topics if they used political\nEric Zeng, Miranda Wei, Theo Gregersen, Tadayoshi Kohno, Franziska Roesner\nmessaging or context to advertise products ordinarily unrelated to\npolitics. For instance, this covers investment firms marketing their\nstock reports in the context of election uncertainty.\nC.4.3 Political Services. Political services includes ads promoting\nservices directly involved in political industry such as lobbying or\nelection prediction sites.\nC.5 Political News and Media Ads\nWe define political news and media ads as those advertising a\nspecific political news article, video, program, or event, regardless\nof the content style or quality. This categorization encompasses\npolitical clickbait and tabloid-style coverage of political figures as\nwell as traditional news and media. We further define two mutually\nexclusive subcategories: sponsored articles / direct links to stores,\nand news outlets, programs, and events.\nC.5.1 Sponsored Articles / Direct Links to Stories. We coded ads\nas sponsored articles / direct links to stories if they advertised a\nspecific news article or media piece, e.g. an authored story or video\nregarding a current event. We automatically assigned 1,038 ads to\nthis category from Zergnet, a well-known content recommendation\ncompany, as we determined via their advertisement methods that\nall ads from their domain fit this category.\nC.5.2 News Outlets, Programs, Events, And Related Media. News\noutlets, programs, and events ads are distinguished from sponsored\narticles / direct links to stories in specificity, longevity, or reference.\nThis category includes ads for political news outlets (as opposed\nto individual news pieces), lasting programs such as NBC election\nshows (in contrast to a single media clip), or future events such as\npanels or livestreams (rather than already existing news). We also\nincluded ads that were related media, such as podcasts, books, and\ninterviews.\nD WORD FREQUENCY ANALYSIS OF\nPOLITICAL NEWS ADS\nUnique Word Frequency Analysis. We looked at the most common\nwords in political article ads by first deduplicating ads (Sec. 3.2.2),\nthen tokenizing and lemmatizing the ad text. The top 10 words and\ntheir frequencies, as well as a word cloud of the top 50 words, is\nshown in Fig. 15. Among the top 50, we find frequent mentions of\n\u201ctrump\u201d (1,050 times, more than double the next most common word,\n\u201cbiden\u201d), as well as other politically relevant terms and names. Many\nof top 50 words reveal the general tone of these article ads, which\noften emphasize urgency, e.g., \u201cnew,\u201d \u201ctop,\u201d or scandal, e.g., \u201cjust,\u201d\n\u201cclaim,\u201d \u201creveal,\u201d \u201cwatch.\u201d The colloquialism \u201cturn heads\u201d was par-\nticularly common, e.g., \u201cWhat Michigan\u2019s Governor Just Revealed\nMay Turn Some Heads.\u201dWord Freq.\ntrump 1,050\nbiden 415\nelect 314\nread 235\nnew 219\ntop 215\narticl 196\npresid 176\nthi 170\nvideo 162\nFigure 15: Frequencies of the top 10 words in political news\narticle ads, and a word cloud showing the top 50. Ad text was\ndeduplicated by ad, and then tokenized and lemmatized.\nPolls, Clickbait, and Commemorative $2 Bills:\nProblematic Political Advertising on News and Media Websites Around the 2020 U.S. Elections\nFigure 17: The landing page of the poll from Figure 9c. View-\ners are asked to submit an email address to vote in the poll,\nand are signed up a newsletter. Prior reporting has shown\nthis is typically a scheme to generate mailing lists and au-\ndiences for political campaigns to advertise to. \u00a9rightwing.\norg\n(a)\n (b)\nFigure 18: Examples of political news and media ads about\npolitical outlets and events. Images \u00a9CBS and \u00a9Faith and\nFreedom Coalition\n(a)\n (b)\nFigure 16: Other misleading campaign ads: an RNC ad imi-\ntates a system popup (a), and a Trump campaign meme-style\nad attacking Biden (b). Images \u00a9Republican National Com-\nmittee and \u00a9Trump Make America Great Again Committee.\nE ADDITIONAL AD SCREENSHOTS\nE.0.1 Other Misleading Campaign Ads: Phishing Ads and Memes.\nThough many campaign and advocacy ads in the dataset were\npotentially misleading or factually incorrect, we highlight two types\nthat appeared particularly egregious.\nIn December, the Republican National Committee ran ads that\nimitate a system alert popup, like an impersonation attack (Fig-\nure 16a). We found 162 ads of this style in our dataset. Though the\npopup\u2019s style is outdated, it is generally misleading for ads and\nwebsites to imitate operating system dialogues or other programs.\nBefore the general election, the Trump Make America Great\nAgain Campaign launched several attack ads in the style of an\n\u201cimage macro\u201d meme. They featured (obviously) doctored photos\nof Joe Biden, holding Chinese flags, handfuls of cash, or depicting\nhim approving of rioting (Fig. 16b). We found 119 meme-style ads\nin our dataset. Though attack ads and smears are fairly normalized,\nwe did not observe the use of memes for attacks by any other\ncampaigns. These ads contrast with more polished ads placed by\nother campaigns, and could be misleading if users assume meme-\nstyle ads are placed by other users, not an official political campaign.\nE.0.2 Misleading Political Polls. Figure 17 shows the landing page\nof the misleading political poll depicted in Figure 9c.\nE.0.3 Political News and Media. .\nFigures 18a and 18b show examples of political news ads in the\noutlets, events, and programs subcategory. These ads, rather than\nadvertising a sponsored link to a news article, instead advertise\nthe outlet as a whole, or a larger event or program hosted by the\noutlet.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Polls, clickbait, and commemorative $2 bills: problematic political advertising on news and media websites around the 2020 US elections", "author": ["E Zeng", "M Wei", "T Gregersen", "T Kohno"], "pub_year": "2021", "venue": "Proceedings of the 21st \u2026", "abstract": "Online advertising can be used to mislead, deceive, and manipulate Internet users, and  political advertising is no exception. In this paper, we present a measurement study of online"}, "filled": false, "gsrank": 613, "pub_url": "https://dl.acm.org/doi/abs/10.1145/3487552.3487850", "author_id": ["y5yR8WIAAAAJ", "UM8Yu2YAAAAJ", "xKwYmSgAAAAJ", "s_YDrrgAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:qifl8nLq9JcJ:scholar.google.com/&output=cite&scirp=612&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D610%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=qifl8nLq9JcJ&ei=dbWsaILdNb_SieoPzJnloAQ&json=", "num_citations": 40, "citedby_url": "/scholar?cites=10949634373466793898&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:qifl8nLq9JcJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://par.nsf.gov/servlets/purl/10308665"}}, {"title": "Large-scale digital signatures of emotional response to the COVID-19 vaccination campaign", "year": "2024", "pdf_data": "Bertanietal. EPJDataScience           (2024) 13:20 \nhttps://doi.org/10.1140/epjds/s13688-024-00452-7\nRESEARCH OpenAccess\nLarge-scaledigitalsignaturesofemotional\nresponsetotheCOVID-19vaccination\ncampaign\nAnnaBertani1,2*,RiccardoGallotti2,StefanoMenini3,PierluigiSacco4,5andManlioDeDomenico6\n*Correspondence:\nanna.bertani@unitn.it\n1DepartmentofInformation\nEngineeringandComputerScience,\nUniversityofTrento,ViaSommarive,\n9,38123Povo(TN),Italy\n2CHuBLab,FondazioneBruno\nKessler,ViaSommarive,18,38123\nPovo(TN),Italy\nFulllistofauthorinformationis\navailableattheendofthearticleAbstract\nThesameindividualscanexpressverydi\ufb00erentemotionsinonlinesocialmediawith\nrespecttoface-to-faceinteractions,partiallybecauseofintrinsiclimitationsofthe\ndigitalenvironmentsandpartiallybecauseoftheiralgorithmicdesign,whichis\noptimizedtomaximizeengagement.Suchdi\ufb00erencesbecomeevenmore\npronouncedfortopicsconcerningsociallysensitiveandpolarizingissues,suchas\nmassivepharmaceuticalinterventions.Here,weinvestigatehowonlineemotional\nresponseschangeduringthelarge-scaleCOVID-19vaccinationcampaignwith\nrespecttoabaselineinwhichnospeci\ufb01ccontentioustopicdominates.Weshowthat\ntheonlinediscussionsduringthepandemicgenerateavastspectrumofemotional\nresponsecomparedtothebaseline,especiallywhenwetakeintoaccountthe\ncharacteristicsoftheusersandthetypeofinformationsharedintheonlineplatform.\nFurthermore,weanalyzetheroleofthepoliticalorientationofsharednews,whose\ncirculationseemstobedrivennotonlybytheiractualinformationalcontentbutalso\nbythesocialneedtostrengthenone\u2019sa\ufb03liationto,andpositioningwithin,aspeci\ufb01c\nonlinecommunitybymeansofemotionallyarousingposts.Our\ufb01ndingsstressthe\nimportanceofbetterunderstandingtheemotionalreactionstocontentioustopicsat\nscalefromdigitalsignatures,whileprovidingamorequantitativeassessmentofthe\nongoingonlinesocialdynamicstobuildafaithfulpictureofo\ufb04inesocialimplications.\nKeywords: Computationalsocialscience;Socio-technicalsystems;Exceptional\nevents;COVID-19vaccination;Emotions\n1 Introduction\nFace-to-faceinteractionisnotoriouslyimportanttofacilitatecivilizedexchangeandsocial\ncooperation [ 1,2]. Through their nonverbal language[ 3], interacting humans send com-\nplexarraysofsignals(ofdominance,trust,composureetc.)[ 4]thatfavormutualalignment\n[5]andevenelicitbehavioralmimicry[ 6].Forthesereasons,face-to-faceinteractionsare\nnormallyexpectednottoescalateintoviolentandconfrontationalbehaviors[ 7],andeven\nfunctionasadriverofsocialcohesion[ 8]withdistinctivepsycho-physiologicalsignatures\n[9]. With the advent of online interactions, however, this carefully evolved package of\nsocio-cognitive skills has been put to a hard test. In digital interaction, the moderating\n\u00a9TheAuthor(s)2024. OpenAccess ThisarticleislicensedunderaCreativeCommonsAttribution4.0InternationalLicense,which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit\ntotheoriginalauthor(s)andthesource,provide a linktotheCreative Commonslicence,and indicateifchangeswere made.The\nimagesorotherthirdpartymaterialinthisarticleareincludedinthearticle\u2019sCreativeCommonslicence,unlessindicatedotherwise\nin a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not\npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright\nholder.Toviewacopyofthislicence,visit http://creativecommons.org/licenses/by/4.0/ .\nBertanietal. EPJDataScience           (2024) 13:20 Page2of16\nroleofnonverbalcuesislargelylost[ 10],andsubjectsmustlearnhowthea\ufb00ordancesof\ndigital communication enable alternative ways to signal pro-sociality and a\ufb00ection [ 11].\nHowever, developing alternative systems that work on a large social scale is challenging,andasaresultitiswidelyobservedthatdigitallybasedexchangeshavehigherchancesto\nbecomevitriolicandpronetoescalationthanface-to-faceones[ 12].\nIn this context, a particularly crucial role is played by emotions. The once widely\nheld conviction that human emotions were biologically programmed, universal \u2018naturalkinds\u2019,spannedbyacoregroupofsixbasicemotionsandcharacterizedbyspeci\ufb01c,inter-culturallyreadablesetsofbodilycues,hasgraduallygivenwaytoanalternativeparadigmthatidenti\ufb01esawholespectrumofemotionalstatesmappedbyahigh-dimensionalsetofverbalandnonverbalsignals[ 13]whosesocio-cognitiveindexingdependsonsociallearn-\ningandculturalframes[ 14].Inviewofthecentralroleofemotionsinsocialcognition[ 15],\ntheuseandinterpretationofsuchemotionally-relatedsignalsispivotalinhumaninterac-tion.Inparticular,theexpressionanddecodingofemotionsislikelytobeoneofthemostcriticalaspectsrelatedtotheshiftfromface-to-facetoonlineinteractions[ 16].\nIf emotions arenot \u2018naturalkinds\u2019, a radical change in the socio-cognitive environment\nsuchasthatbroughtaboutbythedigitalshiftmaynotonlya\ufb00ecthowemotionsareinter-preted, but also how they are de\ufb01ned [ 17], expressed [ 18], and socially transmitted [ 19].\nDespitethatthea\ufb00ordancesofthedigitalenvironmentfacilitateemotionalover-reaction[20], it is also true that, unlike face-to-face interaction where nonverbal signals are ob-\nserved at a millisecond scale and are largely automatic and not controlled [ 21], in online\ninteractiontheemotionalresponsemaybedistilledandevenconstructedonthescaleof\nseconds and even minutes or hours [ 22]. On the other hand, nonverbal cues are largely\nsubstituted by verbal equivalents that intentionally amplify the a\ufb00ective content of com-municationtocompensateforthelower-dimensionalnatureofthesignal[ 23].Moreover,\ninonlineinteractionssomeoftheparticipantsmaybenonhuman(bots)whichareexplic-itlydesignedtoimplementacertainstrategyofa\ufb00ectivecommunicationtoelicittypesofemotionalresponsesfromusers[ 24].\nThereisthenabasicdi\ufb00erencebetweenemotionsexpressedinface-to-faceinteraction\nand \u2018emotions\u2019 as constructed in online exchanges [ 25]. And such di\ufb00erence is likely to\nbe more substantial and relevant the more the topic of the exchange concerns sociallysensitive,polarizingissues[ 26,27].Inparticular,fakeormisleadingcontentthatembeds\ncertainemotionalreferences(whichcanbepositivesuchasanticipationortrust,orneg-ativesuchasanger,accordingtocases)ismorelikelytogoviralandhasalongerexpectedlifetime[28].\nMovingfromthispremise,theaimofourpaperistoinvestigatehowonline\u2018emotional\u2019\nresponseschangewhencomparingtwodi\ufb00erentkindsofexchanges:oneregardingabase-line, non-polarizing topic and one regarding one of the most polarizing topics of today:\nvaccination [ 29]. It is intuitive to conjecture that more socially controversial issues in-\ncitemoreemotionalreaction(andviceversa)onlinethanlesscontroversialones[ 30],not\nunlike what happens o\ufb04ine, although speci\ufb01c features of the online interaction environ-mentmaymakeasigni\ufb01cantdi\ufb00erence[ 31].However,itisofinteresttounderstandwhat\nkind of emotional reaction is incited in what circumstances, depending on the charac-teristics of the users involved. The emotional response patterns that are found give usadditionalinsightonthenatureofthesocialcontentiousnessoftheissue,andatthesametime help us understand better how online \u2018emotions\u2019 are constructed to pursue speci\ufb01c\nBertanietal. EPJDataScience           (2024) 13:20 Page3of16\nsocial goals. In this regard, vaccination is clearly a relevant test bed [ 32], and we are in\nparticularinterestedinstudyingsocialexchangesonTwitterwithspecialattentiontotheveri\ufb01ed/un-veri\ufb01eduserandhuman/botdyads.Towhatextenttheemotionalcommuni-cation of veri\ufb01ed users will distinguish itself from that of un-veri\ufb01ed ones? Will there bemajor di\ufb00erences between human and arti\ufb01cial \u2018emotional\u2019 responses when the topic is\npotentiallymorein\ufb02ammatory,andinwhichsense?\n2 RelatedWork\nThereisarapidlygrowingliteraturethatisexploringvariousmethodologicalapproaches\nto the measurement of emotions and of their socio-behavioral e\ufb00ects in online interac-tions,withspecialemphasisoncontentiousandpolarizingtopics.Theworkpresentedin[33] introduces a method using change point detection and topic modeling to measure\nonline emotional reactions to o\ufb04ine polarizing events such as the COVID-19 pandemic\nandracialjusticeprotests,e\ufb00ectivelydis-aggregatingdi\ufb00erenttopicstomeasuretheemo-\ntionalandmoralreactionsofthepublicopinion.\nGuo et al. [ 34] use natural language processing methods to highlight a surprising rise\ninpositive a\ufb00ectduringtheearlystages oftheCOVID-19pandemic,likelyre\ufb02ecting therole of social media as a tool for emotion regulation and reducing fear of the unknown\nduring the pandemic, while also revealing a partisan divide in emotional and moral re-\nactions. Vemprala et al. [ 35] also develop a natural language processing approach to the\nstudyofpublicnegativeemotionalresponsesduringtheCOVID-19pandemictodiscernhowemotionalpatternsreacttotheprevailingfocusofthepublicdiscussionbeingplaceduponhealthvs.economicconcerns,withfeardominatinginhealth-relatedconversations,\nand a more complex mix of emotions, mainly anger and fear, emerging in economics-\nrelatedones.Wangetal.[ 36]alsodevelopanaturallanguageprocessingapproachbased\nupon a BERT transformer model for sentiment classi\ufb01cation to show that the COVID-19 pandemic caused a signi\ufb01cant drop in expressed sentiment globally, followed by anasymmetric, slower recovery, with minor e\ufb00ects of sentiment expression related to lock-\ndown policies, although with signi\ufb01cant variation across countries. Zhang et al. [ 37]u s e\ninsteadamachinelearningclassi\ufb01cationmodelbasedupondeeplearninglanguagemod-els to identify positive and negative emotions in online conversations about COVID-19,and including an additional, so far not analyzed, ambivalent emotional expression (jok-ing), \ufb01nding a rapid burst and a slower decline in the online conversations in all of the 6\nlanguagestheyanalyze.\nAlthough a comprehensive review of this literature is beyond the scope of the present\npaper,thesefewexamplessu\ufb03cetoshowtherichnessofthemethodologicalandanalyti-calcontributionsofcomputationalsocialscienceapproachestotheemotionalanalysisofonlineinteractionsonpolarizingtopicsandespeciallyofCOVID-19relatedones.\nInthisrapidlyevolving\ufb01eldofresearch,thespeci\ufb01ccontributionofourpaperismaking\nuseagainofanaturallanguageprocessingapproachandofthesamelexiconusedby[ 35],\nNRC Word-Emotion Association Lexicon, to analyze speci\ufb01cally the di\ufb00erences in emo-tional expression between a baseline non-contentious topic and a highly polarizing onesuchasCOVID-19vaccination,andtestingspeci\ufb01callyfordi\ufb00erencesrelatedtowhether\nonlineparticipantsarehumansorbots.\nTheNRCLexiconconsiderseight\u2018emotions\u2019,fourofwhichwithpositivevalenceandfour\nwithnegativevalence.Withthischoice,weintentionallymovefromthesixbasicemotions\nBertanietal. EPJDataScience           (2024) 13:20 Page4of16\nofEkman[ 38]notbecauseweagreewiththe\u2018naturalkinds\u2019framework(withinwhichthere\nisasigni\ufb01cantlackofconsensusaboutthespeci\ufb01clistofwhatemotionsarebasic;see[ 39]),\nbutbecausethismaybeasimplebenchmarkfortheanalysisoftheironlinecounterparts.\nSpeci\ufb01cally,thefour\u2018basic\u2019negativeemotions(fear,anger,sadness,anddisgust)andoneof\nthepositiveones(surprise)arekeptintheNRCLexicon,whereastheother(happiness)is\nunpackedintotwopositiveemotions(anticipationandjoy).Finally,trusthasbeenaddedasa\ufb01nalpositiveemotion.Despitethattrustistechnicallynotconsideredanemotion[ 40],\nitcanbeinterestingtoconsideraspeci\ufb01cemotionalsignalrelatedtotrustintherelatively\nunfavorable conditions given by social signaling through a digital medium. This set of\n\u2018emotions\u2019 as encapsulated in the NRC Lexicon should therefore be seen as a useful \ufb01rst\nbenchmark rather than as an invitation to consider them as more basic or foundational\nthanothers.\nWithinthisframework,we\ufb01ndasigni\ufb01cantdi\ufb00erenceinemotionalreactionsbetween\nno-contentious and polarizing conversations, and moreover we \ufb01nd that veri\ufb01ed users,\nnomatterwhetherhumansofbots,exhibitmorepositivelyvalencedemotionalresponses\nthanunveri\ufb01edones.Inthisregard,ourpaperprovidesfreshinsightsonspeci\ufb01caspects\nthat have not been covered in the previous literature, while at the same time connecting\ntoasolidadgrowingstreamofstudiesbothinmethodologicalandthematicterms.\nGettingdeeperinsightsintotheseissuesiscrucialforthefuturedesignofenvironments\nthatfavormorecivilizedonlineinteraction.Asarguedby[ 41],civilizedneednotamount\nto\u2018polite\u2019accordingtopre-digitalstandardsofsocialetiquette.Onlinediscussionmaybe\nmoreemotionallychargedthano\ufb04ineonesforreasonsthatmostlyconcernthedi\ufb00erent\na\ufb00ordancesofthesocialenvironments[ 42].However, astheexperienceofthepandemic\nhastaughtus,thesocialimplicationsofmassiveuncivilizedexchangeonissuesofprimary\npublic interest may be devastating [ 43]and mayo\ufb00er ampleopportunitiesformanipula-\ntionbymalevolentparties[ 44].Therefore,adeeperunderstandingofthesocio-emotional\ngrammar of online interactions is a key issue for both computational social science and\npublicpolicy.\n3 Method\n3.1 Overviewofthedataset\nWe collected social media data through the special Twitter\u2019s endpoint dedicated to\nCOVID-19research,1whichallowedresearcherstostudythecomprehensive,publiccon-\nversationaboutCOVID-19inreal-time.Wefocusedourattentionondatacapturedbythe\ufb01lter of the Twitter Firehose on COVID-19 in 18 among the most represented languages\non Twitter. More speci\ufb01cally, we focused on terms related to the vaccination, to anti-vax\ncampaignsbutalsotothemostknownvaccinebrands,suchasP\ufb01zer,Astrazeneca,John-\nson&Johnson,Moderna,SputnikV(seefurtherdetailsinAdditional\ufb01le 1Appendix).\nIn addition, we considered only a small fraction of the overall data, the about 1% of\ntweets that are geotagged, to guaranteean accurateinformation also as to their location,\nas signalled by the user\u2019s device. The data collected covers the period between August\n31, 2020 and July 15, 2021, that is from the announcement of the availability of the \ufb01rst\nCOVID-19 vaccinesupto thepeakof thevaccinecampaigninEuropeandin theUnited\nStates.Wecompareoursampleoftweetsrelatedtothevaccinationtopicswitha10million\n1https://blog.twitter.com/developer/en_us/topics/tools/2020/covid19_public_conversation_data\nBertanietal. EPJDataScience           (2024) 13:20 Page5of16\nTable 1Statisticsaboutthedatasets.Thetableshowsthenumberofmessages,uniqueusersand\nthetimeframeconsideredforboththedatasets\nDataset Messages UniqueUsers Timeframe\nVaccine 9.6million 3million 31/08/2020-15/07/2021\nBaseline 10million 6million 21/04/2021-24/04/2021\nof messages posted on Twitter as baseline sample in which no speci\ufb01c contentious topic\nd o m i n a t es,ass h o wni nT a b l e 1. The baseline sample was obtained using the stream API\nwithout speci\ufb01c \ufb01ltering or keyword searches. As a result, the dataset encompasses a di-verserangeoflanguages,includingbutnotlimitedtoEnglish,Japanese,Spanish,Korean,Portuguese,Thai,Indianlanguages,French,German,andItalian,amongothers.\n3.2 Veri\ufb01edaccounts\nVeri\ufb01ed users are those having the blue veri\ufb01ed badge, a blue check mark, that de\ufb01nesthose accounts that are of public interest because they are considered authentic, notable\nand active on Twitter. The veri\ufb01cation process of users is given by the blue check mark\nt h a tc a nb ef o u n dn e x tt ot h eu s e r n a m e ,w h i l eu n v e r i \ufb01 e da c c o u n t sd on o th a v et h i sd i s -tinctivesignal.However,thisde\ufb01nitionpertainstotheperiodbeforeMusk\u2019stakeover,dur-\ningwhichaccountswererequiredtoundergorequestveri\ufb01cation.StartingfromApril1st,\n2023,therehasbeenachangeintherule.Foruserstoacquiretheveri\ufb01cationbadgenow,theyarerequiredtosubscribetoTwitterBlue[ 45].Regardingthisresearch,weadhereto\ntheinitialde\ufb01nitionsincethenewrulewasnotine\ufb00ectduringtheconsideredtimeframe.\n3.3 Newsreliabilityandpoliticalleaning\nIn this analysis, we also considered some metadata associated to the textual content ofthe messages. We collected manuallychecked web domains from various publicly acces-\nsible databases, encompassing scienti\ufb01c andjournalisticsources.Inparticular,we exam-\nineddataprovidedbytheMediaBiasFactCheck[ 46].Thisisanorganizationthatprovide\na huge database continuously updated whose methodology is to systematically evaluate\ntheideologicalleaningsandfactualaccuracyofmediaandinformationoutletsthrougha\nmulti-facetedapproachthatincorporatesbothquantitativemetricsandqualitativeassess-ments.Wefoundatotalof4988domains,reducedto4417afterremovinghardduplicatesacrossdatabases.Giventhenatureofourmultilingualandmulticulturalanalysis,weeval-\nuatedthelanguagecoverageoftheweb-domainsclassi\ufb01ed,takingintoaccounttheEnglish\ncentricnatureoftheweb.Buildingupon[ 47],wegatheredstatistics fromAmazonAlexa\n(www.alexa.com/topsites/countries ) about web tra\ufb03c (the top 50 most visited websites)\nforallcountriesacrosstheglobe,matchingtheselistswiththelistofdomainsusedinour\nanalysis.For127countrieswefoundatleastonedomaininthereliabletop-50newssource,and for 21 (iso2 codes: AE, AR, BB, BE, CA, DK, FR, KE, MX, NG, PA, PE, PH, PR, PT,QA,SD,SE,TT,USandVE)theyhaveatleastonedomaininthetop-50websiteslabelled\nasunreliable.In fact, this is a lowerbound, because Alexa provided onlymajor domains,\ndisregardingsubdomainsthatweinsteadclassi\ufb01edaswell.Thislargepresenceamongthevery top tier of websites suggests that the results are robust for multilanguage/multicul-turalanalysis.\nWeconsideredtheURLscontainedinmessages,andlabeledeachURLaccordingtothe\npoliticalleaning(left,left-center,neutral,right-centerandright)ofitsmediasourceand\nBertanietal. EPJDataScience           (2024) 13:20 Page6of16\nthe type of source (political, satire, mainstream media, science, conspiracy/junk science,\nclickbait, fake/hoax) as manually classi\ufb01ed by external experts. In particular, building on[47],wehaveclassi\ufb01edournewssourcesas reliable(whenbelongingtotheScience,Main-\nstreamMediacategories)and unreliable (whenbelongingtotheSatire,Clickbait,Political,\nFakeorHoax,ConspiracyandJunkscienceandShadowcategories).Finally,asathirdcat-egoryweconsidertweetsnotcontaininganyurltobeclassi\ufb01ed(i.e.mereopinionswithout\na source). We excluded all the web-domains classi\ufb01ed as \u2018Other\u2019 as they point to general\ncontentthatcannotbeeasilyclassi\ufb01ed,suchasvideosonYouTubeorpostonInstagram.\n3.4 Botdetection\nSocialbotsareautomatedaccountsthatmimichumanbehavior,createposts,commentsand likes on social networks. Analyzing the role of social bots in the emotional response\nto controversial topics such as those studied in this paper is important, given that they\nare being systematically used for the manipulation of the public opinion through socialmedia[48].Inparticular,botshaveprovensuccessfulinspreadinglow-credibilitycontent\nby strategically targeting in\ufb02uential human users [ 49]. Their typical mode of operation\nalsoincludesampli\ufb01cationofin\ufb02ammatorycontentsbyhumanusers[ 50],whereasactual\ninstigationofemotionsisrarer.However,casesofsuccessfulbot-to-humantransmission\nofnegativeemotionslikeangerhavebeendocumentedinonlineCOVID-19relatedcon-\nversations[ 51].Botscanthereforebeconsideredasigni\ufb01cantthreattopublichealth[ 52],\nwhosemodeofoperationalsoincludesemotionalmanipulation.Itisthereforeimportant\nto investigate thepotential roleofbots inthe socialdynamics of emotional responses on\ncontroversialtopics.\nTo distinguish a bot from a human, we chose some criteria associated to some forms\nofunusualsocialbehavior.Inparticular,basedon[ 44],automatedaccountstendtoshow\nan important productivity on social media by posting excessive number of content and,especially, in their concentration in particular moments during the day (e.g. overnight\nor all day long). We identi\ufb01ed automated accounts using a machine learning algorithm\ndesigned to classify Twitter accounts as humans or bots and used in previous research[50,53]Theclassi\ufb01cationofusersinto\u201chuman\u201dor\u201cbot\u201disbasedontenfeaturesthatyield\nthebest classi\ufb01cation accuracyaccordingtoseveral authors[ 50,54]. The features are (1)\nstatusescount;(2)followerscount;(3)friendscount;(4)favoritescount;(5)listedcount;(6) default pro\ufb01le; (7) geo enabled; (8) pro\ufb01le use background image; (9) protected; and\n(10)veri\ufb01ed,followingthesameprescriptionsofpreviousstudies[ 50,53\u201355].Themodels\nundergo training using 80% of the data and are subsequently validated on the remaining20%.Thedivisionbetweenthesetwosetsisperformedwhileensuringabalancebetweenbots and humans at the level of each individual original dataset. The models gave us the\nhighestaccuracy(>90%)andprecisioninidentifyingbots(>95%)[ 54].\nIn a previous work [ 53], we tested the ability of the algorithm to generalize the clas-\nsi\ufb01cation out of the data sample used for training and validation by applying the model\non an independent data set [ 56], consisting of labeled information about 8,092 humans\nand42,446bots.Theresultsoftheclassi\ufb01eraresatisfactory,withanaccuracyof60%,anF1-scorehigherthan70%,andarecallof58%.Inthisspeci\ufb01cwork,wedidnotmanually\ninspecttheusersbutweassumethatourresultswoouldbeconsistentwiththeonesfound\nintheabovementionedwork.\nBertanietal. EPJDataScience           (2024) 13:20 Page7of16\n3.5 Emotionaldetection\nThe NLP pipeline used to process the tweets allows processing of text and emotions in\nmultiple languages. For the emotions we rely on the NRC Word-Emotion AssociationLexicon [ 57], containing 14,182 English words associated with one or more of eight ba-\nsic emotions ( anger,fear,anticipation ,trust,surprise,sadness,joy,a n ddisgust) and two\nsentiments( negativeandpositive).Thesewordsarethentranslatedintoseverallanguages,\nincludingthe18usedinthiswork.AsecondresourceusedinthepipelineistheNRC-VADLexicon [ 58], containing 19,971 English words associated with three scores representing\nrespectivelyvalence,arousal,anddominance.AsinthepreviousresourcetheNRC-VAD\nisalsoincludingthetranslationofeachwordinallthe18neededlanguages.Takingasin-\nputthetextofthetweetanditslanguage,thepipelinereturnsthelistofwordsassociatedwithemotions,theamountofeachemotioncontainedinthemessageandthetotalvaluesof valence, arousal, and dominance. Being based on a lexicon, the emotions extraction isprecededbytwopreprocessingstepsinordertoincreasetheamountofemotionsretieved\nfrom the text. The \ufb01rst preprocessing involves elements that are relevant in social media\nashashtagsandemoji,oftencarryinganimportantpartofthecontentofatweet.Topro-cess the hashtags we expand the Ekphrasis library [ 59], originally developed for English,\nto cover additional languages. This allows us to identify hashtags in tweets and to splitthem in the words composing it, e.g. #staysafe into \u2018stay safe\u2019. To detect emotions related\ntoemojisweadoptedthestrategyofreplacingthemwiththeirtextualdescriptionsinthe\nlanguageofthetweet.ThedescriptioncanthenbeusedtosearchformatchesintheNRClexicons,e.g.\u201cworriedface\u201d.SincetheNRClexiconsdoesn\u2019tcontainallthein\ufb02ectedformsof annotated words, we preprocess the tweets lemmatizing them (including the text ex-tractedfromhashtagsandemoji),toincreasethenumberofmatcheswiththewordsinthe\nlexicon.ThelemmatizationisdoneusingtheSpacylibrary.Thepipelinealsohasthepos-\nsibilityofusingalistofwordsthatneedtobeexcludedfromtheemotionsextraction,forinstance when working on tweets about Covid, we can decide to exclude words as \u2018virus\u2019beingpresentindata,astopic,regardlessoftheemotionsexpressedbythemessages.\n3.6 Emotionalaggregation\nVarious emotional algorithms have been already tested on di\ufb00erent kind of data. One ofthemissurelytheValence-Arousal-Dominancemodel[ 60].Wefoundthatthethreesen-\ntiments are highly correlated among them in our dataset, as shown in Additional \ufb01le 1.\nBased on the purpose of this research, we decide to adopt another algorithm with eight\nemotions,fourofwhichwithpositivevalence(Trust,Anticipation,Joy,Surprise)andfourwithnegativevalence(Anger,Sadness,Fear,Disgust)inordertobettercaptureeachsingleemotion.Beforestartingtheanalysis,apreprocessingphasehasbeenfundamentaltoun-derstandhowtonormalizethedata,sincetheemotionalrangeofeachemotiondoesnot\nfollow the same scale. In particular, we found the total emotional value of the messages\npostedonTwitterbysummingeachsingularemotion.Then,wedividedeachemotionbythe total emotional value found in order to obtain that the sum of each emotion shouldbe equal to 1. We grouped the positive emotions (Anticipation, Surprise, Joy and Trust)and the negative emotions (Sadness, Anger, Fear, Disgust) into two di\ufb00erent categories\n(positive vs negative emotions). After this procedure, we normalized the valence of each\ntweetacrosstheemotionalrangeof\u20131(completelynegativevalence)and+1(completelypositive valence). In the emotional analysis of the three reliability-ranked categories of\nBertanietal. EPJDataScience           (2024) 13:20 Page8of16\nnews Fig. 1and Fig.2, we also decided to delete the emotional values equal to 0 because\nourinterest wastoobserve thedistribution ofemotionalresponse andthe0valuemightindicatetheabsenceofanyemotioninthemessageposted.\n4R e s u l t s\n4.1 EmotionsdistributionamongtheCovid-19vaccinationandthebaseline\nsample\nWe consider online discussions in which no speci\ufb01c contentious topic dominates con-\nsisting of more than 10 millions messages posted to a popular microblogging platform,Twitter, between 21/04/2021 and 24/04/2021 from 6 millions of unique users. We alsoconsidertheonlinediscussionsconcerningthemassiveCOVID-19vaccinationcampaignbetween August 31, 2020 and July 15, 2021, that is from the announcement of the avail-ability of the \ufb01rst COVID-19 vaccines up to the peak of the vaccine campaign in EuropeandintheUnitedStates,consistingof9.6millionofpostsfrom3millionofuniqueusers.We\ufb01ndthattheconversationrelatedtoacontentioustopicsuchasvaccinationgeneratea spectrum of emotional response that di\ufb00ers from that of the baseline, as illustrated inFig.1.\nInparticular,thevaccinesampleshowsanoverrepresentationoffouremotionswithre-\nspecttothebaseline:Fear,Anticipation,SadnessandTrust.Indeed,thesethreeemotionshave a mean value greater than the threshold value. If we imagine an ideal benchmarkin which each of the eight emotions is equally represented, each of them should be by a\nfrequency of 0.125. We therefore draw this level as a dotted line in the \ufb01gure to make it\nmore readable which emotions are actually over- vs. under-represented with respect tothe benchmark. We can notice that the baseline distribution is more evenly distributedthanthevaccinationone.Inparticular,forthebaselineweseethatJoy,SadnessandAngerare very close to the benchmark value. On the contrary, the distribution is considerablylessuniformforthevaccinationsample,havinganover-representationofFear,Anticipa-tion, Sadness and Trust. This suggests that a contentious topic may lead to the selectiveampli\ufb01cationofcertainemotionswithrespecttoothers.AKolmogorov-Smirnovtesthas\nFigure1 EmotionMeanValue.Foreachsample(baseline(leftpanel)andvaccine(rightpanel)),wequanti\ufb01ed\nthemeanvalueofeachemotion:Anger,Sadness,Surprise,Fear,Anticipation,Trust,Disgust,Joy.The\nhorizontallineisplacedatthe0.125valuetobettervisualizewhichemotionsareover-vsunder-represented\nwithrespecttoourbenchmarkvalue.Thegreyverticallinesshowthestandarddeviationofeachdistribution.\nTobetterunderstandthestatisticallysigni\ufb01canceofeachemotions,weperformedaKolmogrov-SmirnovTest\nwiththeresultthatallthedistributionarestatisticallydi\ufb00erentatthe95%ofcon\ufb01dencelevelwithap-value\nlowerthan0.01bothacrossemotionsandamongthebaselineandvaccinesamples.Inaddition,we\ncalculatedthestandarderrorofthemean,resultingtobenegligibleamongalltheemotiondistribution\nBertanietal. EPJDataScience           (2024) 13:20 Page9of16\nbeen performed to evaluate the statistical signi\ufb01cance of the two samples. We \ufb01nd that\nthedistributionsareactuallystatisticallydi\ufb00erentbetweenthetwosamples.\n4.2 Emotionalresponsestodifferentlevelsofnewsreliability\nAtthispoint,however,itmaybereasonabletoconjecturethatthisselectiveampli\ufb01cationofcertainemotionsinthecaseofcontentiouseventsmaybeinturnfurthermodulatedbys p e c i \ufb01 cf e a t u r e s .F i r s to fa l l ,w ec a na s kw h e t h e rg e t t i n gi nc o n t a c tw i t hu n r e l i a b l en e w srelatedtothecontentioustopicisdi\ufb00erentthangettingincontactwithreliableonesasfar\nastheemotionalresponseisconcerned.\nTo test this, Fig. 2shows the distribution of emotions for the three reliability-ranked\ncategories of news: reliable, unreliable and opinions. The eight emotions have been nor-malized in a range between \u20131 (the most negative emotional valence) and +1 (the mostpositiveemotionalvalence)toimprovereadability.\nInterestingly, we \ufb01nd that in the baseline sample there are no systematic di\ufb00erences in\nterms of emotional response to the three categories of news. In particular, the median is0forallthenewscategories,showingthatusersnotonlydonotdiscriminateacrossnewscategories in terms of emotional response, but also present a balanced overall responsetothenews,sincethereisnotaskewtowardspositiveornegativeresponses,indicatinga\nmorebalancedemotionalresponse,speci\ufb01callywithrespecttothevaccinationsample.\nAlsointhecaseofthedi\ufb00erencesbetweenreliableandunreliablenewsweperformeda\nnon-parametric Kolmogrov-Smirnov Test, \ufb01nding that the two distributions are not sta-tisticallysigni\ufb01cantlydi\ufb00erent,withap-valueequalto0.22.\nIn the case of the vaccine sample, the pattern is completely di\ufb00erent, and the distribu-\ntionsofemotionsarestatisticallydi\ufb00erentforeachtypeofnewsconsidered.\nFigure2 Emotionalrangefordi\ufb00erentcategoriesofnews.Foreachsample(baseline(leftpanel)vsvaccine\n(rightpanel)),wequanti\ufb01edthedistributionofemotionalvaluesfrom\u20131to1,respectivelyfortweets\ncontainingreliablenews(top),unreliablenews(mi ddle)orjustcommentswithoutsources(bottom).Allthe\ndistributionhavebeentestedthroughaKolmogrov-SmirnovTestinordertoevaluatethedi\ufb00erenceamong\nthesamplesandacrosscategoriesofnewsconsidered.Thedistributionarestatisticallysigni\ufb01cantatthe95%\nofcon\ufb01dencelevelwithap-valuelowerthan0.01amongthetwosamplesandwithinthesamesample,with\ntheonlyexceptionoftweetscontainingunreliablenewsandreliablenewsintheBaselinesample,havinga\np-valueequalto0.22\nBertanietal. EPJDataScience           (2024) 13:20 Page10of16\nReliablenewsarecharacterizedbybeingskewedtowardthepositivevalencesideofthe\nspectrumwhencomparedtounreliableones.Inotherwords,reliablenewselicitinusersmorepositiveemotionsinthecaseofacontentioustopicwithrespecttounreliableones.\n4.3 Emotionalresponsesofveri\ufb01edvsunveri\ufb01edusers\nIn the Fig. 3panel (A), we compare the emotional responses of veri\ufb01ed vs. un-veri\ufb01ed\nusers. We \ufb01nd that veri\ufb01ed users are characterized by a more positive emotional re-sponse.Onceagain,thedi\ufb00erencebetweenthetwodistributionsisstatisticallysigni\ufb01cantasshownbytheKolmogrov-SmirnovTest.Moreover,alsoforthebaselinesamplewe\ufb01ndmore positive valenced responses for veri\ufb01ed users (an average level of 0.31) in contrastwiththeunveri\ufb01edusers(0.27),asshowninTable 2.Interestingly,inthecaseofthebase-\nlinethepositiveemotionalresponseoftheveri\ufb01edusersisstrongerthaninthecaseofthevaccine sample. Even if veri\ufb01ed users, as a consequence of their incentives to reputationmanagement and accountability, tend to favor positively valenced emotional reactions,it turns out that the contentious nature of the vaccine topic in\ufb02uences their own mode\nof response and determines a less positively valenced response, although maintaining an\noverallpositiveemotionaltone.Interestingly,forun-veri\ufb01eduserstheemotionalresponse\nFigure3 EmotionalAnalysisfordi\ufb00erenttypesofuseraccounts.Wequanti\ufb01ed,rangingfrom\u20131to+1,across\n(A)veri\ufb01edandunveri\ufb01edusers,(B)bothhumanandbot,overtime.Thedashedhorizontallinesindicatethe\nemotionalmeanvaluesforeachusertype,basedonabaselineof10millionrandomlysampledtweets.This\nhighlightsthatthemeanemotionoftherandomdistributionishigherthatofourspeci\ufb01cinterest.Wepresent\nthedistributionofemotionalvalues,aggregatedbyday,foreachcategoryconcerningthevaccinesample\n(C-D).WeconductedaKolmogorov-SmirnovTestacrosstheentiredistributionforeachcategory.Theresults\nindicatestatisticalsigni\ufb01canceatthe95%con\ufb01dencelevel,withap-valuebelow0.01,notonlywithin\ncategoriesbutalsoacrossthevarioussamplesconsidered\nBertanietal. EPJDataScience           (2024) 13:20 Page11of16\nTable 2SummaryStatisticsofeachtypeofusersconsideredbothintheVaccineandBaseline\nsample.Weshowthemeanvalueandthestandarddeviationofeachdistributionforthefollowing\ncategories:veri\ufb01edandunveri\ufb01edusers,botsandhumans.We\ufb01ndthatonaveragethebaseline\nsampleischaracterizedbyhavinghighervaluesforeachtypeofusersandthatthediscoursescarriedoutbyveri\ufb01edaccountsaremarkedbyamorepositiveemotionalfeatures\nType Vaccine Baseline\nUsers Mean(SD) Mean(SD)\nVeri\ufb01edUsers 0.041(0.81) 0.316(0.668)\nUnveri\ufb01edUsers \u20130.131(0.8) 0.275(0.744)\nVeri\ufb01edBot 0.052(0.81) 0.290(0.682)\nUnveri\ufb01edBot \u20130.126(0.799) 0.273(0.745)\nVeri\ufb01edHuman 0.022(0.811) 0.378(0.626)\nUnveri\ufb01edHuman \u20130.134(0.8) 0.276(0.743)\nisnotonlylesspositive,butparticularlysointhe\ufb01rstmonthsofthevaccinationcampaign,\nwhenthelevelofcontentiousnesswasparticularlyhigh.\nIn the second panel (B), we then \ufb01nd the emotional responses of the veri\ufb01ed and un-\nveri\ufb01ed users as distinguished between bots and humans. Discerning bots from humans\nis usually carried out through the observation of the respective behaviors on social me-\ndia platforms. More speci\ufb01cally, automated accounts are usually characterized by some\nforms of unusual behavior: very large volume of content created, or very high frequency\nof creation, for instance (see Method section for further details). Interestingly, the most\nsigni\ufb01cantdistinctionintermsofemotionalexpressioninoursampleisnotthatbetween\nbotsandhumans,butratherthatbetweenveri\ufb01edandun-veri\ufb01eduserswithineachcat-\negory. It turns out that when we compare the entire distribution of emotional responses\nbetween any pair of these four categories of users, each couple of distributions is signif-\nicantly di\ufb00erent in all cases. In particular, the C panel shows the emotional distribution\nacross time respectively for veri\ufb01ed and unveri\ufb01ed users, while the D panel shows the\nsameanalysisdiscerningbetweenbotsandhumanuserstobettervisualizethestatistical\ndistributionofeachcategory.\nWhen an user is veri\ufb01ed, be it a human or bot, their emotional response is more posi-\ntively valenced over time. Likewise, responses are less positively valenced for un-veri\ufb01ed\nusers, be them humans or bots. Comparing our results with the baseline sample, we no-\nticeanimportantdi\ufb00erence.Whereasveri\ufb01edhumanstendtopresentamorepositivere-\nsponseinthebaseline,forveri\ufb01edbotstheresponseismorepositiveinthevaccinesample.\nThe di\ufb00erences between the four categories of users tend to stabilize as the vaccination\ncampaignunfolds,whereasinthevery\ufb01rstmonthstheresponsesarelesspredictable(see\nfurtherdetailsinAdditional\ufb01le 1Appendix).\n4.4 Emotionalresponsesbasedonthepoliticalorientation\nAnotherimportantaspecttoconsideristheroleofnewsmediasourcesaccordingtotheir\npoliticalorientation.InFig. 4,weanalyzedthepoliticalorientationofthesourcesinrela-\ntiontotheveri\ufb01ed/un-veri\ufb01edstatusoftheusersandtheiremotionalresponse.Asalways\nwehavenormalizedthevalenceofthetweetsacrosstheemotionalrange(\u20131,+1)foreach\nofthecategoriesmentioned.Inparticular,eachURLappearinginthemessageshasbeen\nmanuallyinspectedbyexperts(seeMethodsection),classifyingthevarioussourcesinone\nofthefollowingcategories:left,left-center,center,right-center,right.\nBertanietal. EPJDataScience           (2024) 13:20 Page12of16\nFigure4 EmotionalrangeformessagespostedonTwitterbyveri\ufb01ed-unveri\ufb01edaccountsandwithURLs\nassociatedto\ufb01vedi\ufb00erentpoliticalleaning.Foreachpanel,wequanti\ufb01edstatisticsoftweetsacrossthe\nemotionalrange(\u20131,+1)accordingtothepoliticalleaningofthesourcesandtheveri\ufb01ed/un-veri\ufb01edstatus\noftheusers.Theleftpanelreportsthe10Mrandomsampleoftweetsasabaseline,whilethecentraloneis\naboutthevaccinetweets.Therightpanelshowsthepercentagedi\ufb00erencebetweenvaccineandbaseline\ndataforeachcell.Weshowhowemotionalresponsescanbecameasortofmarkerfordi\ufb00erentideological\nposition,especiallyfortheleft-center,centerandcenter-rightcategories.Aninterestingresultisshownbythe\nveri\ufb01edaccountsthroughwhichtheemotionalresponsesvaryatdi\ufb00erentscales,reachinghighnegativeand\npositivevalues\nWe quanti\ufb01ed the distribution of the political orientation of the sources and the status\nof the users both for the baseline and the vaccination samples. We obtain an interesting\nresult: the variation between the two samples is apparent, and the di\ufb00erence is statisti-callysigni\ufb01cantamongallthepoliticalorientations,whethertheusersareveri\ufb01edornot.\nThe \ufb01rst panel of the Figure indicates the statistics for tweets from the baseline sample,\nshowingthatahighernumberoftweetshasaleft-centerorientationcomparedtotheoth-\ners. However, in the vaccine sample the incidence of political orientations changes, and\nalsotheemotionalresponsesareprettydi\ufb00erent,especiallyfortheleft-center,centerand\ncenter-right categories. The di\ufb00erence is substantial in the third panel where we can ob-\nservesigni\ufb01cantvariationsinemotionalresponsesespeciallyfortheleft-center,centerand\ncenter-rightcategories.Thisclearlyshowshowemotionalresponsesbecomemarkersofa\ndi\ufb00erentiation between di\ufb00erent ideological positions in the case of a highly contentious\ntopicsuchasvaccination.Interestingly,thepositionswhereweobservethemosthetero-\ngeneity in response are the relatively moderate ones, and not the most extreme, as theformer are the ones that need to di\ufb00erentiate more from their relatively closer analogs.\nExtreme positions are already well di\ufb00erentiated and their emotional response patterns\narelesscharacteristic.\nIn the bottom panel, where we further di\ufb00erentiate between veri\ufb01ed and un-veri\ufb01ed\nusers,we\ufb01ndthatagaintheemotionalresponsesaredi\ufb00erentbetweenthetwocategories.\nForunveri\ufb01edusers,thereisscarcedi\ufb00erentiationintermsofdominantemotions.How-\never, in the case of veri\ufb01ed users, the emotional response changes signi\ufb01cantly, with the\nemotionalvalencethattendstoradicalizetowardhighlypositiveorhighlynegativelevels,\nrespectively(seeFig. 4),similarlytowhatweobservedin 3.\nBertanietal. EPJDataScience           (2024) 13:20 Page13of16\n5 Discussion\nAreonline\u2018emotions\u2019animportantsourceofsocialcognition,whichismodulatedbythe\nstatusandbyotherspeci\ufb01ccharacteristicsofthesubjects?Ouranalysisshowsthatthisis\nthecase,especiallywhenthetopicofthediscussioniscontentious.Aswehaveseen,acon-tentioustopicsuchasvaccinationelicitsstrongerandmorenuancedemotionalresponsesthan baseline conversations. Moreover, users who are characterized by a speci\ufb01c status,that of veri\ufb01ed users, have an emotional response whose valence is more positive thanthat of unveri\ufb01ed users. The higher social status related to the veri\ufb01cation, and the con-sequentincentivetoreputationmanagementandhigheraccountability,impliesthatsuch\nusers feel more pressured toward providing constructive, inspiring emotional responses\nratherthandismissiveandconfrontationalones.Interestingly,thisisthecasewhethertheusersarehumansorbots.However,giventherecentchangesintheconditionsofaccesstoaveri\ufb01edstatusonTwitter,itispossiblethatsuchsocialpressuree\ufb00ectsonemotionalre-sponseswouldnotbefoundunderthenewregime,asveri\ufb01edaccountsnowdonotsignalsocialstatusanylongerbutonlymarkthepurchaseofaspeci\ufb01cservice.\nOntheotherhand,alsopoliticalorientationsofthesourcescitedhaveanimportantim-\nplication for emotional response, and in the case of contentious topics there is a general\ntendency to di\ufb00erentiate the emotional response so that to better di\ufb00erentiate political\nidentity accordingly. However, unlike what could be expected, it is especially in the caseofthedi\ufb00erentiationbetweenrelativelymoderatepositionsthatemotionalresponsesareused as a di\ufb00erentiating factor, whereas in the case of more extreme positions this ef-fectislessmarked,asthepositioningatanextremesideofthepoliticalspectrumalreadysu\ufb03ces to ensure di\ufb00erentiation. This con\ufb01rms that emotional responses may be strate-gicallyin\ufb02atedinonlineinteractionstoconstructaspeci\ufb01c,andoptimallydi\ufb00erentiated,\nsocial identity [ 61]. This may be especially the case for contentious topics, where emo-\ntionalarousalispursuedbythepartiesinvolvedalsotoenhancethesalienceofintentionalsignalsoffalseinformationthatfunctionasdemonstrationsofcommitmenttothegroup\u2019scause,aidinginthestrengtheningofgroupsolidarityagainstopponentsinpolarizingdis-cussions[ 62].\nThisimplies,inparticular,thatuserswithdi\ufb00erentpoliticalorientationsmightdevelop\ntheir own speci\ufb01c \u2018emotion playbook\u2019 which is characteristic of their political position\nandthatmightallowverycomplexformsofsocialcoordinationandevensynchronization\nwiththeirbasethroughasuitable,skillfuluseofemotionalsignals.Apoliticalbasethatisparticularlysensitivetofearordisgustcouldthereforebeactivatedbyanykindofcontentwhere such emotions are balanced with others in a certain, characteristic proportion. Itis possible that this kind of strategy has been already experimented by populist leadersworldwide, where the emphasis on the emotional component often overrides that of thespeci\ufb01ccontent[ 63].Thisiscertainlyatopicthatdeservesfurtheranalysisandthatcould\nproveofgreatimportanceinunderstandingfuturesocialresponsestomajorcontentious\ntopicsandevents.\nSupplementaryinformation\nSupplementaryinformation accompaniesthispaperat https://doi.org/10.1140/epjds/s13688-024-00452-7 .\nAdditional\ufb01le1 .(PDF6.5MB)\nBertanietal. EPJDataScience           (2024) 13:20 Page14of16\nAcknowledgements\nWewanttothanktheTwitterCOVID-19workinggroupforprovidingusthedatastream.Wealsowouldliketo\nacknowledgeSaraTonelliandElisaLeornardelliforusefuldiscussions.\nFunding\nNotapplicable.\nDataavailability\nDataanalyzedinthisworkareavailablefromthecorrespondingauthoruponreasonablerequest.\nDeclarations\nCompetinginterestsTheauthorsdeclarethattheyhavenocompetinginterests.\nAuthorcontributions\nAB,RGperformednumericalexperimentsandanalyzedthedata.SMdevelopedthealgorithm.PSandMDDdesignedthestudy.AB,PSandMDDwrotethemanuscript.Allauthorsreadandapprovedthe\ufb01nalmanuscript.\nAuthordetails\n1DepartmentofInformationEngineeringandComputerScience,UniversityofTrento,ViaSommarive,9,38123Povo(TN),\nItaly.2CHuBLab,FondazioneBrunoKessler,ViaSommarive,18,38123Povo(TN),Italy.3DigitalHumanities,Fondazione\nBrunoKessler,ViaSommarive,18,38123Povo(TN),Italy.4DiSFiPEQ,UniversityofChieti-Pescara,VialePindaro42,65127\nPescara,Italy.5metaLAB(at)Harvard,42KirklandSt,02138Cambridge,MA,USA.6DepartmentofPhysicsandAstronomy,\nG.Galilei,UniversityofPadua,ViaFrancescoMarzolo8,35131Padua,Italy.\nReceived:13July2023 Accepted:9February2024\nReferences\n1. BehrensF,KretME(2019)Theinterplaybetweenface-to-facecontactandfeedbackoncooperationduringreal-life\ninteractions.JNonverbalBehav43(4):513\u2013528\n2. DroletAL,MorrisMW(2000)Rapportincon\ufb02ictresolution:accountingforhowface-to-facecontactfostersmutual\ncooperationinmixed-motivecon\ufb02icts.JExpSocPsychol36(1):26\u201350\n3. KurzbanR(2001)Thesocialpsychophysicsofcooperation:nonverbalcommunicationinapublicgoodsgame.J\nNonverbalBehav25:241\u2013259\n4. BurgoonJK,WangX,ChenX,PentlandSJ,DunbarNE(2021)Nonverbalbehaviors\u201cspeak\u201drelationalmessagesof\ndominance,trust,andcomposure.FrontPsychol12:624177\n5. JiangJ,DaiB,PengD,ZhuC,LiuL,LuC(2012)Neuralsynchronizationduringface-to-facecommunication.JNeurosci\n32(45):16064\u201316069\n6. FeeseS,ArnrichB,Tr\u00f6sterG,MeyerB,JonasK(2012)Quantifyingbehavioralmimicrybyautomaticdetectionof\nnonverbalcuesfrombodymotion.In:2012internationalconferenceonprivacy,security,riskandtrustand2012internationalconferneceonsocialcomputing.IEEE,LosAlamitos,pp520\u2013525\n7. BakerJ(2019)Theempathicfoundationsofsecuritydilemmade-escalation.PolitPsychol40(6):1251\u201312668. DunbarRIM(1996)Grooming,gossip,andtheevolutionoflanguage\n9. Sari\u00f1ana-Gonz\u00e1lezP,Romero-Mart\u00ednez\u00c1,Moya-AlbiolL(2018)Cooperationbetweenstrangersinface-to-facedyads\nproducesmorecardiovascularactivationthancompetitionorworkingalone.JPsychophysiol\n10. HalpinJ,WilsonC(2022)Howonlineinteractionradicaliseswhilegroupinvolvementrestrains:acasestudyofaction\nzealandiafrom2019to2021.PolitSci74(1):18\u201333\n11. AntheunisML,SchoutenAP,ValkenburgPM,PeterJ(2012)Interactiveuncertaintyreductionstrategiesandverbal\na\ufb00ectionincomputer-mediatedcommunication.CommunRes39(6):757\u2013780\n12. LidskyLB(2011)Incendiaryspeechandsocialmedia.TexTechLawRev44:14713. CowenA,SauterD,TracyJL,KeltnerD(2019)Mappingthepassions:towardahigh-dimensionaltaxonomyof\nemotionalexperienceandexpression.PsycholSciPublicInterest20(1):69\u201390\n14. LindquistKA,SiegelEH,QuigleyKS,BarrettLF(2013)Thehundred-yearemotionwar:areemotionsnaturalkindsor\npsychologicalconstructions?commentonlench,\ufb02ores,andbench(2011)\n15. GalleseV,KeysersC,RizzolattiG(2004)Aunifyingviewofthebasisofsocialcognition.TrendsCognSci8(9):396\u2013403\n16. GarciaD,KappasA,K\u00fcsterD,SchweitzerF(2016)Thedynamicsofemotionsinonlineinteraction.RSocOpenSci\n3(8):160059\n17. Beneito-MontagutR(2015)Encountersonthesocialweb:everydaylifeandemotionsonline.SociolPerspect\n58(4):537\u2013553\n18. Beneito-MontagutR(2017)Emotions,everydaylife,andthesocialweb:age,gender,andsocialwebengagement\ne\ufb00ectsononlineemotionalexpression.SociolResOnline22(4):87\u2013104\n19. ChmielA,SienkiewiczJ,ThelwallM,PaltoglouG,BuckleyK,KappasA,Ho\u0142ystJA(2011)Collectiveemotionsonline\nandtheirin\ufb02uenceoncommunitylife.PLoSONE6(7):22207\n20. FoxJ,MorelandJJ(2015)Thedarksideofsocialnetworkingsites:anexplorationoftherelationalandpsychological\nstressorsassociatedwithFacebookuseanda\ufb00ordances.ComputHumBehav45:168\u2013176\n21. JahngJ,KralikJD,HwangD-U,JeongJ(2017)Neuraldynamicsoftwoplayerswhenusingnonverbalcuestogauge\nintentionstocooperateduringtheprisoner\u2019sdilemmagame.NeuroImage157:263\u2013274\n22. PhirangeeK,HewittJ(2016)Lovingthisdialogue!!!!:expressingemotionthroughthestrategicmanipulationof\nlimitednon-verbalcuesinonlinelearningenvironments.In:Emotions,technology,andlearning,pp69\u201385\nBertanietal. EPJDataScience           (2024) 13:20 Page15of16\n23. WaltherJB,LohT,GrankaL(2005)Letmecounttheways:theinterchangeofverbalandnonverbalcuesin\ncomputer-mediatedandface-to-facea\ufb03nity.JLangSocPsychol24(1):36\u201365\n24. ShumH-Y,HeX-d,LiD(2018)FromElizatoxiaoice:challengesandopportunitieswithsocialchatbots.FrontInf\nTechnolElectronEng19(1):10\u201326\n25. RiordanMA,KreuzRJ(2010)Emotionencodingandinterpretationincomputer-mediatedcommunication:reasons\nforuse.ComputHumBehav26(6):1667\u20131673\n26. SobkowiczP,SobkowiczA(2012)Two-yearstudyofemotionandcommunicationpatternsinahighlypolarized\npoliticaldiscussionforum.SocSciComputRev30(4):448\u2013469\n27. ZhuQ,WeeksBE,KwakN(2021)Implicationsofonlineincidentalandselectiveexposureforpoliticalemotions:\na\ufb00ectivepolarizationduringelections.NewMediaSoc26(1):450\u2013472\n28. Pr\u00f6llochsN,B\u00e4rD,FeuerriegelS(2021)Emotionsexplaindi\ufb00erencesinthedi\ufb00usionoftruevs.falsesocialmedia\nrumors.SciRep11(1):22721\n29. M\u00f8nstedB,LehmannS(2022)Characterizingpolarizationinonlinevaccinediscourse\u2014alarge-scalestudy.PLoSONE\n17(2):0263746\n30. ChenK,HeZ,ChangR-C,MayJ,LermanK(2023)Angerbreedscontroversy:analyzingcontroversyandemotionson\nreddit.In:Internationalconferenceonsocialcomputing,behavioral-culturalmodelingandpredictionandbehaviorrepresentationinmodelingandsimulation.Springer,Berlin,pp44\u201353\n31. JakobJ,DobbrickT,FreudenthalerR,Ha\ufb00nerP,WesslerH(2023)Isconstructiveengagementonlinealostcause?\nToxicoutrageinonlineusercommentsacrossdemocraticpoliticalsystemsanddiscussionarenas.CommunRes\n50(4):508\u2013531\n32. TomljenovicH,BubicA,ErcegN(2020)Itjustdoesn\u2019tfeelright\u2013therelevanceofemotionsandintuitionforparental\nvaccineconspiracybeliefsandvaccinationuptake.PsycholHealth35(5):538\u2013554\n33. GuoS,HeZ,RaoA,JangE,NanY,MorstatterF,BrantinghamJ,LermanK(2023)Measuringonlineemotionalreactions\ntoo\ufb04ineevents.arXivpreprint. arXiv:2307.10245\n34. GuoS,BurghardtK,RaoA,LermanK(2022)Emotionregulationanddynamicsofmoralconcernsduringtheearly\ncovid-19pandemic.arXivpreprint. arXiv:2203.03608\n35. VempralaN,BhattP,ValechaR,RaoH(2021)EmotionsduringtheCovid-19crisis:ahealthversuseconomyanalysisof\npublicresponses.AmBehavSci65(14):1972\u20131989\n36. WangJ,FanY,PalaciosJ,ChaiY,Guetta-JeanrenaudN,ObradovichN,ZhouC,ZhengS(2022)Globalevidenceof\nexpressedsentimentalterationsduringtheCovid-19pandemic.NatHumBehav6(3):349\u2013358\n37. ZhangX,YangQ,AlbaradeiS,LyuX,AlamroH,SalhiA,MaC,AlshehriM,JaberII,TifrateneFetal(2021)Riseandfallof\ntheglobalconversationandshiftingsentimentsduringtheCovid-19pandemic.HumanitSocSciCommun8(1):1\u201310\n38. EkmanP(1999)Basicemotions.In:Handbookofcognitionandemotion98(45-60),p16\n39. TracyJL,RandlesD(2011)Fourmodelsofbasicemotions:areviewofEkmanandcordaro,izard,levenson,and\npankseppandwatt.EmotRev3(4):397\u2013405\n40. LahnoB(2020)Trustandemotion.In:TheRoutledgehandbookoftrustandphilosophy,pp147\u201315941. PapacharissiZ(2004)Democracyonline:civility,politeness,andthedemocraticpotentialofonlinepolitical\ndiscussiongroups.NewMediaSoc6(2):259\u2013283\n42. AskerD,DinasE(2019)Thinkingfastandfurious:emotionalintensityandopinionpolarizationinonlinemedia.Public\nOpinQ83(3):487\u2013509\n43. NgE(2020)ThepandemicofhateisgivingCovid-19ahelpinghand.AmJTropMedHyg102(6):1158\n44. SaccoPL,GallottiR,PilatiF,CastaldoN,DeDomenicoM(2021)Emergenceofknowledgecommunitiesand\ninformationcentralizationduringtheCovid-19pandemic.SocSciMed285:114215\n45. O\u2019kaneC(2023)Twitteriso\ufb03ciallyendingitsoldveri\ufb01cationprocessonApril1.Togetabluecheckmark,you\u2019llhave\ntopay.https://www.cbsnews.com/news/twitter-blue-check-veri\ufb01cation-ending-new-subscription-april-1-elon-musk/ ,CBS\nNews.[Accessed11-November-2023]\n46. FactCheckM(2020)MediaBiasFactCheck. https://mediabiasfactcheck.com/\n47. GallottiR,ValleF,CastaldoN,SaccoP,DeDomenicoM(2020)Assessingtherisksof\u2018infodemics\u2019inresponseto\nCovid-19epidemics.NatHumBehav4(12):1285\u20131293\n48. FerraraE,VarolO,DavisC,MenczerF,FlamminiA(2016)Theriseofsocialbots.CommunACM59(7):96\u2013104\n49. ShaoC,CiampagliaGL,VarolO,YangK-C,FlamminiA,MenczerF(2018)Thespreadoflow-credibilitycontentby\nsocialbots.NatCommun9(1):1\u20139\n50. StellaM,FerraraE,DeDomenicoM(2018)Botsincreaseexposuretonegativeandin\ufb02ammatorycontentinonline\nsocialsystems.ProcNatlAcadSci115(49):12435\u201312440\n51. ShiW,LiuD,YangJ,ZhangJ,WenS,SuJ(2020)Socialbots\u2019sentimentengagementinhealthemergencies:a\ntopic-basedanalysisoftheCovid-19pandemicdiscussionsonTwitter.IntJEnvironResPublicHealth17(22):8701\n52. AllemJ-P,FerraraE(2018)Couldsocialbotsposeathreattopublichealth?AmJPublHealth108(8):100553. Gonz\u00e1lez-Bail\u00f3nS,DeDomenicoM(2021)Botsarelesscentralthanveri\ufb01edaccountsduringcontentiouspolitical\nevents.ProcNatlAcadSci118(11):2013443118\n54. FerraraE(2017)Disinformationandsocialbotoperationsintherunuptothe2017frenchpresidentialelection.arXiv\npreprint.arXiv:1707.00086\n55. StellaM,CristoforettiM,DeDomenicoM(2019)In\ufb02uenceofaugmentedhumansinonlineinteractionsduringvoting\nevents.PLoSONE14(5):0214210\n56. YangKC,VarolOnur,VarolOnurHuiPM,MenczerF(2020)Scalableandgeneralizablesocialbotdetectionthrough\ndataselection.In:ProceedingsoftheAAAIconferenceonarti\ufb01cialintelligencepp1096\u20131103\n57. MohammadSM,TurneyPD(2013)Crowdsourcingaword-emotionassociationlexicon.ComputIntell29(3):436\u201346558. MohammadSM(2018)Obtainingreliablehumanratingsofvalence,arousal,anddominancefor20,000English\nwords.In:Proceedingsoftheannualconferenceoftheassociationforcomputationallinguistics(ACL),Melbourne,Australia\n59. BaziotisC,PelekisN,DoulkeridisC(2017)Datastoriesatsemeval-2017task4:deeplstmwithattentionfor\nmessage-levelandtopic-basedsentimentanalysis.In:Proceedingsofthe11thinternationalworkshoponSemanticEvaluation(SemEval-2017).Assoc.Comput.Linguistics,Vancouver,pp747\u2013754\nBertanietal. EPJDataScience           (2024) 13:20 Page16of16\n60. WarrinerAB,KupermanV,BrysbaertM(2013)Normsofvalence,arousal,anddominancefor13,915Englishlemmas.\nBehavResMethods45(4):1191\u20131207\n61. GrahamJ,HaidtJ,NosekBA(2009)Liberalsandconservativesrelyondi\ufb00erentsetsofmoralfoundations.JPersSoc\nPsychol96(5):1029\n62. PetersenMB,OsmundsenM,ToobyJ(2021)Theevolutionarypsychologyofcon\ufb02ictandthefunctionsoffalsehood.\nIn:ThepoliticsoftruthinpolarizedAmerica,p131\n63. RicoG,GuinjoanM,AnduizaE(2017)Theemotionalunderpinningsofpopulism:howangerandfeara\ufb00ectpopulist\nattitudes.SwissPolitSciRev23(4):444\u2013461\nPublisher\u2019sNote\nSpringerNatureremainsneutralwithregardtojurisdictionalclaimsinpublishedmapsandinstitutionala\ufb03liations.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Large-scale digital signatures of emotional response to the COVID-19 vaccination campaign", "author": ["A Bertani", "R Gallotti", "S Menini", "P Sacco"], "pub_year": "2024", "venue": "EPJ Data \u2026", "abstract": "The same individuals can express very different emotions in online social media with  respect to face-to-face interactions, partially because of intrinsic limitations of the digital"}, "filled": false, "gsrank": 616, "pub_url": "https://epjds.epj.org/articles/epjdata/abs/2024/01/13688_2024_Article_452/13688_2024_Article_452.html", "author_id": ["xZ0hhTcAAAAJ", "3XaGvR0AAAAJ", "G9bzM4kAAAAJ", "xnwu184AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:PAWB705D_i0J:scholar.google.com/&output=cite&scirp=615&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D610%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=PAWB705D_i0J&ei=dbWsaILdNb_SieoPzJnloAQ&json=", "num_citations": 1, "citedby_url": "/scholar?cites=3314160382095983932&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:PAWB705D_i0J:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://link.springer.com/content/pdf/10.1140/epjds/s13688-024-00452-7.pdf"}}, {"title": "Misinformation, radicalization and hate through the lens of users", "year": "2019", "pdf_data": "UNIVERSIDADE FEDERAL DE MINAS GERAIS Instituto de Ci\u00eancias Exatas da Universidade Federal de Minas Gerais Programa de P\u00f3s-Gradua\u00e7\u00e3o em Ci\u00eancia da Computa\u00e7\u00e3o        Manoel Horta Ribeiro      Desinforma\u00e7\u00e3o, Radicaliza\u00e7\u00e3o e \u00d3dio na Perspectiva dos Usu\u00e1rios                                Belo Horizonte 2019 \nMANOEL HORTA RIBEIRO\nDESINFORMA\u00c7\u00c3O, RADICALIZA\u00c7\u00c3O E \u00d3DIO\nNA PERSPECTIVA DOS USU\u00c1RIOS\nDisserta\u00e7\u00e3o apresentada ao Programa de\nP\u00f3s-Gradua\u00e7\u00e3o em Ci\u00eancia da Computa\u00e7\u00e3o\ndo Instituto de Ci\u00eancias Exatas da Univer-\nsidade Federal de Minas Gerais como req-\nuisito parcial para a obten\u00e7\u00e3o do grau de\nMestre em Ci\u00eancia da Computa\u00e7\u00e3o.\nOrientador: Wagner Meira Jr. Coorientador:\nVirg\u00edlio Augusto Fernandes Almeida\nBelo Horizonte\nJulho de 2019\nMANOEL HORTA RIBEIRO\nMISINFORMATION, RADICALIZATION AND\nHATE THROUGH THE LENS OF USERS\nDissertation presented to the Graduate\nProgram in Computer Science of the Uni-\nversidade Federal de Minas Gerais in par-\ntial ful\ufb01llment of the requirements for the\ndegree of Master in Computer Science.\nAdvisor: Wagner Meira Jr. Coorientador: Virg\u00edlio\nAugusto Fernandes Almeida\nBelo Horizonte\nJuly 2019\n\u00a9 2019, Manoel Horta Ribeiro . \n.   Todos os direitos reservados  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n              Ribeiro , Manoel Horta .  R484m      Misinformation, radicalization and hate through the lens of                users  [manuscrito] / Manoel Horta Ribeiro .  \u00b2  2019.                   90 f. il.; 29 cm.                    Orientador: Wagner Meira J\u00fanior .                   Coorientador: Virg\u00edlio Augusto Fernandes Almeida .                   Disserta\u00e7\u00e3o (mest rado) - Universidade Federal de Minas                Gera is \u00b1 Departamento  de Ci\u00eancia da Computa\u00e7\u00e3o                    Refer\u00eancias: f . 78-90.                    1. Computa\u00e7\u00e3o \u00b1 Teses. 2.  Redes sociais on -line \u00b1 Teses.                3. Desinforma\u00e7\u00e3o  - Teses. 4 . Fake news  \u00b1 Teses . I. Meira                J\u00fanior , Wagner . II. Almeida , Virg\u00edlio Augusto Fernandes . III.                Univer sidade Federal de Minas Gerais,  Instituto de Ci\u00eancias                Exatas, Departamento de Computa\u00e7\u00e3o. IV. T\u00edtulo .  CDU 51 9.6*04 (043)  Ficha Ficha catalogr\u00e1fica elaborada pela bibliotec\u00e1ria Belkiz Inez Rezende \nCosta CRB 6/1510 Universidade Federal de Minas Gerais - ICEx  \n\npara Jessica, com amor\nAcknowledgments\nEm 2013, quando ingressei na gradua\u00e7\u00e3o, pedi uma oportunidade de IC para o Meira,\nagora meu orientador. Durante a gradua\u00e7\u00e3o e o mestrado, ele me ajudou a descobrir\nminha paix\u00e3o por pesquisa, me deu a liberdade para buscar t\u00f3picos que me motivavam,\ne me aconselhou de maneira que me fez crescer como pesquisador e como pessoa. Em\n2017, tive o prazer de come\u00e7ar a trabalhar tamb\u00e9m com o Virg\u00edlio, que me co-orientou.\nEle me ensinou a procurar (e responder) perguntas cient\u00ed\ufb01cas que fazem diferen\u00e7a na\nsociedade, e a enxergar o mundo de maneira multidisciplinar e cosmopolita. O que\naprendi com voc\u00eas vou levar para sempre comigo.\nMeus dias no ICEx foram alegrados pela presen\u00e7a de amigos: Derick, Carlos, Wal-\nter, Rodrigo, Osvaldo, Ewerton, Karen, Roberto, T\u00falio, Emanuel, Evandro, Gabriel,\nJosemar. Obrigado pelas conversas e pela ajuda durante a temida monitoria de AEDS\nIII. No DCC, tamb\u00e9m encontrei pessoas que viriam a ser meus co-autores: gostaria de\nagradecer ao Yuri, ao Pedro Calais e ao Rapha, que acrescentaram muito nos papers\nque originaram essa disserta\u00e7\u00e3o.\nOs funcion\u00e1rios (docentes ou n\u00e3o) do DCC tamb\u00e9m n\u00e3o poderiam deixar de ser\nmencionados. Recebi todo o apoio que podia esperar da secretaria da p\u00f3s do DCC. No\ns\u00e9timo andar, n\u00e3o s\u00f3 roubei caf\u00e9 dos professores como \ufb01z amizades e recebi conselhos.\nQueria agradecer em especial ao Fabr\u00edcio e ao Renato Assun\u00e7\u00e3o pelos bate-papos.\nNos paragr\u00e1fos abaixo, aproveito para agradecer pessoas com contribui\u00e7\u00f5es menos\ndiretas para esse trabalho, por\u00e9m n\u00e3o menos importantes. Meus amigos de Col\u00e9gio\nSanto Ant\u00f4nio e de joguinho me acompanharam durante toda essa jornada, e sem\nGansos nem Meusba Clan, ela seria bem menos divertida. Meus amigos de gradua\u00e7\u00e3o\ntamb\u00e9m sempre estiveram presentes: Nini, Araroba, Nanato e Vivi.\nLast (European) summer, I had the opportunity to do a research internship in\nSwitzerland at EPFL. There, Gli, Tizi, Ramtin, Kiriusha, Luci and Bob taught me a\nlot about doing research (and about the internal workings of orange juice machines).\nLooking forward to join (most of) you guys soon. Also, I would like to thank the folks\nat iDrama, particularly \"Jimmy\" Blackburn, for all the feedback he gave me.\nTamb\u00e9m tive a oportunidade de trabalhar em um projeto interdisciplinar no HC\ndurante boa parte do mestrado, o CODE. Em reuni\u00f5es semanais, aprendi com Paulo,\nGabriela, J\u00e9ssica, Derick (de novo), Milton, meu pai e meu irm\u00e3o, um pouco mais\nsobre as di\ufb01culdades do trabalho interdisciplinar e de trabalhar com m\u00e9dicos em geral\n(hahaha).\nE por falar em fam\u00edlia, sem o apoio dela, nada disso seria poss\u00edvel, j\u00e1 que eles v\u00eam\nme apoiando desde quando eu era uma crian\u00e7a que \"perguntava demais\" na escola. S\u00e3o\nminhas refer\u00eancias como pesquisadores e como pessoas; e compartilharam comigo as\nminhas aventuras nos con\ufb01ns da internet em jantares animados. Minha m\u00e3e esteve do\nmeu lado nas horas mais complicadas da minha vida. Ela foi minha con\ufb01dente, minha\namiga, e me deu todo o carinho que um \ufb01lho pode precisar (muitas vezes com um\nsuquinho de laranja). Obrigado por me entender (mesmo eu sendo t\u00e3o bobo). Meu pai\ncolocou algum ju\u00edzo em mim e me ajudou a sonhar meu futuro pessoal e pro\ufb01ssional.\nTenho muita sorte de ter um pai que sempre cativou o melhor em mim. Quero ser\num cinquent\u00e3o bacana igual voc\u00ea (apesar de que provavelmente serei mais careca).\nMeu irm\u00e3o foi meu companheiro de cursos online, de projetos paralelos ligeiramente\nmegaloman\u00edacos, e de discuss\u00f5es sobre assunto dos quais ele claramente sabia mais\ndo que eu. Minhas cadelinhas, Laika, Cacau e Jasmim me acompanharam durante\nincont\u00e1veis tardes de trabalho, e voltinhas no quarteir\u00e3o para descansar um pouco.\nAmo voc\u00eas todos.\nNos \u00faltimos meses, Jessica foi minha companheira de modo deadline ,ed ed i s -\ncuss\u00f5es animadas sobre os t\u00f3picos que eu estudei. Durante a escrita desse texto, voc\u00ea\nfoi minha namorada, minha noiva e minha esposa. Dedico essa disserta\u00e7\u00e3o pra voc\u00ea.\n\u201cThe metaphysicians of Tl\u00f6n do not seek for the truth\nor even for verisimilitude, but rather for the astounding.\u201d\n(Tl\u00f6n, Uqbar, Orbis Tertius (1940))\nResumo\nAp o p u l a r i z a \u00e7 \u00e3 od a sr e d e ss o c i a i sm u d o uad i n \u00e2 m i c ad ec r i a \u00e7 \u00e3 oec o n s u m od ec o n -\nte\u00fado. Barreiras para disseminar textos, imagens e v\u00eddeos tornaram-se signi\ufb01cativa-\nmente menores do que em \u00e9pocas anteriores, capacitando os usu\u00e1rios a criar, com\npoucos recursos, conte\u00fado de impacto e de longo alcance. Neste cen\u00e1rio, a sociedade\ntestemunhou a ampli\ufb01ca\u00e7\u00e3o de fen\u00f4menos como a desinforma\u00e7\u00e3o e discurso de \u00f3dio.\nPesquisas recentes tentam resolver esses problemas estudando conte\u00fado odioso ou falso.\nNo entanto, abordar com robustez esses fen\u00f4menos desta maneira muitas vezes n\u00e3o \u00e9\nvi\u00e1vel. Considere, por exemplo, a tarefa de detectar o discurso de \u00f3dio. Pode haver\ndois textos iguais que, em diferentes contextos (por exemplo, uma letra de rap e o\ndiscurso de um pol\u00edtico) podem ser considerados odiosos ou n\u00e3o. Al\u00e9m disso, pode\nser que, dado um texto dentro de um contexto, dois indiv\u00edduos discordem sobre se o\nconte\u00fado \u00e9 odioso ou n\u00e3o. Esta disserta\u00e7\u00e3o tenta abordar estes problemas tomando\noutra perspectiva: a do usu\u00e1rio. Atrav\u00e9s da perspectiva do usu\u00e1rio, \u00e9 poss\u00edvel abor-\ndar conte\u00fados possivelmente falsos ou odiosos com o contexto circundante: orienta\u00e7\u00e3o\npol\u00edtica, padr\u00f5es de atividade, conex\u00f5es, etc. Al\u00e9m disso, somos capazes de estudar\nfen\u00f4menos mais complexos, como a radicaliza\u00e7\u00e3o de usu\u00e1rios, onde devemos estudar\nas trajet\u00f3rias dos indiv\u00edduos \u2014ou, mais realisticamente, seus tra\u00e7os on-line. Em tr\u00eas\nestudos de caso em redes sociais, n\u00f3s: (i) fornecemos insights sobre como a percep\u00e7\u00e3o\ndo que \u00e9 desinforma\u00e7\u00e3o \u00e9 alterada pela opini\u00e3o pol\u00edtica; (ii) propomos uma metodolo-\ngia para estudar o discurso de \u00f3dio no n\u00edvel do usu\u00e1rio, mostrando que a estrutura de\nrede dos usu\u00e1rios pode melhorar muito a detec\u00e7\u00e3o do fen\u00f4meno; e (iii) caracterizamos\nar a d i c a l i z a \u00e7 \u00e3 od eu s u \u00e1 r i o se mc a n a i sn oY o u T u b ea ol o n g od ot e m p o ,m o s t r a n d ou m a\nmigra\u00e7\u00e3o crescente para canais mais extremos. Cada estudo de caso contribui para\nseu assunto mais espec\u00ed\ufb01co: desinforma\u00e7\u00e3o, discurso de \u00f3dio e radicaliza\u00e7\u00e3o. Em con-\njunto, eles suportam um argumento central: o de que devemos estudar fen\u00f4menos mal\nde\ufb01nidos como desinforma\u00e7\u00e3o e o discurso de \u00f3dio sob a perspectiva dos usu\u00e1rios.\nPalavras-chave: redes sociais, desinforma\u00e7\u00e3o, discurso de \u00f3dio, radicaliza\u00e7\u00e3o\nAbstract\nThe popularization of Online Social Networks has changed the dynamics of content\ncreation and consumption. Barriers to disseminate texts, images and videos became\nsigni\ufb01cantly lower than in earlier times, empowering users to create, with little re-\nsources, content with far-reaching impact. In this setting, society has witnessed an\nampli\ufb01cation in phenomena such as misinformation and hate speech. Recent research\nattempts to address these issues by studying hateful or fake content. Yet, robustly\naddressing these phenomena in this fashion is often not feasible. Consider for exam-\nple the task of detecting hate speech. There can be two exact texts that, in di \ufb00erent\ncontexts (e.g. a rap lyric and a politician\u2019s speech), may be considered to be hateful\nor not. Moreover, there may even be that, given a text within a context, two individ-\nuals disagree on whether the content is hateful or not. This dissertation attempts to\naddress these issues by taking another perspective: that of the user. Through the lens\nof users, it is possible to approach possibly fake or hateful content with surrounding\ncontext: political orientation, activity patterns, connections, etc. Furthermore, we can\nstudy more complex phenomenon, such as user radicalization, where one must study\nthe trajectories of individuals \u2013 or, more realistically, their online traces. In three case\nstudies on social networks, we: (i) provide insight on how the perception of what is\nmisinformation is altered by political opinion; (ii) propose a methodology to study hate\nspeech on a user-level, showing that the network structure of users can improve the\ndetection of the phenomenon; (iii) characterize user radicalization in far-right channels\non YouTube through time, showing a growing migration towards the consumption of\nextreme content in the platform. Each case study contributes to their more speci\ufb01c\nsubject: misinformation, hate speech and user radicalization. Yet, altogether, they\nadvance a central argument: that studying users rather than the content itself is more\nproductive to better understand (and eventually mitigate) ill-de\ufb01ned social phenomena\nsuch as hate speech and fake news.\nKeywords: Social networks, Misinformation, Hate Speech, Radicalization\nList of Figures\n2.1 Network of retweets showing Democrats (blue) and Republicans (red) di-\nvided into two distinct communities. What is the impact of such polariza-\ntion in what is perceived as fake news? . . . . . . . . . . . . . . . . . . . . 23\n2.2 Methodology to collect the: (i)URLs/tweets \ufb02agged as fake news; (ii)\ngeneral tweets that tweeted this URL/tweets; and (iii) general tweets on\npolitics, and then build a dataset that encompasses the polarized reactions\nof users to a URL. We also exemplify how the calculation of the polarization\nof the URL is performed on the right-hand side. This is discussed in detail\nin Section 2.2.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n2.3 Average absolute user polarization for the users in the FN-Related and the\nPolitics dataset. The error bars are the 95% con\ufb01dence intervals calcu-\nlated using bootstrap. The increase in the polarization in the FN-Related\ndataset suggests that the theme of misinformation increases polarization in\nan already polarized topic (politics). . . . . . . . . . . . . . . . . . . . . . 31\n2.4 (i)Average polarization per number of reactions to a URL (quartiles). Er-\nror bars represent the 95% con\ufb01dence interval. (ii)Average p olarization\nper ratio of tweets with the URL containing the misinformation related\nkeywords (quartiles). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n2.5 Domain clouds of #FakeNews-related tweets. Notice the presence of web-\nsites with the same ideology as the users in the polarized groups. This\nindicates that users are reacting to sources they agree with on fake-news\nrelated narratives. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n3.1 Network of 100,386users sampled from Twitter after our di \ufb00usion process.\nRed nodes indicate the proximity of users to those who employed words in\nour lexicon. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.2 Toy examp \u013aeo ft h ed i \ufb00usion process. (i)We begin with the sampled retweet\ngraph Gtextit(ii) We revert the direction of the edges (the way in\ufb02uence\n\ufb02ows), add self loops to every node, and mark the users who employed words\nin our lexicon; (iii) We iteratively update the belief of other nodes. . . . . 42\n3.3 KDEs of the creation dates of user accounts. The white dot indicates the\nmedian and the thicker bar indicates the \ufb01rst and third quartiles. . . . . . 44\n3.4 Average values for several activity-related statistics for hateful users, normal\nusers, users in the neighborhood of those, and suspended/active users. The\navg(interval) was calculated on the 200tweets extracted for each user.\nError bars represent 95% con\ufb01dence intervals. The legend used in this graph\nis kept in the remainder of the chapter. . . . . . . . . . . . . . . . . . . . . 44\n3.5 Boxplots for the distribution of metrics that indicate spammers. Hateful\nusers have slightly lessfollowers per followee, lessURLs per tweet, and less\nhashtags per tweet. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n3.6 Network centrality metrics for hateful and normal users, their neighborhood,\nand suspended/non-suspended users calculated on the sampled graph. . . . 46\n3.7 Average values for the relative occurrence of several categories in Empath .\nNotice that not all Empath categories were analyzed and that the to-be-\nanalyzed categories were chosen before-hand to avoid spurious correlations.\nError bars represent 95% con\ufb01dence intervals. . . . . . . . . . . . . . . . . 47\n3.8 Boxplots for the distribution of sentiment and subjectivity and bad-words\nusage. Suspended users, hateful users and their neighborhood are more\nnegative, and use more bad words than their counterparts. . . . . . . . . . 47\n3.9 Corhort-like depiction of the banning of users. We \ufb01nd that in the period\nafter Twitter\u2019s guideline change the number of bans a day increased 1.5\ntimes, from 6to9.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 9\n4.1 In the top row (a)-(e), for each community and for the control channels,\nwe have the cumulative number of active channels (that posted at least one\nvideo), of videos published, of likes, views and of comments. Recall that the\nnumber of likes and views is obtained at the moment of the data collection.\nIn the bottom row, we have CDFs for engagement metrics, and the CCDF of\nvideos published, zoomed in in the range [40% ,100%] on the y-axis. Notice\nthat for comments, we know only the year when they were published, and\nthus the CDFs granularity is coarser (years rather than seconds). . . . . . 63\n4.2 In (a),t h en u m b e ro fu n i q u ec o m m e n t i n gu s e r sp e ry e a ri nt h et o p\ufb01 g u r e\nand the CDF of comments per user for each one of the communities in the\nbottom \ufb01gure. In (b)\u2014(d)we show two similarity metrics (Jaccard and\nOverlap Coe \ufb03cient) for di \ufb00erent pairs of sets of commenting users across\nthe years. In (b)these pairs are the sets of users of each community in\nsubsequent years. In (c)these pairs are the sets of users of each one of\nthe communities of interest. In (d)these pairs are the sets of users of the\ncommunities compared with the users who commented on control channels.\nNotice that comments are clumped together per year, so here, unlike in\nFig 4.1, 2017 means from 2017 to 2018, and so forth. . . . . . . . . . . . . 65\n4.3 We show how users \"migrate\" towards Alt-right content. For users who\nconsumed only videos in the communities indicated by the labels in the\nrows (Alt-lite or I.D.W., only Alt-lite, only I.D.W. or Control), we show the\nprobability of them becoming consuming Alt-right content. We consider\nthree levels of \"infection\": light (commented on 1 to 2 Alt-right videos),\nmild (3, 5) and severe (6+). Each column tracks users in a di \ufb00erent starting\ndate. Initially, their infection rates are 0 (as they did not consume any Alt-\nright content). As time passes, we show the infection rates in the y-axis,\nfor each of the years, in the x-axis. . . . . . . . . . . . . . . . . . . . . . . 67\n4.4 We show the percentage of users that can be traced back as not-infected\nusers who commented on other communities. Each line represents users\nwho, in a given start date, commented only Alt-lite or I.D.W. content, the\ny-axis shows the percentage of the total Alt-right commenting users they\nwent to become (notice that all lines begin at 0 as users initially did not\nconsume any Alt-right content). . . . . . . . . . . . . . . . . . . . . . . . . 68\n4.5 We show the results for the simulation of random walks for channels (a) and\nvideos (b). The top row shows the chance of the random walker being in\nan Alt-right channel at each step, while the bottom row shows the chance\nof the random walker being in any of the other communities. The di \ufb00erent\ncolumns portray di \ufb00erent starting rules: in any channel, only in channels of\nthe Alt-right, and so forth. . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\nList of Tables\n2.1 General characterization of the data sources. The intersection between the\nPolitics dataset and FN-Related is important as we use it to charac-\nterize the polarization of the users, and consequently of the URLs in the\nFN-Related datasets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n3.1 Number of users in each group. . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.2 Occurrence of the edges between hateful (red) and normal (blue) users,\nand between suspended (lemon) and active (dark yellow)users. Results are\nnormalized w.r.t. to the type of the source node, as in: P(source type !dest\ntype|source type). Notice that the probabilities do not add to 1 in hateful\nand normal users as we don\u2019t present the statistics for non-annotated users. 48\n3.3 Percentage/number of accounts that got suspended up before and after the\nguidelines changed. Notice that accounts may be suspended for reasons\nother than hateful conduct. . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n3.4 Prediction results and standard deviations for the two proposed settings:\ndetecting hateful users and detecting suspended users. The semi-supervised\nnode embedding approach performs better than state-of-the-art supervised\nlearning algorithms in all the assessed criteria, suggesting the bene\ufb01ts of\nexploiting the network structure to detect hateful and suspended users. . 50\n4.1 Top 16YouTube channels with the most views per each community and for\ncontrols. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n4.2 Overview of our dataset. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n4.3 Percentage of edges in-between communities in the channel recommendation\ngraph (normalized per weight). . . . . . . . . . . . . . . . . . . . . . . . . 72\n4.4 Percentage of edges in-between communities in the video recommendation\ngraph (normalized per weight). . . . . . . . . . . . . . . . . . . . . . . . . 72\nContents\nAcknowledgments\nResumo\nAbstract\nList of Figures\nList of Tables\n1I n t r o d u c t i o n 1 8\n2P o l a r i z e d U s e r s a n d M i s i n f o r m a t i o n 2 2\n2.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n2.2 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n2.2.1 Data Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n2.2.2 Estimating User\u2019s Political Polarization . . . . . . . . . . . . . . 28\n2.2.3 Estimating URL\u2019s Political Polarization . . . . . . . . . . . . . . 29\n2.2.4 Domains and Impactful URLs . . . . . . . . . . . . . . . . . . . 30\n2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n2.3.1 Polarization and Tweets . . . . . . . . . . . . . . . . . . . . . . 31\n2.3.2 Polarization and URL Domains . . . . . . . . . . . . . . . . . . 33\n2.3.3 Analyzing Top Reacted URLs . . . . . . . . . . . . . . . . . . . 33\n2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n3H a t e f u l U s e r s o n T w i t t e r 3 7\n3.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n3.1.1 Hateful Users . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n3.1.2 Retweet Graph . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n3.1.3 O \ufb00ensive Language . . . . . . . . . . . . . . . . . . . . . . . . . 40\n3.1.4 Suspended Accounts . . . . . . . . . . . . . . . . . . . . . . . . 40\n3.2 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n3.2.1 Data Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n3.2.2 Choosing the Subsample to Annotate . . . . . . . . . . . . . . . 41\n3.2.3 Annotating the Users . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.3.1 Activity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.3.2 Centrality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n3.3.3 Lexicon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n3.3.4 Connections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n3.3.5 Suspension of Users . . . . . . . . . . . . . . . . . . . . . . . . . 49\n3.3.6 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n4U s e r R a d i c a l i z a t i o n o n Y o u T u b e 5 4\n4.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n4.1.1 Radicalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.1.2 Auditing recommendation systems . . . . . . . . . . . . . . . . 58\n4.1.3 Previous research from/on YouTube . . . . . . . . . . . . . . . . 59\n4.2 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2.1 Data Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n4.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n4.3.1 The Rise of Contrarians . . . . . . . . . . . . . . . . . . . . . . 63\n4.3.2 User Intersection . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.3.3 User Migration . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.3.4 Recommendation Algorithm . . . . . . . . . . . . . . . . . . . . 69\n4.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n5C o n c l u s i o n 7 5\n5.1 Major Themes across the Chapters . . . . . . . . . . . . . . . . . . . . 76\nBibliography 78\n18\nChapter 1\nIntroduction\nIn 1998, a publication by the U.S. Department of Defense (DoD) de\ufb01ned the term\ninformation environment as: the aggregate of individuals, organizations, or sys-\ntems that collect, process, or disseminate information, including the information it-\nself[Shelton, 1998]. At the time, institutions pointed out as key parts of the informa-\ntion environment included industry, academia, government, and commercial networks.\nAlthough ahead of its time, the report did not foresee the role of social media, which\nundoubtedly shapes today\u2019s information environment [Rainie et al., 2017].\nThe information environment in the times of social networks is very di \ufb00erent\nfrom the picture painted by the DoD in 1998. The news ecosystem was deeply trans-\nformed: users increasingly consume more news-pieces and opinion content in social\nmedia [Gottfried and Shearer, 2016]; business models for traditional media organiza-\ntions evolved [Newman, 2011], and their importance as gatekeepers diminished in favor\nof alternative sources [Lianne and Simmonds, 2013]. On darker corners of the inter-\nnet, fringe websites, like 4chan ,a n ds u b r e d d i t s ,l i k e /r/TheDonald ,h a v eg r e a ti n \ufb02 u -\nence over which memes and news are shared in large social networks, such as Twit-\nter [Zannettou et al., 2018b, Zannettou et al., 2018a], and often promote harassment\ncampaigns and hateful narratives [Nagle, 2017]. On social media, in the feeds of web-\nsites like Facebook and Twitter, the information users are exposed to is selected through\nrecommendation algorithms [Liao and Fu, 2013]. These black-boxes, tuned to optimize\nengagement, were accused of separating users from news and opinions they disagreed\nwith [Pariser, 2011].\nOverall, we can identify two troublesome phenomena that were ampli\ufb01ed by this\nnew information environment: the dissemination of hateful content and of misinforma-\ntion (or fake news ). As we discuss later, part of the challenges we propose to approach\nhave to do with the subjectivity of such concepts. Yet, we broadly de\ufb01ne them:\n19\n\u2022Fake news is a recently popularized term which refers to fabricated or excessively\nbiased news created with the intention to manipulate, deceive or (in case of satire)\nentertain users [Tandoc Jr et al., 2018].\n\u2022Hate speech is speech that targets a group, or individuals as members of a\ngroup, causing or intending harm, often as actions beyond the speech itself. It is\noften expressed publicly or directly at members of the group, in a context where\nviolent response is possible [Sellars, 2016].\nAlthough it is often hard to pinpoint exactly what constitutes either of these phe-\nnomena, their societal impact is signi\ufb01cant. Fringe ideologies like White Supremacy got\ntheir voices ampli\ufb01ed through online movements such as the Alt-right [ADL, 2019b],\nmotivating terrorist attacks such as the one in Christchurch, NZ [Mann et al., 2019].\nIn the U.S., during the 2016 presidential election, researchers estimate that the average\nAmerican \"read and remember on the order of one, or perhaps several fake news ar-\nticles\" [Allcott and Gentzkow, 2017]. Worryingly, researchers and the media indicate\nthat these false pieces of information were partially driven by an orchestrated e \ufb00ort to\npromote political turmoil [Ferrara et al., 2016, Zannettou et al., 2019].\nIn this scenario, ways of mitigating the di \ufb00usion of hate speech and fake news are\nclearly necessary. Yet, there are several challenges involved with that task. Moderating\nthis content is hard due to the sheer amount of images and comments produced every\nday by users in OSNs [Schmidt and Wiegand, 2017], due to the inherent friction with\nvalues such as freedom of expression [Rainie et al., 2017], and due to the hardness to de-\ntermine what exactly is fake or what is hateful [Davidson et al., 2017], which may di \ufb00er,\nfor example, in di \ufb00erent cultures [NW et al., 2015]. These di \ufb03culties have been ob-\nstacles for both mass-hired human moderators [Julia Angwin, 2017], and for attempts\nto use automated techniques to characterize and detect these issues. For example,\nhate speech detection models fail to di \ufb00erentiate between harmful speech and o \ufb00ensive\nspeech [Davidson et al., 2017], and several fake news detection models capture only\nstylistic cues, often not su \ufb03cient to tell whether something is fake [Shu et al., 2017].\nMoreover, the dissemination of such information happens in an \"adversarial\" environ-\nment, in which agents may be spreading content to further a certain political agenda,\nwhich means that borderline cases will be exploited [Zannettou et al., 2019]. These\nchallenges, which are largely shared in the task of tackling both hate speech and mis-\ninformation, make it logical to study these phenomena together. We argue that the\ndevelopment of methods to characterize and detect hate speech will further our under-\nstanding of fake-news (and vice-versa), as a big part of the challenge with dealing with\nboth these phenomena is to deal with the elusiveness of their de\ufb01nition.\n20\nIn this dissertation, we propose automated methodologies to characterize and\ndetect hate speech and fake news by aggregating data at the user level .W e a r g u e\nthat: (i)these two steps \u2014characterization and detection\u2014 are essential parts in the\nlarger task of mitigating these phenomena; and (ii)adopting a user-centric perspective\n\u2014one which considers users as the central unit of study\u2014 allows to better address\nthe aforementioned challenges. Throughout the chapters, we show examples of how\nfocusing on users allowed us to better understand the nuances of these ill-de\ufb01ned but\nhigh-impact social phenomena; to develop detection methods better suited for the real\nworld; and, lastly, to study more complicated processes, such as the alleged radicaliza-\ntion of users which is happening on YouTube [Lewis, 2018] \u2014amidst plenty of hateful\nand fake content.\nBefore further describing the achievements of this work, we use hate speech de-\ntection as a motivating example for our user-centric approach. Consider the tweet:\nTimesup, yall getting w should have happened long ago .W h i c h w a s i n r e p l y\nto another tweet that mentioned the holocaust. Although the tweet, whose author\u2019s\npro\ufb01le contained white-supremacy imagery, incited violence, it is hard to conceive how\nthis could be detected as hateful with only textual features. Furthermore, the lack\nof hate-related words makes it di \ufb03cult for this kind of tweet to even be sampled in\ntext-based approaches.\nFortunately, the data in posts, tweets or messages are not the only signals we\nmay use to study hate speech in OSNs. Most often, these signals are linked to a pro\ufb01le\nrepresenting a person or organization. Considering this pro\ufb01le, we could use plenty of\nother information to try to determine if the user is engaging in hateful behavior: other\ntweets, their network of friends and retweets, their activity patterns. The case can be\nmade that this wider context is sometimes needed to de\ufb01ne hate speech, such as in the\nexample, where the abuse was made clear by the neo-nazi signs in the user\u2019s pro\ufb01le.\nThis motivates our user-centered approach, which is able to take into consid-\neration all this extra information. Characterizing and detecting hateful users shares\nmuch of the bene\ufb01ts of detecting hateful content and presents plenty of opportunities\nto explore a richer feature space. Furthermore, on a practical hate speech guideline\nenforcement process, containing humans in the loop, its is natural that content needs\nto be surrounded with user context1.\n1This is present, for example, in YouTube\u2019s [Google, 2019] and Twitter\u2019s [Twitter, 2019] hateful\nconduct guidelines. To quote directly from Twitter Rules: Some Tweets may appear to be hateful when\nviewed in isolation, but may not be when viewed in the context of a larger conversation. For example,\nmembers of a protected category may refer to each other using terms that are typically considered as\nslurs. When used consensually, the intent behind these terms is not abusive, but a means to reclaim\nterms that were historically used to demean individuals.\n21\nThe aforementioned example inspired one of the three case studies we present in\nthe following chapters \u2014one case study per chapter. They go as follows.\n\u2022In Chapter 2 we explore the connections between political polarization and the\nspread of fake news. By monitoring URLs and tweets associated with fake news\nrelated hashtags and keywords, we \ufb01nd the concept of \"fake\" to be highly associ-\nated with political polarization, and that users employ terms such as fake news to\nrefer to content that they particularly disagree with. This touches on the already\nmentioned topic of the ill-de\ufb01nition of what is hate or fake.\n\u2022In Chapter 3 we explore \u2014as mentioned in the motivating example\u2014 how it may\nbe helpful to characterize and detect hateful users ,r a t h e rt h a nh a t e f u l content .\nOur characterization sheds light on how hateful users di \ufb00er from normal ones\nwith respect to their user activity patterns, network centrality measurements,\nand the content they produce. We show that these di \ufb00erences can be exploited\nto robustly detect such users.\n\u2022Lastly, in Chapter 4 we study the phenomena of user radicalization on YouTube.\nWe \ufb01nd that users consistently migrate from milder to more extreme content;\nand that a large percentage of users who consume extreme content now, con-\nsumed milder content in the past. We also probe YouTube\u2019s recommendation\nalgorithm, showing that more extreme content was not particularly favored by\nrecommendation systems in the platform.\nEach case study contributes to their more speci\ufb01c subject: misinformation, hate\nspeech and user radicalization. Yet, altogether, they advance a central argument: that\nstudying users rather than content itself is more productive to better understand (and\neventually mitigate) ill-de\ufb01ned social phenomena such as hate speech and fake news.\n22\nChapter 2\nPolarized Users and Misinformation\nTwo phenomena have been increasingly receiving attention due to their\npotential impact on important societal processes [Allcott and Gentzkow, 2017,\nBakshy et al., 2015]: the rapid spread of a growing number of unsubstantiated or\nfalse information online [Vicario et al., 2016], coined as fake news ,a n dt h ei n c r e a s e\nof opinion polarization [Allcott and Gentzkow, 2017]. Previous studies suggest a dual\ninteraction between the two: polarized communities are more susceptible to the dissem-\nination of misinformation [Vicario et al., 2016], and, conversely, misinformation plays\nak e yr o l ei nc r e a t i n gp o l a r i z e dg r o u p s[ Z o l l oe ta l . ,2 0 1 5 ] . A n o t h e rw a yt h e s ep h e n o m -\nena may interact is when users incorrectly classify news as being fake because they\ndisagree with the narrative the news-piece pushes, and not because it reports actual\nfalse or imprecise facts [Lewandowsky et al., 2012]. This behavior creates alternative\nnarratives of what is actually fake, which depends on one\u2019s political ideology, and that,\nultimately, make the line between biased and fake information blurrier.\nIn this chapter, we conduct an initial analysis of the relationships and interactions\nbetween polarized debate and the perception of misinformation on Twitter data. We\nexamine the following research questions:\nRQ1 How is polarization related to information perceived as or related to fake news?\nRQ2 Are users designating content that they disagree with as misinformation?\nWe analyze a dataset composed of tweets of content associated with fake news and\ngeneral tweets about U.S. politics. Our methodology employs a community detection\nmethod designed to estimate the degree of polarization of each user leaning towards the\nDemocratic or Republican parties, as depicted in Figure 2.1. Based on these estimates,\nwe correlate user polarization levels to their interactions with fake news related tweets\n23\nFigure 2.1. Network of retweets showing Democrats (blue) and Republicans\n(red) divided into two distinct communities. What is the impact of such polar-\nization in what is perceived as fake news?\nand external URLs (links to other websites). We analyze how the polarization of\nsuch tweets and URLs are related to their popularity and to the frequency they are\nassociated with the theme of misinformation by users. We also analyze the polarization\ndi\ufb00erences between users who merely discuss politics versus those who engage with\ntweets and URLs labeled by other users as fake news.\nOur analyses show three main \ufb01ndings: (i)Fake news is associated with polar-\nization. Users that associate content with fake news are more polarized, and content\nthat is associated with fake news receives engagement from users of mostly one side of\nthe political spectrum. (ii)Polarized groups cite sources on their side of the political\nspectrum to tag or condemn news and statements given by the other opposite group\nas fake; (iii) Polarized users employ terms such as fake news to refer to content that\nthey particularly disagree with;\nWe discuss the impact of these \ufb01ndings in the ongoing battle against the spread\nof fake news, particularly to detect them. We suggest, for example, that approaches\nbased on crowd-sourcing [Ratkiewicz et al., 2011] may become biased towards political\nideologies \u2014once the narratives on what is fake seems to di \ufb00er among groups with\ndi\ufb00erent ideologies\u2014 and how machine learning models trained with naive data can do\nharm, as the umbrella term designates content with di \ufb00erent characteristics.\n24\n2.1 Background\nOnline social networks have deeply transformed the news ecosys-\ntem [Kumar and Shah, 2018]. In 2016, 62% of adults in the U.S. allegedly\nobtained news from social media, whereas in 2012, only 49% reported seeing\nnews on social media at all [Gottfried and Shearer, 2016]. This popularization\nprocess has drastically changed the way newsrooms function and how news out-\nlets pro\ufb01t. In the last years, traditional news organizations have struggled to\nmaintain a sustainable business model [Newman, 2011, Pew Research, 2018], which\nhas increased the sensationalism of headlines in order to attract clicks from social\nmedia [Chakraborty et al., 2016], and decreased resources and time available for news-\nrooms to conduct research [Mitchell and Page, 2015]. The way individuals consume\nnews also changed. The information to which users are exposed is selected through\nrecommendation algorithms [Liao and Fu, 2013], which may separate users from infor-\nmation (and news) that disagree with their viewpoints [Pariser, 2011]. Moreover, users\nand alternative sources began to signi\ufb01cantly in\ufb02uence the \ufb02ow of information, dimin-\nishing mainstream media importance as gatekeepers [Lianne and Simmonds, 2013].\nIn this dynamic setting, social media platforms have become a fertile terrain for\nthe phenomenon broadly referred to as fake news [Zannettou et al., 2018b]. Although\ntrendy, the term fake news is highly ambiguous, as it may be used to refer to distinct\nphenomena [Lazer et al., 2018, Zannettou et al., 2018b] The term may be used to re-\nfer to: (i)information that is plainly false; (ii)information that intends to mislead\nor in\ufb02uence readers; (iii) information published without proper research or editorial\nprocess; and (iv)information that is biased.\nDi\ufb00erent news sources are often accused of spreading fake news for di \ufb00erent rea-\nsons. Traditional media outlets like CNN, for example, have been accused by right-wing\npoliticians and pundits as being fake news due to bias [Trump, 2017, Fox News, 2019];\nWhereas alternative news sources have been called fake news for, among other reasons,\nlack of accountability and due journalistic process [Shafer, 2017]. We avoid adopt-\ning an explicit de\ufb01nition for the phenomena, as part of the reason of this work is\nto explore the di \ufb00erent perceptions of fake news by users in social media. Some\nsurveys attempt to create di \ufb00erent categories, which leaves less room for ambigu-\nity [Zannettou et al., 2018b, Kumar and Shah, 2018]. Zannettou et al., for example,\ncreate a typology which di \ufb00erentiates between Fabricated News, Propaganda, Conspir-\nacy Theories, Biased News, Clickbaits, and others [Zannettou et al., 2018b].\nRegardless of the particular de\ufb01nition of fake news, news-pieces with the afore-\nmentioned characteristics (false, biased, misleading, lacking editorial process, etc) have\n25\nbeen playing an important role in shaping the public discourse and global politics.\nIn Brazil, WhatsApp was in the center of political debate during the 2018 presiden-\ntial election, as it was widely used to disseminate hoaxes [Tard\u00e1guila et al., 2018].\nIn the U.S., during the 2016 presidential election, researchers estimate that the av-\nerage American \u201cread and remembered on the order of one or perhaps several fake\nnews articles\u201d [Allcott and Gentzkow, 2017]. In India, rumours have motivated a\nseries of mob-lynchings, which, in 2018 alone, were responsible for more than 20\ndeaths [Bengaluru et al., 2018]. Overall, fake news has become an e \ufb00ective mecha-\nnism to in\ufb02uence public opinion and to create discord, possibly to the bene\ufb01t of a\ncertain agenda [Zannettou et al., 2019].\nRecent research has found clever workarounds against the fuzzy de\ufb01nition of the\nphenomena. Grinberg et. al [Grinberg et al., 2019], for example, divided fake news\nsources into three categories which ranged from publishing almost exclusively fabricated\nstories, to websites where the creation of falsehoods was not systematic. Although\ncareful consideration of sub-categories of fake news may be a successful approach for\nresearchers, what is less clear is whether the general public is correctly parsing through\nthis multi-faceted concept. In this setting, a major concern is that the term fake news\nis increasingly used to refer to news perceived as biased. This misuse may even be\na strategy to dismiss opposing narratives. In the Philippines, for instance, President\nDuterte dismissed serious criticism by the news outlet Rappler as fake news [Yap, 2018].\nIn a similar fashion, in Burma, the army has dismissed accusations of Rohingya ethnic\ncleansing as fake [Lloyd Parry, 2017]. The hijacking of the term fake news, if wide-\nspread, would largely contribute to post-factual or post-truth politics, where emotion\nand ideological biases triumph over facts and empirical data [Corner, 2017]. It would\npotentially make fake news even harder to moderate, as the disagreement over what is\nfake news would make the line between content moderation and censorship blurrier.\nThe usage of the term fake news to refer to a narrative one disagrees with does\nnot happen in a vacuum, but in a world stage increasingly marked by political po-\nlarization [Cunha et al., 2018]. In the US, between 2004 and 2014, the number of\nrepublicans that viewed the democratic party very unfavourably doubled (from 21% to\n43%) [Mitchel et al., 2014]. In Europe, national and European parliament seats have\nbecome more polarized than ever, with a rising number of seats being held by far-right\nparties and a decreasing number seats being held by center parties [Groskopf, 2016].\nIn Brazil, the 2018 election was marked by extreme political polarization, raising\nconcerns about the country\u2019s future [Brooks and Boadle, 2018]. In such scenarios, a\ntroubling possibility is that individuals start perceiving any opposing narrative as ex-\ntremely biased, and thus, fake [Habgood-Coote, 2017, Funke, 2018]. This has the po-\n26\nFigure 2.2. Methodology to collect the: (i)URLs/tweets \ufb02agged as fake news;\n(ii) general tweets that tweeted this URL/tweets; and (iii) general tweets on\npolitics, and then build a dataset that encompasses the polarized reactions of users\nto a URL. We also exemplify how the calculation of the polarization of the URL\nis performed on the right-hand side. This is discussed in detail in Section 2.2.3\ntential to poison public debate and lead to the adoption of a decision-making process\ngrounded only in ideology and not in reality. Interestingly, it is important to notice\nthat this increase in polarization has often been associated with Online Social Net-\nworks [Garimella et al., 2017]. The possibilities given by these platforms for users to\n\ufb01lter only content they agree with, as well as algorithms which more often recommend\ncontent which gets engagement from users, have been accused of creating \u201c\ufb01lter bub-\nbles\u201d: tailored information environments where users only \ufb01nd information they agree\nwith [Pariser, 2011].\n2.2 Methods\nWe describe our methodology to collect the data; estimate the political polarization of\nusers, URLs, and Tweets; and how we analyse domains and impactful URLs.\n2.2.1 Data Collection\nOur data collection strategy is shown in Figure 2.2. We study two datasets in conjunc-\ntion, both obtained from Twitter . The \ufb01rst dataset, FN-Related ,w a sb u i l tt om o n i t o r\nnarratives and discussions surrounding fake news. While the second, Politics ,i su s e d\nto infer the political orientation (Republican vs. Democrats) of the users in the \ufb01rst\ndataset. Notice that we collect tweets from all over the world in all cases, as \ufb01ltering\nonly geo-tagged tweets would greatly diminish the amount of data available.\n27\nTo collect the FN-Related dataset, we performed, from May 07 2017 toMay 25\n2017,t w os i m u l t a n e o u sd a t ac o l l e c t i o ne \ufb00orts, using the Stream API (which allows you\nto gather large amounts of data currently being tweeted) and the Search API (which\nallows you to search for tweets mentioning speci\ufb01c keywords, among other parameters),\naccording to the following steps:\n(Step 1) We collect tweets with the following keywords/hashtags from the Stream API:\n{fakenews, #fakenews, fake-news, #fake-news, posttruth, #posttruth,\npost-truth, #post-truth, alternativefact, #alternativefact, alternative-\nfact, #alternative-fact}\nWe then proceed to store the URLs being mentioned, whether it is an external\nURL or a URL to another tweet. In the examples below, we store external URL\nin the \ufb01rst case and the tweet being commented in the second case:\nURL Trump Schools CNN Reporter in 1990 {URL} #fakenews\nTweet (RT) This is an abuse of his office #posttruth {Tweet}\n(Step 2) In the second step, we use the stored external URLs and tweets. We use Twitter\u2019s\nSearch API to extract tweets that include relevant URLs and tweets stored, and\nmetadata about the users who tweeted about them. Unlike the Stream API,\nthese tweets may have been tweeted a couple of hours or even days ago. For\ninstance, in the aforementioned examples, we could \ufb01nd other users who tweeted\nthe same link of Trump talking to the CNN reporter, or other people who had\nmade a comment on the tweet quoted in the second example. Notice that this\nallows us to capture the URLs being used in di \ufb00erent contexts, as exempli\ufb01ed\nby the two tweets below, which had the same URL. One of them simply shares\na poll from The Hu \ufb03ngton Post, whereas the other one suggests that the poll is\nuntrustworthy:\nFake HuffPost sucks, no one believes their polls #fakenews #MAGA {URL}\nOther Canadian views of US hit an all-time low, poll shows, {URL}\nThis data collection process allows us to build a more complete view of the fake\nnews debate on Twitter: we can see both users who are referring to a piece of content\n(i.e. a URL or another tweet) as a potential source of fake news, and users who are\nciting, propagating or interacting with the same content without attaching to it the\nfake news label.\n28\nWe obtained the second dataset, Politics ,b yc o l l e c t i n gt w e e t sa b o u tU S\nPolitics in general from the Twitter Stream API, from August 2016 toMay 2017 .\nThis is what is referred to as the third step in the Figure 2.2. We use keywords\nand hashtags such as {Hillary Clinton, #potus, Donald Trump, White House,\nDemocrats, Republicans ...}. The reason for collecting this dataset was to have\nsu\ufb03cient data to accurately compute the degree of polarization of users from the\nFN-related dataset with respect to their leanings towards Republicans and Democrats.\nThis is explained in detail in Section 2.2.2.\nUsing these data sources, we are able to analyze URLs that co-occurred with\n#FakeNews tags, the associated reactions to this URL in the form of tweets, and the\npolarization of some of the users who tweeted it. This is depicted on the right-hand\nside of Figure 2.2.\nSome remarks on the methodology are: (i)Retweets and quoted tweets (a retweet\nwith some additional text) are considered to be URLs (to other tweets). (ii)Every 15\nminutes we update the URLs being searched in Step 2 with the search according to\nthe more recent results of the Stream API in Step 1 . This is done due to limitations\nin using Twitter Stream and Search APIs.\n2.2.2 Estimating User\u2019s Political Polarization\nWe want to correlate the degree of polarization of Twitter users to each main side in\nUS Politics \u2014Republicans and Democrats with fake news-related tweets. There are\nap l e t h o r ao fm e t h o d sd e s i g n e dt oc l a s s i f yt h ep o l i t i c a ll e a n i n go fs o c i a lm e d i au s e r s ,\nwhich typically group themselves in well-separated communities [Conover et al., 2011,\nWong et al., 2016]. Finding communities on polarized topics is eased by the fact that it\nis usually simple to \ufb01nd seeds \u2014users that are known to belong to a speci\ufb01c community.\nIn the case of Twitter data, we consider that o \ufb03cial pro\ufb01les of politicians and political\nparties are natural seeds that can be fed to a semi-supervised clustering algorithm that\nexpands the seeds to the communities formed around them [Calais Guerra et al., 2011,\nKloumann and Kleinberg, 2014].\nWe assume that the number of communities Kformed around a topic Tis known\nin advance and it is a parameter of our method. To estimate user leanings toward each\nof the Kgroups (K= 2for Democrats, Republicans), we employ a label propagation-\nlike strategy based on random walk with restarts [Tong et al., 2008]: a random walker\ndeparts from each seed and travels in the user-message retweet bipartite graph by\nrandomly choosing an edge to decide which node it should go next. With a probability\n(1 -\u21b5)=0 .85,t h er a n d o mw a l k e rr e s t a r t st h er a n d o mw a l k i n gp r o c e s sf r o mi t so r i g i n a l\n29\nseed. As a consequence, the random walker tends to spend more time inside the cluster\nits seed belongs to [Calais Guerra et al., 2011]. Each node is then assigned to its closest\nseed (i.e., community), as shown in the node colors in the sample of the graph displayed\nin Figure 2.1.\nThe relative proximity of each node to the two sets of seeds yields a probability\nthat this node belongs to each of two communities, and can be interpreted as an\nestimate of his or her political leaning. For instance, if the proximity of node X to\nRepublican seeds is 0.01 and its proximity to Democrat seeds is 0.04, the random-walk\nbased community detection algorithm outputs that node belongs to the Democrat\ncommunity with 80% probability. Note that this model captures that some nodes may\nbe more neutral than others. For more details on the random walk-based community\ndetection algorithm, please refer to [Calais Guerra et al., 2011].\nIn our speci\ufb01c case study there are only two communities, thus we can de\ufb01ne the\npolarization of a user uwith an assigned polarization value vu2[0.5,1.0][[\u00001.0,\u00000.5]:\nPu=8\n<\n:2(\u0000vu+0.5)ifu2D\n2(vu\u00000.5)ifu2R(2.1)\nWhere RandDare the polarized groups of Republicans and Democrats. Notice that\nwe are simply changing the domain of the value assigned by the polarization algorithm\nto a more intuitive one ( [\u00001,1]). We can further de\ufb01ne the absolute user polarization:\nAu=|Pu| (2.2)\n2.2.3 Estimating URL\u2019s Political Polarization\nAnother aspect of the data that needs to be modeled is the polarization of a URL.\nIntuitively, polarized URLs are those which receive engagement from users of only one\nside of the political spectrum. Thus, we de\ufb01ne the degree of polarization of a URL\nbased on the polarization of users that reacted to it. The polarization of a URL ( k),\ngiven two polarized groups of users ( R,D), is de\ufb01ned as:\nPk=1\nnnX\nu2U(k)Pu (2.3)\n30\nGeneral Statistics Shared Users Shared Act. Users\nSource #users #active users #tweets #urls FN-R Politics FN-R Politics\nFN-R 374,191 101,031 833,962 109,397 - 29.22% - 37,61%\nPolitics 4,164,604 247,435 246,103,385 - 2.62% - 15.72% -\nTable 2.1. General characterization of the data sources. The intersection\nbetween the Politics dataset and FN-Related is important as we use it to\ncharacterize the polarization of the users, and consequently of the URLs in the\nFN-Related datasets.\nThis is depicted in the right-hand side of Figure 2.2. We also de\ufb01ne the absolute URL\npolarization:\nAk=|Pk| (2.4)\nRemember that (i)we consider as URLs any links external to Twitter or links to other\ntweets; and (ii)reactions are tweets of all kinds that interact with the URL.\n2.2.4 Domains and Impactful URLs\nAn important part of our analysis is trying to \ufb01nd evidence that users are employing\nfake news related terms to express disagreement, rather than a factual lack of veracity\nin content they tweet about. To do so, we analyze the URL domains mentioned by\neach polarized side in tweets associated with misinformation. We also qualitatively\nanalyze the content of some of the URLs that generated the most signi\ufb01cant reactions.\nTo generate the domains we extract the external URLs mentioned in the tweets\nand then calculate the political polarization of each domain exactly like we do for\nfull URLs. The wordclouds are generated for all the external URLs with absolute\npolarization Akbigger than 0.5,o n ef o re a c hr e s p e c t i v ep o l a r i z e dg r o u p . F o rt h e\nanalysis of the content of the URLs which garnered the most reactions, we randomly\nselect 75of the top 150URLs, including both URLs referring to other tweets as well as\nURLs to external websites. Of those, we have equally sized stratum where Akbelongs\nto intervals [0,0.32],[0.33,0.66]or[0.67,1].W e t h e n a n a l y z e t h e c o n t e n t o f t h e s e t o p\n75 URLs for insights on di \ufb00erent ways the fake news themes may emerge.\n2.3 Results\nWe begin by characterizing the two datasets in terms of tweets, URLs, and users \u2013 as\ndepicted in Table 2.1. Recall that the analysis concerning URLs are all performed using\nthe tweets of the dataset we call FN-Related ;t h ea n a l y s i sc o n c e r n i n gt h ep o l a r i z a t i o n\n31\nFigure 2.3. Average absolute user polarization for the users in the FN-Related\nand the Politics dataset. The error bars are the 95% con\ufb01dence intervals cal-\nculated using bootstrap. The increase in the polarization in the FN-Related\ndataset suggests that the theme of misinformation increases polarization in an\nalready polarized topic (politics).\nof users is performed using the dataset named Politics . The dataset sizes di \ufb00er\nsigni\ufb01cantly, but the overlap amongst them grants us a signi\ufb01cant number of users\nwith which to perform the analysis ( 29.22% of the 374,191users in the FN-Related\ndataset). If we de\ufb01ne the active users in the Politics dataset as the smallest set of\nusers responsible for 80% of the tweets collected, we have that the intersection with\nthe users from the FN-Related dataset grows signi\ufb01cantly, increasing to 15.72% from\nthe original 2.62%.N o t i c e t h a t t h i s o v e r l a p i s i m p o r t a n t b e c a u s e t h e s e u s e r s p r e s e n t\nin both datasets are the ones used to calculate the polarization of URLs. These are\nthe users we know the political orientation of.\n2.3.1 Polarization and Tweets\nWe analyze the di \ufb00erence in polarization of the users in both datasets. Notice that\nin the Politics dataset we inspect the polarization of all users, whereas in the\nFN-Related dataset we only know the polarization of the users who are also present\nin the Politics dataset. Figure 2.3 shows the average polarization in such datasets,\nconsidering all users, active users, and inactive users. A signi\ufb01cant increase in polar-\nization is evident among the users who were associated with URLs that co-occurred\nwith fake news related terms.\nWe were also interested in investigating the relationship between the polarization\nof URLs and the characteristics of the reactions associated with it. This analysis is\nperformed only in the FN-Related dataset. We analyze two aspects: (i)the impact\nof the number of reactions surrounding a URL on its level of polarization, and (ii)\nthe impact of the percentage of reactions which use fake news-related keywords and\nhashtags to the polarization of the URL. Ordering the URLs according to these metrics,\nwe plot the average polarization of each one of its quartiles in Figure 2.4.\n32\n(i)\n(ii)\nFigure 2.4. (i) Average polarization per number of reactions to a URL (quar-\ntiles). Error bars represent the 95% con\ufb01dence interval. (ii)Average polarization\nper ratio of tweets with the URL containing the misinformation related keywords\n(quartiles).\n(a)Democrat-leaning users.\n(b)Republican-leaning users.\nFigure 2.5. Domain clouds of #FakeNews-related tweets. Notice the presence\nof websites with the same ideology as the users in the polarized groups. This\nindicates that users are reacting to sources they agree with on fake-news related\nnarratives.\nWe analyze other tweets and external URLs separately. Figure 2.4 (i) shows that\nthe increase in the number of reactions has a negative impact on the average polariza-\ntion of a URL, which suggests that more reactions mean less polarization. This is a\nnuanced \ufb01nding, due to how we de\ufb01ned polarization. Notice that a URL with heated\ncomments from both Republican and Democrat leaning users would be considered not\nvery polarized here. A polarized URL, on the other hand, would be one where com-\nments were predominantly from users of just one side of the political spectrum. In\npractice, this means that for popular URLs, you will have engagement from users of\nboth sides of the political spectrum, while for less popular ones, mostly from one side.\nFigure 2.4 (ii) shows that polarization increases when URLs are associated with fake\nnews-related keywords. This, along with Figure 2.3, contributes to the hypothesis that\nthe fake news thematic is a polarizing one.\n33\n2.3.2 Polarization and URL Domains\nWe generate a wordcloud as described in Section 2.2.4, and the results can be seen in\nFigure 2.5 (a)for Democrat-leaning users and in Figure 2.5 (b)for Republican-leaning\nusers. Recall that the users in the FN-Related dataset were put in the Democrat-\nRepublican spectrum using the Politics dataset. Using di \ufb00erent media sources de-\ntermined to be trustworthy on both sides of the political spectrum according to the\nPew Research Center [Mitchel et al., 2014], we can see that Democrat-leaning word-\ncloud contains domains related to news sources such as The Washington Post and\nThe New York Times , which are trusted by liberals and distrusted by conservatives.\nSimilarly, the republican-leaning wordcloud contains domains related to news sources\nsuch as Breitbart andFox News , which are trusted by conservatives and distrusted by\nliberals.\nThe wordcloud analysis implies that polarized groups does not directly mention\nsome reports or news stories as fake, but do react to links from sources that they agree\nwith on the fake news theme. It also indicates that sources that members of a certain\npolitical ideology trust can signi\ufb01cantly impact their view of what is fake, as they rely\non their trusted sources, rather than pieces of information from outside sources that\nthey believe to be misinformation or denounced as fake news.\n2.3.3 Analyzing Top Reacted URLs\nAnalyzing the tweets which received the most reactions, and that co-occurred with fake\nnews related keywords, allows us to better understand how such keywords are being\nused. We perform our qualitative analysis by giving and discussing examples of the\ndi\ufb00erent stratum we de\ufb01ned and inspected.\n2.3.3.1 Opposing Narratives as #FakeNews\nAmong the randomly selected URLs, the top reacted external URL in the highly po-\nlarized stratum is a news-piece on Michael Flynn being cleared by the FBI as innocent\nfrom his relationship with Russia [Tacopino, 2017]:\nNew York Post: FBI clears Michael Flynn in probe linking him to Russia\nIt is important to notice that Flynn\u2019s involvement with Donald Trump\u2019s campaign\nmakes this piece of information more favorable to Republican-leaning users. Con\ufb01rming\nthe result obtained with the analysis of the wordclouds in Section 2.3.2, however, the\nresult is polarized towards individuals supporting the Republican party. This suggests\n34\nthat users are mainly dismissing a narrative of other media sources that suggested the\nlink of Flynn with Russia. The terms associated with fake news are thus not being\nemployed to denote that a piece of content itself is fake, but to denote other pieces of\ninformation as false.\n2.3.3.2 Humor or absurdities as #FakeNews\nAnother usage of the term we can \ufb01nd analyzing the most reacted URLs is to refer to\nnews which can be seen as ridicule. One of the most reacted URLs in the less polarized\nstratum Ak2[0,0.32], is about a prisoner who attempted to escape jail dressed as a\nwoman in Honduras [Boult, 2017]:\nTelegraph: Prisoner dressed as woman in failed escape bid\nThis usage, although not necessarily harmful for the political debate, may present a\nchallenge for automated techniques to detect misinformation. This could be particu-\nlarly troublesome if what users tag as fake is used directly as a feature by a model.\n2.3.3.3 Misinformation as #FakeNews\nWe can also \ufb01nd instances of users tagging actual facts or stories as fake. An example\nof such a case is a highly polarized Democrat-leaning news-piece Ak2[0.67,1]pointing\nthe rebuttal of a supposedly fake news story on the murder of DNC sta \ufb00:\nRaw Story: Family blasts right-wing media for spreading fake news story about\nslain DNC sta \ufb00er as Russia scandal deepens\nThis usage is what is often imagined when we think about the conversational usage of\nthe term fake news. Yet, it is important to understand that this is only one among\nmany of the ways it is employed.\n2.4 Discussion\nThis work is a \ufb01rst attempt to observe correlations between political polarization and\nthe spread of misinformation, in particular fake news. To tackle the practical challenge\nof having access to a preclassi\ufb01ed set of fake news articles or tweets, we monitored the\nexternal URLs and tweets associated with fake news related hashtags and keywords.\nWe searched for tweets reacting to these URLs and calculated the polarization of the\nusers who reacted to them using an auxiliary and more general dataset on politics. We\n35\nexamined the association between polarization and fake news by analyzing the impact\nof various factors in fake news related URLs, and users we knew the polarization\nof. We also analyzed the di \ufb00erent sources that are mentioned as \u201cfake\u201d by users and\nqualitatively described di \ufb00erent scenarios where the terminology is applied.\nOur analyses show three main \ufb01ndings:\n\u2022Fake news is associated with polarization. Users that engage with content labeled\nas fake news are more polarized and that content that is labeled more often as\nfake news by users receives engagement from users of mostly one side of the\npolitical spectrum.\n\u2022Users employ terms such as fake news to refer to content that they particularly\ndisagree with. We \ufb01nd examples of such cases in our qualitative analysis of the\ntweets and the URLs, and observe that content labelled as fake news is very\npolarized.\n\u2022Polarized groups cite sources on their side of the political spectrum to tag or con-\ndemn news and statements given by the other opposite group as fake. This was\nshown by the analysis of the wordclouds of the Democrat and Republican-leaning\nusers and the qualitative examination of the contexts where misinformation-\nrelated keywords and hashtags were employed.\nThese \ufb01ndings present challenges for automated ways of detecting fake news. Two\npopular ideas for doing so are crowd-sourced systems (e.g. [Ratkiewicz et al., 2011])\nand machine learning models (e.g. [Conroy et al., 2015]). Both cases involve humans\nlabelling news-pieces as either true or false: In the \ufb01rst case, to take a crowd-sourced\ndecision about the news-piece, and, in the second, to label a dataset with which a ma-\nchine learning model will be trained. Our \ufb01ndings suggest that the political orientation\nof these humans-in-the-loop may have a signi\ufb01cant impact on what they are labelling\nas fake, which should be taken into consideration. One way this could be addressed\nin crowd-sourcing setups would be to ensure that a news-piece was annotated by users\nfrom di \ufb00erent political orientations. In that sense, actual misinformation could be\nfound in the intersection of what users from groups with con\ufb02icting interests consider\nto be fake. In the machine learning scenario, a possible workaround would be to con-\nsider this factor in the labelling process, ensuring that an ideologically diverse group\nof users looked at each news-piece in the training data.\nMore generally, this work also corroborates with the hypothesis that vague ter-\nminologies to denote misinformation are harmful. To avoid blurring the line between\n36\nwhat is perceived as fake and what is perceived as biased, governments and the media\nshould try to adopt speci\ufb01c terminology to di \ufb00erentiate between di \ufb00erent types of con-\ntent, such as click-baits, biased news, hoaxes, and etc. Several of these typologies\nhave been proposed by researchers [Tandoc Jr et al., 2018, Zannettou et al., 2018b,\nKumar and Shah, 2018], and they could address the ambiguity of the term.\nThe major possibilities to expand this work are tied to its limitations. Firstly,\nalthough we obtain evidence that users employ terms such as fake news to refer to\ncontent that they disagree with, we do not calculate to which extent is that so. In\nthat context, a key direction to expand our understanding of the interactions between\nthese two phenomena is to measure to which extent people actually believe that facts\nthey disagree with are fake. A way to do so would be \ufb01nding statements that are\nproven false by fact-checkers and modeling the complex interactions (such as quotes\nand replies) between users in a social network such as Twitter .\nAnother interesting direction has to do with the causal relationship between fake\nnews and political polarization. Although this work shows a correlation between the\ntwo phenomena, the way they interact is not so clear. If there is a decrease in political\npolarization, is there a decrease in fake news? How about the opposite scenario?\nUnderstanding these e \ufb00ects would enable new ways of \ufb01ghting misinformation and\npolitical polarization.\n37\nChapter 3\nHateful Users on Twitter\nThe importance of understanding hate speech in Online Social Networks (OSNs)\nis manifold. Countries such as Germany have strict legislation against the prac-\ntice [Stein, 1986], the presence of such content may pose problems for advertis-\ners [Solon, 2017] and users [Sabatini and Sarracino, 2017], and manually inspecting all\npossibly hateful content in OSNs is unfeasible [Schmidt and Wiegand, 2017]. Further-\nmore, the trade-o \ufb00between banning such behavior from platforms and not censoring\ndissenting opinions is a major societal issue [Rainie et al., 2017].\nThis scenario has motivated work that aims to understand and de-\ntect hateful content [Greevy and Smeaton, 2004, Warner and Hirschberg, 2012,\nBurnap and Williams, 2016], by creating representations for text in OSNs,\ne.g.word2vec [Mikolov et al., 2013], and then classifying them as hateful or not, of-\nten drawing insights on the nature of hateful speech. However, in OSNs, the mean-\ning of such content is often not self-contained, referring, for instance, to some event\nwhich just happened, and the texts are packed with informal language, spelling er-\nrors, special characters and sarcasm [Dhingra et al., 2016, Rilo \ufb00et al., 2013]. Besides\nthat, hate speech itself is highly subjective, reliant on temporal, social and historical\ncontext, and occurs sparsely [Schmidt and Wiegand, 2017]. These problems, although\nobserved, remain unaddressed [Davidson et al., 2017, Magu et al., 2017]. Consider the\ntweet:\nTimesup, yall getting w should have happened long ago\nWhich was in reply to another tweet that mentioned the holocaust. Although the\ntweet, whose author\u2019s pro\ufb01le contained white-supremacy imagery, incited violence, it\nis hard to conceive how this could be detected as hateful with only textual features.\nFurthermore, the lack of hate-related words makes it di \ufb03cult for this kind of tweet to\neven be sampled in text-based approaches.\n38\nFigure 3.1. Network of 100,386 users sampled from Twitter after our di \ufb00usion\nprocess. Red nodes indicate the proximity of users to those who employed words\nin our lexicon.\nFortunately, as we just hinted, the data in posts, tweets or messages are not the\nonly signals we may use to study hate speech in OSNs. Most often, these signals are\nlinked to a pro\ufb01le representing a person or organization. Characterizing and detecting\nhateful users shares much of the bene\ufb01ts of detecting hateful content and presents\nplenty of opportunities to explore a richer feature space. Furthermore, on a practical\nhate speech guideline enforcement process, containing humans in the loop, it is nat-\nural that user pro\ufb01les will be checked, rather than isolated tweets. The case can be\nmade that this wider context is sometimes needed to de\ufb01ne hate speech, such as in the\nexample, where the abuse was made clear by the neo-nazi signs in the user\u2019s pro\ufb01le.\nAnalyzing hate on a user-level rather than content-level enables our characterization\nto explore not only content, but also dimensions such as the user\u2019s activity and con-\nnections. Moreover, it allows us to use the very structure of Twitter\u2019s network in the\ntask of detecting hateful users [Hamilton et al., 2017b].\nIn this chapter, we characterize and detect hateful users on Twitter, which we de-\n\ufb01ne according to Twitter\u2019s hateful conduct guidelines. We collect a dataset of 100,386\nusers along with up to 200tweets for each with a random-walk-based crawler on Twit-\nter\u2019s retweet graph. We identify users that employed words from a set of hate speech\nrelated lexicon and generate a subsample, selecting users that are in di \ufb00erent \u201cdistances\u201d\n39\nto those. The latter are manually annotated as hateful or not through crowdsourcing.\nThe aforementioned distances are real valued numbers obtained through a di \ufb00usion\nprocess in which the users who used the words in the lexicon are the seeds. With this,\nwe create a dataset containing 4,972 manually annotated users, of which 544 were\nlabeled as hateful1.W e a l s o \ufb01 n d t h e u s e r s t h a t h a v e b e e n s u s p e n d e d a f t e r t h e d a t a\ncollection \u2014before and after Twitter\u2019s 18/Dec/17 guideline changes.\nStudying these users, we \ufb01nd di \ufb00erences between the activity patterns of hateful\nand normal users: hateful users tweet more frequently, follow more people each day\nand their accounts are more short-lived and recent While the media stereotypes hateful\nindividuals as \u201clone wolves\u201d [Burke, 2017], we \ufb01nd that hateful users are not in the\nperiphery of the retweet network we sampled. Although they have fewer followers, the\nmedian for several network centrality measures in the retweet network is higher for\nthose users. We also \ufb01nd that these users do not seem to behave like spammers.\nAl e x i c a la n a l y s i su s i n g Empath [Fast et al., 2016] shows that their choice of vo-\ncabulary is particular: words related to hate, anger and politics occur lessoften when\ncompared to their normal counterparts, and words related to masculinity, love and\ncurses occur more often. This is noteworthy, as much of the previous work directly\nemploys hate-related words as a data-collection mechanism.\nWe compare the neighborhood of hateful with the neighborhood of normal users\nin the retweet graph, as well as accounts that have been suspended with those who\nwere not. We argue that these suspended accounts and accounts that retweeted hateful\nusers are also proxies for hateful speech online, and the similar results found in many\nof the analyses performed increase the robustness of our \ufb01ndings.\nWe also compare users who have been banned before and after Twitter\u2019s recent\nguideline change, \ufb01nding an increase in the number of users banned per day, but little\ndi\ufb00erence in terms of their vocabulary, activity and network structure.\nFinally, we \ufb01nd that hateful users and suspended users are very densely connected\nin the retweet network we sampled. Hateful users are 71times more likely to retweet\nother hateful users and suspended users are 11times more likely to retweet other\nsuspended users. This motivates us to pose the problem of detecting hate speech as\nat a s ko fs u p e r v i s e dl e a r n i n go v e rg r a p h s . W ee m p l o yan o d ee m b e d d i n ga l g o r i t h m\nthat creates a low-dimensional representation of nodes in a network to then classify\nit. We demonstrate robust performance to detect both hateful and suspended users\nin such fashion ( 95% AUC and 93% AUC) and show that this approach outperforms\ntraditional state-of-the-art classi\ufb01ers ( 88% AUC and 88% AUC, respectively).\n1We used Crowd\ufb02ower\u2019 crowdsourcing service to annotate users and Twitter\u2019s hateful\nconduct guidelines as a criteria for what we consider to be a hateful user.\n40\n3.1 Background\n3.1.1 Hateful Users\nWe de\ufb01ne \u201chateful user\u201d and \u201chate speech\u201d according to Twitter\u2019s guidelines. For the\npurposes of this chapter, \u201chate speech\u201d is any type of content that \u2018promotes violence\nagainst or directly attack or threaten other people on the basis of race, ethnicity,\nnational origin, sexual orientation, gender, gender identity, religious a \ufb03liation, age,\ndisability, or disease\u201d [Twitter, 2019]. On the other hand, \u201chateful user\u201d is a user that,\naccording to annotators, endorses such type of content.\n3.1.2 Retweet Graph\nThe retweet graph Gis a directed graph G=(V,E)where each node u2Vrepresents\na user in Twitter, and each edge (u1,u2)2Eimplies that the user u1has retweeted\nu2. Previous work suggests that retweets are better than followers to judge users\u2019\nin\ufb02uence [Cha et al., 2010]. As in\ufb02uence \ufb02ows in the opposite direction of retweets, we\ninvert the graph\u2019s edges.\n3.1.3 O \ufb00ensive Language\nWe employ [Waseem et al., 2017] de\ufb01nition of explicit abusive language, which de\ufb01nes\nit as language that is unambiguous in its potential to be abusive, for example, language\nthat contains racial or homophobic slurs . The use of this kind of language doesn\u2019t\nimply hate speech, although there is a clear correlation [Davidson et al., 2017].\n3.1.4 Suspended Accounts\nMost Twitter accounts are suspended due to spam, however, they are harder to reach\nin the retweet graph as they rarely get retweeted. We use Twitter\u2019s API to \ufb01nd the\naccounts that have been suspended among the 100,386collected users, and use these\nas another source for potentially hateful behavior. We collect accounts that have been\nsuspended two months after the data collection, on 12/Dec/2017, and after Twitter\u2019s\nhateful conduct guideline changes, on 14/Jan/2018. The new guidelines are allegedly\nstricter, considering, for instance, o \ufb00-the-platform behavior. Importantly, not all ac-\ncounts are suspended due to hateful conduct. Yet, it is worth noticing that our analyses\nshow that our accounts not present spammer-like behavior, a common reason for sus-\npension.\n41\n3.2 Methods\n3.2.1 Data Collection\nMost previous work on detecting hate speech on Twitter employs a lexicon-\nbased data collection, which involves sampling tweets that contain speci\ufb01c\nwords [Davidson et al., 2017, Waseem and Hovy, 2016], such as wetb*cks orfagg*t .\nHowever, this methodology is biased towards a very direct, textual and o \ufb00en-\nsive hate speech. It presents di \ufb03culties with statements that subtly dissem-\ninate hate with no o \ufb00ensive words, as in \"Who convinced Muslim girls they\nwere pretty?\" [Davidson et al., 2017]; And also with the usage of code words, as\nin the use of the word \"skypes\" , employed to reference jews [Magu et al., 2017,\nKnow Your Meme, 2018]; In this scenario, we propose collecting users rather than\ntweets, relying on lexicon only indirectly ,a n dc o l l e c t i n gt h es t r u c t u r eo ft h e s eu s e r s\nin the social network, which we will later use to characterize and detect hate.\nWe represent the connections among users in Twitter using the retweet net-\nwork [Cha et al., 2010]. Sampling the retweet network is hard as we can only ob-\nserve out-coming edges (due to API limitations), and as it is known that any unbiased\nin-degree estimation is impossible without sampling most of these \u201chidden\u201d edges in\nthe graph [Ribeiro et al., 2012]. Acknowledging this limitation, we employ Ribeiro\net al. Direct Unbiased Random Walk algorithm, which estimates out-degrees distri-\nbution e \ufb03ciently by performing random jumps in an undirected graph it constructs\nonline [Ribeiro et al., 2010]. Fortunately, in the retweet graph the outcoming edges\nof each user represent the other users she \u2014usually [Guerra et al., 2017]\u2014 endorses.\nWith this strategy, we collect a sample of Twitter retweet graph with 100,386users\nand2,286,592retweet edges along with the 200most recent tweets for each users, as\nshown in Figure 3.1. This graph is unbiased w.r.t. the out degree distribution of nodes.\nNotice that this graph is not biased in any way towards hateful users. It is just\na sample of Twitter\u2019s retweet graph with a nice property (out-degree distribution is\npreserved).\n3.2.2 Choosing the Subsample to Annotate\nAs the sampled graph is too large to be annotated entirely, we need to select a sub-\nsample to be annotated. If we choose tweets uniformly at random, we risk having a\nvery insigni\ufb01cant percentage of hate speech in the subsample. On the other hand, if\nwe choose only tweets that use obvious hate speech features, such as o \ufb00ensive racial\n42\nFigure 3.2. Toy examp \u013aeo ft h ed i \ufb00usion process. (i)We begin with the sampled\nretweet graph Gtextit(ii) We revert the direction of the edges (the way in\ufb02uence\n\ufb02ows), add self loops to every node, and mark the users who employed words in\nour lexicon; (iii) We iteratively update the belief of other nodes.\nslurs, we will stumble in the same problems pointed in previous work. We propose a\nmethod between these two extremes. We:\n1.Create a lexicon of words that are mostly used in the context of hate speech. This\nis unlike other work [Davidson et al., 2017] as we do not consider words that are\nemployed in a hateful context but often used in other contexts in a harmless\nway ( e.g. n*gger ); We use 23words such as holohoax ,racial treason and\nwhite genocide ,h a n d p i c k e df r o mH a t e b a s e . o r g[ H a t e b a s e , 2 0 1 8 ] ,a n dA D L \u2019 s\nhate symbol database [ADL, 2018].\n2.Run a di \ufb00usion process on the graph based on DeGroot\u2019s Learning\nModel [Golub and Jackson, 2010], assigning an initial belief p0\ni=1 to each user\nuiwho employed the at least one of the words in the lexicon; This prevents our\nsample from being excessively small or biased towards some vocabulary.\n3.Divide users into 4strata according to their associated beliefs after the di \ufb00usion\nprocess, and perform a strati\ufb01ed sampling, obtaining up to 1500 user per strata.\nWe brie\ufb02y present our di \ufb00usion model, as illustrated in Figure 3.2. Let Abe the\nadjacency matrix of our retweeted graph G=(V,E)where each node u2Vrepresents\nau s e ra n de a c he d g e (u, v)2Erepresents a retweet. We have that A[u, v]=1 ifu\nretweeted v.W ec r e a t eat r a n s i t i o nm a t r i x Tby inverting the edges in A(as in\ufb02uence\n\ufb02ows from the retweeted user to the user who retweeted him or her), adding a self loop\nto each of the nodes and then normalizing each row in Aso it sums to 1. This means\neach user is equally in\ufb02uenced by every user he or she retweets.\nWe then associate a belief p0\ni=1to every user who employed one of the words in\nour lexicon, and p0\ni=0 to all who did not. Lastly, we create new beliefs ptusing the\nupdate rule: pt=Tpt\u00001.A l lt h eb e l i e f s pt\niconverge to the same value as t!1,t h u s\nwe run the di \ufb00usion process with t=2.W i t h t h i s r e a l v a l u e ( p2\ni2[0,1])a s s o c i a t e d\nwith each user, we get 4 strata by randomly selecting up to 1500 users with piin the\n43\nintervals [0,.25),[.25,.50),[.50,.75)and[.75,1]. This ensures that we annotate users\nthat did not employ any of the words in our lexicon, yet have a high potential to be\nhateful due to homophily.\n3.2.3 Annotating the Users\nWe annotate 4,972users as hateful or not using CrowdFlower , a crowdsourcing service.\nThe annotators were given the de\ufb01nition of hateful conduct according to Twitter\u2019s\nguidelines and asked, for each user:\nDoes this account endorse content that is humiliating, derogatory or insult-\ning towards some group of individuals (gender, religion, race) or support\nnarratives associated with hate groups (white genocide, holocaust denial,\nJewish conspiracy, racial superiority)?\nAnnotators were asked to consider the entire pro\ufb01le (limiting the tweets to the\nones collected) rather than individual publications or isolate words and were given\nexamples of terms and codewords in ADL\u2019s hate symbol database. Each user pro\ufb01le\nwas independently annotated by 3annotators, and, if there was disagreement, up to\n5annotators. In the end, 544hateful users and 4,427normal ones were identi\ufb01ed by\nthem. The sample of the retweet network was collected between the 1st and 7th of\nOct/17, and annotation began immediately after. We also obtained all users suspended\nup to 12/Dec/17 ( 387)a n du pt o1 4 / J a n / 1 8( 668).\n3.3 Results\nWe analyze how hateful and normal users di \ufb00er w.r.t. their activity, vocabulary and\nnetwork centrality. We also compare the neighbors of hateful and of normal users,\nand suspended/active users to reinforce our \ufb01ndings, as homophily suggests that the\nneighbors will share a lot of characteristics with annotated users, and as suspended\nusers may have been banned because of hateful conduct. We compare those in pairs as\nthe sampling mechanism for each of the populations is di \ufb00erent. We argue that each\none of these pairs contains a proxy for hateful speech in Twitter, and thus inspecting\nTable 3.1. Number of users in each group.\nHateful Normal Hateful Neigh. Normal Neigh. Banned Active\n544 4427 3471 33564 668 99718\n44\n2006-032007-032008-032009-032010-032011-032012-032013-032014-032015-032016-032017-03Creation Date of Users\nFigure 3.3. KDEs of the creation dates of user accounts. The white dot indicates\nthe median and the thicker bar indicates the \ufb01rst and third quartiles.\n010.02030#statuses/day\n02040#followers/day\n02.04.06.0#followees/day\n010K20K30K#favorites\n050K100Kavg(interval)Hateful UserNormal UserHateful Neigh.Normal Neigh.SuspendedActive\nFigure 3.4. Average values for several activity-related statistics for hateful users,\nnormal users, users in the neighborhood of those, and suspended/active users. The\navg(interval) was calculated on the 200 tweets extracted for each user. Error\nbars represent 95% con\ufb01dence intervals. The legend used in this graph is kept in\nthe remainder of the chapter.\nthe three increases the robustness of our analysis. P-values given are from unequal\nvariances t-tests to compare the averages across distinct populations. When we refer\nto \u201chateful users\u201d, we refer to the ones annotated as hateful. The number of users in\neach of these groups is given in the table bellow:\n3.3.1 Activity\nThe account creation date of users is depicted in Figure 3.3. Hateful users were created\nlater than normal ones (p-value <0.001). A hypothesis for this di \ufb00erence is that hateful\nusers are banned more often due to Twitter\u2019s guidelines infringement. This resonates\nwith existing methods for detecting fake accounts in which using the account\u2019s creation\ndate have been successful [Viswanath et al., 2015]. We obtain similar results w.r.t. the\n1-neighborhood of such users, where the hateful neighbors were also created more\nrecently (p-value <0.001), and also when comparing suspended and active accounts\n45\n(p-value <0.001).\nOther interesting metrics for analysis are the number of tweets, followers, followees\nand favorite tweets a user has, and the interval in seconds between their tweets. We\nshow these statistics in Figure 3.4. We normalize the number of tweets, followers and\nfollowees by the number of days the users have since their account creation date. Our\nresults suggest that hateful users are \u201cpower users\u201d in the sense that they tweet more, in\nshorter intervals, favorite more tweets by other people and follow other users more (p-\nvalues <0.01). The analysis yields similar results when we compare the 1-neighborhood\nof hateful and normal users, and when comparing suspended and active accounts (p-\nvalues <0.01, except for the number of favorites when comparing suspended/active\nusers, and for the average interval, when comparing the neighborhood).\nWe investigate whether users that propagate hate speech are spammers. We\nanalyze metrics that have been used by previous work to detect spammers, such\nas the numbers of URLs per tweet, of hashtags per tweet and of followers per fol-\nlowees [Benevenuto et al., 2010]. The boxplot of these distributions is shown on Fig-\nure 3.5. We \ufb01nd that hateful users use, in average, lesshashtags (p-value <0.001)\nand less URLs (p-value <0.001) per tweet than normal users. The same analysis\nholds if we compare the 1-neighborhood of hateful and non-hateful, or suspended and\nactive users (with p-values <0.05,e x c e p tf o rt h en u m b e ro ff o l l o w e r sp e rf o l l o w e e s ,\nwhere there is no statistical signi\ufb01cance to the t-test). Additionally, we also \ufb01nd that,\nin average, normal users have more followers per followees than hateful ones (p-value\n<0.05), which also happens for their neighborhood (p-value <0.05). This suggests\nthat the hateful and suspended users do not use systematic and programmatic methods\nto deliver their content. Notice that it is not possible to extrapolate this \ufb01nding to\nTwitter in general, as there maybe be hateful users with other behaviors which our data\ncollection methodology does not consider, as we do not speci\ufb01cally look for trending\ntopics or popular hashtags.\n3.3.2 Centrality\nWe analyze di \ufb00erent measures of centrality for users, as depicted in Figure 3.6. The\nmedian hateful user is more central in all measures when compared to their normal\ncounterparts. This is a counter-intuitive \ufb01nding, as hateful crimes have long been as-\nsociated with \u201clone wolves\u201d, and anti-social people [Burke, 2017]. We observe similar\nresults when comparing the median eigenvector centrality of the neighbors of hateful\nand normal users, as well as suspended and active users. In the latter pair, suspended\nusers also have higher median out degree. When analyzing the average for such statis-\n46\n010.02030#followers/followees\n00.51.01.5#URLs/tweet\n00.51.01.5hashtags/tweet\nFigure 3.5. Boxplots for the distribution of metrics that indicate spammers.\nHateful users have slightly less followers per followee, less URLs per tweet, and\nless hashtags per tweet.\n010K20Kmedian(betweenness)05e-081e-07median(eigenvector)05e-050.0001median(out degree)\n050K100Kavg(betweenness)00.00020.0004avg(eigenvector)00.00020.0004avg(out degree)\nFigure 3.6. Network centrality metrics for hateful and normal users, their neigh-\nborhood, and suspended/non-suspended users calculated on the sampled graph.\ntics, we observe the average eigenvector centrality is higher for the opposite sides of\nthe previous comparisons. This happens because some very in\ufb02uential users distort\nthe value: for example, the 970most central users according to the metric are normal.\nNotice that despite of this, hateful and suspended users have higher average out degree\nthan normal and active users respectively (p-value <0.05).\n3.3.3 Lexicon\nWe characterize users w.r.t. their content with Empath [Fast et al., 2016], as depicted\nin Figure 3.7. Hateful users use lesswords related to hate, anger, shame and terrorism,\nviolence, and sadness when compared to normal users (with p-values <0.001). A\nquestion this raises is how sampling tweets based exclusively in a hate-related lexicon\nbiases the sample of content to be annotated to a very speci\ufb01c type of \u201chate-spreading\u201d\nuser, and reinforces the claims that sarcasm, code-words and very speci\ufb01c slang plays\n47\n00.002Sadness00.002Swearing00.005Independence00.005Pos. Emotions00.001Neg. Emotions00.005Government00.002Love\n00.005Ridicule00.0010.002Masculine00.001Feminine00.0005Violence00.0005Su\u0000ering00.0025Dispute00.002Anger\n00.005Envy00.005Work00.01Politics00.005Terrorism00.001Shame00.002Confusion00.0025HateHateful UserNormal UserHateful Neigh.Normal Neigh.SuspendedActive\nFigure 3.7. Average values for the relative occurrence of several categories in\nEmpath . Notice that not all Empath categories were analyzed and that the to-be-\nanalyzed categories were chosen before-hand to avoid spurious correlations. Error\nbars represent 95% con\ufb01dence intervals.\n00.20.4sentiment0.40.6subjectivity050100bad words\nFigure 3.8. Boxplots for the distribution of sentiment and subjectivity and bad-\nwords usage. Suspended users, hateful users and their neighborhood are more\nnegative, and use more bad words than their counterparts.\na signi\ufb01cant role in de\ufb01ning such users [Davidson et al., 2017, Magu et al., 2017].\nCategories of words more used by hateful users include positive emotions, neg-\native emotions, su \ufb00ering, work, love and swearing (with p-values <0.001), suggest-\ning the use of emotional vocabulary. An interesting direction would be to analyze\nthe sensationalism of their statements, as it has been done in the context of click-\nbaits [Chen et al., 2015]. When we compare the neighborhood of hateful and normal\nusers and suspended vs active users, we obtain very similar results (with p-values\n<0.001 except for when comparing suspended vs. active users usage of anger, ter-\nrorism, sadness, swearing and love). Overall, the non-triviality of the vocabulary of\nthese groups of users reinforces the di \ufb03culties found in the NLP approaches to sample,\nannotate and detect hate speech [Davidson et al., 2017, Magu et al., 2017].\nWe also explore the sentiment in the tweets users write using a corpus based\n48\nTable 3.2. Occurrence of the edges between hateful (red) and normal (blue)\nusers, and between suspended (lemon) and active (dark yellow)users. Results\nare normalized w.r.t. to the type of the source node, as in: P(source type !dest\ntype|source type). Notice that the probabilities do not add to 1 in hateful and\nnormal users as we don\u2019t present the statistics for non-annotated users.\nNode Type ( %)Node Type ( %)\n! 41.50 ! 13.10\n! 15.90 ! 2.86\n! 7.50 ! 92.50\n! 99.35 ! 0.65\napproach, as depicted in Figure 3.8. We \ufb01nd that sentences written by hateful and\nsuspended users are more negative, and are less subjective (p-value <0.001). The\nneighbors of hateful users in the retweet graph are also more negative (p-value <0.001),\nhowever not less subjective. We also analyze the distribution of profanity per tweet\nin hateful and non-hateful users. The latter is obtained by matching all words in\nShutterstock\u2019s \u201cList of Dirty, Naughty, Obscene, and Otherwise Bad Words\u201d2.W e \ufb01 n d\nthat suspended users, hateful users and their neighbors employ more profane words per\ntweet, also con\ufb01rming the results from the analysis with Empath (p-value <0.01).\n3.3.4 Connections\nFinally, we analyze the frequency at which hateful and normal users, as well as sus-\npended and active users, interact within their own group and with each other. Table 3.2\ndepicts the probability of a node of a given type retweeting other types of node. We \ufb01nd\nthat 41% of the retweets of hateful users are to other hateful users, which means that\nthey are 71times more likely to retweet another hateful user, considering the occur-\nrence of hateful users in the graph. We observe a similar phenomenon with suspended\nusers, which have 7%of their retweets redirected towards other suspended users. As\nsuspended users correspond to only 0.68% of the users sampled, this means they are\napproximately 11times more likely to retweet other suspended users. The high den-\nsity of connections among hateful and suspended users suggest strong modularity. We\nexploit this, along with activity and network centrality attributes to robustly detect\nthese users.\n2https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words\n49\nTable 3.3. Percentage/number of accounts that got suspended up before and\nafter the guidelines changed. Notice that accounts may be suspended for reasons\nother than hateful conduct.\nSusp. Accounts Hateful Normal Others\n2017-12-12 9.09%/55 0.32%/14 0.33%/318\n2018-01-14 17.64%/96 0.90%/40 0.55%/532\nFigure 3.9. Corhort-like depiction of the banning of users. We \ufb01nd that in the\nperiod after Twitter\u2019s guideline change the number of bans a day increased 1.5\ntimes, from 6to9.\n3.3.5 Suspension of Users\nTwitter has changed its enforcement of hateful conduct guidelines in 18/Dec/2017. We\nanalyze the di \ufb00erences among accounts that have been suspended two months after\nthe end of the annotation, in 12/Dec/2017 and in 14/Jan/2018.\nThe intersection between these groups and the ones we annotated as hateful or\nnot is shown in Table 3.3. In the \ufb01rst period from the end of the data annotation to\nthe 12/Dec, there were approximately 6.45banned users a day whereas in the second\nperiod there were 9.05. This trend, illustrated in Figure 3.9, suggests an increased\nbanning activity.\nPerforming the lexical analysis we previously applied to compare hateful and nor-\nmal users we do not \ufb01nd statistically signi\ufb01cant di \ufb00erence w.r.t. the averages for users\nbanned before and after the guideline change (except for government-related words,\nwhere p-value <0.05). We also analyze the number of tweets, followers/followees, and\nthe previously mentioned centrality measures, and observe no statistical signi\ufb01cance in\nthe di \ufb00erence between the averages or the distributions (which were compared using\nKS-test). This suggests that Twitter has not changed the type of users banned.\n50\nTable 3.4. Prediction results and standard deviations for the two proposed set-\ntings: detecting hateful users and detecting suspended users. The semi-supervised\nnode embedding approach performs better than state-of-the-art supervised learn-\ning algorithms in all the assessed criteria, suggesting the bene\ufb01ts of exploiting the\nnetwork structure to detect hateful and suspended users.\nHateful/Normal Suspended/Active\nModel Features Accuracy F1-Score AUC Accuracy F1-Score AUC\nGradBoost user+glove 84.6\u00b11.05 2 .0\u00b12.28 8 .4\u00b11.3 81.5\u00b10.64 8 .4\u00b11.18 8 .6\u00b10.1\nglove 84.4\u00b10.55 2 .0\u00b11.38 8 .4\u00b11.3 78.9\u00b10.74 4 .8\u00b10.78 7 .0\u00b10.5\nAdaBoost user+glove 69.1\u00b12.43 7 .6\u00b12.48 5 .5\u00b11.4 70.1\u00b10.13 8 .3\u00b10.98 4 .3\u00b10.5\nglove 69.1\u00b12.53 7 .6\u00b12.48 5 .5\u00b11.4 69.7\u00b11.03 7 .5\u00b10.88 2 .7\u00b10.1\nGraphSage user+glove 90.9\u00b11.16 7 .0\u00b14.19 5 .4\u00b10.284.8\u00b10.35 5 .8\u00b14.09 3 .3\u00b11.4\nglove 90.3\u00b11.96 5 .9\u00b16.29 4 .9\u00b12.6 84.5\u00b11.05 4 .8\u00b11.69 3 .3\u00b11.5\n3.3.6 Prediction\nAs we consider users and their connections in the network, we can use information that\nis not available for models which operate on the granularity level of tweets or comments\nto detect hate speech.\n\u2022Activity/Network: Features such as number of statuses, followers, followees,\nfavorites, and centrality measurements such as betweenness, eigenvector central-\nity and the in/out degree of each node. We refer to these as user .\n\u2022GloVe: We also use spaCy\u2019s o \ufb00-the-shelf 300-dimensional GloVe\u2019s vec-\ntor [Pennington et al., 2014] as features. We average the representation across\nall words in a given tweet, and subsequently, across all tweets a user has. We\nrefer to these as glove .\nUsing these features, we compare experimentally two traditional machine learn-\ning models known to perform very well when the number of instances is not very\nlarge: Gradient Boosted Trees ( GradBoost )a n dA d a p t i v eB o o s t i n g( AdaBoost ); and\nam o d e la i m e ds p e c i \ufb01 c a l l ya tl e a r n i n gi ng r a p h s[ H a m i l t o ne ta l . ,2 0 1 7 a ]( GraphSage ).\nInterestingly, the latter approach is semi-supervised, and allows us to use the neighbor-\nhood of the users we are classifying even though they are not labeled, exploiting the\nmodularity between hateful and suspended users we observed. The algorithm creates\nlow-dimensional embeddings for nodes, given associated features (unlike other node\nembeddings, such as node2vec [Grover and Leskovec, 2016]). Moreover, it is inductive\n- which means we don\u2019t need the entire graph to run it. For additional information on\nnode embeddings methods, refer to [Hamilton et al., 2017b].\n51\nThe GraphSage algorithm creates embeddings for each node given that the nodes\nhave associated features (in our case the GloVe embeddings and activity/network-\ncentrality attributes associated with each user). Instead of generating embeddings for\nall nodes, it learns a function that generate embeddings by sampling and aggregating\nfeatures from a node\u2019s local neighborhood. This strategy exploits the structure of the\ngraph beyond merely using the features of the neighborhood of a given node.\nWe run the algorithms trying to detect both hateful and normal users, as anno-\ntated by the crowdsourcing service, as well as trying to detect which users got banned.\nWe perform a 5-fold cross validation and report the F1-score, the accuracy and the\narea under the ROC curve (AUC) for all instances.\nIn all approaches we accounted for the class imbalance (of approximately 1to10)\nin the loss function. We keep the same ratio of positive/negative classes in both tasks,\nwhich, in practice, means we used the 4981 annotated users in the \ufb01rst setting (where\napproximately 11% were hateful) and, in the second setting, selected 6680 users from\nthe graph, including the 668suspended users, and other 5405 users randomly sampled\nfrom the graph.\nNotice that, as we are dealing with a binary classi\ufb01cation problem, we may control\nthe trade-o \ufb00between speci\ufb01city and sensitivity by varying the positive-class threshold.\nIn this work we simply pick the largest value, and report the resulting AUC score\n\u2014which can be interpreted as the probability of a classi\ufb01er correctly ranking a random\npositive case higher than a random negative case.\nThe results of our experiments are shown in Table 3.4. We \ufb01nd that the node\nembedding approach using the features related to both users and the GloVe embeddings\nyields the best results for all metrics in the two considered scenarios. The Adaptative\nBoosting approach yields good AUC scores, but incorrectly classi\ufb01es many normal\nusers as hateful, which results in a low accuracy and F1-score.\nUsing the features related to users makes little di \ufb00erence in many settings, yield-\ning, for example, exactly the same AUC ,a n dv e r ys i m i l a ra c c u r a c y / F 1 - s c o r ei nt h e\nGradient Boosting models trained with the two sets of parameters. However, the usage\nof the retweet network (in GraphSage) yields promising results, especially because we\nobserve improvements in both the detection of hateful users and of suspended users,\nwhich shows the performance improvement occurs independently of our annotation\nprocess.\n52\n3.4 Discussion\nWe present an approach to characterize and detect hate speech on Twitter at a user-\nlevel granularity. Our methodology di \ufb00ers from previous e \ufb00orts, which focused on\nisolated pieces of content, such as tweets and comments. [Greevy and Smeaton, 2004,\nWarner and Hirschberg, 2012, Burnap and Williams, 2016]. We developed a method-\nology to sample Twitter which consists of obtaining a generic subgraph, \ufb01nding users\nwho employed words in a lexicon of hate-related words and running a di \ufb00usion process\nbased on DeGroot\u2019s learning model to sample for users in the neighborhood of these\nusers. We then used Crowd\ufb02ower , a crowdsourcing service to manually annotate 4,988\nusers, of which 544(11%)w e r ec o n s i d e r e dt ob eh a t e f u l . W ea r g u et h a tt h i sm e t h o d o l -\nogy aids two existing shortcomings of existing work: it allows the researcher to balance\nbetween having a generic sample and a sample biased towards a set of words in a lexi-\ncon, and it provides annotators with realistic context, which is sometimes necessary to\nidentify hateful speech.\nOur \ufb01ndings shed light on how hateful users di \ufb00er from normal ones w.r.t. their\nuser activity patterns, network centrality measurements, and the content they produce.\nWe discover that hateful users have created their accounts more recently and write more\nnegative sentences. They use lexicon associated with categories such as hate, terrorism,\nviolence and anger lessthan normal ones, and categories of words such as love, work\nand masculinity more frequently. We also \ufb01nd that the median hateful user is more\ncentral and that hateful users are densely connected in the retweet network. The latter\n\ufb01nding motivates the use of an inductive graph embedding approach to detect hateful\nusers, which outperforms widely used algorithms such as Gradient Boosted Trees. As\nmoderation of Online Social Networks in many cases analyzes users, characterizing and\ndetecting hate on a user-level granularity is an essential step for creating work\ufb02ows\nwhere humans and machines can interact to ensure OSNs obey legislation, and to\nprovide a better experience for the average user.\nNevertheless, our approach still has limitations that may lead to interesting future\nresearch directions. Firstly, our characterization only considered the behavior of users\non Twitter, and the same scenario in other Online Social Networks such as Instagram\nor Facebook may present di \ufb00erent challenges. Secondly, although classifying hateful\nusers provides contextual clues that are not available when looking only at a piece of\ncontent, it is still a non-trivial task, as hateful speech is subjective, and people can\ndisagree with what is hateful or not. In that sense, an interesting direction would be to\ntry to create mechanisms of consensus, where online communities could help moderate\ntheir content in a more decentralized fashion (like Wikipedia [Shi et al., 2019]). Lastly,\n53\nar e s e a r c hq u e s t i o ni nt h ec o n t e x to fd e t e c t i n gh a t es p e e c ho nau s e r - l e v e lg r a n u l a r i t y\nthat this work fails to address is how much hateful content comes from how many\nusers . This is particularly important as, if we have a Pareto-like distribution where\nmost of the hate is generated by very few users, then analyzing hateful users rather\nthan content becomes even more attractive.\nAn interesting debate which may arise when shifting the focus on hate speech\ndetection from content to users is how this can potentially blur the line between indi-\nviduals and their speech. Twitter, for instance, implied it will consider conduct occur-\nring \u201co \ufb00the platform\u201d in making suspension decisions. In this scenario, approaching\nthe hate speech detection problem as we propose could allow users to be suspended to\n\"contextual\" factors \u2014and not for a speci\ufb01c piece of content he or she wrote. However,\nas mentioned previously, such models can be used as a \ufb01rst step to detect these users,\nwhich then will be assessed by humans or other more speci\ufb01c methods.\nThe broader question this brings is to what extent a \u201cblack-box\u201d model may\nbe used to aid in tasks such as content moderation, where this model may contain\naccidental or intentional bias. These models can be used to moderate Online Social\nNetworks, without the supervision of a human, in which case its bias could be very\ndamaging towards certain groups, even leading to possible suppressions of individual\u2019s\nhuman rights, notably the right to free speech. Another option would be to make\nac l e a rd i s t i n c t i o nb e t w e e nu s i n gt h em o d e lt od e t e c tp o s s i b l yh a t e f u lo ri n a d e q u a t e\ncontent and delegating the task of moderation exclusively to a human. Although there\nare many shades of gray between these two approaches, an important research direction\nis how to make the automated parts of the moderation process fair, accountable and\ntransparent, which is hard to achieve even for content-based approaches.\n54\nChapter 4\nUser Radicalization on YouTube\nOn YouTube, channels that discuss social, political and cultural subjects have \ufb02our-\nished. Among these, one may \ufb01nd individuals such as Jordan Peterson and Joe Rogan,\nassociated with the so-called Intellectual Dark Web (I.D.W.): iconoclastic thinkers,\nacademics and media personalities [Weiss and Winter, 2018], but also openly declared\nwhite nationalists like Richard Spencer and Jared Taylor, which have been broadly\nreferred to as Alt-right [ADL, 2019b].\nThese individuals do not only share the same platform, but often publicly en-\ngage in debates and conversations in the website [Lewis, 2018]. All the previously\nmentioned individuals, for example, are connected by joint video appearances: Jordan\nPeterson was interviewed by Joe Rogan [PowerfulJRE, ], who interviewed YouTuber\nCarl Benjamin [PowerfulJRE, ], who debated Richard Spencer [Andywarski, ], who\nwas in a panel with Jared Taylor in an Alt-right conference [RedIceTV, ]. Accord-\ning to Lewis [Lewis, 2018], this proximity would create \u201cradicalization pathways\u201d for\naudience members and content creators. Anecdotal examples of these journeys are\nplenty, including Roosh V \u2019s content creator trajectory, going from a Pick Up Artist to\nAlt-right supporter [Kutner, 2016, Roosh V, 2016], and Caleb Cain\u2019s testimony of his\nYouTube-driven radicalization [Faraday Speaks, 2019, Roose, 2019].\nThe claim that there is a \u201cradicalization pipeline\u201d on YouTube should be con-\nsidered in the context of decreasing trust in mainstream media and increasing in-\n\ufb02uence of social networks. Across the globe, individuals are skeptical of tradi-\ntional media vehicles and growingly consume news and opinion content on social me-\ndia [Nic et al., 2018, Ingram, 2018]. In this setting, recent research has shown that\nfringe websites (like 4chan )a n ds u b r e d d i t s( l i k e /r/TheDonald )h a v eg r e a ti n \ufb02 u -\nence over which memes [Zannettou et al., 2018a] and news [Zannettou et al., 2017] are\nshared in large social networks, such as Twitter. YouTube is extremely popular, espe-\n55\ncially among children and teenagers [Anderson and Jiang, 2018], and, if the streaming\nwebsite is actually radicalizing individuals, this can push fringe ideologies like white\nsupremacy further into the mainstream [Tufekci, 2018].\nA key problem in dealing with topics like radicalization and hate speech is the lack\nof agreement over what is \u201chateful\u201d or \u201cextreme\u201d [Sellars, 2016]. A work-around this\nissue is to perform community-based analyzes, rather than trying to label what is or is\nnot hateful. For the purpose of this work, we consider three communities that have been\nassociated with user radicalization [Lewis, 2018, Weiss and Winter, 2018, Roose, 2019],\nand that di \ufb00er signi\ufb01cantly in the extremity of their content: the Intellectual Dark\nWeb (I.D.W.), the Alt-lite and the Alt-right. While the I.D.W. discuss controversial\nsubjects like race and I.Q. [Weiss and Winter, 2018], the Alt-right sponsor fringe ideas\nlike that of a white ethnostate [Hankes and Amend, 2018]. Somewhere in the middle,\nindividuals of the Alt-lite deny embracing white supremacist ideology, although they\nconstantly \ufb02irt with concepts associated with it (e.g. the great replacement, globalist\nconspiracies). This community-driven focus allows one to understand where individuals\nconsuming extreme content are coming from and how does YouTube recommendation\nalgorithms lump these communities together.\nPresent Work In this work, we audit whether users are becoming radicalized on\nYouTube, and whether the recommendation algorithm contributes towards this radi-\ncalization. We do so by examining three prominent communities: The Alt-right, the\nIntellectual Dark Web, and the Alt-lite. More speci\ufb01cally, considering Alt-right chan-\nnels as a proxy for extreme content, we ask:\nRQ1 What are the dynamics of the consumption and production of extreme content\non YouTube?\nRQ2 To which extent do users systematically steer towards more extreme content?\nRQ3 Do algorithmic recommendations steer users towards more extreme content?\nWe develop a data collection process where we: (i)obtain a large pool of relevant chan-\nnels from these communities; (ii)obtain metadata and comments for each of the videos\nin the channels; (iii) annotate channels as belonging to several di \ufb00erent communities;\nand(iv)collect YouTube video and channel recommendations. We additionally collect\ntraditional and alternative media channels to employ as control, These e \ufb00orts resulted\nin a dataset with more than 79 million comments in 331,849videos of 350channels,\nand with more than 2 million video and 10 thousand channel recommendations. We\nanalyze this large dataset extensively:\n56\nWe look at the growth of these communities throughout the last decade in terms\nof videos, likes, and views, \ufb01nding a step rise in activity and engagement in the com-\nmunities of interest when compared with the control channels (Sec. 4.3.1). We inspect\nthe intersection of commenting users within the communities, \ufb01nding they increasingly\nshare the same commenting user base (Sec. 4.3.2). Moreover, we \ufb01nd that the intersec-\ntion is not only growing due to new users but that there is signi\ufb01cant user migration\namong the communities being studied. Users that consume only content from the\nI.D.W. or the Alt-lite throughout the years, consistently start to consume Alt-right\ncontent. These users are an expressive fraction of the Alt-right commenting user base.\nInterestingly, although control channels share, on a yearly basis, a signi\ufb01cant number of\nusers with Alt-right channels, we cannot observe signi\ufb01cant user migration from them\nto Alt-right channels (Sec. 4.3.3). Lastly, we take a look at the impact of YouTube\u2019s\nrecommendation algorithms, running simulations on recommendation graphs we con-\nstruct with our data collection. Our analyzes, given the recommender system in the\ntime of the data collection, and without personalization, show that the communities\nare indeed connected by YouTube\u2019s recommendation algorithms, but that it does not\nsigni\ufb01cantly steer users towards the Alt-right (Sec. 4.3.4).\nThis is, to our best knowledge, the \ufb01rst large scale quantitative audit of user\nradicalization on YouTube. We \ufb01nd strong evidence for radicalization among YouTube\nusers, and that YouTube\u2019s algorithm does bind these communities together. However,\nits e\ufb00ect, given our experimental setup, is not as strong as suggested by anecdotal\nevidence. Yet, there are several limitations to our work, especially given that the\nrecommendations obtained are not personalized. We discuss our \ufb01ndings and our lim-\nitations in light of the research questions further in Sec. 4.4. We argue that regardless\nof the in\ufb02uence of the recommender system in the process of radicalizing users, there is\nsigni\ufb01cant evidence that this process is happening, and that appropriate measurements\nshould be taken.\n4.1 Background\nWe discuss three of YouTube\u2019s prominent communities: the Alt-Right, the Alt-lite,\nand the Intellectual Dark Web. We argue that all of them are contrarians ,i nt h es e n s e\nthat they strongly oppose mainstream views or attitudes. According to Nagle, these\ncommunities \ufb02ourished in the wave of \u201canti-PC\u201d culture of the 2010s, where social-\npolitical movements (e.g. the transgender rights movement, the anti-sexual assault\nmovement) were portrayed as hysterical, and their claims, as absurd [Nagle, 2017].\n57\nAccording to the Anti Defamation League [ADL, 2019a], the Alt-Right is a loose\nsegment of the white supremacist movement consisting of individuals who reject main-\nstream conservatism in favor of politics that embrace racist, anti-Semitic and white\nsupremacist ideology. The Alt-right skews younger than other far-right groups, and\nhas a big online presence, particularly on fringe web sites like 4chan, 8chan and certain\ncorners of Reddit.\nThe term Alt-lite was created to di \ufb00erentiate right-wing activists who deny to\nembracing white supremacist ideology. Atkison argues that the Unite the Rally in\nCharlottesville was deeply related to this change, as participants of the rally revealed\nthe movement\u2019s white supremacist leanings and a \ufb03liations [Atkinson, 2018]. Alt-right\nwriter and white supremacist Greg Johnson [ADL, 2019b] describes the di \ufb00erence be-\ntween Alt-right and Alt-lite by the origin of its nationalism: \"The Alt-light is de\ufb01ned\nby civic nationalism as opposed to racial nationalism, which is a de\ufb01ning character-\nistic of the Alt-right\". This distinction was also highlighted in a The New Yorker\narticle [Marantz, 2017]. Yet it is important to point out that the line between the Alt-\nright and the Alt-lite is blurry [ADL, 2019b], this is particularly tricky because many of\nthe Alt-liters are accused of dog-whistling: attenuating their real beliefs to appeal to a\nmore general public and to prevent getting banned [Lopez G., 2019, Joel Kelly, 2017].\nTo address this problem, in this paper we take a very conservative approach to our\nlabeling, naming only the most extreme content creators as Alt-right. This is explained\nin further detail in Sec. 4.2.1.\nThe Intellectual Dark Web (I.D.W.), is a term coined by Eric Ross Weinstein to\nrefer to a particular group of academics and podcast hosts. The neologism was later\npopularized in a New York Times opinion article [Weiss and Winter, 2018], where it\nis employed to describe: \"collection of iconoclastic thinkers, academic renegades and\nmedia personalities who are having a rolling conversation about all sorts of subjects,\n(...) touching on controversial issues such as abortion, biological di \ufb00erences between\nmen and women, identity politics, religion, immigration, etc\" .\nThe group described in the NYT piece includes Sam Harris, Jordan Peterson,\nBen Shapiro, Dave Rubin, and Joe Rogan, and also mentions a website with an\nuno\ufb03cial list of members. Members of the so-called I.D.W have been accused of\nbigotry, including Islamophobia [Beydoun, 2018], transphobia [Lott, 2017] and sex-\nism [Foderaro, 2018]. Moreover, a recent report by Data & Society research institute\nhas claimed these channels are \u201cpathways to radicalization\u201d [Lewis, 2018]: they would\nact as an entry point to more radical channels, such as those in Alt-right. Broadly,\nmembers of this loosely de\ufb01ned movement see these critics as a consequence of dis-\ncussing controversial subjects [Weiss and Winter, 2018] and largely ignored/dismissed\n58\nthe report [The Rubin Report, 2018]. Similarly to what happens between Alt-right and\nAlt-lite, there is also blurry lines between the I.D.W. and the Alt-lite, especially for\nnon-core members, like those listed in the website. Here, again, we take a conservative\napproach, considering borderline cases to belong to the Alt-lite.\n4.1.1 Radicalization\nWe approach this central concept with the de\ufb01nition of McCauley and\nMoskalenko [McCauley and Moskalenko, 2008]: Functionally, political radicalization is\nincreased preparation for and commitment to intergroup con\ufb02ict. Descriptively, radical-\nization means a change in beliefs, feelings, and behaviors in directions that increasingly\njustify intergroup violence and demand sacri\ufb01ce in defense of the ingroup .W eu s et h e\nconsumption of Alt-right content as a proxy for radicalization. We argue this is reason-\nable because the rhetoric preached by the Alt-right has been associated with multiple\nrecent terrorist attacks (e.g. the Christchurch mass shooting [Mann et al., 2019]), and\nbecause it champions ideas associated with intergroup con\ufb02ict (e.g. a white ethnos-\ntate [Hankes and Amend, 2018]). Our conservative strategy when labeling channels\nis of particular importance here: Alt-right channels are closely related to these ideas,\nwhile the Alt-lite and the I.D.W. are given the bene\ufb01t of the doubt.\n4.1.2 Auditing recommendation systems\nAs algorithms play an ever-larger role in our lives, it is increasingly important\nfor researchers and society at large to reverse engineer algorithms input-output\nrelationships [Diakopoulos, 2014]. Previous large scale algorithmic auditing in-\nclude measuring discrimination on AirBnB [Edelman and Luca, 2014], personaliza-\ntion on web search [Hannak et al., 2013] and price discrimination on e-commerce web\nsites [Hannak et al., 2014]. We argue this work is an audit in the sense that it sheds\nlight into a troublesome phenomenon (user radicalization) in a content-sharing social\nenvironment heavily in\ufb02uenced by algorithms (YouTube). Unfortunately, it is impossi-\nble to obtain the entire history of YouTube recommendation, so we must limit algorith-\nmic analyzes to a time slice of a constantly changing black-box. Although comments\nmay give us insight into the past, it is impossible to tease apart the in\ufb02uence of the al-\ngorithm in previous times. Another limitation of our auditing is that we do not account\nfor user personalization. Despite these \ufb02aws, we argue that: (i)our analyzes provide\nanswers to important question related with impactful societal processes that are al-\nlegedly happening on YouTube, and (ii)our framework for auditing user radicalization\n59\ncan be replicated through time, and expanded to handle personalization. Regardless\nof the extent of the contribution of YouTube\u2019s algorithm towards the process of user\nradicalization, understanding this process and \ufb01nding ways to \ufb01ght it is still a timely\nquestion.\n4.1.3 Previous research from/on YouTube\nPrevious work by Google sheds light into some of the high-level technicalities of\nYouTube\u2019s recommender system [Covington et al., 2016, Davidson et al., 2010]. Their\nlatest paper indicates they use feed embeddings for video searches and video his-\ntories into a dense feed-forward neural network [Davidson et al., 2010]. There also\nexists a large body of work studying violent [Giannakopoulos et al., 2010], hate-\nful or extremist [Sureka et al., 2010, Agarwal and Sureka, 2014] and disturbing con-\ntent [Papadamou et al., 2019] on the platform. Much of the existing work fo-\ncuses on creating detection algorithms for these types of content using features\nof the comments, the commenting users and the videos [Agarwal and Sureka, 2014,\nGiannakopoulos et al., 2010]. Notably, Sureka et al. [Sureka et al., 2010] use a seed-\nexpanding methodology to track extremist user communities, which yielded high pre-\ncision in including relevant users. This is somewhat analogous to what we do, although\nwe use YouTube\u2019s recommender system while they use user friends, subscriptions and\nfavourites. Ottoni et al. perform an in-depth textual analysis of 23 channels (13 broadly\nde\ufb01ned as Alt-right), \ufb01nding signi\ufb01cantly di \ufb00erent lexicon and topics across the two\ngroups [Ottoni et al., 2018].\n4.2 Methods\n4.2.1 Data Collection\nWe are interested three communities on YouTube: the Alt-lite, the I.D.W., and the\nAlt-right. Identifying such communities and the channels which belong to them is no\neasy task: the membership of channels to these communities is volatile and fuzzy; and\nthere is disagreement between how members of these communities view themselves,\nand how they are considered by scholars and the media; These particularities make\nour challenges multi-faceted: on one hand, we want to study user radicalization, and\nknow, for example, if users who start watching videos by communities like the I.D.W.\neventually go on to consume Alt-right content. On the other, there is often no clear\nagreement on who belongs to which community.\n60\nTable 4.1. Top16YouTube channels with the most views per each community\nand for controls.\nAlt-right Views Alt-lite Views\n1 James Allsup 62.20M StevenCrowder 727.01M\n2 Black Pigeon Speaks 49.97M Rebel Media 405.12M\n3 ThuleanPerspective 44.55M Paul Joseph Watson 356.37M\n4 Red Ice TV 41.98M MarkDice 333.95M\n5 The Golden One 12.06M Stefan Molyneux 193.29M\n6 AmRenVideos 9.08M hOrnsticles3 144.98M\n7 NeatoBurrito Productions 7.19M MILO 133.03M\n8 The Last Stand 6.52M Styxhexenhammer666 132.17M\n9 MillennialWoes 6.15M OneTruth4Life 111.97M\n10 Mark Collett 5.58M No Bullshit 104.07M\n11 AustralianRealist 5.29M SJWCentral 89.99M\n12 Jean-Fran\u00e7ois Gari\u00e9py 4.80M Computing Forever 86.69M\n13 Prince of Zimbabwe 4.61M The Thinkery 86.43M\n14 The Alternative Hypothesis 4.60M Bearing 81.16M\n15 Matthew North 4.41M RobinHoodUKIP 64.00M\n16 Faith J Goldy 4.07M patcondell 63.67M\nIntellectual Dark Web Views Control Views\n1 PowerfulJRE 1.07B Vox 1.29B\n2 JRE Clips 716.55M Young Turks 1.12B\n3 PragerUniversity 634.77M GQ Magazine 1.09B\n4 SargonofAkkad100 257.76M Vice News 1.06B\n5 The Daily Wire 246.64M WIRED 1.05B\n6 The Rubin Report 206.03M ABC News 973.14M\n7 ReasonTV 137.68M MSNBC 824.96M\n8 JordanPetersonVideos 90.49M RT News 677.68M\n9 Bite-sized Philosophy 62.39M BBC 660.06M\n10 Timcast 40.42M Vanity Fair 639.13M\n11 Owen Benjamin 34.80M The Verge 636.06M\n12 AgatanFoundation 32.92M Glamour Magazine 619.75M\n13 Essential Truth 32.44M Fox News 584.50M\n14 Ben Shapiro 29.99M Business Insider 522.80M\n15 YAFTV 29.61M Next News Network 465.01M\n16 joerogandotnet 24.66M CBS News 452.79M\nDue to these nuances, we devise a careful methodology to (a)collect a large pool\nof relevant channels; (b)collect data and the recommendations given by YouTube for\nthese channels; (c)label these channels according to the communities of interest using\nboth manual input and the data that we collected for each channel.\n61\n(a)For each community, we create a pool of channels as follows. We refer to channels\nobtained in the i-th step as Type ichannels. We do as follows:\n1.We choose a set of seed channels .S e e d s w e r e e x t r a c t e d f r o m t h e I . D . W . u n o \ufb03cial\nwebsite [Anonymous, ], ADL\u2019s report on the Alt-lite/the Alt-right [ADL, 2019b]\nand Data & Society\u2019s report on YouTube Radicalization [Lewis, 2018]. The idea\nis to pick popular channels that are representative of the community we are\ninterested in. Each seed was independently annotated two times and discarded\nin case there was any disagreement.\n2.We choose a set of keywords related to the sub-communities. For each keyword,\nwe use YouTube\u2019s search functionality and consider the \ufb01rst 200results in English.\nWe then add channels that broadly relate in topic to the community in questions.\n3.We iteratively search the related and featured channels collected in steps 1 and\n2, adding relevant channels (as de\ufb01ned in 2). We repeat the procedure for the\nrecently collected channels. This, as well as Step (2), was done by an individual\nwith more than 50hours of watch-time of the communities of interest. Notice\nthat here we are not labeling the channels, but creating a pool of channels to be\nfurther inspected and labeled in subsequent steps.\n(b)For each channel, we collect the number of subscribers and views, and for their\nvideos, all the comments and captions. Video and channel recommendations were\ncollected separately using custom-made crawlers. We collected multiple \"rounds\" of\nrecommendations, 20for channel recommendations and 10for video recommendations.\nEach \"round\" consists of collecting all recommended channels (on the channel web\npage) and all recommended video (on the video web page). To circumvent possible\nlocation bias in the data we collected we used VPNs from 7 di \ufb00erent locations: 3 in\nthe U.S.A, 2 in Canada, 1 in Switzerland and 1 in Brazil. Moreover, channels were\nalways visited at random order, to prevent any biases from arising from session-based\nrecommendations.\n(c)Channel labeling was done in multiple steps. All channels are either seeds ( Type 1 )\nor obtained through YouTube\u2019s recommendation/search engine ( Types 2 and 3) .N o t i c e\nthat Type 1 channels were assigned labels at the time of their collection. For the others,\nwe had 2 researchers annotate them carefully. They both had signi\ufb01cant experience\nwith the communities being studied, and were given the following instructions:\nGiven the set of channels in this table, you should carefully inspect each one\nof them: taking a look at the most popular videos and watching, altogether,\n62\nTable 4.2. Overview of our dataset.\nChannels 350\nVideos 331,849\nComments 79,180,534\nVideo recommendation rounds 19\nVideo recommendations 2,474,044\nChannel recommendation rounds 22\nChannel recommendations 14,283\nat least 5 minutes of content from that channel. Then you should decide if the\nchannel belongs to the Alt-right, the Alt-lite, the Intellectual Dark Web (I.D.W.),\nor whether you think it doesn\u2019t \ufb01t any of the communities. To get a grasp on\nwho belongs to the I.D.W., read [link], and check out the website with some of\nthe alleged members of the group [link]. Yet, we ask you to consider the label\nholistically, including channels that have content from these creators and with a\nsimilar spirit to also belong in this category. To distinguish between the Alt-right\nand the Alt-lite, read [link] and [link]. It is important to stress the di \ufb00erence\nbetween civic nationalism and racial nationalism in that case. Please consider\nthe Alt-right label only to the most extreme content. You are encouraged to\nsearch on the internet for the name of the content creator to help you make your\ndecision.1\nThe annotation process lasted for 3 weeks. In case they disagreed, they had to discuss\nthe cases individually until a conclusion was reached. Interanotator agreement was of\n75.57%. We ended up with 91I.D.W., 114Alt-lite and 88Alt-right channels.\nControls Additionally we collect news-related channels as control channels. These\nwere obtained from the mediabiasfactcheck.com [Check, 2019]. For each media source\nof the categories on the website ( Left, Left-Center, Center, Right-Center, Right )w e\nsearch for its name on YouTube, and consider it if there is a match in the \ufb01rst page\nof results [Check, 2019]. Some of the channels were not considered because they had\ntoo many videos ( 15,000+ ) and we weren\u2019t able to retrieve all their videos (which is\nimportant, because our analysis are temporal). In total, we collect 68channels in that\nway.\nWe summarize the dataset collected in the Tab. 4.2. Data collection was per-\nformed from the 19th to the 30th of May 2019, and the collection of the recommenda-\ntions between May and July 2019.\n1Links were ADL\u2019s report [ADL, 2019b] and the New Yorker\u2019s article on Alt-right/lite\n[Marantz, 2017], the NYT a article on the I.D.W. [Weiss and Winter, 2018], and the I.D.W.\nwebsite [Anonymous, ]\n63\n4.3 Results\n4.3.1 The Rise of Contrarians\nWe present an overview of the channels in the communities of interest, and show results\nabout their growth in the last years. Tab. 4.1 shows the 16most viewed YouTubers\nfor each of the communities and for the controls, and Figure 4.1 shows information on\nthe number of videos published, channels created, likes, views and comments per year,\nas well as several engagement metrics.\nRecent rise in activity. Figs. 4.1(a)\u2014(e) show the rise in channel creation, video\npublishing, likes, views and comments in the last decade. The four latter are growing\nexponentially for all the communities of interest and for the control channels. Notice-\nably, the rise in the number of active channels is much more recent for the communities\nof interest than for control channels, as shown in Fig. 4.1(a). In mid 2015, for example,\nwhile more than 75% of the channels in the control group were already created, only\nslightly more than 50% of Alt-lite and less than 50% of Alt-right and I.D.W. channels\nhad been created. This growth in the communities of interest during 2015 may also\nbe noted in Fig. 4.1(i), which shows the CDF of number comments per videos, and\ncan also be seen between early 2014 and late 2016 in Figs. 4.1(f)\u2014(g), which show\nthe number of likes and views per video, respectively. Notice that the number of likes\nand views is obtained during data collection, and thus, it might be that older videos\n080910111213141516171819050100(a) Active Channels\n080910111213141516171819101K100K(b) Videos Published\n08091011121314151617181910010K1M100M(c) Like CountAlt-rightAlt-liteIntellectual Dark WebControl\n08091011121314151617181910K1M100M10B(d) View Count\n08091011121314151617181910K1M(e) Comment Count\n08091011121314151617181901K2K(f) Likes/Video\n080910111213141516171819050K100K(g) Views/Video\n08091011121314151617181940%60%80%100%(h) CCDF Videos Pub.\n0809101112131415161718190250500750(i) Comments/Video\n0809101112131415161718190.0000.0050.0100.015(j) Comments/View\nFigure 4.1. In the top row (a)-(e), for each community and for the control\nchannels, we have the cumulative number of active channels (that posted at least\none video), of videos published, of likes, views and of comments. Recall that the\nnumber of likes and views is obtained at the moment of the data collection. In\nthe bottom row, we have CDFs for engagement metrics, and the CCDF of videos\npublished, zoomed in in the range [40% ,100%] on the y-axis. Notice that for\ncomments, we know only the year when they were published, and thus the CDFs\ngranularity is coarser (years rather than seconds).\n64\nfrom those channels became popular later. Altogether, our data corroborates with\nthe narrative that these communities gained traction (and forti\ufb01ed) Donald Trump\u2019s\ncampaign during the 2016 presidential elections [Campaigns et al., , Gray, 2015].\nEngagement. Ak e yd i \ufb00erence between the communities of interest and the control\nchannels is the level of engagement with the videos, as portrayed by the number of\nlikes per video comments per video and comments per view, portrayed in Figs. 4.1(f),\n(i), and (j), respectively. For all these metrics, the communities of interest have more\nengagement than the control channels: Although control channels have more views\nper video, as shown in Figs. 4.1(g), these views are less often converted into likes and\ncomments. Notably, Alt-right channels have, since 2017, become the ones with the\nhighest number of comments per view, with nearly 1comment per 50views by 2018.\nDormant Alt-right Channels. Although by 2013, approximately the same number\nof channels of all three communities had been created ( \u21e030), as it can be seen in\nFig. 4.1(a) the number of videos they published by the Alt-right was very low before\n2016. This can be better seen in the CCDF in Fig. 4.1(h): while control and Alt-lite\nchannels had published nearly 40% of their content, the Alt-right had published a bit\nmore than 20%. This is not because the most popular channels did not yet exist: 4 out\nof the 5 current top Alt-right channels (accumulating approximately 150M views) had\nalready been created by 2013. Moreover, it is noteworthy that many of the channels\nnow dedicated to Alt-right content have initial videos related to other subjects. Take\nfor example the channel \u201cThe Golden One\u201d, number 5on Tab. 4.1. Most of the initial\nvideos in the channel are about working out or video-games, with politics related\nvideos becoming increasingly occurring. When taking into account that the growth in\nengagement metrics such as likes per video and comments per video of the Alt-right\nsucceeds that of the I.D.W. and of the Alt-lite, this resonates with the narrative that\nthe rise of Alt-Lite and I.D.W. channels created fertile grounds for individuals with\nfringe ideas to prosper [Nagle, 2017, Lewis, 2018].\nAlthough our data-driven analysis sheds light into existing narratives on the\ncommunities of interest, it is still very di \ufb03cult to see, from these simple CDFs, whether\nthere is a radicalization pipeline. To do so, in the following two sections, we dig\ndeeper into the relationship between these communities looking closely at the users\nwho commented on them.\n4.3.2 User Intersection\nWe begin our in-depth analysis of users who commented on the channels of interest\nby analyzing the intersection between the users in di \ufb00erent channels and communities.\n65\n080910111213141516171810010K1Mper Year(a) Commenting UsersAlt-rightAlt-liteI.D.W.Control\n08091011121314151617180%10%20%30%Jaccard(b) Self-Similarity\n08091011121314151617180%10%20%30%(c) Similarity among CommunitiesAlt-right\u0000I.D.W.Alt-right\u0000Alt-liteAlt-lite\u0000I.D.W.\n08091011121314151617180%10%20%30%(d) Similarity with Control\n08091011121314151617180%25%50%75%Overlap Coef.08091011121314151617180%25%50%75%\n08091011121314151617180%25%50%75%\n1101001k60%80%100%Comments/User\nFigure 4.2. In(a), the number of unique commenting users per year in the top\n\ufb01gure and the CDF of comments per user for each one of the communities in the\nbottom \ufb01gure. In (b)\u2014(d)we show two similarity metrics (Jaccard and Overlap\nCoe\ufb03cient) for di \ufb00erent pairs of sets of commenting users across the years. In (b)\nthese pairs are the sets of users of each community in subsequent years. In (c)\nthese pairs are the sets of users of each one of the communities of interest. In (d)\nthese pairs are the sets of users of the communities compared with the users who\ncommented on control channels. Notice that comments are clumped together per\nyear, so here, unlike in Fig 4.1, 2017 means from 2017 to 2018, and so forth.\nIn that context, we use two set similarity metrics: the Jaccard Similarity|A\\B|\n|A[B|;a n d\nthe Overlap Coe \ufb03cient|A\\B|\nmin(|A|,|B|). Noticeably, previous studies have shown that com-\nmenting users are not a representative sample of users who engage with online content.\nIn news, for example, they tend to be older and more often male [Ziegele et al., 2013].\nHere, it is reasonable to assume they are more engaged with the content than simple\nviewers.\nColumn (a) of Fig. 4.2 characterizes commenting users. The top \ufb01gure shows the\nabsolute number of commenting users per year, while the bottom one shows the CDF\nof the number of comments per users per community. It is interesting to compare these\nplots with that of Fig. 4.1(e), as we can see that the communities of interest have way\nmore assiduous commenters. This supports the hypothesis that users that consume\ncontent in the communities of interest are more \"engaged\" than those that consume\nthe content from the control channels. Notice that although the Alt-right commenters\nhave, in average, fewer comments than the Alt-lite or the I.D.W., the community is\nway younger (as discussed in Sec. 4.3.1), and thus it is hard to tell whether their users\nare less engaged.\nIn columns (b)\u2014(d) of Fig.4.2 we consider the intersection between the comment-\ning users of the communities of interest and the control channels. The top \ufb01gure for\neach column shows the Jaccard similarity and the bottom one the Overlap Coe \ufb03cient.\n66\nColumn (b) in Fig. 4.2 shows the self-similarity for a community with itself a year\nbefore. We \ufb01nd that the retention of users among the three communities is growing\nwith time for both metrics. However, for control channels, we \ufb01nd that the Jaccard\nsimilarity is actually decreasing since 2015 ,a n dt h a tt h eo v e r l a pc o e \ufb03cient only recently\nstarted to grow, perhaps due to the sharp increase in commenting users since 2015 .\nCommenting users from the communities of interest seem to go back more often than\nthose in the control channels.\nColumn (c) in Fig. 4.2 shows the similarity within the three communities. No-\ntably, the Jaccard similarity between the Alt-lite and the I.D.W. is higher than the\nself-similarity of these both communities, reaching almost 30%. Moreover, the Overlap\nCoe\ufb03cient of the Alt-right with the Alt-lite and the Alt-right is high: approximately\n55% for the I.D.W. and 70% for the Alt-lite. This means more than half of the users\nwho commented on Alt-right channels commented on both the other two communities.\nLastly, column (d) in Fig. 4.2 shows the similarity of the three communities with\nthe control channels. We have that the Jaccard similarity between the I.D.W. and\nthe Alt-lite and the control channels is not so di \ufb00erent from the similarity between\nthese communities and the Alt-right. This is a subtle \ufb01nding. On one hand, it means\nthat individuals on this communities actually make up a signi\ufb01cant portion of the\nmassive media channels we use for control, which gather billions of views. On the\nother, it shows that the Alt-right, a group of channels with an order of magnitudes\nless views, subscribers and comments, are actually on par with these large channels.\nInspecting the Overlap Coe \ufb03cient, however, we get a di \ufb00erent panorama: there we have\nthat the communities overlap more with themselves than with the control channels.\nInterestingly, for both similarity measurements, we can see a sharp growth in the\nsimilarity with control after 2016 . This may be explained due to the sharp increase\nin popularity of these communities since 2015 ,a sd i s c u s s e di nS e c .4 . 3 . 1 ,a st h e s e\ncommunities become more \"mainstream\", it may be that users who comment watch\nthe control channels eventually stumble upon them.\n4.3.3 User Migration\nIn the previous section we portrayed a scenario where the commenting user bases among\nthe communities is increasingly similar. Although that may indicate a growing per-\ncentage of users consuming extreme (here the Alt-right) content on YouTube while also\nconsuming content from other milder communities (here the Alt-lite and the I.D.W.),\nit does not, per se ,i n d i c a t et h a tt h e r ei sar a d i c a l i z a t i o np i p e l i n ei nt h ew e b s i t e . T o\nbetter address this question, we \ufb01nd users who did not comment in Alt-right content\n67\n0%4%8%12%Alt-lite orI.D.W.Start: 2006-2012Start: 2013-2015lightmildsevereStart: 2016Start: 2017\n0%4%8%12%Alt-lite0%4%8%12%I.D.W.\n2006-20122013-20152016201720180%4%8%12%Control2013-201520162017201820162017201820172018\nFigure 4.3. We show how users \"migrate\" towards Alt-right content. For users\nwho consumed only videos in the communities indicated by the labels in the\nrows (Alt-lite or I.D.W., only Alt-lite, only I.D.W. or Control), we show the\nprobability of them becoming consuming Alt-right content. We consider three\nlevels of \"infection\": light (commented on 1 to 2 Alt-right videos), mild (3, 5)\nand severe (6+). Each column tracks users in a di \ufb00erent starting date. Initially,\ntheir infection rates are 0 (as they did not consume any Alt-right content). As\ntime passes, we show the infection rates in the y-axis, for each of the years, in the\nx-axis.\nin a given year and look into their activity into subsequent years.\nFor four time brackets [(2016 \u00002012) ,(2013 \u00002015) ,(2016) ,(2017)] we track four\nsets of users: those who only commented on videos of the Alt-lite, those who did so\nonly in the I.D.W., those who did so in either, and those who commented only in\ncontrol channels. Then, for subsequent years, we track the same users. Notice that\nwhen users are tracked for one year they aren\u2019t eligible for selection in upcoming years.\nWe consider these users to be \"infected\" if they commented on 1to2(light), 3to5\n(mild) or 6or more (severe) Alt-right videos.\nThe results for this analysis are shown in Fig. 4.3. We show the percentage\nof users who become infected of the users we managed to track. We \ufb01nd that a high\npercentage of users became \"infected\", according to our criteria. Consider, for example,\nusers who in 2006 \u00002012 commented only on I.D.W. or Alt-lite content ( 227,945users).\nBy2018 ,21.83% were still active, and from those, 17.9%(around 9,000users) were\n\"infected\" in one of the three levels, around 4.8%of them severely so. From the ones\nwho in 2017 commented only in Alt-lite or I.D.W. videos ( 1,253,751users) 50% were\nactive the following year, and approximately 12% of them became infected, around\n3.6%mildly or severely so \u2014more than 26,000users. Interestingly, when considering\nusers who commented both in the Alt-lite and the I.D.W., or only in the Alt-lite, the\n68\n0%10%20%30%Alt-lite orI.D.W.(0.0%)(5.9%)(20.4%)(33.0%)(37.8%)Light Infection(0.0%)(7.0%)(25.3%)(38.9%)(44.9%)Mild InfectionStart: 2006-2012Start: 2013-2015Start: 2016Start: 2017(0.0%)(8.1%)(25.1%)(36.1%)(43.0%)Severe Infection\n0%6%12%18%Alt-lite(0.0%)(4.0%)(10.7%)(19.3%)(23.3%)(0.0%)(4.7%)(12.2%)(20.9%)(26.4%)(0.0%)(5.4%)(11.8%)(17.8%)(23.8%)\n0%1.5%3%4.5%I.D.W.(0.0%)(1.1%)(3.8%)(5.2%)(7.0%)(0.0%)(1.2%)(4.2%)(5.3%)(6.7%)(0.0%)(1.3%)(3.8%)(4.7%)(5.6%)\n2006-20122013-20152016201720180%1.5%3%4.5%Control(0.0%)(2.2%)(3.9%)(4.3%)(5.5%)2006-20122013-2015201620172018(0.0%)(2.3%)(3.7%)(4.0%)(4.9%)2006-20122013-2015201620172018(0.0%)(2.5%)(3.4%)(3.5%)(4.0%)\nFigure 4.4. We show the percentage of users that can be traced back as not-\ninfected users who commented on other communities. Each line represents users\nwho, in a given start date, commented only Alt-lite or I.D.W. content, the y-axis\nshows the percentage of the total Alt-right commenting users they went to become\n(notice that all lines begin at 0 as users initially did not consume any Alt-right\ncontent).\nspeed of infection seems to be increasing year by year. When comparing the infection\nrates of the communities of interest with the control channels, we \ufb01nd that they all\npresent higher infection rates, particularly for mild and severe infections. Moreover,\nwe also \ufb01nd that, for light infections, the rate is signi\ufb01cantly smaller for users who\nwere tracked after 2013. When teasing apart users that commented only on Alt-lite or\nonly on I.D.W. content, we \ufb01nd that, not only users that commented only on content\nfrom the I.D.W. get less infected, but increasingly less so, as with the control channels.\nFor example, the radicalization rates of users who watched only Alt-lite or only I.D.W.\ncontent are much more similar for those tracked in 2006 \u00002012 than for those tracked\nin2017 (infection is less prevalent among I.D.W. commenters in recent years).\nThe previous experiment suggests that the pipeline e \ufb00ect does exist, and that\nindeed, users systematically go from milder communities to the Alt-right. However, it\ndoes not give insight into how expressive the e \ufb00ect is in terms of what part of the Alt-\nright user base has gone through it. We address this question by tracking users exactly\nas we did before, and then analyzing what percentage of \"infected\" users at each year\ncan be traced back to users who initially watched content from other communities. In\nother terms, for each year we calculate, of the users who are infected (i.e. who watched\nAlt-right videos), which percentage belongs to each one of the sets of tracked users we\njust described.\nThe results for this analysis are shown in Fig. 4.4. We \ufb01nd that these users are\n69\nac o n s i d e r a b l ef r a c t i o no ft h eA l t - r i g h tc o m m e n t i n ga u d i e n c e . I n2 0 1 8 ,f o re x a m p l e ,\nfor all kinds of infections, roughly 40% of commenting users can be traced back from\ncohorts of users that commented only in Alt-lite or I.D.W. videos in the past. Moreover,\nwe can observe that, consistently, users who consumed Alt-lite or I.D.W. content in\nag i v e ny e a r ,g oo nt ob e c o m eas i g n i \ufb01 c a n tf r a c t i o no ft h eA l t - r i g h tu s e rb a s ei nt h e\nfollowing year. This generates the peaks in the \ufb01rst row of Fig. 4.4. Moreover, looking\nat the second and third row of Fig. 4.4, we \ufb01nd a substantial di \ufb00erence between the\nI.D.W. and the Alt-lite. Whereas in Sec. 4.3.2 we \ufb01nd that the intersection between\nthem both and the Alt-right are very similar, here we see that users who commented\nonly on I.D.W. channels constitute a way less signi\ufb01cant percentage of the Alt-right\nconsumer base in upcoming years when compared to the Alt-lite. For all levels of\ninfection, at all times, there are more than roughly 3times more users in the Alt-right\ncommenting base that commented exclusively in the I.D.W. than in the Alt-lite. So,\nwhile in 2018 ,23.3%of users who were lightly infected can be traced back to users who\ncommented on Alt-lite channels in previous years, only 7.6%can be traced back to\nI.D.W. channels. As a matter of fact, the percentages of the I.D.W. are not so di \ufb00erent\nfrom those of the control channels, shown in the last row of Fig. 4.4.\nThe experiments performed in this section are particularly relevant given our\nanalysis (in Sec. 4.3.2) showing the growing intersection of the commenting user bases.\nThere we \ufb01nd that (i)the di \ufb00erences in user intersection on a yearly basis are not so\ndire between the milder communities and the Alt-right, and control channels and the\nAlt-right; and (ii)that the intersection between the I.D.W. and the Alt-right is similar\nto that of the Alt-lite of the Alt-right. Here we see that, despite that, the systematic\nmigration of users who consume Alt-lite content (along with I.D.W. or not) to Alt-right\ncontent is signi\ufb01cantly more expressive than that of users who consume only I.D.W.\ncontent initially, which in itself is only marginally more expressive than that of users\nwho consumed only control channels initially. Importantly, we mean \"expressive\" both\nin terms of the percentage of the users we manage to track through the comments (as\nin Fig. 4.3), but also in terms of the percentage of users comment on the Alt-right\nvideos (as in Fig. 4.4).\n4.3.4 Recommendation Algorithm\nIn this section, we inspect the impact of YouTube\u2019s recommendation algorithm, ex-\namining if the algorithm prioritizes more extreme content, as it has often been\nclaimed [Tufekci, 2018]. We perform our analysis in a recommendation graph, built\nusing the data collected. A noteworthy fact is that channels recommendations were\n70\n0%2%4%6%% Alt-rightStart: Wherever20%Start: Alt-rightStart: Alt-liteAlt-rightAlt-liteIntellectual Dark WebControlStart: I.D.W.0%0.25%0.5%0.75%0.01%Start: Control\n01234567891011Steps0%25%50%75%100%% Others01234567891011Steps01234567891011Steps01234567891011Steps01234567891011Steps0%2.5%5.0%7.5%10%\n(a)\n0%2%4%6%% Alt-rightStart: WhereverStart: Alt-rightStart: Alt-liteStart: I.D.W.0%0.25%0.5%0.75%0.01%Start: Control\n01234567891011Steps0%25%50%75%100%% Others01234567891011Steps01234567891011Steps01234567891011Steps01234567891011Steps0%2.5%5.0%7.5%10%\n(b)\nFigure 4.5. We show the results for the simulation of random walks for channels\n(a) and videos (b). The top row shows the chance of the random walker being\nin an Alt-right channel at each step, while the bottom row shows the chance of\nthe random walker being in any of the other communities. The di \ufb00erent columns\nportray di \ufb00erent starting rules: in any channel, only in channels of the Alt-right,\nand so forth.\ndisabled for most of the channels we are studying following a dispute between Carlos\nMaza and Steven Crowder [Goggin, 2019], where the former asked for YouTube to act\nin response of several episodes of personal attacks, where Crowder made fun of Carlos\u2019\nethnicity and sexual orientation. The graph is built as follows: for each channel, we join\ntogether all recommendations obtained in all rounds of data collection. Each channel is\nan o d e ,a n de d g e sb e t w e e nn o d e si n d i c a t er e c o m m e n d a t i o n sf r o mac h a n n e lt oa n o t h e r\n(for both video and channel recommendations). Each edge is weighted proportionally\nto the number of times that recommendation appeared in the data collection, and\nweights are normalized so that outcoming edges of each node sums to 1(thus creating\nas t o c h a s t i cg r a p h ) .\nThe percentage of edges \ufb02owing from each community to another (normalized by\ntheir weight) is shown in Tab. 4.3 and Tab. 4.4 for channel and video recommendations,\nrespectively. We have very di \ufb00erent scenarios for each one of the recommender systems.\nFor channel recommendations, we have that control channels are recommended scarcely\nby the communities of interest, although they do have some edges to Alt-lite and I.D.W.\nchannels. Alt-lite and I.D.W. channels recommend another channel from the same\n71\ncommunity around 68% of the time, and recommend each other around 25% of the time.\nAlt-right channels are rarely recommended by both the Alt-lite ( 2.72)a n dt h eA l t - r i g h t\n4, although signi\ufb01cantly more by the Alt-lite. Moreover, these channels recommend\nAlt-lite channels more than they recommend other Alt-right channels. They also very\nrarely recommend control channels. For video recommendations, control channels are\nrecommended very often across the communities, more than communities themselves\nfor the Alt-lite and the Alt-right. The only community that remains recommending\nitself often is the I.D.W (around 60% of the time). Whereas the Alt-lite recommends\nthe I.D.W. about the same percentage of times as it recommends itself (roughly 30%),\nthe I.D.W. recommends the Alt-lite less frequently.\nGiven these graphs, we perform experiments considering a random walker. We\ndo as follows. The random walker begins in a random node, chosen with chance pro-\nportional to the number of subscribers in each channel. Then the random walker\nrandomly navigates the graph for 11 steps, choosing edges at random with probabili-\nties proportional to their weights. We collect the communities of the channels visited\nby the random walker and calculate the probability of it visiting channels from each\nof the communities. We consider the case where the random-walk start in any node,\nbut also the case where its starting point is con\ufb01ned within a single community. The\nprobabilities of the random walker to be in each of the communities, at each step, given\ndi\ufb00erent starting conditions is shown in Fig. 4.5, for channel and video recommenda-\ntions respectively.\nFor channel recommendations, we \ufb01nd that, from the three communities of in-\nterest, users eventually reach 2%of chance of being in an Alt-right channel. This is\napproximately 2/3of the probability a user would land in such channel if they picked\nthe channels at random given their number of subscribers ( \u21e03%). When starting in\nthe Alt-right, the chance of visiting another Alt-right channel quickly drops, reaching\n4%in the third hop. For the other communities, the situation changes according to the\nstarting rule. If we start wherever or in control channels, we actually tend to increas-\ningly consume control channels. On the other cases, we end up being steered towards\nAlt-lite content.\nVideo recommendations yield very low chances for users to be recommended Alt-\nright channels in all scenarios. The chance quickly converges to 0in all cases. Here,\ncontrol channels are also favoured much more than in the channel recommendation\ngraph, and the chance of being in a control channel eventually increases to more than\n50% in all cases. Moreover, in this scenario, the recommender system clearly steer users\ntowards the I.D.W.. in detriment of the Alt-lite. For example, if you start in I.D.W.\nvideos, there is less than 5%of chance that you will go towards Alt-lite channels, and\n72\nTable 4.3. Percentage of edges in-between communities in the channel recom-\nmendation graph (normalized per weight).\nSrc#|D e s t . !Alt-lite Alt-right Control I.D.W.\nAlt-lite 68.02 2.72 4.27 25.00\nAlt-right 46.38 35.64 1.75 16.23\nControl 11.06 0.00 75.23 13.71\nI.D.W. 26.29 0.43 5.52 67.76\nTable 4.4. Percentage of edges in-between communities in the video recommen-\ndation graph (normalized per weight).\nSrc#|D e s t . !Alt-lite Alt-right Control I.D.W.\nAlt-lite 28.67 2.59 38.86 29.88\nAlt-right 17.74 16.39 38.00 27.88\nControl 2.25 0.05 91.19 6.52\nI.D.W. 12.70 0.52 26.95 59.82\nif you start in Alt-lite videos, after 5steps you are more likely to be in an I.D.W. video\nthan in an Alt-lite video.\nOverall, these \ufb01ndings are nuanced. Channel recommendations do seem to steer\nusers towards the Alt-lite, which is de\ufb01nitely closer to some far-right talking points,\nwhile video recommendations do seem to steer users towards I.D.W. content. Yet,\nin both cases, the recommender systems seem particularly unfavorable to Alt-right\ncontent, which is less represented than it would be if we randomly picked the channel\namong our pool. It is worthwhile to mention that there are several limitations to our\napproach, and these are discussed further in Sec. 4.4.\n4.4 Discussion\nWe performed a throughout analysis of three YouTube communities \u2014the Alt-right,\nthe Alt-lite and the Intellectual Dark Web\u2014 by inspecting a large dataset containing\nmillions of comments and recommendations from thousands of videos. We \ufb01nd several\ndata-driven insights associated with the questions proposed in the introduction.\nThe communities studied sky-rocketed in terms of views, likes, videos published\nand comments, particularly since 2015 , coinciding with the turbulent presidential elec-\ntion of that year. The communities of interest also have a lot of engagement in the\nform of comments, gathering much more comments per video and comments per views\nthan the Even if one considers only Alt-right content as \"extreme\", our results indicate\n73\nthat content production and consumption are growing exponentially.\nWe \ufb01nd that the user bases for the three communities are increasingly similar,\nand, considering Alt-right channels as a proxy for extreme content, that a signi\ufb01cant\namount of commenting users systematically migrates from commenting exclusively on\nmilder content to commenting on more extreme content. We argue that this \ufb01nding\ncomprises signi\ufb01cant evidence that there has been, and there continues to be, user\nradicalization on YouTube, and our analyzes of the activity of these communities are\nconsistent with the theory that more extreme content \"piggybacked\" the surge in pop-\nularity of I.D.W. and Alt-lite content [Nagle, 2017]. The analyzes done in Sec. 4.3.3\nshow the phenomenon is not only consistent throughout the years, but also that it is\nvery expressive in its absolute quantity. Even if one considers that only a very small\nquantity commenting users we deem as infected are getting radicalized, they are still\nin the magnitude of the thousands (not to mention those who only watch the videos\nand never comment on them). Our results also show that there is a signi\ufb01cant di \ufb00er-\nence for Alt-lite and for I.D.W. channels. Users who consume only the latter seems to\nmigrate to more extreme content less, and are a less signi\ufb01cant portion of the Alt-right\ncommenting user base.\nOur analysis also suggests that YouTube\u2019s recommendation algorithm, given our\nexperimental setup, does not strongly favor the Alt-right, although it does bind the\nthree communities of interest together. In our simulations, the representativity of Alt-\nright channels was smaller than what it should be considering the number of subscribers\nthese channels have. Still the question of \"is that enough?\" does not seem to have a\nclear answer, particularly when Alt-lite content, seems to be in violation of YouTube\u2019s\nown guidelines against hate speech. Noticeably, our analysis has several shortcomings\nwhich do not allow us to make bold claims about this research question. Firstly, we\nare able to look only at a tiny fraction of actual recommendations \u2014it could very well\nbe that this content was being promoted in the past. Secondly, our analysis does not\ntake into account personalization, which could reveal a completely di \ufb00erent picture.\nAlthough it may be impossible to completely tease apart from historical data what\nrole the algorithm had in in\ufb02uencing users to migrate from milder to more extreme\ncontent, techniques like the one employed here could be adapted to audit YouTube in\nyears to come.\nFuture Work In this chapter, we focused almost exclusively at the trajectory of\nusers, be they inferred through comments or simulated in the recommendation graphs.\nAnother interesting direction would be to trace the evolution of the speech of content\ncreators and commenting users throughout the years: what are the narratives that\narose, how did their tone change. Looking at text could also improve the criteria\n74\nused for radicalization, we could rule out users that are having negative responses to\nextremist videos (although this is not substantial in the dataset). Moreover, we intend\nto extend the existing framework to audit radicalization to take into account user\npersonalization \u2014this is not trivial, as knowing the trajectories of radicalized users\nthrough YouTube content is unfeasible.\n75\nChapter 5\nConclusion\nIn the introduction, we argued that a user-centric approach may aid towards better\ncharacterization and detection of ill-de\ufb01ned social phenomena. Here, we elaborate on\nthis argument, and explain how this approach was bene\ufb01cial in each of the three case\nstudies presented. Moreover, we argue that each case study illustrate a way of thinking\nof users when modeling ill-de\ufb01ned phenomena such as hate speech and fake news.\nIn the \ufb01rst case study, we looked at the interaction between political polarization\nand misinformation, analyzing users and their social networks to understand who was\nengaging with content. Broadly, we want to associate users\u2019 characteristics (in this\nparticular case, political leaning) to content. One could even try to infer one\u2019s political\nopinion from a piece of content, but this characteristic is not of the piece of content\nitself, but of the user who created or shared it. To put it more broadly, in Chapter 2, our\nuser-centric focus allow us to relate a user-centered characteristic with content. Closer\ninspection shows that this is a surprisingly common modeling recipe. For example,\n[Shi et al., 2019] proposes correlating political polarization of users with the quality of\nWikipedia articles. There, analogously to our paper, we are able to learn something\nabout a piece of content (a Wiki article) by analyzing users associated with it.\nIn Chapter 3 we propose characterizing and detecting hateful users. This is\ndi\ufb00erent from what we did in Chapter 2, as here, we largely disassociate the content\nof the research question \u2014it becomes just another dimension that we can analyze for\nusers. Instead of trying to employ user context to detect whether a piece of content is\nhateful, we abandon the idea of trying to classify content altogether, and focus strictly\non users. This abstraction is particularly useful for problems such as hate speech, as\nit is very hard to disassociate content from user \u2014what is hateful depends on who\nis saying it\u2014 and as we are usually interested in taking action against the user who\nspread hateful content [Google, 2019, Twitter, 2019].\n76\nLastly, in Chapter 4 we study a phenomenon that only makes sense in the gran-\nularity level of users \u2014radicalization. If in Chapter 2 and 3, you could still argue that,\nat least ignoring trickier cases, there are things that are inherently hateful or inherently\nfake, regardless of who is sharing this content, here you cannot. In a sense, this is a\nthird category of how we can better understand hate speech and fake news focusing\non users: it allows us to study relevant phenomena which happens in a more complex\nlevel of abstraction.\nIn retrospect, we can pinpoint the di \ufb00erent roles users had in each Chapter:\n\u2022In Chapter 2, we enriched our understanding of a kind of content by analyzing\nusers.\n\u2022In Chapter 3, we studied a phenomenon associated with content at the user level.\n\u2022In Chapter 4, we studied a phenomenon associated with users.\nAfter more explicitly de\ufb01ning what we mean by \"user-perspective\", understand-\ning why it is bene\ufb01cial to focus on users when studying ill-de\ufb01ned phenomena such as\nhate speech of fake news becomes clearer. As it is particularly hard to indicate what\nis hate or fake, focusing on users may help us: (i)to understand the nuances of these\nde\ufb01nitions (as done in Chapter 2); (ii)to simplify the problem, ignoring some of these\nnuances by adopting a more granular model of the world (as in Chapter 3); and, lastly,\n(iii)to explore phenomena that are associated with hateful and fake content, but which\nrequires this more granular model to be studied.\n5.1 Major Themes across the Chapters\nAfter we have established in broad strokes the contribution of this dissertation, we take\nthe opportunity to take a step back and intertwine the major themes from the di \ufb00erent\nchapters. We identify 3 of such major themes:\nDrawing Boundaries is Hard. In Chapters 3 and 4, we have posed questions\nthat ultimately require that we subjectively categorize content or users \u2014we had to\nmake decisions about which user pro\ufb01les were hateful, or which channels were radical.\nThis kind of research is particularly challenging, as it requires that we approach the\nmaterial with depth and domain experience. Yet, this close inspection is crucial, after\nall, the most important questions in the study of online hate or misinformation will\nrequire ground truth for what is hate speech and misinformation. In Chapter 3, draw-\ning boundaries between hateful and non-hateful users allowed us to characterize the\n77\nbehavior of such users and to develop a method to better detect them. In Chapter 4,\nclassifying YouTube channels as belonging to one of the three categories of interest\n\u2014I.D.W., Alt-lite, and Alt-right\u2014 allowed us to study the migration of such users\nacross these communities. Importantly, drawing these boundaries is no easy task, and\nthus all e \ufb00orts must be made that the methodology is throughout and transparent.\nNo Silver Bullet. Fake news, hate speech and radicalization are complex and ever-\nchanging phenomena. As mentioned before, our work suggests that enriching the un-\nderstanding of content related to these phenomena through user context and studying\nsuch phenomena at the user level presents many bene\ufb01ts. In that sense, our work\nproposes approaches that are distant from an out-of-the-box-solution framework. This\navoidance of over-simplistic approaches towards these intricate issues is the driving\nforce behind the methodologies of Chapters 2 and 3, and our \ufb01ndings suggest that\nover-simpli\ufb01cation may indeed cause problems. In Chapter 2, we avoided simplistic\nde\ufb01nitions of what is fake or what is hateful and, as a consequence, realized that rely-\ning on what people name as fake in social media is not practical (as they disagree over\nwhat is fake news). In Chapter 3, we examined hate speech at a user level and devel-\noped a sampling methodology less dependant on a speci\ufb01c set of words. Not only this\nallowed us to paint a more realistic portrait of hate speech on Twitter, but it showed\nus that the choice of words of hateful users is highly counter-intuitive: they use fewer\nwords related to anger and more words related to love, for example.\nResearch as an Auditing Tool. Lastly, a major theme across the dissertation is the\nrole of research as a way to uncover social phenomena that: 1) interests society at large,\n2) takes place in online social networks. In many cases, here particularly in Chapter\n4, it is particularly hard to study such phenomena because social network platforms\nare not designed to be crawled or studied. Yet, this kind of research is important\nto understand the impact of di \ufb00erent technologies in our society. In Chapter 3 we\nwere able to characterize the users that Twitter banned as hateful during a months-\nlong period. This is particularly important, as the discussion over what should be\nmoderated is of tremendous interest to society. In Chapter 4 we were able to con\ufb01rm\nthat user radicalization indeed took place on YouTube in the last couple of years. Our\nwork provides quantitative evidence previous anectotes and qualitative research that\nsuggested so.\n78\nBibliography\n[ADL, 2018] ADL (2018). Hate on DisplayTMHate Symbols Database.\n[ADL, 2019a] ADL (2019a). Alt Right: A Primer about the New White Supremacy.\n[ADL, 2019b] ADL (2019b). From Alt Right to Alt Lite: Naming the Hate.\n[Agarwal and Sureka, 2014] Agarwal, S. and Sureka, A. (2014). A Focused Crawler for\nMining Hate and Extremism Promoting Videos on YouTube. In Proceedings of the\n25th ACM Conference on Hypertext and Social Media ,H T\u2019 1 4 ,p a g e s2 9 4 - - 2 9 6 ,N e w\nYork, NY, USA. ACM. event-place: Santiago, Chile.\n[Allcott and Gentzkow, 2017] Allcott, H. and Gentzkow, M. (2017). Social Media and\nFake News in the 2016 Election. Journal of Economic Perspectives ,3 1 ( 2 ) : 2 1 1 - - 2 3 6 .\nISSN 0895-3309.\n[Anderson and Jiang, 2018] Anderson, M. and Jiang, J. (2018). Teens, Social Media\n& Technology 2018. Technical report, Pew Research Center.\n[Andywarski, ] Andywarski. Richard Spencer, Styx and Sargon Have a Chat - Andy\nand JF moderate.\n[Anonymous, ] Anonymous. The Intellectual Dark Web.\n[Atkinson, 2018] Atkinson, D. C. (2018). Charlottesville and the alt-right: a turning\npoint? Politics, Groups, and Identities ,6 ( 2 ) : 3 0 9 - - 3 1 5 .I S S N2 1 5 6 - 5 5 0 3 .\n[Bakshy et al., 2015] Bakshy, E., Messing, S., and Adamic, L. A. (2015). Exposure to\nideologically diverse news and opinion on Facebook. Science ,3 4 8 ( 6 2 3 9 ) : 1 1 3 0 - - 1 1 3 2 .\nISSN 0036-8075, 1095-9203.\n[Benevenuto et al., 2010] Benevenuto, F., Magno, G., Rodrigues, T., and Almeida, V.\n(2010). Detecting spammers on twitter. In In Collaboration, Electronic messaging,\nAnti-Abuse and Spam Conference (CEAS .\n79\n[Bengaluru et al., 2018] Bengaluru, C., Dhule, G., Hyderabad, K., and Raipur, R.\n(2018). Murderous mob \u2014 9 states, 27 killings, one year: And a pattern to the\nlynchings. The Indian Express .\n[Beydoun, 2018] Beydoun, K. A. (2018). US liberal Islamophobia is rising \u2013 and more\ninsidious than rightwing bigotry | Khaled A Beydoun. The Guardian .I S S N 0 2 6 1 -\n3077.\n[Boult, 2017] Boult, A. (2017). Prisoner dressed as woman in failed escape bid. The\nTelegraph .I S S N0 3 0 7 - 1 2 3 5 .\n[Brooks and Boadle, 2018] Brooks, B. and Boadle, A. (2018). Divisive Brazil election\ncareens into \u2019dangerous\u2019 polarization. Reuters .\n[Burke, 2017] Burke, J. (2017). The myth of the \u2018lone wolf\u2019 terrorist. The Guardian .\nISSN 0261-3077.\n[Burnap and Williams, 2016] Burnap, P. and Williams, M. L. (2016). Us and them:\nidentifying cyber hate on Twitter across multiple protected characteristics. EPJ\nData Science ,5 ( 1 ) : 1 1 .I S S N2 1 9 3 - 1 1 2 7 .\n[Calais Guerra et al., 2011] Calais Guerra, P. H., Veloso, A., Meira, Jr., W., and\nAlmeida, V. (2011). From Bias to Opinion: A Transfer-learning Approach to Real-\ntime Sentiment Analysis. In Proceedings of the 17th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining , KDD \u201911, pages 150--158,\nNew York, NY, USA. ACM.\n[Campaigns et al., ] Campaigns, i., Elections, Parties, Action, C., Groups, I., Papers,\nPolitics, Government, Research, and Technology. \u201cAlt-Lite\u201d Bloggers and the Con-\nservative Ecosystem.\n[Cha et al., 2010] Cha, M., Haddadi, H., Benevenuto, F., and Gummadi, K. P. (2010).\nMeasuring User In\ufb02uence in Twitter: The Million Follower Fallacy. In Fourth Inter-\nnational AAAI Conference on Weblogs and Social Media .\n[Chakraborty et al., 2016] Chakraborty, A., Paranjape, B., Kakarla, S., and Ganguly,\nN. (2016). Stop Clickbait: Detecting and Preventing Clickbaits in Online News\nMedia. In Proceedings of the 2016 IEEE/ACM International Conference on Advances\nin Social Networks Analysis and Mining , ASONAM \u201916, pages 9--16, Piscataway, NJ,\nUSA. IEEE Press.\n[Check, 2019] Check, M. B. F. (2019). Media Bias Fact Check.\n79\n80\n[Chen et al., 2015] Chen, Y., Conroy, N. J., and Rubin, V. L. (2015). Misleading\nOnline Content: Recognizing Clickbait As \"False News\". In Proceedings of the 2015\nACM on Workshop on Multimodal Deception Detection , WMDD \u201915, pages 15--19,\nNew York, NY, USA. ACM. event-place: Seattle, Washington, USA.\n[Conover et al., 2011] Conover, M. D., Ratkiewicz, J., Francisco, M. R., Gon\u00e7alves,\nB., Menczer, F., and Flammini, A. (2011). Political Polarization on Twitter. In\nAdamic, L. A., Baeza-Yates, R. A., and Counts, S., editors, Proceedings of the Fifth\nInternational Conference on Weblogs and Social Media, ICWSM 2011 ,B a r c e l o n a ,\nCatalonia, Spain. AAAI Press.\n[Conroy et al., 2015] Conroy, N. J., Rubin, V. L., and Chen, Y. (2015). Automatic\nDeception Detection: Methods for Finding Fake News. In Proceedings of the 78th\nASIS&T Annual Meeting: Information Science with Impact: Research in and for the\nCommunity , ASIS&T \u201915, pages 82:1--82:4, Silver Springs, MD, USA. ASIS&T.\n[Corner, 2017] Corner, J. (2017). Fake news, post-truth and media\u2013political change.\nMedia, Culture & Society ,3 9 ( 7 ) : 1 1 0 0 - - 1 1 0 7 .I S S N0 1 6 3 - 4 4 3 7 .\n[Covington et al., 2016] Covington, P., Adams, J., and Sargin, E. (2016). Deep Neural\nNetworks for YouTube Recommendations. In Proceedings of the 10th ACM Confer-\nence on Recommender Systems ,R e c S y s\u2019 1 6 ,p a g e s1 9 1 - - 1 9 8 ,N e wY o r k ,N Y ,U S A .\nACM. event-place: Boston, Massachusetts, USA.\n[Cunha et al., 2018] Cunha, E., Magno, G., Caetano, J., Teixeira, D., and Almeida,\nV. (2018). Fake News as We Feel It: Perception and Conceptualization of the Term\n\u201cFake News\u201d in the Media. In Staab, S., Koltsova, O., and Ignatov, D. I., editors,\nSocial Informatics , Lecture Notes in Computer Science, pages 151--166. Springer\nInternational Publishing.\n[Davidson et al., 2010] Davidson, J., Liebald, B., Liu, J., Nandy, P., Van Vleet, T.,\nGargi, U., Gupta, S., He, Y., Lambert, M., Livingston, B., and Sampath, D. (2010).\nThe YouTube Video Recommendation System. In Proceedings of the Fourth ACM\nConference on Recommender Systems ,R e c S y s\u2019 1 0 ,p a g e s2 9 3 - - 2 9 6 ,N e wY o r k ,N Y ,\nUSA. ACM. event-place: Barcelona, Spain.\n[Davidson et al., 2017] Davidson, T., Warmsley, D., Macy, M., and Weber, I. (2017).\nAutomated Hate Speech Detection and the Problem of O \ufb00ensive Language. In\nEleventh International AAAI Conference on Web and Social Media .\n80\n81\n[Dhingra et al., 2016] Dhingra, B., Zhou, Z., Fitzpatrick, D., Muehl, M., and Cohen,\nW. (2016). Tweet2vec: Character-Based Distributed Representations for Social Me-\ndia. In Proceedings of the 54th Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers) ,p a g e s2 6 9 - - 2 7 4 ,B e r l i n ,G e r m a n y .A s s o c i a t i o n\nfor Computational Linguistics.\n[Diakopoulos, 2014] Diakopoulos, N. (2014). Algorithmic Accountability Reporting:\nOn the Investigation of Black Boxes.\n[Edelman and Luca, 2014] Edelman, B. G. and Luca, M. (2014). Digital Discrimina-\ntion: The Case of Airbnb.com. SSRN Scholarly Paper ID 2377353, Social Science\nResearch Network, Rochester, NY.\n[Faraday Speaks, 2019] Faraday Speaks (2019). My Descent into the Alt-Right\nPipeline.\n[Fast et al., 2016] Fast, E., Chen, B., and Bernstein, M. S. (2016). Empath: Under-\nstanding Topic Signals in Large-Scale Text. In Proceedings of the 2016 CHI Con-\nference on Human Factors in Computing Systems , CHI \u201916, pages 4647--4657, New\nYork, NY, USA. ACM. event-place: San Jose, California, USA.\n[Ferrara et al., 2016] Ferrara, E., Varol, O., Davis, C., Menczer, F., and Flammini, A.\n(2016). The Rise of Social Bots. Commun. ACM ,5 9 ( 7 ) : 9 6 - - 1 0 4 .I S S N0 0 0 1 - 0 7 8 2 .\n[Foderaro, 2018] Foderaro, L. W. (2018). Alexandria Ocasio-Cortez Likens $10,000\nDebate O \ufb00er by Conservative Columnist to Catcalling. The New York Times .I S S N\n0362-4331.\n[Fox News, 2019] Fox News (2019). Tucker: No American citizen has been charged\nwith collusion. Fox News .\n[Funke, 2018] Funke, D. (2018). Reporters: Stop calling everything \u2018fake news\u2019. Poyn-\nter.\n[Garimella et al., 2017] Garimella, K., De Francisci Morales, G., Gionis, A., and Math-\nioudakis, M. (2017). Reducing Controversy by Connecting Opposing Views. In\nProceedings of the Tenth ACM International Conference on Web Search and Data\nMining , WSDM \u201917, pages 81--90, New York, NY, USA. ACM. event-place: Cam-\nbridge, United Kingdom.\n81\n82\n[Giannakopoulos et al., 2010] Giannakopoulos, T., Pikrakis, A., and Theodoridis, S.\n(2010). A Multimodal Approach to Violence Detection in Video Sharing Sites. In\n2010 20th International Conference on Pattern Recognition ,p a g e s3 2 4 4 - - 3 2 4 7 .\n[Goggin, 2019] Goggin, B. (2019). YouTube\u2019s week from hell: How the debate over\nfree speech online exploded after a conservative star with millions of subscribers was\naccused of homophobic harassment. Business Insider .\n[Golub and Jackson, 2010] Golub, B. and Jackson, M. O. (2010). Na\u00efve Learning in\nSocial Networks and the Wisdom of Crowds. American Economic Journal: Microe-\nconomics ,2 ( 1 ) : 1 1 2 - - 1 4 9 .I S S N1 9 4 5 - 7 6 6 9 .\n[Google, 2019] Google (2019). Hate speech policy.\n[Gottfried and Shearer, 2016] Gottfried, J. and Shearer, E. (2016). News Use Across\nSocial Media Platforms 2016. Technical report, Pew Research Center.\n[Gray, 2015] Gray, R. (2015). How 2015 Fueled The Rise Of The Freewheeling, White\nNationalist Alt- Movement. BuzzFeed News .\n[Greevy and Smeaton, 2004] Greevy, E. and Smeaton, A. F. (2004). Classifying Racist\nTexts Using a Support Vector Machine. In Proceedings of the 27th Annual In-\nternational ACM SIGIR Conference on Research and Development in Information\nRetrieval , SIGIR \u201904, pages 468--469, New York, NY, USA. ACM. event-place:\nShe\ufb03eld, United Kingdom.\n[Grinberg et al., 2019] Grinberg, N., Joseph, K., Friedland, L., Swire-Thompson, B.,\nand Lazer, D. (2019). Fake news on Twitter during the 2016 U.S. presidential elec-\ntion. Science ,3 6 3 ( 6 4 2 5 ) : 3 7 4 - - 3 7 8 .I S S N0 0 3 6 - 8 0 7 5 ,1 0 9 5 - 9 2 0 3 .\n[Groskopf, 2016] Groskopf, C. (2016). European politics is more polarized than ever,\nand these numbers prove it. Quartz .\n[Grover and Leskovec, 2016] Grover, A. and Leskovec, J. (2016). Node2vec: Scalable\nFeature Learning for Networks. In Proceedings of the 22Nd ACM SIGKDD Inter-\nnational Conference on Knowledge Discovery and Data Mining , KDD \u201916, pages\n855--864, New York, NY, USA. ACM. event-place: San Francisco, California, USA.\n[Guerra et al., 2017] Guerra, P. C., Nalon, R., Assun\u00e7\u00e3o, R., and Jr, W. M. (2017).\nAntagonism Also Flows Through Retweets: The Impact of Out-of-Context Quotes\n82\n83\nin Opinion Polarization Analysis. In Proceedings of the Eleventh International Con-\nference on Web and Social Media, ICWSM 2017 , pages 536--539, Montr\u00e9al, Qu\u00e9bec,\nCanada. AAAI Press.\n[Habgood-Coote, 2017] Habgood-Coote, J. (2017). The term \u2019fake news\u2019 is doing great\nharm. The Conversation .\n[Hamilton et al., 2017a] Hamilton, W., Ying, Z., and Leskovec, J. (2017a). Inductive\nrepresentation learning on large graphs.\n[Hamilton et al., 2017b] Hamilton, W. L., Ying, R., and Leskovec, J. (2017b). Rep-\nresentation Learning on Graphs: Methods and Applications. Bulletin of the IEEE\nComputer Society Technical Committee on Data Engineering .\n[Hankes and Amend, 2018] Hankes, K. and Amend, A. (2018). The Alt-Right is Killing\nPeople.\n[Hannak et al., 2013] Hannak, A., Sapiezynski, P., Molavi Kakhki, A., Krishnamurthy,\nB., Lazer, D., Mislove, A., and Wilson, C. (2013). Measuring Personalization of Web\nSearch. In Proceedings of the 22Nd International Conference on World Wide Web ,\nWWW \u201913, pages 527--538, New York, NY, USA. ACM. event-place: Rio de Janeiro,\nBrazil.\n[Hannak et al., 2014] Hannak, A., Soeller, G., Lazer, D., Mislove, A., and Wilson, C.\n(2014). Measuring Price Discrimination and Steering on E-commerce Web Sites. In\nProceedings of the 2014 Conference on Internet Measurement Conference , IMC \u201914,\npages 305--318, New York, NY, USA. ACM. event-place: Vancouver, BC, Canada.\n[Hatebase, 2018] Hatebase (2018). Hatebase.\n[Ingram, 2018] Ingram, M. (2018). Most Americans say they have lost trust in the\nmedia. Columbia Journalism Review .\n[Joel Kelly, 2017] Joel Kelly, B. (2017). Lauren Southern: The alt-right\u2019s Canadian\ndog whistler.\n[Julia Angwin, 2017] Julia Angwin, H. G. (2017). Facebook\u2019s Secret Censorship Rules\nProtect White Men From Hate Speech But Not Black Children.\n[Kloumann and Kleinberg, 2014] Kloumann, I. M. and Kleinberg, J. M. (2014). Com-\nmunity Membership Identi\ufb01cation from Small Seed Sets. In Proceedings of the 20th\nACM SIGKDD International Conference on Knowledge Discovery and Data Mining ,\nKDD \u201914, pages 1366--1375, New York, NY, USA. ACM.\n83\n84\n[Know Your Meme, 2018] Know Your Meme (2018). Operation Google.\n[Kumar and Shah, 2018] Kumar, S. and Shah, N. (2018). False Information on Web\nand Social Media: A Survey. arXiv:1804.08559 [cs] .a r X i v :1 8 0 4 . 0 8 5 5 9 .\n[Kutner, 2016] Kutner, M. (2016). Roosh V\u2019s journey from pickup artist to right-wing\nprovocateur. Newsweek .\n[Lazer et al., 2018] Lazer, D. M. J., Baum, M. A., Benkler, Y., Berinsky, A. J., Green-\nhill, K. M., Menczer, F., Metzger, M. J., Nyhan, B., Pennycook, G., Rothschild, D.,\nSchudson, M., Sloman, S. A., Sunstein, C. R., Thorson, E. A., Watts, D. J., and\nZittrain, J. L. (2018). The science of fake news. Science ,3 5 9 ( 6 3 8 0 ) : 1 0 9 4 - - 1 0 9 6 .I S S N\n0036-8075, 1095-9203.\n[Lewandowsky et al., 2012] Lewandowsky, S., Ecker, U. K. H., Seifert, C. M., Schwarz,\nN., and Cook, J. (2012). Misinformation and Its Correction: Continued In\ufb02uence and\nSuccessful Debiasing. Psychological Science in the Public Interest ,1 3 ( 3 ) : 1 0 6 - - 1 3 1 .\nISSN 1529-1006.\n[Lewis, 2018] Lewis, R. (2018). Alternative in\ufb02uence: Broadcasting the reactionary\nright on YouTube. Technical report, Data and Society.\n[Lianne and Simmonds, 2013] Lianne, C.-F. and Simmonds, H. (2013). Rede\ufb01ning\nGatekeeping Theory For A Digital Generation. The McMaster Journal of Com-\nmunication ,8 .\n[Liao and Fu, 2013] Liao, Q. V. and Fu, W.-T. (2013). Beyond the Filter Bubble:\nInteractive E \ufb00ects of Perceived Threat and Topic Involvement on Selective Exposure\nto Information. In Proceedings of the SIGCHI Conference on Human Factors in\nComputing Systems , CHI \u201913, pages 2359--2368, New York, NY, USA. ACM.\n[Lloyd Parry, 2017] Lloyd Parry, R. (2017). Rohingya ethnic cleansing is fake news,\nsays Burma army. The Times .I S S N0 1 4 0 - 0 4 6 0 .\n[Lopez G., 2019] Lopez G., C. (2019). Stefan Molyneux is MAGA Twitter\u2019s favorite\nwhite nationalist.\n[Lott, 2017] Lott, T. (2017). Jordan Peterson and the transgender wars.\n[Magu et al., 2017] Magu, R., Joshi, K., and Luo, J. (2017). Detecting the Hate Code\non Social Media. In Eleventh International AAAI Conference on Web and Social\nMedia .\n84\n85\n[Mann et al., 2019] Mann, A., Nguyen, K., and Gregory, K. (2019). \u2019Emperor Cottrell\u2019:\nAccused Christchurch shooter had celebrated rise of the Australian far-right.\n[Marantz, 2017] Marantz, A. (2017). The Alt-Right Branding War Has Torn the Move-\nment in Two. ISSN 0028-792X.\n[McCauley and Moskalenko, 2008] McCauley, C. and Moskalenko, S. (2008). Mecha-\nnisms of Political Radicalization: Pathways Toward Terrorism. Terrorism and Po-\nlitical Violence ,2 0 ( 3 ) : 4 1 5 - - 4 3 3 .I S S N0 9 5 4 - 6 5 5 3 .\n[Mikolov et al., 2013] Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Ef-\n\ufb01cient Estimation of Word Representations in Vector Space. arXiv:1301.3781 [cs] .\narXiv: 1301.3781.\n[Mitchel et al., 2014] Mitchel, A., Eva Matsa, K., Gottfried, J., and Kiley, J. (2014).\nPolitical Polarization & Media Habits. Technical report, Pew Research Center.\n[Mitchell and Page, 2015] Mitchell, A. and Page, D. (2015). State of News Media 2015.\nTechnical report, Pew Research Center.\n[Nagle, 2017] Nagle, A. (2017). Kill All Normies: Online Culture Wars From 4Chan\nAnd Tumblr To Trump And The Alt-Right . John Hunt Publishing. ISBN 978-1-\n78535-544-8.\n[Newman, 2011] Newman, N. (2011). Mainstream media and the distribution of news\nin the age of social media. Technical report.\n[Nic et al., 2018] Nic, N., Fletcher, R., Kalogeropoulos, A., Levy, D. A., and Nielsen,\nR. K. (2018). Reuters Institute Digital News Report 2018. Technical report, Reuters\nInstitute for the Study of Journalism.\n[NW et al., 2015] NW, . L. S., 800Washington, S., and Inquiries, D. U.-.-. |. M.-.-. |.\nF.-.-. |. M. (2015). Global Support for Principle of Free Expression, but Opposition\nto Some Forms of Speech.\n[Ottoni et al., 2018] Ottoni, R., Cunha, E., Magno, G., Bernardina, P., Meira Jr., W.,\nand Almeida, V. (2018). Analyzing Right-wing YouTube Channels: Hate, Violence\nand Discrimination. In Proceedings of the 10th ACM Conference on Web Science ,\nWebSci \u201918, pages 323--332, New York, NY, USA. ACM. event-place: Amsterdam,\nNetherlands.\n85\n86\n[Papadamou et al., 2019] Papadamou, K., Papasavva, A., Zannettou, S., Blackburn,\nJ., Kourtellis, N., Leontiadis, I., Stringhini, G., and Sirivianos, M. (2019). Disturbed\nYouTube for Kids: Characterizing and Detecting Inappropriate Videos Targeting\nYoung Children. arXiv:1901.07046 [cs] .a r X i v :1 9 0 1 . 0 7 0 4 6 .\n[Pariser, 2011] Pariser, E. (2011). The Filter Bubble: What The Internet Is Hiding\nFrom You . Penguin UK. ISBN 978-0-14-196992-3.\n[Pennington et al., 2014] Pennington, J., Socher, R., and Manning, C. (2014). Glove:\nGlobal Vectors for Word Representation. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Processing (EMNLP) ,p a g e s1 5 3 2 - - 1 5 4 3 ,\nDoha, Qatar. Association for Computational Linguistics.\n[Pew Research, 2018] Pew Research (2018). Trends and Facts on Newspapers. Tech-\nnical report, Pew Research Center.\n[PowerfulJRE, ] PowerfulJRE. Jordan Peterson - Joe Rogan Experience #1208.\n[PowerfulJRE, ] PowerfulJRE. Sargon of Akkad - Joe Rogan Experience #979.\n[Rainie et al., 2017] Rainie, H., Anderson, J. Q., and Albright, J. (2017). The future of\nfree speech, trolls, anonymity and fake news online. Technical report, Pew Research\nCenter Washington, DC.\n[Ratkiewicz et al., 2011] Ratkiewicz, J., Conover, M. D., Meiss, M. R., Gon\u00e7alves, B.,\nFlammini, A., and Menczer, F. (2011). Detecting and Tracking Political Abuse in\nSocial Media. In Adamic, L. A., Baeza-Yates, R. A., and Counts, S., editors, Pro-\nceedings of the Fifth International Conference on Weblogs and Social Media, ICWSM\n2011, Barcelona, Catalonia, Spain. AAAI Press.\n[RedIceTV, ] RedIceTV. NPI 2016 Panel Discussion: Jared Taylor, Peter Brimelow,\nKevin MacDonald & Millennial Woes.\n[Ribeiro et al., 2012] Ribeiro, B., Murai, a. F., and Towsley, D. (2012). Sampling\ndirected graphs with random walks. In 2012 Proceedings IEEE INFOCOM ,p a g e s\n1692--1700.\n[Ribeiro et al., 2010] Ribeiro, B., Wang, P., and Towsley, D. (2010). On Estimating\nDegree Distributions of Directed Graphs through Sampling. Technical Report UM-\nCS-2010-046, University of Massachusetts.\n86\n87\n[Rilo \ufb00et al., 2013] Rilo\ufb00, E., Qadir, A., Surve, P., De Silva, L., Gilbert, N., and\nHuang, R. (2013). Sarcasm as Contrast between a Positive Sentiment and Negative\nSituation. In Proceedings of the 2013 Conference on Empirical Methods in Natu-\nral Language Processing ,p a g e s7 0 4 - - 7 1 4 ,S e a t t l e ,W a s h i n g t o n ,U S A .A s s o c i a t i o nf o r\nComputational Linguistics.\n[Roose, 2019] Roose, K. (2019). The Making of a YouTube Radical. The New York\nTimes .I S S N0 3 6 2 - 4 3 3 1 .\n[Roosh V, 2016] Roosh V (2016). I Do Not Disavow Richard Spencer.\n[Sabatini and Sarracino, 2017] Sabatini, F. and Sarracino, F. (2017). Online Networks\nand Subjective Well-Being. Kyklos ,7 0 ( 3 ) : 4 5 6 - - 4 8 0 .I S S N1 4 6 7 - 6 4 3 5 .\n[Schmidt and Wiegand, 2017] Schmidt, A. and Wiegand, M. (2017). A Survey on Hate\nSpeech Detection using Natural Language Processing. In Proceedings of the Fifth\nInternational Workshop on Natural Language Processing for Social Media ,p a g e s\n1--10, Valencia, Spain. Association for Computational Linguistics.\n[Sellars, 2016] Sellars, A. (2016). De\ufb01ning Hate Speech. SSRN Scholarly Paper ID\n2882244, Social Science Research Network, Rochester, NY.\n[Shafer, 2017] Shafer, J. (2017). The Case for Accrediting Breitbart. POLITICO\nMagazine .\n[Shelton, 1998] Shelton, H. (1998). Joint Doctrine for Information Operations. Tech-\nnical report JOINT-PUB-3-13, JOINT CHIEFS OF STAFF WASHINGTON DC.\n[Shi et al., 2019] Shi, F., Teplitskiy, M., Duede, E., and Evans, J. A. (2019). The\nwisdom of polarized crowds. Nature Human Behaviour ,3 ( 4 ) : 3 2 9 .I S S N2 3 9 7 - 3 3 7 4 .\n[Shu et al., 2017] Shu, K., Sliva, A., Wang, S., Tang, J., and Liu, H. (2017). Fake News\nDetection on Social Media: A Data Mining Perspective. SIGKDD Explor. Newsl. ,\n19(1):22--36. ISSN 1931-0145.\n[Solon, 2017] Solon, O. (2017). Google\u2019s bad week: YouTube loses millions as adver-\ntising row reaches US. The Observer .I S S N0 0 2 9 - 7 7 1 2 .\n[Stein, 1986] Stein, E. (1986). History against Free Speech: The New German Law\nagainst the \"Auschwitz\": And Other: \"Lies\". Michigan Law Review ,8 5 ( 2 ) : 2 7 7 - - 3 2 4 .\nISSN 0026-2234.\n87\n88\n[Sureka et al., 2010] Sureka, A., Kumaraguru, P., Goyal, A., and Chhabra, S. (2010).\nMining YouTube to Discover Extremist Videos, Users and Hidden Communities. In\nCheng, P.-J., Kan, M.-Y., Lam, W., and Nakov, P., editors, Information Retrieval\nTechnology , Lecture Notes in Computer Science, pages 13--24. Springer Berlin Hei-\ndelberg.\n[Tacopino, 2017] Tacopino, J. (2017). FBI clears Michael Flynn in probe linking him\nto Russia. New York Post .\n[Tandoc Jr et al., 2018] Tandoc Jr, E. C., Lim, Z. W., and Ling, R. (2018). De\ufb01ning\n\u201cFake News\u201d. Digital Journalism ,6 ( 2 ) : 1 3 7 - - 1 5 3 .I S S N2 1 6 7 - 0 8 1 1 .\n[Tard\u00e1guila et al., 2018] Tard\u00e1guila, C., Benevenuto, F., and Ortellado, P. (2018).\nOpinion | Fake News Is Poisoning Brazilian Politics. WhatsApp Can Stop It. The\nNew York Times .I S S N0 3 6 2 - 4 3 3 1 .\n[The Rubin Report, 2018] The Rubin Report (2018). Eric Weinstein: The Future of\nThe Intellectual Dark Web.\n[Tong et al., 2008] Tong, H., Faloutsos, C., and Pan, J.-Y. (2008). Random walk\nwith restart: fast solutions and applications. Knowledge and Information Systems ,\n14(3):327--346. ISSN 0219-3116.\n[Trump, 2017] Trump, D. J. (2017). The Fake News Media has never been so wrong\nor so dirty. Purposely incorrect stories and phony sources to meet their agenda of\nhate. Sad!\n[Tufekci, 2018] Tufekci, Z. (2018). Opinion | YouTube, the Great Radicalizer. The\nNew York Times .I S S N0 3 6 2 - 4 3 3 1 .\n[Twitter, 2019] Twitter (2019). Hateful conduct policy.\n[Vicario et al., 2016] Vicario, M. D., Bessi, A., Zollo, F., Petroni, F., Scala, A., Cal-\ndarelli, G., Stanley, H. E., and Quattrociocchi, W. (2016). The spreading of misin-\nformation online. Proceedings of the National Academy of Sciences ,1 1 3 ( 3 ) : 5 5 4 - - 5 5 9 .\nISSN 0027-8424, 1091-6490.\n[Viswanath et al., 2015] Viswanath, B., Bashir, M. A., Zafar, M. B., Bouget, S., Guha,\nS., Gummadi, K. P., Kate, A., and Mislove, A. (2015). Strength in Numbers: Robust\nTamper Detection in Crowd Computations. In Proceedings of the 2015 ACM on\nConference on Online Social Networks , COSN \u201915, pages 113--124, New York, NY,\nUSA. ACM. event-place: Palo Alto, California, USA.\n88\n89\n[Warner and Hirschberg, 2012] Warner, W. and Hirschberg, J. (2012). Detecting Hate\nSpeech on the World Wide Web. In Proceedings of the Second Workshop on Language\nin Social Media , pages 19--26, Montr\u00e9al, Canada. Association for Computational\nLinguistics.\n[Waseem et al., 2017] Waseem, Z., Davidson, T., Warmsley, D., and Weber, I. (2017).\nUnderstanding Abuse: A Typology of Abusive Language Detection Subtasks. In\nProceedings of the First Workshop on Abusive Language Online ,p a g e s7 8 - - 8 4 ,V a n -\ncouver, BC, Canada. Association for Computational Linguistics.\n[Waseem and Hovy, 2016] Waseem, Z. and Hovy, D. (2016). Hateful Symbols or Hate-\nful People? Predictive Features for Hate Speech Detection on Twitter. In Proceedings\nof the NAACL Student Research Workshop , pages 88--93, San Diego, California. As-\nsociation for Computational Linguistics.\n[Weiss and Winter, 2018] Weiss, B. and Winter, D. (2018). Opinion | Meet the Rene-\ngades of the Intellectual Dark Web. The New York Times .I S S N0 3 6 2 - 4 3 3 1 .\n[Wong et al., 2016] Wong, F. M. F., Tan, C. W., Sen, S., and Chiang, M. (2016). Quan-\ntifying Political Leaning from Tweets, Retweets, and Retweeters. IEEE Transactions\non Knowledge and Data Engineering ,2 8 ( 8 ) : 2 1 5 8 - - 2 1 7 2 .I S S N1 0 4 1 - 4 3 4 7 .\n[Yap, 2018] Yap, C. (2018). Duterte Decries \u2018Fake News\u2019 as Critics Warn of Media\nCrackdown. Bloomberg .\n[Zannettou et al., 2018a] Zannettou, S., Caul\ufb01eld, T., Blackburn, J., De Cristofaro,\nE., Sirivianos, M., Stringhini, G., and Suarez-Tangil, G. (2018a). On the Origins\nof Memes by Means of Fringe Web Communities. In Proceedings of the Internet\nMeasurement Conference 2018 , IMC \u201918, pages 188--202, New York, NY, USA. ACM.\nevent-place: Boston, MA, USA.\n[Zannettou et al., 2017] Zannettou, S., Caul\ufb01eld, T., De Cristofaro, E., Kourtelris, N.,\nLeontiadis, I., Sirivianos, M., Stringhini, G., and Blackburn, J. (2017). The Web\nCentipede: Understanding How Web Communities In\ufb02uence Each Other Through\nthe Lens of Mainstream and Alternative News Sources. In Proceedings of the 2017\nInternet Measurement Conference , IMC \u201917, pages 405--417, New York, NY, USA.\nACM. event-place: London, United Kingdom.\n[Zannettou et al., 2019] Zannettou, S., Caul\ufb01eld, T., De Cristofaro, E., Sirivianos, M.,\nStringhini, G., and Blackburn, J. (2019). Disinformation Warfare: Understanding\nState-Sponsored Trolls on Twitter and Their In\ufb02uence on the Web. In Companion\n89\n90\nProceedings of The 2019 World Wide Web Conference ,W W W\u2019 1 9 ,p a g e s2 1 8 - - 2 2 6 ,\nNew York, NY, USA. ACM. event-place: San Francisco, USA.\n[Zannettou et al., 2018b] Zannettou, S., Sirivianos, M., Blackburn, J., and Kourtellis,\nN. (2018b). The Web of False Information: Rumors, Fake News, Hoaxes, Clickbait,\nand Various Other Shenanigans. arXiv:1804.03461 [cs] .a r X i v :1 8 0 4 . 0 3 4 6 1 .\n[Ziegele et al., 2013] Ziegele, M., Johnen, M., Bickler, A., Jakobs, I., Setzer, T., and\nSchnauber, A. (2013). Male, Hale, Comments? Factors In\ufb02uencing the Activity of\nCommenting Users on Online News Websites. SCM Studies in Communication and\nMedia ,2 ( 1 ) : 6 7 - - 1 1 4 .I S S N2 1 9 2 - 4 0 0 7 .\n[Zollo et al., 2015] Zollo, F., Novak, P. K., Vicario, M. D., Bessi, A., Mozeti \u010d,I . ,S c a l a ,\nA., Caldarelli, G., and Quattrociocchi, W. (2015). Emotional Dynamics in the Age\nof Misinformation. PLOS ONE ,1 0 ( 9 ) : e 0 1 3 8 7 4 0 .I S S N1 9 3 2 - 6 2 0 3 .\n90", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Misinformation, radicalization and hate through the lens of users", "author": ["MH Ribeiro"], "pub_year": "2019", "venue": "NA", "abstract": "A populariza\u00e7\u00e3o das redes sociais mudou a din\u00e2mica de cria\u00e7\u00e3o e consumo de conte\u00fado.  Barreiras para disseminar textos, imagens e v\u00eddeos tornaram-se significativamente menores"}, "filled": false, "gsrank": 617, "pub_url": "https://repositorio.ufmg.br/handle/1843/46708", "author_id": ["IN55QyEAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:sAYU9ML5WswJ:scholar.google.com/&output=cite&scirp=616&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D610%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=sAYU9ML5WswJ&ei=dbWsaILdNb_SieoPzJnloAQ&json=", "num_citations": 1, "citedby_url": "/scholar?cites=14725356547355182768&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:sAYU9ML5WswJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://repositorio.ufmg.br/bitstream/1843/46708/5/Master_Thesis.pdf"}}, {"title": "To Kavanaugh or not to Kavanaugh: That is the polarizing question", "year": "2018", "pdf_data": "To Kavanaugh or Not to Kavanaugh: That is the Polarizing Question\nKareem Darwish\nQatar Computing Research Institute, HBKU\nDoha, Qatar\nKDarwish@HBKU.edu.qa\nAbstract\nOn October 6, 2018, the US Senate con\ufb01rmed Brett Ka-\nvanaugh with the narrowest margin for a successful con\ufb01r-\nmation since 1881 and where the senators voted overwhelm-\ningly along party lines. In this paper, we examine whether\nthe political polarization in the Senate is re\ufb02ected among the\ngeneral public. To do so, we analyze the views of more than\n128 thousand Twitter users. We show that users supporting or\nopposing Kavanaugh\u2019s nomination were generally using di-\nvergent hashtags, retweeting different Twitter accounts, and\nsharing links from different websites. We also examine char-\nacterestics of both groups.\nIntroduction\nOn October 6, 2018, the US senate con\ufb01rmed Brett Ka-\nvanaugh to become a justice on the US Supreme Court\nwith a 50 to 48 vote that was mostly along party lines.\nThis was the closest successful con\ufb01rmation since the Stan-\nley Matthews con\ufb01rmation in 18811. In this paper, we ex-\namine whether the political polarization at play in the US\nSenate between Republicans, who overwhelmingly voted\nfor Kavanaugh, and Democrats, who overwhelmingly voted\nagainst him, re\ufb02ects polarization among the general pub-\nlic. We analyze more than 23 million tweets related to the\nKavanaugh con\ufb01rmation. Initially, we semi-automatically\ntag more than 128 thousand Twitter users as supporting\nor opposing his con\ufb01rmation. Next, we bucket hashtags,\nretweeted accounts, and cited websites according to how\nstrongly they are associated with those who support or op-\npose. Further, we visualize users according to their similarity\nbased on their hashtag usage, retweeted accounts, and cited\nwebsites. All our analysis show strong polarization between\nboth camps. Lastly, we highlight some of the main differ-\nences between both groups.\nTimeline\nOn July 9, 2018, Brett Kavanaugh (BK), a US federal judge,\nwas nominated by the US president Donald Trump to serve\nas a justice on the US supreme court to replace outgoing Jus-\n1https://www.senate.gov/pagelayout/\nreference/nominations/Nominations.htmtice Anthony Kennedy2. His nomination was marred by con-\ntroversy with Democrats complaining that the White House\nwithheld documents pertaining to BK\u2019s record and later a\nfew women including a University of California profes-\nsor accused him of sexual assault. The accusations of sex-\nual misconducted led to a public congressional hearing on\nSeptember 27, 2018 and a subsequent investigation by the\nFederal Bureau of Investigation (FBI). The US Senate voted\nto con\ufb01rm BK to a seat on the Supreme Court on October\n6 with a 50\u201348 vote, which mostly aligned with party loyal-\nties. BK was sworn in later the same day.\nData Collection\nWe collected tweets pertaining to the nomination of BK in\ntwo different time epochs, namely September 28-30, which\nwere the three days following the congressional hearing con-\ncerning the sexual assault allegation against BK, and Octo-\nber 6-9, which included the day the Senate voted to con\ufb01rm\nBK and the following three days. We collected tweets using\nthe twarc toolkit3, where we used both the search and \ufb01l-\ntering interfaces to \ufb01nd tweets containing any of the follow-\ning keywords: Kavanaugh, Ford, Supreme, judiciary, Blasey,\nGrassley, Hatch, Graham, Cornyn, Lee, Cruz, Sasse, Flake,\nCrapo, Tillis, Kennedy, Feinstein, Leahy, Durbin, White-\nhouse, Klobuchar, Coons, Blumenthal, Hirono, Booker, or\nHarris. The keywords include the BK\u2019s name, his main ac-\ncuser, and the names of the members of the Senate\u2019s Judi-\nciary Committee. The per day breakdown of the collected\ntweets is as follows:\nDate Count\n28-Sep 5,961,549\n29-Sep 4,815,160\n30-Sep 1,590,522\nsubtotal 12,367,231\n6-Oct 2,952,581\n7-Oct 3,448,315\n8-Oct 2,761,036\n9-Oct 1,687,433\nsubtotal 10,849,365\nTotal 23,216,596\n2https://en.wikipedia.org/wiki/Brett_\nKavanaugh\n3https://github.com/edsu/twarcarXiv:1810.06687v1  [cs.SI]  15 Oct 2018\nIn the process we collected 23 million tweets that were au-\nthored by 687,194 users. Our \ufb01rst step was to label as many\nusers as possible by their stance as supporting (SUPP) or op-\nposing (OPP) the con\ufb01rmation of BK. The labeling process\nwas done in three steps, namely:\n\u000fManual labeling of users. We manually labeled 43 users\nwho had the most number of tweets in our collection. Of\nthem, the SUPP users were 29 compared to 12 OPP users.\nThe two remaining users were either neutral or spammers.\n\u000fLabel propagation. Label propagation automatically la-\nbels users based on their retweet behavior (Darwish et al.\n2017b; Kutlu, Darwish, and Elsayed 2018; Magdy et al.\n2016). The intuition behind this method is that users that\nretweet the same tweets on a topic most likely share the\nsame stance. Given that many of the tweets in our collec-\ntion were actually retweets or duplicates of other tweets,\nwe labeled users who retweeted 15 or more tweets that\nwere authored or retweeted by the SUPP group or 7 or\nmore times by OPP group and no retweets from the other\nside as SUPP or OPP respectively. We elected to increase\nthe minimum number for the SUPP group as they were\nover represented in the initial manually labeled set. We it-\neratively performed such label propagation 4 times, which\nis when label propagation stopped labeling new accounts.\nAfter the last iteration, we were able to label 65,917 users\nof which 26,812 were SUPP and 39,105 were OPP. Since\nwe don\u2019t have golden labels to compare against, we opted\nto spot check the results. Thus, we randomly selected\n10 automatically labeled accounts, and all of them were\nlabeled correctly. This is intended as a sanity check. A\nlarger random sample is required for a more thorough\nevaluation. Further, this labeling methodology naturally\nfavors users who actively discuss a topic and who are\nlikely to hold strong views.\n\u000fRetweet based-classi\ufb01cation. We used the labeled users\nto train a classi\ufb01cation model to guess the stances of users\nwho retweeted at least 20 different accounts, which were\nusers who were actively tweeting about the topic. For\nclassi\ufb01cation, we used the FastText classi\ufb01cation toolkit,\nwhich is an ef\ufb01cient deep neural network classi\ufb01er that\nhas been shown to be effective for text classi\ufb01cation\n(Joulin et al. 2016). We used the accounts that each user\nretweeted as features. Strictly using the retweeted ac-\ncounts has been shown to be effective for stance classi-\n\ufb01cation (Magdy et al. 2016). To keep precision high, we\nonly trusted the classi\ufb01cation of users where the classi\ufb01er\nwas more than 90% con\ufb01dent. Doing so, we increased\nthe number of labeled users to 128,096, where 57,118\nbelonged to the SUPP group with 13,095,422 tweets\nand 70,978 belonged to the OPP group with 12,510,134\ntweets. Again, we manually inspected 10 random users\nwho were automatically tagged and all of them were clas-\nsi\ufb01ed correctly. It is noteworthy that the relative number\nof SUPP to OPP users in not necessarily meaningful. To\ndetermine the exact ratio, we would need to determine the\nstances of a large sample of users.Data Analysis\nNext we analyzed the data to ascertain the differences in in-\nterests and focus between both groups as expressed using\nthree elements, namely the hashtags that they use, the ac-\ncounts they retweet, and the websites that they cite (share\ncontent from). Doing so can provide valuable insights into\nboth groups (Darwish et al. 2017b; Darwish, Magdy, and\nZanouda 2017). For all three elements, we bucketed them\ninto \ufb01ve bins re\ufb02ecting how strongly they are associated\nwith the SUPP and OPP groups. These bins are: strong\nSUPP, SUPP, Neutral, OPP, and strong OPP. To perform the\nbucketing, we used the so-called valence score (Conover et\nal. 2011). The valence score for an element eis computed as\nfollows:\nV(e) = 2tfSUPP\ntotalSUPP\ntfSUPP\ntotalSUPP+tfOPP\ntotalOPP\u00001 (1)\nwhere tfis the frequency of the element in either the SUPP\nor OPP tweets and total is the sum of all tfs for either the\nSUPP or OPP tweets. We accounted for all elements that\nappeared in at least 100 tweets. Since the value of valence\nvaries between -1 (strong OPP) to +1 (strong SUPP), we\ndivided the ranged into 5 equal bins: strong OPP [-1.0 \u2013 -\n0.6), OPP [-0.6 \u2013 -0.2), Neutral [-0.2 \u2013 0.2), SUPP [0.2 \u2013\n0.6), and strong SUPP [0.6 \u2013 1.0).\nFigures 1, 2, and 3 respectively provide the number of dif-\nferent hashtags, retweeted accounts, and cited websites that\nappear for all \ufb01ve bins along with the number of tweets in\nwhich they are used. As the \ufb01gures show, there is strong po-\nlarization between both camps. Polarization is most evident\nin the accounts that they retweet and the websites that they\nshare content from, where \u201cstrong SUPP\u201d and \u201cstrong OPP\u201d\ngroups dominate in terms of the number of elements and\ntheir frequency.\nTables 1, 2, and 3 respectively show the 15 most com-\nmonly used hashtags, retweeted accounts, and most cited\nwebsites for each of the valence bins. Since the \u201cStrong\nSUPP\u201d and \u201cstrong OPP\u201d groups are most dominant, we fo-\ncus here on their main characteristics.\nFor the \u201cStrong SUPP\u201d group, the hashtags can be split\ninto the following topics (in order of importance as deter-\nmined by frequency):\n\u000fTrump related: #MAGA (Make America Great Again),\n#Winning.\n\u000fPro Kavanaugh con\ufb01rmation: #Con\ufb01rmKavanaugh,\n#Con\ufb01rmKavanaughNow, #JusticeKavanaugh.\n\u000fAnti-Democratic Party: #walkAway (campaign to walk-\naway from liberalism), #Democrats, #Feinstein\n\u000fConspiracy theories: #QAnon (an alleged Trump admin-\nistration leaker), #WWG1WGA (Where We Go One We\nGo All)\n\u000fMidterm elections: #TXSen (Texas senator Ted Cruz),\n#Midterms, #V oteRed2018 (vote Republican)\n\u000fConservative media: #FoxNews, #LDTPoll (Lou Dobbs\n(Fox News) on Twitter poll)\nFigure 1: Count of hashtags and the number of times they are used for different valence bins\nFigure 2: Count of retweeted account and the number of times they are retweeted for different valence bins\nFigure 3: Count of websites and the number of times they are cited for different valence bins\nStrong SUPP SUPP Neutral OPP Strong OPP\nMAGA SCOTUS KavanaughHearings Kavanaugh DelayTheV ote\nWinning ChristineBlaseyFord KavanaughV ote MeToo StopKavanaugh\nCon\ufb01rmKavanaugh kavanaughcon\ufb01rmation Breaking BrettKavanaugh GOP\nCon\ufb01rmKavanaughNow Ford FBI Trump BelieveSurvivors\nwalkaway kavanaughcon\ufb01rmed \ufb02ake republican SNLPremiere\nJusticeKavanaugh SaturdayMorning JeffFlake DrFord SNL\nQAnon TedCruz SupremeCourt KavanaghHearing TheResistance\nDemocrats FridayFeeling Grassley BelieveWomen Resist\nTXSen HimToo LindseyGraham Republicans V oteno\nMidterms TCOT KavanaughHearing RT SusanCollins\nLDTPoll SundayMorning WhiteHouse NeverTrump V ote\nFoxNews USA America Resistance SmartNews\nWWG1WGA KavanaughCon\ufb01rmationHearing WomensMarch Kavanagh JulieSwetnick\nFeinstein RememberInNovember Senate BrettKavanuagh KavaNo\nV oteRed2018 KavanaughFord FBIInvestigation Collins V oteBlue\nTable 1: Top hashtags\nStrong SUPP SUPP Neutral OPP Strong OPP\nrealDonaldTrump PollingAmerica RiegerReport AP krassenstein\nmitchellvii cspan lachlan CBSNews kylegrif\ufb01n1\ndbongino JenniferJJacobs SenJoeManchin Reuters KamalaHarris\ncharliekirk11 JerryDunleavy AaronBlake USATODAY SenFeinstein\nFoxNews JulianSvendsen WSJ Phil Mattingly EdKrassen\nRealJack jamiedupree markknoller dangercart thehill\nDineshDSouza CNNSotu Bencjacobs WalshFreedom MichaelAvenatti\nThomas1774Paine AlBoeNEWS lawrencehurley 4YrsToday SethAbramson\nAnnCoulter elainaplott AureUnnie byrdinator funder\nfoxandfriends AlanDersh choi bts2 MittRomney Lawrence\nJackPosobiec FoxNewsResearch happinesspjm MacFarlaneNews tedlieu\npaulsperry SCOTUSblog threadreaderapp TexasTribune MSNBC\nRealCandaceO W7VOA soompi HotlineJosh JoyceWhiteVance\nMcAllisterDen scotusreporter yami yami BBCWorld tribelaw\nIngrahamAngle CraigCaplan savtwopointoh Mediaite Amy Siskind\nTable 2: Top retweeted accounts\nStrong SUPP SUPP Neutral OPP Strong OPP\nthegatewaypundit.com usatoday.com dr.ford nytimes.com hill.cm\nfoxnews.com mediaite.com twitter.com/michaelavenatti/ twitter.com/thehill/ wapo.st\nfxn.ws twitter.com/realdonaldtrump/ dailym.ai thehill.com washingtonpost.com\ndailycaller.com theweek.com lawandcrime.com politi.co rawstory.com\nbreitbart.com twitter.com/lindseygrahamsc/ nypost.com abcn.ws vox.com\ntwitter.com/foxnews/ nyp.st twitter.com/donaldjtrumpjr/ usat.ly huf\ufb01ngtonpost.com\nthefederalist.com twitter.com/senfeinstein/ twitter.com/senjudiciary/ axios.com nyti.ms\nwesternjournal.com twitter.com/kamalaharris/ twitter.com/mediaite/ politico.com nbcnews.com\ntwitter.com/politico/ twitter.com/newsweek/ c-span.org reut.rs cnn.com\nilovemyfreedom.org twitter.com/natesilver538/ twitter.com/gop/ po.st apple.news\nchicksonright.com twitter.com/mcallisterden/ twitter.com/kendilaniannbc/ twitter.com/nbcnews/ dailykos.com\nhannity.com twitter.com/foxandfriends/ realclearpolitics.com twitter.com/brithume/ cnn.it\nnationalreview.com chn.ge twitter.com/senatorcollins/ dailymail.co.uk palmerreport.com\ndailywire.com fastcompany.com twitter.com/samstein/ twitter.com/cnnpolitics/ hillreporter.com\nbigleaguepolitics.com rollcall.com twitter.com/johncornyn/ abcnews.go.com newyorker.com\nTable 3: Top websites\nIt is interesting to see hashtags expressing support for Trump\n(#MAGA and #Wining) feature more prominently than\nthose that indicate support for BK. Further, anti-Democratic\nparty hashtags (ex. #WalkAway and #Democrats) and hash-\ntags related to conspiracy theories (ex. #QAnon) may also\nindicate polarization.\nRetweeted accounts re\ufb02ect a similar trend to that of hash-\ntags. The retweeted accounts can be grouped as follows (in\norder of importance):\n\u000fTrump related: realDonaldTrump, mitchellvii (Bill\nMitchell \u2013 social media personality who staunchly\nsupports Trump), RealJack (Jack Murphy \u2013 co-owner\nofILoveMyFreedom.org (pro-Trump website)), Di-\nneshDSouza (Dinesh D\u2019Souza \u2013 commentator and \ufb01lm\nmaker)\n\u000fConservative media: dbongino (Dan Bongino \u2013 author\nwith podcast), FoxNews, FoxAndFriends (Fox News),\nJackPosobiec (Jack Posobiec \u2013 One America News Net-\nwork), IngrahamAngle (Laura Ingraham \u2013 Fox News)\n\u000fConservative/Republican personalities: charliekirk11\n(Charlie Kirk \u2013 founder of Turning Point USA), An-\nnCoulter (Ann Coulter \u2013 author and commentator),\nThomas1774Paine (Thomas Paine \u2013 author), paulsperry\n(Paul Sperry \u2013 author and media personality), RealCan-\ndaceO (Candace Owens \u2013 Turning Point USA), McAllis-\nterDen (D C McAllister \u2013 commentator)\nThe list above show that speci\ufb01cally pro-Trump accounts\nfeatured even more prominently than conservative accounts.\nFor the same group, cited website were gener-\nally right-leaning, with some of them being far-right\nand most of them having mixed credibility. We list\nhere their leanings and their credibility levels ac-\ncording to the Media Bias/Fact Check website4:\n4mediabiasfactcheck.comSource Bias Credibility\nthegatewaypundit.com far right questionable\nfoxnews.com right mixed\nfxn.ws (Fox News) right mixed\ndailycaller.com right mixed\nbreitbart.com far right questionable\ntwitter.com/foxnews/ right mixed\nthefederalist.com right high\nwesternjournal.com right mixed\ntwitter.com/politico/ left-center high\nilovemyfreedom.org far right questionable\nchicksonright.com not listed not listed\nhannity.com (Fox News) not listed not listed\nnationalreview.com right mixed\ndailywire.com right mixed\nbigleaguepolitics.com right mixed\nFor the \u201cstrong OPP\u201d group, the top hashtags can be top-\nically grouped as follows (in order of importance):\n\u000fAnti Kavanaugh: #DelayTheV ote, #StopKavanaugh,\n#KavaNo (no to Kavanaugh), #voteNo.\n\u000fRepublican Party related: #GOP, #SusanCollins (GOP\nsenator voting for Kavanaugh).\n\u000fSexual assault related: #BelieveSurvivors, #JulieSwet-\nnick (woman accusing Kavanaugh).\n\u000fMedia related: #SNLPremiere (Saturday Night Live\nsatirical show), #SNL, #SmartNews (anti-Trump/GOP\nnews)\n\u000fAnti Trump: #TheResistance, #Resist\n\u000fMidterms: #vote, #voteBlue (vote democratic)\nAs the list shows that the most prominent hashtags were re-\nlated to opposition to the con\ufb01rmation of BK. Opposition to\nthe Republican Party (#GOP) and Trump (#TheResistance)\nmay indicate polarization.\nAs for their retweeted accounts, media related accounts\ndominated the list. The remaining accounts belonged to\nprominent Democratic Party of\ufb01cials and anti-Trump ac-\ncounts. The details are as follows (in order of importance):\n\u000fMedia related: krassenstein (Brian Krassenstein \u2013\nHillReporter.com ), kylegrif\ufb01n1 (Kyle Grif\ufb01n \u2013\nMSNBC producer), EdKrassen (Ed Krassenstein \u2013\nHillReporter.com ), theHill, funder (Scott Dworkin\n\u2013 Dworkin Report and Democratic Coallition), Lawrence\n(Lawrence O\u2019Donnell \u2013 MSNBC), MSNBC, JoyceWhite-\nVance (Joyce Alene \u2013 law professor and MSNBC contrib-\nutor), Amy Siskind (Amy Siskind \u2013 The Weekly List)\n\u000fDemocratic Party: KamalaHarris (Senator Kamala Har-\nris), SenFeinstein (Senator Dianne Feinstein), tedlieu\n(Representative Ted Lieu)\n\u000fAnti Kavanaugh: MichaelAvenatti (Michael Avenatti \u2013\nlawyer of Kavanaugh accuser)\n\u000fAnti Trump: SethAbramson (Seth Abramson \u2013 author of\nProof of Collusion), tribelaw (Laurence Tribe \u2013 Harvard\nProfessor and author of \u201cTo End a Presidency\u201d)\nConcerning cited websites, they mostly left or\nleft-of-center leaning sources. The credibility of the\nsources were generally higher than those for the\n\u201cstrong SUPP\u201d group. Their details are as follows:\nSource Bias Credibility\nhill.cm (the Hill) left-center high\nwapo.st (Washington Post) left-center high\nwashingtonpost.com left-center high\nrawstory.com left mixed\nvox.com left high\nhuf\ufb01ngtonpost.com left high\nnyti.ms (New York Times) left-center high\nnbcnews.com left-center high\ncnn.com left mixed\napple.news not list not listed\ndailykos.com left mixed\ncnn.it left mixed\npalmerreport.com left mixed\nhillreporter.com not listed not listed\nnewyorker.com left high\nLastly, we examined the retweeted accounts and cited\nwebsites that appeared in the \u201cneutral\u201d bin, meaning that\nthey were shared by both SUPP and OPP groups. We were\ninterested speci\ufb01cally in the media accounts. They are as fol-\nlows:\n\u000fRetweeted accounts: RiegerReport (JM Rieger \u2013 Wash-\nington Post video editor), lachlan (Lachlan Markay \u2013\nDaily Beast reporter), AaronBlake (Aaron Blake \u2013 Wash-\nington Post reporter), WSJ (Wall Street Journal), mark-\nknoller (Mark Knoller \u2013 CBS News correspondent),\nBencjacobs (Ben Jacobs \u2013 the Guardian), lawrencehurley\n(Lawrence Hurley \u2013 Reuters reporter)\n\u000fWebsites/URLs: dailym.ai (Daily Mail), LawAnd-\nCrime.com (Live trial network), nypost.com (New York\nPost), twitter.com/mediaite (MediaITE), c-span.org\n(CSPAN), twitter.com/kendilaniannbc/ (Ken Dila-\nnian \u2013 NBC News), RealClearPolitics.com, twit-\nter.com/samstein/ (Sam Stein \u2013 Daily Beast/MSNBC\nnewsletter)\nOne interesting thing in this list is that although some\nsources such as the Washington Post was most cited by the\u201cstrong OPP\u201d group, some Post\u2019s reporters featured promi-\nnently for the \u201cneutral\u201d group.\nThree politicians also appeared in the neutral column,\nnamely Sen JoeManchin (Democratic Senator Joe Manchin\nfrom West Virginia), SenatorCollins (Republican Senator\nSusan Collins from Maine), and JohnCornyn (Republican\nSenator John Cornyn from Texas). Incidentally, all three of\nthem voted to con\ufb01rm Kavanaugh.\nNext, we examined polarization in terms of the similarity\nbetween users based on the three aforementioned elements,\nhashtags, retweeted accounted, and cited websites. We com-\nputed the cosine similarity between users based on their us-\nage of the different elements. For user vectors given element\ntypes, we normalized the occurrence values of elements to\nensure that sum of values in the vector add up to 1. For exam-\nple, if user \u201cA\u201d uses three hashtags with frequencies 5, 100,\nand 895, the corresponding feature values would be 5/1,000,\n100/1,000, and 895/1,000, where 1,000 is the sum of the fre-\nquencies. We computed the similarities of users who used a\nminimum of 10 elements to ensure a minimum level of en-\ngagement in the topic. Next, we visualized the network of\nusers using the NetworkX toolkit5which uses Fruchterman-\nReingold force-directed algorithm to space out the nodes.\nIn essence, the distance between users would correlate posi-\ntively with their similarity (Darwish et al. 2017a).\nFigures 4, 5, and 6 respectively show the similarity\nbetween 5,000 randomly selected users using hashtags,\nretweeted accounts, and cited websites. The red dots rep-\nresent SUPP users and the blue dots represent OPP users.\nAs the Figures clearly show, both groups are clearly sepa-\nrable, which indicates polarization. For retweeted accounts\nand cited websites, polarization is more evident.\nConclusion\nIn this paper, we examined whether the political polariza-\ntion between Republican and Democratic senators on dis-\nplay during the Supreme Court con\ufb01rmation hearings of\njudge Brett Kavanaugh re\ufb02ects polarization of social media\nusers. To do so, we analyzed more than 128 thousand Twitter\nusers. In the process, we showed that those who support and\noppose the con\ufb01rmation of Kavanaugh were generally us-\ning divergent hashtags and were following different Twitter\naccounts and websites.\nReferences\n[Conover et al. 2011] Conover, M.; Ratkiewicz, J.; Fran-\ncisco, M. R.; Gonc \u00b8alves, B.; Menczer, F.; and Flammini, A.\n2011. Political polarization on twitter. Icwsm 133:89\u201396.\n[Darwish et al. 2017a] Darwish, K.; Alexandrov, D.; Nakov,\nP.; and Mejova, Y . 2017a. Seminar users in the arabic twitter\nsphere. In International Conference on Social Informatics ,\n91\u2013108. Springer.\n[Darwish et al. 2017b] Darwish, K.; Magdy, W.; Rahimi, A.;\nBaldwin, T.; and Abokhodair, N. 2017b. Predicting online\nislamophopic behavior after# parisattacks. The Journal of\nWeb Science 3(1).\n5https://networkx.github.io/\nFigure 4: User similarity given the hashtags they used. Red is for SUPP users and Blue is for OPP users.\nFigure 5: User similarity given the accounts they retweeted. Red is for SUPP users and Blue is for OPP users.\nFigure 6: User similarity given the websites they cited. Red is for SUPP users and Blue is for OPP users.\n[Darwish, Magdy, and Zanouda 2017] Darwish, K.; Magdy,\nW.; and Zanouda, T. 2017. Trump vs. hillary: What went vi-\nral during the 2016 us presidential election. In International\nConference on Social Informatics , 143\u2013161. Springer.\n[Joulin et al. 2016] Joulin, A.; Grave, E.; Bojanowski, P.; and\nMikolov, T. 2016. Bag of tricks for ef\ufb01cient text classi\ufb01ca-\ntion. arXiv preprint arXiv:1607.01759 .\n[Kutlu, Darwish, and Elsayed 2018] Kutlu, M.; Darwish, K.;\nand Elsayed, T. 2018. Devam vs. tamam: 2018 turkish elec-\ntions. arXiv preprint arXiv:1807.06655 .\n[Magdy et al. 2016] Magdy, W.; Darwish, K.; Abokhodair,\nN.; Rahimi, A.; and Baldwin, T. 2016. # isisisnotislam or#\ndeportallmuslims?: Predicting unspoken views. In Proceed-\nings of the 8th ACM Conference on Web Science , 95\u2013106.\nACM.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "To Kavanaugh or not to Kavanaugh: That is the polarizing question", "author": ["K Darwish"], "pub_year": "2018", "venue": "arXiv preprint arXiv:1810.06687", "abstract": "On October 6, 2018, the US Senate confirmed Brett Kavanaugh with the narrowest margin for  a successful confirmation since 1881 and where the senators voted overwhelmingly along"}, "filled": false, "gsrank": 618, "pub_url": "https://arxiv.org/abs/1810.06687", "author_id": ["y7tlR6UAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:4O7e49RkhO4J:scholar.google.com/&output=cite&scirp=617&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D610%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=4O7e49RkhO4J&ei=dbWsaILdNb_SieoPzJnloAQ&json=", "num_citations": 15, "citedby_url": "/scholar?cites=17186972943471537888&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:4O7e49RkhO4J:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/1810.06687"}}, {"title": "A Multidimensional Analysis of Text for Automated Detection of Computational Propaganda in Twitter", "year": "2025", "pdf_data": "A Multidimensional Analysis of Text for\nAutomated Detection of Computational\nPropaganda in Twitter\nby\nMarco Emanuel Casavantes Moreno\nA dissertation submitted in partial fulfillment of the requirements\nfor the degree of\nPh.D. IN COMPUTER SCIENCE\nDoctoral Advisors:\nDr. Manuel Montes-Y-G\u00b4 omez,\nINAOE, Mexico\nDr. Luis Carlos Gonz\u00b4 alez Gurrola,\nUniversidad Aut\u00b4 onoma de Chihuahua, Mexico\nDr. Alberto Barr\u00b4 on Cede\u02dc no,\nAlma Mater Studiorum\u2013Universit` a di Bologna, Italy\nMarch, 2025\nSanta Mar\u00b4 \u0131a de Tonantzintla, Puebla, CP 72840, Mexico.\nInstituto Nacional de Astrof\u00b4 \u0131sica, \u00b4Optica y Electr\u00b4 onica\n\u00a9INAOE 2025\nThe author grants INAOE permission to make partial or total\ncopies of this work and distribute them, provided that the source\nis mentioned.\nAbstract\nThe way we consume news has been transformed, with technological ad-\nvancements allowing people to easily express their views to vast audiences, in-\ncluding political opinions. These opinions can enhance a richer public dialogue;\nhowever, they also possess the potential to elevate extreme ideas that seek to ma-\nnipulate or skew political narratives for personal benefit or agendas. Social media\nis frequently praised for its ability to boost political involvement, to the extent\nthat its role in the spread of misinformation has even sparked worries about its\nimpact on democracy. The importance of propaganda spread via social media\ncan be linked to its influence in political matters, representing a domain where\npolitical factions compete for influence and control.\nIn the past few years, there has been a noticeable surge in the volume of\nresearch studies focused on the detection of propaganda across various domains,\nreflecting a growing recognition of the significance and impact of propaganda in\ncontemporary society.\nIn this research study, we aim to contribute to the ongoing expansion of aca-\ndemic research surrounding the phenomenon of propaganda distributed through\nsocial networks, while also acknowledging the importance of various contextual\nfactors that significantly influence the expression of propaganda in these environ-\nments. To facilitate this goal, we introduce a novel corpus specifically centered\non propaganda posted and spread on Twitter, which has been collected from a\ndiverse array of news media accounts.\nBy leveraging this unique dataset, we are putting forth a classification ap-\nproach that incorporates a multitude of contextual attributes, thereby enabling\na more effective detection of propaganda, particularly in comparison to a base-\nline strategy that focuses solely on the textual content of the messages without\nconsidering a broader context.\nWe have carried out an evaluation of the performance of our proposed ap-\nproach across multiple data collections to assess its capabilities. From our evalua-\ntions, we report that our approach consistently outperforms the baseline classifier,\ndemonstrating its superior effectiveness in detecting propaganda. Our analyses\nprovide insights into what kind of contributions different contexts bring when\ndetecting propaganda. Remarkably, we have even managed to secure the highest\nrankings in an international workshop dedicated to propaganda detection, where\nwe competed against a multitude of other participating methodologies, further\nvalidating the significance of our contributions to this field of study.\nResumen\nLa forma en que consumimos noticias se ha transformado gracias a los avances\ntecnol\u00b4 ogicos que permiten a las personas expresar f\u00b4 acilmente sus opiniones a un\np\u00b4 ublico amplio, incluyendo sus opiniones pol\u00b4 \u0131ticas. Estas opiniones pueden en-\nriquecer el di\u00b4 alogo p\u00b4 ublico; sin embargo, tambi\u00b4 en tienen el potencial de impulsar\nideas extremas que buscan manipular o distorsionar las narrativas pol\u00b4 \u0131ticas para\nbeneficio propio. Las redes sociales son frecuentemente elogiadas por su capaci-\ndad para impulsar la participaci\u00b4 on pol\u00b4 \u0131tica, hasta el punto de que su papel en la\ndifusi\u00b4 on de desinformaci\u00b4 on ha suscitado incluso preocupaci\u00b4 on por su impacto en\nla democracia. La importancia de la propaganda difundida a trav\u00b4 es de las redes\nsociales puede vincularse a su influencia en asuntos pol\u00b4 \u0131ticos, representando un\n\u00b4 ambito donde las facciones pol\u00b4 \u0131ticas compiten por influencia y el control.\nEn los \u00b4 ultimos a\u02dc nos, se ha observado un notable aumento en el volumen\nde estudios de investigaci\u00b4 on centrados en la detecci\u00b4 on de propaganda en diversos\n\u00b4 ambitos, lo que refleja un creciente reconocimiento de la importancia y el impacto\nde la propaganda en la sociedad contempor\u00b4 anea.\nEn este estudio de investigaci\u00b4 on, buscamos contribuir a la continua ex-\npansi\u00b4 on de la investigaci\u00b4 on acad\u00b4 emica en torno al fen\u00b4 omeno de la propaganda\ndistribuida a trav\u00b4 es de las redes sociales, reconociendo al mismo tiempo la im-\nportancia de diversos factores contextuales que influyen significativamente en la\nexpresi\u00b4 on de la propaganda en estos entornos. Para facilitar este objetivo, presen-\ntamos un novedoso corpus centrado espec\u00b4 \u0131ficamente en la propaganda publicada\ny difundida en Twitter, recopilado a partir de diversas cuentas de medios de\ncomunicaci\u00b4 on.\nAl aprovechar este conjunto de datos \u00b4 unico, proponemos un enfoque de\nclasificaci\u00b4 on que incorpora una multitud de atributos contextuales, lo que permite\nuna detecci\u00b4 on m\u00b4 as eficaz de la propaganda, especialmente en comparaci\u00b4 on con\nuna estrategia de l\u00b4 \u0131nea base que se centra \u00b4 unicamente en el contenido textual de\nlos mensajes sin considerar un contexto m\u00b4 as amplio.\nHemos llevado a cabo una evaluaci\u00b4 on del rendimiento de nuestro enfoque\npropuesto en m\u00b4 ultiples conjuntos de datos para evaluar sus capacidades. A partir\nde nuestras evaluaciones, reportamos que nuestro enfoque supera continuamente\nal clasificador de l\u00b4 \u0131nea base, lo que demuestra su eficacia superior en la detecci\u00b4 on\nde propaganda. Nuestros an\u00b4 alisis proporcionan informaci\u00b4 on sobre las contribu-\nciones de los diferentes atributos contextuales a la detecci\u00b4 on de propaganda. Cabe\ndestacar que incluso logramos las mejores clasificaciones en un taller internacional\ndedicado a la detecci\u00b4 on de propaganda, donde competimos con numerosas otras\nmetodolog\u00b4 \u0131as participantes, lo que valida a\u00b4 un m\u00b4 as la importancia de nuestras\ncontribuciones a este campo de estudio.\nAgradecimientos\nAl Consejo Nacional de Humanidades, Ciencias y Tecnolog\u00b4 \u0131as (CONAHCYT), y\nal Instituto Nacional de Astrof\u00b4 \u0131sica, \u00b4Optica y Electr\u00b4 onica (INAOE), por su apoyo\notorgado durante la realizaci\u00b4 on de este trabajo.\nA mis asesores, Dr. Manuel Montes y G\u00b4 omez, Dr. Luis Carlos Gonz\u00b4 alez\nGurrola, y Dr. Luis Alberto Barr\u00b4 on Cede\u02dc no, por compartir conmigo sus valiosos\nconocimientos y experiencias, as\u00b4 \u0131 como brindarme su gu\u00b4 \u0131a, lo cual me ayud\u00b4 o a\ncrecer tanto como investigador como persona.\nA mis sinodales, Dr. Luis Villase\u02dc nor Pineda, Dr. Jes\u00b4 us Ariel Carrasco\nOchoa, Dr. Aurelio L\u00b4 opez L\u00b4 opez, Dra. Delia Iraz\u00b4 u Hern\u00b4 andez Far\u00b4 \u0131as, y Dr.\nArkaitz Zubiaga, por sus valiosos comentarios, retroalimentaci\u00b4 on y observaciones,\nlos cuales han sido fundamentales para mejorar este trabajo.\nA mi familia y amigos, por su invaluable apoyo moral y emocional, sin\nimportar la distancia. Gracias a mi padre Manuel, a mi madre Blanca, a mi\nhermana Cony, a mi abuelo Manuel y a mi abuela Cuqui.\nA mis compa\u02dc neros del INAOE, por acompa\u02dc narme durante mi estancia en el\ninstituto.\nDedicatoria\nA mi familia, por estar conmigo y apoyarme en todo momento.\nA mi madre, Blanca, que siempre me ha dado su apoyo incondicional, alen-\ntado a perseguir mis sue\u02dc nos, e impulsado a perseverar ante todos los desaf\u00b4 \u0131os que\nhe encontrado en mi camino. Gracias por todas las conversaciones diarias que\nhemos compartido. Aunque me encuentre lejos de casa, siempre est\u00b4 as presente\nen mi vida.\nA mi querido perro Buzz, porque aunque ya no est\u00b4 e con nosotros, su re-\ncuerdo es suficiente para alegrar mi d\u00b4 \u0131a.\nA mis amigos Fabi\u00b4 an y Josu\u00b4 e, por su apoyo, por reunirnos siempre que la\noportunidad lo permite, porque a pesar de que hemos tomado caminos distintos,\ns\u00b4 e que cuento con su amistad, lealtad y hermandad.\nA mi asesor, el Dr. Manuel Montes y G\u00b4 omez, porque no puedo imaginar a\notra persona que hubiera hecho m\u00b4 as amena, interesante y divertida mi experien-\ncia. Gracias por compartir conmigo su conocimiento, pero sobre todo gracias por\ndarme la oportunidad de trabajar a su lado y por creer en m\u00b4 \u0131.\nContents\n1 Introduction 1\n1.1 Problem Statement . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2 Research Questions . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.3 Main Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.3.1 Specific objectives . . . . . . . . . . . . . . . . . . . . . . . 5\n1.4 Summary of Contributions . . . . . . . . . . . . . . . . . . . . . . 6\n1.4.1 Academic Production . . . . . . . . . . . . . . . . . . . . . 6\n1.5 Document Outline . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2 Background 10\n2.1 Machine Learning Algorithms . . . . . . . . . . . . . . . . . . . . 10\n2.1.1 Traditional Method for Text Classification . . . . . . . . . 11\n2.1.2 Deep Learning Method for Text Classification . . . . . . . 17\n2.2 Evaluation measures . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.3 Types of information disorders . . . . . . . . . . . . . . . . . . . . 21\n2.4 Propaganda techniques . . . . . . . . . . . . . . . . . . . . . . . . 22\n3 Related work 30\n3.1 Computational Propaganda Detection Outside Social Networks . . 30\n3.1.1 Propaganda as part of Fake News Analysis . . . . . . . . . 31\n3.1.2 Propaganda Detection as a Standalone Task . . . . . . . . 32\n3.1.3 Fine-Grained Analysis of Propaganda . . . . . . . . . . . . 33\n3.1.4 Propaganda from Digital Newspapers and Web Pages . . . 35\n3.2 Computational Propaganda Detection in Social Networks . . . . . 37\n3.2.1 Propaganda Disseminated in Twitter . . . . . . . . . . . . 37\n3.2.2 Propaganda Disseminated in Reddit . . . . . . . . . . . . . 40\n3.2.3 Propaganda Disseminated in Facebook . . . . . . . . . . . 42\n3.3 Discussion of Related Work Shortcomings . . . . . . . . . . . . . 42\n3.3.1 Scarcity of Data and Format Differences . . . . . . . . . . 43\n3.3.2 Manual Annotation and Distant Supervision . . . . . . . . 43\n3.3.3 Contextual Information . . . . . . . . . . . . . . . . . . . . 44\n3.3.4 Concept Drift . . . . . . . . . . . . . . . . . . . . . . . . . 44\n4 Propitter, a Corpus of Propaganda in Twitter 46\n4.1 Construction Methodology . . . . . . . . . . . . . . . . . . . . . . 47\n4.1.1 Stage 1: Data collection by distant supervision . . . . . . . 47\n4.1.2 Stage 2: Cross-domain tweets filtering . . . . . . . . . . . 48\n4.1.3 Stage 3: In-domain data expansion . . . . . . . . . . . . . 49\n4.2 Propitter\u2019s Classification Results . . . . . . . . . . . . . . . . . . . 54\n4.3 Propitter\u2019s Qualitative Analysis . . . . . . . . . . . . . . . . . . . 54\n4.4 Creating PropitterX : Adding Contextual Information . . . . . . . 55\n4.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n5 The Influence of Contextual Features for Propaganda Detection in Tweets 60\n5.1 PropitterX-LR : On the Role of Political Bias . . . . . . . . . . . . 60\n5.2 PropitterX-TIME : On the Evolution of Trending Topics . . . . . 63\n5.3 PropitterX-EMO : On the Relevance of Affective Information . . . 65\n5.4 PropitterX-GEO : On the Role of Region-Centered Content . . . . 67\n5.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n6 A Contextual-Aware Approach to Improve Propaganda Classification 72\n6.1 Contextual-aware Approach . . . . . . . . . . . . . . . . . . . . . 73\n6.2 Experimental settings . . . . . . . . . . . . . . . . . . . . . . . . . 75\n6.2.1 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n6.2.2 Baseline . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\n6.3 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n6.3.1 On the impact of adding context during the classification\nprocess. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n6.3.1.1 Fixed and new classification mistakes by BERT-CA 79\n6.3.2 On the impact of adding context when using limited train-\ning data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n6.3.3 Classifying Tweets from Unknown Sources . . . . . . . . . 83\n6.3.4 Classifying Tweets from Diplomatic Profiles and Govern-\nment Authorities . . . . . . . . . . . . . . . . . . . . . . . 86\n6.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n7 Conclusions and Future Work 92\nBibliography 100\nAppendices 114\n7.A List of propagandist sources considered for Propitter \u2019s construction. 115\n7.B List of non-propagandist sources considered for Propitter \u2019s con-\nstruction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n7.C Bias distribution of tweets in main partitions of PropitterX . . . . 117\n7.D Training partitions with proportional sampled emotions in PropitterX-\nEMO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\nList of Figures\n2.1 Text classification with conventional techniques. . . . . . . . . . . 11\n2.2 Example of a Linear Support Vector Machine. . . . . . . . . . . . 16\n2.3 General pre-training and fine-tuning mechanisms in BERT. . . . . 19\n2.4 Example of BERT input representation. . . . . . . . . . . . . . . 19\n2.5 The four outcomes of a confusion matrix. . . . . . . . . . . . . . . 20\n2.6 Types of information disorder. . . . . . . . . . . . . . . . . . . . . 22\n2.7 Venn diagram of false information on the Internet. . . . . . . . . . 22\n2.8 Frequency of propaganda techniques in the PTC corpus. . . . . . 29\n3.1 Contributions and shortcomings of relevant related work about\npropaganda detection. . . . . . . . . . . . . . . . . . . . . . . . . 45\n4.1 Construction Methodology - Diagram of Stage 2. . . . . . . . . . 48\n4.2 Construction Methodology - Diagram of Stage 3. . . . . . . . . . 51\n5.1 Word clouds of left-wing and right-wing propaganda. . . . . . . . 62\n5.2 Classification results over the propaganda class with chronological\ntraining splits. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n5.3 Word clouds of propaganda from chronological splits. . . . . . . . 65\n5.4 Word clouds of propagandist tweets that exhibit a predominant\nemotion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n5.5 Word clouds of propaganda by region. . . . . . . . . . . . . . . . 70\n6.1 BERT\u2019s auxiliary input diagram with the contextual features con-\ncatenated to the tweet\u2019s text. . . . . . . . . . . . . . . . . . . . . 74\n6.2 Average classification scores incorporating contextual features and\nchanging the volume of train data. . . . . . . . . . . . . . . . . . 83\n6.3 Average classification scores obtained by incorporating contextual\nfeatures as a secondary input. . . . . . . . . . . . . . . . . . . . . 85\n6.4 Box plots of the results for Task 1 of DIPROMATS 2023. . . . . . 89\nList of Tables\n2.1 Example of a Bag-of-Words . . . . . . . . . . . . . . . . . . . . . . 12\n2.2 Example of a Bag of Character 3-grams. . . . . . . . . . . . . . . 12\n3.1 News articles in TSHP-17 corpus. . . . . . . . . . . . . . . . . . . 31\n3.2 News articles in QProp corpus. . . . . . . . . . . . . . . . . . . . 33\n3.3 News articles in PTC corpus. . . . . . . . . . . . . . . . . . . . . 34\n3.4 Top Official Results for NLP4IF SLC Task. . . . . . . . . . . . . . 35\n3.5 Top Results for SemEval-2020 Task 11 Span Identification. . . . . 36\n3.6 Distribution of cross-domain corpora. . . . . . . . . . . . . . . . . 38\n3.7 Dataset statistics for TWEETSPIN corpus. . . . . . . . . . . . . . 40\n3.8 Data distribution of DIPROMATS corpora. . . . . . . . . . . . . 41\n3.9 Reddit propaganda dataset distribution. . . . . . . . . . . . . . . 41\n4.1 Examples of reliable tweets at Stage 2. . . . . . . . . . . . . . . . 49\n4.2 Examples of noisy tweets filtered by the classifier at Stage 2. . . . 50\n4.3 Examples of reconsidered tweets filtered by the classifier of Stage 3. 52\n4.4 Examples of discarded or non-reconsidered tweets from Stage 3. . 53\n4.5 General statistics of Propitter . . . . . . . . . . . . . . . . . . . . . 53\n4.6 Classification baseline results on Propitter . . . . . . . . . . . . . . 54\n4.7 Linguistic features between propagandist andnon-propagandist tweets\nand news articles. . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n4.8 Sample tweets from Propitter that display the use of different pro-\npaganda techniques in the collection. . . . . . . . . . . . . . . . . 56\n4.9 Bias statistics of main partitions from PropitterX corpus . . . . . 57\n4.10 Emotion statistics of main partitions from PropitterX corpus . . . 58\n4.11 Region statistics of main partitions from PropitterX corpus . . . . 58\n5.1 Statistics of PropitterX-LR according to the amount of left-wing\nand right-wing tweets per partition. . . . . . . . . . . . . . . . . . 61\n5.2 Results of the political bias experiment. . . . . . . . . . . . . . . . 62\n5.3 Date ranges for each temporal split in PropitterX-TIME. . . . . . 63\n5.4 Distribution of tweets per primary emotion evoked and class in\nPropitterX . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n5.5 Comparison of the performance of emotional and neutral classifiers. 67\n5.6 Distribution of tweets per region in the PropitterX-GEO subcol-\nlection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n5.7 Results of training with one region and making predictions on the\nrest. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n6.1 Examples of input token sequences. . . . . . . . . . . . . . . . . . 75\n6.2 Statistics of main partitions from PropitterX corpus . . . . . . . . 77\n6.3 F1 classification results of BERT-CA over the propaganda class\nadding different contextual features. . . . . . . . . . . . . . . . . . 78\n6.4 Examples of fixed and new mistakes by adding contextual features\nto the classifier. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n6.5 Classification results obtained by predicting the bias and the region\nof the tweets in the test set. . . . . . . . . . . . . . . . . . . . . . 85\n6.6 Data distribution for the English and Spanish corpora. . . . . . . 87\n6.7 Official results obtained by our submissions in the DIPROMATS\n2023 shared task. . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n6.8 Bayesian Signed-Rank Test results applied in DIPROMATS Span-\nish Train set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\nA1 Propagandist sources of data considered for the construction of\nPropitter (124 sources). . . . . . . . . . . . . . . . . . . . . . . . . 115\nA2 Non-propagandist sources of data considered for the construction\nofPropitter (120 sources). . . . . . . . . . . . . . . . . . . . . . . 116\nA3 Distribution of tweets per bias and class in main partitions of Pro-\npitterX . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\nA4 Partition considered to train a classifier with proportional sampled\nemotions in PropitterX-EMO . . . . . . . . . . . . . . . . . . . . . 118\nChapter 1\nIntroduction\nThe influence of social networks is widely regarded as incredible when it comes\nto the magnitude, extent, and speed of expansion they achieve constantly, evolv-\ning into a phenomenon that appears everywhere in our current daily lives [1].\nUnfortunately, research findings suggest that these platforms have the potential\nto serve as channels for the dissemination of harmful, deceptive, or manipulative\ninformation [2]. Within the categorization of such information lies the concept of\npropaganda , which can be defined as \u201can expression of opinion or action by indi-\nviduals or groups, deliberately designed to influence opinions or actions of other\nindividuals or groups with reference to predetermined ends\u201d [3].\nPropaganda is frequently linked to the dissemination of news articles and\npolitical campaigns through conventional media outlets (like newspapers or web-\nsites) that prioritize news as their primary content. Nonetheless, certain investi-\ngations have suggested that the sources of information that individuals turn to\nand engage with have undergone a transformation, leading social media to branch\nout from its conventional role as a source of entertainment to also function as an\nonline news provider [4]. In this context, it is observed that the content shared\non such platforms tends to be noticeably shorter in length, noisier yet simpler to\n1\ndigest, allowing for the rapid propagation of messages to a vast audience within a\nmatter of seconds. Social networks are often praised for their potential to enhance\npolitical engagement, so much so that their influence in exacerbating political\npolarization and the widespread dissemination of misinformation has even raised\nconcerns regarding their potential impact on democracy [5]. In this domain, po-\nlitical entities, including both democratic and antidemocratic factions, compete\nfor power and control [6]. This situation raises several concerns regarding the ease\nwith which populations can be influenced or manipulated, and more alarmingly,\nthe potential for such influence to be carried out with malicious intent. Let us\nconsider, for instance, the volume of information that was spread during the 2016\nUS Presidential campaign , aimed to smear the reputation of specific candidates,\nor the safety and health measures that were not handed properly at the peak of\ntheCOVID-19 global infodemic due to the quantity of disinformation disguised\nas reliable news [7].\nPropaganda detection as a computational task has surprisingly not been ex-\nplored as thoroughly in comparison to other categories of information disorders,\nsuch as Fake News or Hoaxes [8]. Consequently, there exist numerous aspects\nwithin this domain that have been neglected or treated in isolation when devel-\noping detection strategies, including but not limited to bias levels, geographical\norigin and emotions evoked. Each of these contextual variables represents a dis-\ntinct dimension or perspective that is linked to techniques of propaganda. For\nexample, Political bias can manifest through \u201cName calling or labeling\u201d and\u201cSlo-\ngans\u201d .Geographical background can relate to \u201cFlag-waving\u201d .Emotions play a vi-\ntal role in techniques such as \u201cLoaded Language\u201d and\u201cAppeal to fear/prejudice\u201d .\nA possible link between propaganda techniques and types of tweets is the neces-\nsity of identifying whether a message is directed specifically at another account\n(perhaps to incite \u201cName calling\u201d or\u201cDoubt\u201d ), simply retweeted (as a form of\n\u201cRepetition\u201d ), or citing other sources (conceivably in an attempt to \u201cAppeal to\n2\nauthority\u201d ) (more information about propaganda techniques can be consulted in\nSection 2.4). These aspects can be crucial when trying to influence the course of\na discussion or argument.\nIn today\u2019s world, there is a pressing need for automated tools designed to\nassist in combating the challenges posed by propagandistic content. The main\ngoal of this research is to explore the concept of propaganda within a social net-\nwork, making comparisons to traditional propaganda while developing customized\nstrategies that correspond to the various forms and degrees this content takes on\na social platform. Through our investigation, this study seeks to evaluate mes-\nsages disseminated on Twitter (currently referred to as \u201cX\u201d) by media outlets\nthat have been classified as either reliable or dubious based on their promotion\nof propaganda. To achieve this goal, we curate a novel corpus specifically fo-\ncused on Twitter-based propaganda, which has been meticulously gathered from\na wide range of news media accounts. Using this distinctive dataset, we propose\na classification methodology that integrates numerous contextual factors, thus\nenhancing the efficacy of propaganda detection, especially when contrasted with\na baseline method that exclusively focuses on the texts of the messages without\naccounting for a broader context.\n1.1 Problem Statement\nPropaganda can be spread from many different sources, social networks being one\nof them. The volume of text-based exchanges in social media have made human\nintervention approaches unfeasible, and recent decisions and rulings by regula-\ntory authorities explicitly mention automatic systems as tools to help mitigate\nthe spread of mischievous content [9], proving their high social relevance.\nShared tasks are being held online to tackle this challenge and research is pub-\nlished to test new algorithms and approaches. The problem is that most of this\n3\nresearch is focused on propaganda extracted exclusively from news articles. Be-\ncause of the lack of resources and limitations of previous work, there is research\nthat acknowledges the room for improvement and necessity of further research\non this subject [10]. To better solve the detection of computational propaganda\nissue, further exploration outside the news articles scope is needed. Since every\nday the influence of social networks grows as they become the main means of dis-\nseminating information, including malicious news and data, the goal of this work\nis to conduct a multidimensional analysis of computational propaganda. One of\nthe characteristics that we have identified as a challenge is the existence of re-\nsources within the domain of news articles. In the course of our research, we have\nposed the question of whether these resources can be not only beneficial but also\npotentially adapted in some manner to facilitate the development of propaganda\ndetection systems that could be implemented within the environment of a social\nnetwork.\nFurthermore, the second challenge that we have detected in our investiga-\ntion is related to the prevalent manner in which propaganda tends to be analyzed.\nIt is often executed with a primary focus on the textual content of the messages,\nthereby neglecting to take into account other factors that are intrinsically linked\nto the dissemination of propaganda. In a study about computational propaganda\nand political big data, Bolsover and Howard [11] suggest that a clear drawback of\nresearch based on big-data platforms is its dependence on readily available infor-\nmation (e.g., join date of the poster, friend counter, number of followers, number\nof total posts, etc.). Nevertheless, factors like geographic location, religious be-\nliefs, political preferences, gender, level of education, and other variables that are\ntypically linked to social behaviors are nearly impossible to obtain from Twitter\ndata. This results in a significantly limited understanding of how these elements\ninfluence the spread of computational propaganda [11]. Consequently, we exam-\nine the question of whether the detection of propaganda could be substantially\n4\nenhanced by incorporating a broader consideration of the context surrounding\nthe messages being analyzed.\n1.2 Research Questions\n\u2022How can the resources from the domain of news articles be used to detect\ncomputational propaganda in Twitter?\n\u2022What are the differences (in terms of topics covered, emotions evoked) in\ncomputational propaganda from tweets based on its context?\n\u2022How can contextual information of messages be incorporated to improve\nthe effectiveness of propaganda detection in them?\n1.3 Main Objective\nTo assess a model for a multidimensional analysis of computational propaganda\nin tweets, taking advantage of resources on news articles, and considering dif-\nferent types of context, allowing to significantly improve the efficacy of current\napproaches.\n1.3.1 Specific objectives\n\u2022To create a new propaganda corpus by collecting a minimum of 200,000\ntweets from both non-propagandist and propagandist news sources on social\nmedia.\n\u2022To evaluate the performance of contextual classifiers on a dataset segmented\nby contextual features (at least two classifiers per feature), and measuring\n5\nperformance using accuracy, precision, recall, and F1 score, with the goal of\nidentifying differences in propaganda based on the contextual features and\nthese metrics.\n\u2022To enhance propaganda detection in a statistically significant manner by\ntraining a classifier that incorporates contextual features such as bias, coun-\ntry of origin, and emotions evoked by texts, evaluating performance using\naccuracy, precision, recall, and F1 score.\n1.4 Summary of Contributions\n\u2022A new corpus of propaganda from Twitter, Propitter , with over 385k tweets\nand extended with Political Bias ,Temporal information ,Affective informa-\ntion andGeographic origin as contextual features.\n\u2022Some insights about propaganda from social media. Through a comprehen-\nsive analysis, this study identifies differences in propaganda depending of\nthe contextual features associated with it. In particular, it highlights the\nrole of context in the detection of propagandist tweets, which was previously\nunderexplored in the existing body of related literature.\n\u2022An approach that combines the tweet content and multiple contextual fea-\ntures for a better detection of propaganda in tweets. Using Propitter , our\ncontext-aware classifier exhibited a relative improvement of 7.08% (F1-score\nin the propaganda class) over a baseline without context.\n1.4.1 Academic Production\n1. Casavantes, M., Montes-y-G\u00b4 omez, M., Gonz\u00b4 alez, L. C., & Barr\u00b4 o\u02dc n-Cedeno,\nA. (2023, November). Propitter: A Twitter Corpus for Computational Pro-\n6\npaganda Detection. In Mexican International Conference on Artificial In-\ntelligence. Springer (pp. 16-27). Cham: Springer Nature Switzerland\nThis article introduces Propitter , the Twitter propaganda corpus that\nwe created in the course of this study. The content of this article is included\nin Chapter 4.\n2. Casavantes, M., Montes-y-G\u00b4 omez, M., Hern\u00b4 andez-Far\u00b4 \u0131as, D. I., Gonz\u00b4 alez-\nGurrola, L. C., & Barr\u00b4 on-Cede\u02dc no, A. (2023, January). PropaLTL at\nDIPROMATS: Incorporating Contextual Features with BERT\u2019s Auxiliary\nInput for Propaganda Detection on Tweets. InProceedings of the Iberian\nLanguages Evaluation Forum (IberLEF 2023) co-located with the Confer-\nence of the Spanish Society for Natural Language Processing (SEPLN 2023),\nJa\u00b4 en, Spain, September 26, 2023.\nThis article describes our participation in the DIPROMATS 2023 work-\nshop (a propaganda detection task organized in IberLEF 2023). DIPRO-\nMATS datasets contain propaganda from Twitter accounts of diplomats,\nambassadors, and governmental entities [12]. Part of the content of this\narticle is included in Chapter 6.\n3. Casavantes, M., Montes-y-G\u00b4 omez, M., Hern\u00b4 andez-Far\u00b4 \u0131as, D. I., Gonz\u00b4 alez-\nGurrola, L. C., & Barr\u00b4 on-Cede\u02dc no, A. (2024, January). PropaLTL at\nDIPROMATS 2024: Cross-lingual Data Augmentation for Propaganda De-\ntection on Tweets. InProceedings of the Iberian Languages Evaluation Fo-\nrum (IberLEF 2024) co-located with the Conference of the Spanish Society\nfor Natural Language Processing (SEPLN 2024), Valladolid, Spain, Septem-\nber, 2024.\nThis article describes our participation in the DIPROMATS 2024 work-\nshop (a propaganda detection task organized in IberLEF 2024).\n4. Casavantes, M., Arag\u00b4 on, M. E., Gonz\u00b4 alez, L. C., & Montes-y-G\u00b4 omez, M.\n7\n(2023, October). Leveraging posts\u2019 and authors\u2019 metadata to spot several\nforms of abusive comments in twitter. Journal of Intelligent Information\nSystems, 61(2), 519-539.\nThis article is about experiments conducted to improve the detection\nof multiple types of Hate Speech using contextual attributes of users and\ntheir posts on Twitter.\n5. Casavantes, M., Montes-y-G\u00b4 omez, M., Hern\u00b4 andez-Far\u00b4 \u0131as, D. I., Gonz\u00b4 alez-\nGurrola, L. C., & Barr\u00b4 o\u02dc n-Cedeno, A. (2024). PropitterX : A Twitter-based\nPropaganda Corpus Extended with Multiple Contextual Features. Submitted\nand currently under review in Language Resources & Evaluation.\nThis article describes the extension of Propitter with contextual at-\ntributes, creating PropitterX , data sub-collections, and corresponding ex-\nperiments. The content of this article is included in Chapter 5.\n6. Casavantes, M., Montes-y-G\u00b4 omez, M., Hern\u00b4 andez-Far\u00b4 \u0131as, D. I., Gonz\u00b4 alez-\nGurrola, L. C., & Barr\u00b4 o\u02dc n-Cedeno, A. (2024). A Contextual-Aware Approach\nto Detect Propaganda by News Outlets in Twitter. Work in progress, with\nthe intention of submitting it to IEEE Transactions on Computational So-\ncial Systems.\nThis article details how we added contextual features to BERT-based\nclassifiers, and the experiments performed on PropitterX and DIPRO-\nMATS . The content of this article is included in Chapter 6.\n7. Casavantes, M., Hern\u00b4 andez-Far\u00b4 \u0131as, D. I., & Montes-y-G\u00b4 omez, M. (2025).\nEntre la Informaci\u00b4 on y la Manipulaci\u00b4 on: Detectando Propaganda en Tuits.\nSubmitted (December 2024) and accepted (February 2025) in the Komputer\nSapiens journal.\nThis article summarizes in Spanish our findings on propaganda detec-\ntion using contextual features in the Propitter corpus.\n8\n1.5 Document Outline\nThe remainder of this thesis is organized as follows:\nChapter 2 contains an overview about the theoretical background concepts\nthat serve as the foundation for the experiments and analyses that follow.\nChapter 3 presents related work on computational propaganda detection,\nwhere we discuss the main contributions and shortcomings of previous studies.\nChapter 4 describes the construction stages of our propaganda dataset from\nTwitter, denoted as \u201c Propitter \u201d.\nChapter 5 introduces 4 sub-collections from Propitter and experiments\nbased on contextual features ( political bias, geographical origin, affective infor-\nmation, temporal splits ).\nChapter 6 describes a classification approach for propaganda detection that\nleverages both content of tweets and contextual features.\nChapter 7 ends with our conclusions, scope, limitations and future work.\n9\nChapter 2\nBackground\n2.1 Machine Learning Algorithms\nNatural Language Processing (NLP) is a branch of computer science and Artificial\nIntelligence (AI) that employs machine learning techniques to allow computers\nto comprehend and interact using human language1.Machine Learning (ML)\nfocuses on enabling computers to change or adjust their actions (like making\npredictions), ensuring that these actions become increasingly accurate by mea-\nsuring how closely the selected actions reflect the correct ones [13]. The multi-\ndisciplinary nature of machine learning becomes apparent as it is inspired by\nconcepts from neuroscience and biology, statistics, mathematics, and physics, al-\nlowing computers to learn.\nMachine Learning systems can be classified into broad groups according to the\namount and type of supervision they get during training. Some of these cate-\ngories are: supervised learning, unsupervised learning, semi supervised learning,\nand reinforcement learning [14]. When we feed data and the desired solutions or\nlabels to an algorithm, we are talking about supervised learning, and a typical\n1https://www.ibm.com/topics/natural-language-processing\n10\nFigure 2.1: Text classification with conventional techniques in each segment. Identifi-\ncation of key features is vital for traditional approaches, whereas deep learning meth-\nods can automatically extract features. Flowchart adapted from [16].\ntask in this category is classification .\nThe classification problem consists of taking input vectors and deciding which\nof N classes they belong to, based on training from instances of each class. In\none-class and multi-class classification problems, each example has one or more\nlabels respectively, but for both tasks the set of classes covers the whole possible\noutput space [13].\nFor this research, computational propaganda detection is treated as a text\nclassification or categorization task, which is to assign a new document to one of\na pre-existing set of document classes [15]. Text classification can be carried out\nunder a traditional or a deep analysis (see Figure 2.1).\n2.1.1 Traditional Method for Text Classification\nFeature Extraction\nTraditional Machine Learning uses a prominent feature representation to analyze\nand extract relevant insights from text data in NLP problems: Bag-of-Words .\nThis representation model, commonly abbreviated as BoW, treats each word in\na collection of documents as a feature, and since each document only contains a\nsmall subset of the whole vocabulary, BoW is an extremely sparse representation.\n11\nThe value assigned to individual features can be either positive (if a given word\nexists within the document) or zero (if a given word is absent). The positive\nvalues can be term frequencies or simple binary indicators. For example, let us\nconsider the next two documents:\n\u2022Doc1: \u201cthe weenie dog chases a cat\u201d\n\u2022Doc2: \u201cmy cat likes dry food\u201d\nA BoW representation of these sentences, filled with binary indicators,\nwould look like Table 2.1, where each column refers to a term and each row\nis a document.\nTable 2.1: Example of a Bag-of-Words .\nthe weenie dog chases acat my likes dry food\nDoc1 1 1 1 1 1 1 0 0 0 0\nDoc2 0 0 0 0 0 1 1 1 1 1\nAlternatively, a BoW can also consider character n-grams (sequences of n\nnumber of items, in this case characters) as features (Table 2.2):\nTable 2.2: Example of a Bag of Character 3-grams.\nthe wee een eni nie dog cha has ase ses cat ...\nDoc1 1 1 1 1 1 1 1 1 1 1 1 ...\nDoc2 0 0 0 0 0 0 0 0 0 0 1 ...\nThere may be some applications (where a binary input is strictly required, or\nwhen presence is more important than frequency) for which binary representations\nare good enough. However, if frequency is indeed relevant for the task at hand,\nthe use of frequencies of terms is a better way to fill the weights in the BoW.\nTo achieve this, we attribute a weight to each term within a document, which is\n12\ndetermined by how frequently that term appears in the document. Our aim is to\ncalculate a score that reflects the relationship between a term tand a document\nd, taking into account the weight of tind. The most straightforward method is\nto set the weight equal to the number of times term tappears in document d.\nThis method of assigning weights is known as term frequency [17].\nRaw term frequency, as explained above, faces an issue: all terms are treated\nwith equal importance. To mitigate the influence of terms that appear too fre-\nquently, a weighted variant is essential. For this reason, it is standard practice\nto use the document frequency d ft, which is defined as the number of documents\nwithin the collection that includes a term t.\nHow is the document frequency dfof a term used to adjust its weight?\nDenoting the total number of documents in a collection as N, we define the\ninverse document frequency ( idf) of a term tin the following manner:\nid ft= logN\nd ft(2.1)\nAs stated in [17], we can combine the definitions of term frequency and\ninverse document frequency to create a combined weight for each term in each\ndocument.\nThe tf-idf weighting method assigns a weight to term tin document d\nrepresented by\ntf-id ft,d=tft,d\u00d7id ft (2.2)\nIn other words, tf-id ft,dprovides a weight for term t in document d that is\n1. at its peak when tappears frequently within a limited number of documents\n13\n(thereby giving those documents significant discriminating power);\n2. diminished when the term appears less frequently in a document, or is found\nin numerous documents (thus providing a weaker relevance indication);\n3. at its lowest when the term is present in nearly all documents.\nTo summarize the BoW model, the universe of words (or terms) corresponds\nto the dimensions (or features), turning them into a sparse multidimensional\nrepresentation, where the ordering of the terms is not used.\nWord Embeddings\nWord ordering conveys semantics that cannot be inferred from the bag-of-words\nrepresentation. For example, consider the following pair of sentences:\n\u2022\u201cThe cat chased the mouse\u201d\n\u2022\u201cThe mouse chased the cat\u201d\nClearly, the two sentences are very different but they are identical from the\npoint of view of the bag-of-words representation. For longer segments of text, term\nfrequency usually conveys sufficient evidence to robustly handle simple machine\nlearning decisions. This is one of the reasons that sequential information is rarely\nused in simpler settings. On the other hand, more sophisticated applications with\nfine-grained nuances require a greater degree of linguistic intelligence. A common\napproach is to convert text sequences to multidimensional embeddings because\nof the wide availability of machine learning solutions for multidimensional data.\nHowever, the goal is to incorporate the sequential structure of the data within\nthe embedding. Such embeddings can only be created with the use of sequencing\ninformation because of its semantic nature [18]. The simplest approach is to use\na 2-gram embedding:\n14\n\u2022For each pair of terms tiandtjthe probability P(tj|ti)that term tjoccurs\njust after tiis computed.\n\u2022A matrix Sis created in which Sijis equal to [ P(ti|tj)+P(tj|ti)]/2.\n\u2022Values of Sijbelow a certain threshold are set to zero.\n\u2022The diagonal entries are set to be equal to the sum of the remaining entries\nin that row. This is done in order to ensure that the matrix is positive\nsemi-definite.\n\u2022The top- keigenvectors of this matrix can be used to generate a word em-\nbedding.\nThe linguistic power (semantic representation) in the embedding depends\nalmost completely on the type of word-word similarity function that is leveraged\n[18].\nThe main idea behind this technique is that words that are similar in context\n(at least according to the text from which the embeddings algorithm trained with)\nappear closer to each other in a multidimensional space. Based on this, one can\nuse the position of the words in this space to compute the similarity and relation\nthat the text has with its surroundings.\nLinear Support Vector Machine as Classifier\nIntroduced by Vapnik in 1992 [19], the Support Vector Machine (SVM) is a\npopular model in machine learning due to its versatility and power. This method\nworks by finding the support vectors , the most useful data points in each class in\na dataset that lie closest to a line called classification line . This line separates\nthe classes in the best way possible (maximizing the margin or largest radius\naround it) before we hit a data point. This leads to an interesting feature of\n15\nthese algorithms: after training this model we can throw away all data except for\nthe support vectors, and use them for classification [13].\nFigure 2.2: Example of a Linear Support Vector Machine. The solid diagonal line\nrepresents the classification line, while the dotted lines enclosing it represent the max-\nimum margin between the classes.\nIn the \u201clinear\u201d version of the SVM (with a linear kernel), we can determine\nour classifier line by using the standard equation of the straight line:\ny=w\u00b7x+b (2.3)\nwhere wis the weight vector, xis the particular input vector, and bis the\nbias weight. For instance, we can use the classifier shown in Figure 2.2 by saying\nthat any xvalue that gives a positive value for yis above the line and therefore an\nexample of the orange class, and any xthat gives a negative value becomes part\nof the blue class. A few distance constraints need to be added to take account of\nthe margin. If we consider Mto be the perpendicular distance between a dashed\nline and the classification line, we need to check if the absolute value of yis less\n16\nthan M:\nclass (x) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3orange, ify=w\u00b7x+b\u2265M\nblue, ify=w\u00b7x+b\u2264 \u2212M\nHowever, this technique alone is not appropriate for datasets with outliers,\nsince these kind of data points can make a classification problem non-linearly\nseparable. In order to generalize and be useful for most real world cases, it needs\nto allow for some mistakes. This would be called a Soft Margin Classifier , as\nit has to look for the widest margin with the fewest classification mistakes, also\nnamed margin violations [14]. In a mathematical way, the function that we want\nto minimize is:\nL(w, \u03f5) =w\u00d7w+\u03bbRX\ni=1\u03f5i (2.4)\nwhere Ris the number of misclassified points, and each \u03f5iis the distance\nto the correct boundary line for the missing point [13]. We can see that a new\nparameter is included, \u03bb(which is also known as the Chyperparameter). A small\n\u03bbmeans that we prioritize a large margin over a few errors, a large value of \u03bb\nrepresents the opposite.\n2.1.2 Deep Learning Method for Text Classification\nDeep Neural Networks, commonly referred to as DNNs, are intricate systems that\nsimulate the complex functionality of the human brain, enabling them to auto-\nmatically learn and extract high-level features from data, and as a result, they\noften outperform traditional modeling techniques in various domains [16]. De-\npending on the specific characteristics of the data used, the corresponding input\n17\nword vectors are fed into the DNN for the purpose of training, and this process\ncontinues iteratively until a termination condition is satisfied. The effectiveness\nand performance of the training model are subsequently assessed and validated\nthrough various downstream tasks. These downstream tasks not only serve to\nevaluate the model\u2019s accuracy but also highlight the practical applicability of the\nDNN in real-world scenarios.\nPre-trained language models [20] are remarkable at grasping global semantic rep-\nresentations and elevate NLP tasks. They typically use unsupervised techniques\nto automatically discover semantic knowledge and then set up pre-training tar-\ngets, allowing machines to learn how to comprehend semantics better [16].\nAs a pre-trained language model, the Transformer architecture relies on a compre-\nhensive attention model and demonstrates efficacy in the domains of language, vi-\nsion, and reinforcement learning, with its key component being the self-attention\nmechanism, which can be perceived as a graph-like induction bias that links all\nthe tokens in a sequence through association-driven pooling operations [21].\nBidirectional Encoder Representations from Transformers\nThis representation technique better known as BERT by its initials, that can also\nbe used to perform classification, solves a restriction that previous pre-trained lan-\nguage models had, unidirectional architectures. By masking a portion of tokens\nfrom the input in a random process called \u201cmasked language model\u201d , a BERT\nrepresentation is able to combine left and right contexts, generating a deep bidi-\nrectional Transformer. BERT\u2019s framework consists of a pre-training step, which\ninvolves training parameters on unlabeled data, and a fine-tuning step that con-\ntinues adjusting these parameters, only this time with labeled data from down-\nstream tasks. This process is illustrated in Figure 2.3 as a question-answering\nexample.\n18\nFigure 2.3: General pre-training and fine-tuning mechanisms in BERT, borrowed from\n[22]. Both pre-training and fine-tuning of parameters use the same architecture.\nIn BERT, a \u201csentence\u201d refers to an arbitrary span of adjacent text, and a\n\u201csequence\u201d indicates the input token sequence. Each sequence has special tokens,\nsuch as \u201c[CLS]\u201d which symbolizes the beginning of the input, and \u201c[SEP]\u201d , which\nseparates sentences. The construction of an input representation for a given token,\npictured in Figure 2.4, is the sum of the token, segment and position embeddings.\nFigure 2.4: Example of BERT input representation, adopted from [22].\n19\n2.2 Evaluation measures\nClassification tasks in supervised learning involves comparing predictions against\nthe true labels of instances to train models. The possible outcomes of this com-\nparison are shown in Figure 2.5.\nFigure 2.5: The four outcomes of a confusion matrix.\nThe typical metric used to assess tasks such as classification is the F1 score,\nwhich serves as a balanced indicator of both precision (ratio of accurate positive\nresults among all instances labeled as positive by a model) and recall (ratio of\naccurate positive results among all the actual positive instances in the data)\nin the detection process [23]. The F1 score is precisely the harmonic mean of\nthese values. The calculations for precision, recall, and F1 are derived from the\nfollowing terms:\n\u2022True positives ( TP): the count of items that have been accurately assigned\nthe class label;\n\u2022False positives ( FP): the count of items that have been inaccurately assigned\nthe class label; and\n\u2022False negatives ( FN): the count of items that have been mistakenly labeled\nwith a non-class label or a different class label.\n20\nSubsequently, the per-class precision, recall, and F1 score are defined as\nfollows:\nPrecision =TP\nTP+FP(2.5)\nRecall =TP\nTP+FN(2.6)\nF1-score = 2\u00b7precision \u00b7recall\nprecision +recall(2.7)\n2.3 Types of information disorders\nAccording to [24], there are two main kinds of information disorders based on the\npurpose behind it: on one hand we have misinformation , which includes unin-\ntentional falseness such as inaccurate dates, statistics or translations; and on the\nother hand there\u2019s malinformation , genuine information deliberately shared with\nan intent to harm, such as moving data intended for confidentiality into the open\ndomain. A middle ground between these two exist in the form of disinformation\n(see Figure 2.6), intentionally false content created with the purpose of causing\nharm (false context, imposter, manipulated or fabricated content).\nNot all propaganda content is generated with bad intentions. For example,\nthere are cases in which propaganda is used to distribute positive messages such\nas raising awareness about the importance of voting, racial equity and the fight\nto promote women\u2019s rights [25]. It can also be used by content creators to find\nattractive ways to address news or events and catch the eye of potential readers.\nHowever, some other disorders fit inside the definition of it as a whole, such as\n21\nFigure 2.6: Types of information disorder, borrowed from [24].\nHoaxes orOpinion Spamming (see Figure 2.7) [2].\nFigure 2.7: Venn diagram of false information on the Internet, borrowed from [2].\n2.4 Propaganda techniques\nThe effectiveness of propaganda and misinformation is based on ideology and\npolarization [26], conveyed through different tactics. Clyde Miller, co-founder of\n22\nthe Institute for Propaganda Analysis (IPA) [27] proposed in 1937 seven devices\nthat appeal to emotions instead of reason [3]: name-calling ,glittering generalities ,\ntransfer ,testimonial ,plain folks ,card stacking , and band wagon . In 2019, Da San\nMartino et al. [28] listed the following 18 techniques:\n1.Loaded language.- To affect an audience by using words and phrases with\nintense emotional connotations (either positive or negative).\nExample: \u201cThe brave volunteers are risking everything to pro-\ntect our children from the dangerous, radical forces trying to\ntear apart our community \u201d.\nIn this example, \u201cbrave volunteers\u201d paints the individuals as heroic\nand selfless, \u201crisking everything\u201d adds urgency and sacrifice to their actions,\n\u201cdangerous, radical forces\u201d uses loaded terms to cast the opposition as\nextreme and harmful, and \u201ctear apart our community\u201d implies that the\nopposition threatens the very fabric of society, adding a sense of fear and\nurgency.\n2.Name calling or labeling.- Using something the target audience either hates\nor loves to label the object of the propaganda campaign.\nExample: \u201cThose reckless communists want to destroy everything we\u2019ve\nworked for. Don\u2019t let their radical agenda ruin our country.\u201d\nIn this example, \u201creckless communists\u201d and\u201cradical agenda\u201d are used\nto label the opposition with negative, derogatory terms, painting them as\ndangerous and out of control.\n3.Repetition.- Delivering the same message in a sustained manner until the\naudience finally accepts it.\nExample: \u201cA strong economy will make us a strong country and lead us\nto a strong future!\u201d\n23\nThe repeated use of \u201cstrong\u201d here reinforces the idea that strength is\nkey to success.\n4.Exaggeration or Minimization.- Representing something either in an exag-\ngerated way or making it seem less important than it really is.\nExample: \u201cWith this new policy, every family will have a perfect\nlife\u2014no poverty , no struggles , just endless prosperity! \u201d\nThe exaggeration of the policy\u2019s potential impact creates unrealistic\nexpectations and plays on people\u2019s hopes and desires.\n5.Doubt.- To question the credibility of someone or something.\nExample: \u201cCan we really trust leaders who have failed us before? Are\nwe sure they\u2019ll make the right choice this time?\u201d\nThis creates doubt about the competence and reliability of the current\nleadership, making people question their past actions and motivations.\n6.Appeal to fear/prejudice.- Seeking to support an idea by infusing anxiety\nand/or panic in a population towards an alternative, sometimes based on\npreconceived judgements.\nExample: \u201cIf we don\u2019t pass this new security law now, our nation will\nbe vulnerable to terrorist attacks . The enemy is already plotting against us ,\nand without these measures, we could lose everything .\u201d\nIn this example, fear of terrorism and danger is used to persuade people\nto support the law, without addressing the actual merits of the law itself.\nThe focus is on stoking fear to push for action.\n7.Flag-waving.- Playing on intense national sentiment (regarding a particular\ngroup, such as race, gender, or political affiliation) to advocate for a specific\naction or idea.\n24\nExample: \u201cOur country, our pride! United we rise, divided we fall. To-\ngether, we are unstoppable!\u201d\nThis example aims to stir strong nationalistic feelings and a sense of\nunity among the people. The message appeals directly to the sense of\nownership and pride citizens feel toward their nation, with a classic rallying\ncry that emphasizes the importance of unity.\n8.Causal oversimplification.- Attributing a problem to a single cause when\nthere are several factors contributing to it.\nExample: \u201cCrime is rising because our borders are weak. Close them,\nand our streets will be safe again.\u201d\nThis example reduces a complex issue (rising crime) to a single cause\n(weak borders) and implies that solving the simplified issue (closing borders)\nwill immediately fix the problem. It ignores other potential contributing\nfactors to crime, making the situation seem much more straightforward\nthan it is.\n9.Slogans.- A brief phrase that may include labeling and stereotyping.\nExample: \u201cMake America Great Again. \u201d\nThis slogan, widely used in political campaigns, invokes a sense of nos-\ntalgia and national pride, implying that a return to past greatness is possible\nif the right leader is chosen, while subtly casting doubt on current leader-\nship or societal changes. The ultimate goal is to unite supporters under a\nvision of a restored ideal.\n10.Appeal to authority.- Claiming that a statement is accurate solely based\non the endorsement of a credible authority or expert in the field, without\nany additional evidence.\nExample: \u201cExperts agree this is the only solution for our future.\u201d\n25\nThis example uses the authority of \u201cexperts\u201d (without specifying who\nthey are or what their credentials are) to persuade people that a partic-\nular course of action is the right one. This technique relies on the im-\nplied trustworthiness of experts to make people accept the message without\nquestioning it. It subtly suggests that dissent is unreasonable because the\n\u201cauthoritative\u201d opinion has already been established.\n11.Black-and-white fallacy, dictatorship.- Offering just two alternative choices\nas if they are the only options, despite the existence of additional possibil-\nities.\nExample: \u201cYou\u2019re either with us, or you\u2019re against us.\u201d\nThis slogan presents only two extreme options, ignoring any middle\nground or nuance. It forces people to choose between two opposing sides,\npainting the situation as if there are no other alternatives, which simplifies\ndecision-making but manipulates emotions and stifles critical thinking.\n12.Thought-terminating clich\u00b4 e.- Expressions or terms that discourage critical\nthinking regarding a specific subject.\nExample: \u201cIt is what it is.\u201d\nBy providing a simple, seemingly final explanation, this phrase discour-\nages questioning or deeper analysis. Such clich\u00b4 es are used to stop people\nfrom challenging the narrative or considering alternative perspectives, ef-\nfectively ending the conversation.\n13.Whataboutism.- Undermine an adversary\u2019s stance by accusing them of\nhypocrisy without explicitly refuting their claims.\nExample: \u201cWhy are you criticizing our government? What about all\nthe corruption in other countries?\u201d\nThis techinque deflects attention from the issue at hand by shifting focus\n26\nto a different, often unrelated problem. Instead of addressing the original\ncriticism, it redirects the conversation, implying that because other nations\nmight have worse issues, the current problem should be ignored or dismissed.\nIt\u2019s used to avoid accountability and derail meaningful discussion.\n14.Reductio ad Hitlerum.- Convincing an audience to reject a particular action\nor concept by indicating that it is favored by groups that the target audience\ndespises.\nExample: \u201cYou support this policy? Well, Hitler also believed in strong\nnational borders, so you must be a Nazi. \u201d\nThis argument is dismissed by drawing a comparison to Hitler or Nazis,\nregardless of the actual merits of the policy or idea, in an attempt to de-\nmonize the position by associating it with a universally condemned figure,\nthereby avoiding any real discussion or analysis.\n15.Red herring.- Introducing unrelated content to the topic at hand, causing\nthe focus of everyone to shift away from the arguments presented.\nExample: \u201cWe shouldn\u2019t worry about the government\u2019s new surveil-\nlance program. Think about how much we\u2019ve improved our public trans-\nportation system! More buses and trains mean less traffic, fewer accidents,\nand cleaner air.\u201d\nThe message diverts attention from the controversial surveillance pro-\ngram (the real issue) by shifting the focus to unrelated improvements in\npublic transportation, which does nothing to address concerns about pri-\nvacy.\n16.Bandwagon.- Seeking to convince the intended audience to participate and\nfollow the same course of action because \u201ceveryone around them is doing\nit\u201d.\n27\nExample: \u201cEveryone\u2019s switching to this new energy drink.\nIt\u2019s the most popular choice among athletes and fitness enthusiasts!\nDon\u2019t get left behind\u2014join the trend and try it for yourself!\u201d\nThis example encourages people to buy the energy drink simply because\nit\u2019s popular, implying that if everyone else is doing it, they should too,\nwithout offering any real reasons why it\u2019s a better product.\n17.Obfuscation, intentional vagueness, confusion.- Employing intentionally\nvague language, so that the audience may have its own interpretation.\nExample: \u201cOur new program\u2019s holistic approach\nto optimizing fiscal efficiency aligns perfectly with our\nlong-term strategic objectives , enhancing overall national prosperity.\u201d\nIn this example, the language is vague with words like \u201cholistic ap-\nproach\u201d ,\u201coptimizing fiscal efficiency\u201d , and \u201clong-term strategic objectives\u201d ,\nwhich make the program sound very positive without actually explaining\nwhat it entails or how it will impact the average person. The lack of clear,\nspecific information is meant to obscure the true nature of the program,\nleading the audience to trust it without questioning the details.\n18.Straw man.- When an opponent\u2019s suggestion is replaced with a comparable\none that is subsequently countered instead of the original.\nExample: \u201cOpponents of our healthcare reform\nargue that we shouldn\u2019t invest in better healthcare at all , claiming that\nit\u2019s a waste of taxpayer money. But we know that a healthier nation is\na stronger nation, and rejecting improvements to our healthcare system\nwould be irresponsible and harmful.\u201d\nIn this example, the opposition\u2019s argument is misrepresented as being\nagainst all healthcare investment, which is likely not their actual position.\nThe strawman simplifies and distorts the argument to make it easier to\n28\nFigure 2.8: Frequency of propaganda techniques in the PTC corpus [28].\nrefute by framing it as a reckless stance, rather than addressing the real\nconcerns or nuanced points raised by the opposition.\nThe Figure 2.8 shows an approximation of the most used propaganda tech-\nniques based on the frequency with which they appear in the PTC corpus from\n[28], with the five most popular techniques being:\n1. Loaded language\n2. Name calling\n3. Repetition\n4. Exaggeration or Minimization\n5. Doubt\n29\nChapter 3\nRelated work\nThe emergence of the Internet and social media has significantly altered the\nlandscape of propaganda, enabling a broader range of individuals and groups\nto create and spread propaganda messages, a task that was previously exclusive\nto governments and major organizations [11]. Additionally, it has introduced\nnew opportunities for the swift dissemination of propaganda, achieved through\nthe manipulation of online information algorithms and processes, as well as the\ntargeting of specific audiences using advanced data analysis techniques. Thus, in\nthis section we divide the research focused on computational propaganda outside\nandinside social networks.\n3.1 Computational Propaganda Detection Outside So-\ncial Networks\nThis particular section is dedicated to presenting an overview of the foundational\nresearch studies conducted in the area of computational propaganda detection. At\nthe beginning of these efforts, the primary focus was on identifying propagandistic\n30\nTable 3.1: News articles in TSHP-17 corpus, adapted from [29].\nNews Type Source # of Documents\nTrusted Gigaword News 13,995\nPropagandaThe Natural News 15,580\nActivist Report 17,869\ncontent within various news articles, which required linguistic analyses of these\ndocuments in their entirety. Later studies followed in with a deconstruction of\nthese documents into individual sentences for further investigation. The most\nnoteworthy research contributions that have been made in this particular domain\nare summarized in the subsequent sections below.\n3.1.1 Propaganda as part of Fake News Analysis\nPropaganda detection can be conceptualized as a text classification task. A group\nof scholars conducted an investigation into the language employed by news media\nwithin the realm of political fact-checking and identification of fake news [29].\nTheir study involved a comparison of the linguistic features of authentic news\ncontent against those of satire, fraudulent information in the form of hoaxes, and\npropaganda, with the aim of identifying distinctive characteristics indicative of\nuntrustworthy texts. In order to examine linguistic patterns among various genres\nof articles, a selection of trusted news articles from the \u201cEnglish Gigaword\u201d corpus\n[30] (a large collection of newswire text data in English amassed by the Linguistic\nData Consortium over the course of several years) was analyzed alongside articles\nretrieved from seven unreliable news websites spanning different categories. One\nof the categories explored was propaganda, defined as content designed to deceive\nreaders into believing a specific political or social ideology. Table 3.1 shows the\nquantity of articles in the TSHP-17 dataset introduced by [29].\nThe crawled articles were used for the purpose of News Reliability Predic-\n31\ntion, where a Max-Entropy classifier with L2 regularization was trained. This\ninvolved feeding the classifier with feature vectors consisting of unigrams, bi-\ngrams, and trigrams with a tf-idf weighting scheme.\nAs part of their research findings, they delineate that the most significant\nweighted n-grams associated with reliable news frequently refer to particular lo-\ncations (e.g., \u201cwashington\u201d ) or temporal references ( \u201con monday\u201d ), while promi-\nnently weighted characteristics indicative of propaganda lean towards concep-\ntual generalizations ( \u201ctruth\u201d ,\u201cfreedom\u201d ) along with specific topics ( \u201cvaccines\u201d ,\n\u201csyria\u201d ) [29].\n3.1.2 Propaganda Detection as a Standalone Task\nBeginning with an examination of the shortcomings associated with the TSHP-17\ncorpus, it lacks in providing information about the origins of each news article in\nit. Furthermore, it managed to gather information from a relatively small selec-\ntion of sources, eleven in total, with only two of those sources being categorized\nas propagandistic. Consequently, conducting thorough experiments and anal-\nyses considering the source factor was unfeasible. These limitations prompted\nthe development of a new corpus named QProp [31]. In that study, a binary\nclass classification was conducted, starting to shape propaganda detection as a\nstandalone task and distancing it further from the fake news scope. They con-\nsidered 94 sources of non-propaganda and 10 sources of propaganda (Table 3.2\ndisplays the distribution of their collection). The labels were produced by us-\ning the news sources as labeling mechanisms\u2014commonly referred to as distant\nsupervision from MediaBias/FactCheck1, a website that categorizes media, jour-\nnalists, and politicians. By increasing the size of their corpus selecting more\npropagandist news sources, systems trained with this data could learn to dis-\ntinguish propaganda instead of learning the writing and publishing style of the\n1https://mediabiasfactcheck.com/\n32\nTable 3.2: News articles in QProp, adapted from [31].\nNews Type Sources # of Documents\nTrustworthy 94 45,557\nPropagandistic 10 5,737\nnews outlets. Their hypothesis was that representations based on writing style\nand readability can generalize better than approaches based on word-level rep-\nresentations. For their experiments, they used a Max-Entropy classifier with L2\nregularization, feeding it features based on word n-grams, lexicons, vocabulary,\nandNELA [32]. Their findings demonstrate that models capturing writing style\nand text complexity exhibit superior effectiveness compared to word n-grams.\n3.1.3 Fine-Grained Analysis of Propaganda\nIn 2019, the PTC corpus was introduced [28], encompassing new features com-\npared to those present in previous collections. This dataset was characterized\nby its manual annotation process, which stood in contrast to the conventional\npractice of employing news sources as distant supervision . Furthermore, this\ndataset was annotated at the span level , an advancement that involved identify-\ning and marking specific snippets of text, thereby allowing for a more granular\nanalysis rather than categorizing entire documents as a whole. The second twist\nof this research was the transition from a binary classification framework to a\nmore complex multi-class classification scheme, which took into account a total\nof 18 distinct propaganda techniques, thereby enhancing the depth of the study\nin terms of propaganda analysis. Although there are some techniques that appear\nonly a few times in the collection (e.g. \u201cstraw man\u201d , with only 15 instances out of\na total of 7,485), it is worth mentioning that Loaded language andName calling\nor labeling , the two most popular techniques (appearing 3,841 times combined,\nmore than half the instances in the whole collection) share an association with\n33\nTable 3.3: News articles in PTC, adapted from [28].\nNews Type Sources # of Docs. Prop. Techniques Instances\nNon-propagandistic 36 79 \u2013 \u2013\nPropagandistic 13 372 18 7,485\nthe use of emotions as a way to \u201cpush\u201d propagandistic content into the messages.\nAs an interesting fact, the authors of PTC corpus now labeled the \u201ctrust-\nworthy\u201d class as \u201cnon-propagandistic\u201d , perhaps as a result of the difference in\ntask purpose between fake news and propaganda detection. Table 3.3 shows the\ndistribution of the PTC corpus.\nNLP4IF 2019\nIn 2019, the second workshop on NLP for Internet Freedom (NLP4IF)2presented\ntwo subtasks involving propaganda detection using the PTC corpus, one for iden-\ntification of propagandist texts at the fragment-level and a binary classification\ntask at the sentence-level [33]. In the Sentence-Level Classification in the Test\nSet, 9 out of 10 teams reported the use of BERT [22] in some form to predict\nlabels, either independently or as part of an ensemble. Other teams from the top\nscores (shown in Table 3.4) found useful to consider lexical features, sentiments,\nand tackling the class imbalance of the set to achieve their final results. We can\nobserve that all the top strategies proposed for this task were dependent on Trans-\nformer architectures. For the Sentence-Level Classification in the Development\nSet, the best performance was achieved by a combination of three classifiers [34]:\ntwo based on BERT [22] and one on Google\u2019s Universal Sentence Encoder [35].\n2http://www.netcopia.net/nlp4if/2019/index.html\n34\nTable 3.4: Top Official Results for NLP4IF SLC Task - Test Set. Adapted from [33].\nRank Classifier F1 System Description\n1 BERT 0.6323Attention Transformer trained on Wikipedia and\nBookCorpus.\n2 BERT 0.6249Over-sampled training data and performed cost-\nsensitive classification.\n3 BERT 0.6249 Ensemble of models.\n4BERT + LR\n+ CNN0.6230V oting ensemble with features from FastText em-\nbeddings, readability, emotions and sentiments.\n5 N/A 0.6183 Not reported.\n6BERT +\nUSE0.6138Ensemble of two BERTs and Universal Sentence\nEncoder.\n7BERT +\nbi-LSTM +\nXGBoost0.6112Ensemble with features from GloVe embeddings,\naffective and lexical representations.\nSemEval-2020 Task 11\nTask 11 of SemEval-2020 focused on the detection of propaganda techniques\nin news articles [36], concentrating on fine-grained analysis of texts that could\ncomplement existing strategies. Again, practically all approaches submitted for\nthis task relied on systems based on Transformers. The best ranked team for\nSpan Identification [37] trained several of these architectures and combined them\nin the end as an ensemble. This result, along with the rest of participants among\nthe top five teams, is displayed in Table 3.5.\n3.1.4 Propaganda from Digital Newspapers and Web Pages\nPolonijo et al. [38] introduced a deep learning technique to merge sentiment scores\nwith Word2Vec [39] vectors, resulting in a representation that encompasses both\nsemantic and emotional data, which leads to a more accurate model for propa-\n35\nTable 3.5: Top Results for SemEval-2020 Task 11 Span Identification - Test Set.\nAdapted from [36].\nRank Classifier F1 System Description\n1Ensemble\nof 6+ archi-\ntectures51.74Complex heterogeneous multi-layer neural net-\nwork with BIO encoding, Part-of-Speech and\nNamed Entity embeddings.\n2 RoBERTa 49.88Ensemble of models with oversampling by produc-\ning silver data.\n3 RoBERTa 49.59Ensemble with attached CRF for sequence label-\ning.\n4BERT+\nBiLSTM48.16Model with extra features (PoS, NE, sentiment)\nand fine-tuned on 10k additional propaganda arti-\ncles.\n5 BERT 46.63Used masked language modeling to domain-adapt\ntheir base model with 9M articles (fake, suspi-\ncious, hyperpartisan news).\nganda classification. Word2Vec vectors serve as an effective tool for understanding\nthe semantic significance of words, and an emotional lexicon incorporated into\nVADER\u2019s [40] sentiment analysis yields a sentiment score for the text that en-\ncapsulates emotional insights. This approach maintains the adaptability of the\nWord2Vec vector by fusing it with the outcomes of sentiment analysis. The data\nthey analyzed consists of two parts: propaganda data from texts in English from\nthe Xinhuanet3and CGTN4newspapers\u2019 Internet portals, and non-propaganda\ntexts from news articles from Reuters5and TheHill6. The experiments they con-\nducted using a Word2Vec model with sentiment data alongside standard deep\nlearning techniques for propaganda detection from extracted text from web pages\n(comprising 37,503 lines of propaganda and 43,613 lines of nonpropaganda text)\nshowed that their strategy enhances the accuracy of propaganda classification.\n3http://www.xinhuanet.com/english/\n4https://www.cgtn.com/\n5https://www.reuters.com/\n6https://thehill.com/\n36\n3.2 Computational Propaganda Detection in Social\nNetworks\nThe research studies summarized below considered and evaluated forms of pro-\npaganda that have been disseminated across various social networking platforms,\nthereby highlighting the growing impact and influence that these digital commu-\nnication channels apply on public opinion and societal discourse.\n3.2.1 Propaganda Disseminated in Twitter\nWang et al. [10] explored propaganda from different sources. They hypothesize\nthat propagandistic sources are sophisticated and creative, and that they will find\nnew ways to deceive by evading trained classifiers. The novelty of their approach\nlies in cross-domain learning , recognizing the scarcity of labeled data where do-\nmains represent different types of sources, such as news articles, social media\nposts, and public speeches. The data collections used for their experiments fall\ninto precisely these three types of sources. Table 3.6 shows distribution of these\ncorpora. They created a collection of speech transcripts from four politicians,\narranged in ordered pairs. Trump andObama as contemporary speakers. Trump\nwas seen as more propagandist than Obama . They also use Joseph Gobbels (Nazi\nPropaganda Minister) and Winston Churchill (UK Prime Minister) as important\nfigures around the time of World War II, Gobbels supposedly being more pro-\npagandistic than Churchill . All four of these politicians have given propaganda\nspeeches, and the author\u2019s supposition is that two of the speakers exhibit less\npropaganda than the other two.\nWith news as a source, they combined and reorganized the datasets used in \u201cHack\nthe News\u201d7, to build an article-level corpus and a sentence-level corpus. With\n7https://www.datasciencesociety.net/hack-news-datathon/\n37\nTable 3.6: Distribution of cross-domain corpora from [10]. \u201c+Prop\u201d and\u201c-Prop\u201d\nmeans a politician was considered more propagandistic or less propagandistic, respec-\ntively.\nSource Class Documents Sentences\nSpeechesTrump (+Prop) 100 7,985\nObama (-Prop) 100 8,336\nGoebbels (+Prop) 44 4,482\nChurchill (-Prop) 44 4,131\nTOTAL 288 24,934\nNewsPropagandistic 3,899 3,938\nNormal 3,899 3,938\nTOTAL 7,798 7,876\nTweetsPropagandistic \u2013 8,963\nNormal \u2013 8,963\nTOTAL \u2013 17,926\ntweets as a source, they combined two collections, Twitter Internet Research\nAgency Dataset ( Twitter IRA )8from 2018, and twitter7 [41], a 2009 collection of\nalmost 476 million tweets. They used the 8,963 tweets from the Twitter IRA as\nexamples of propaganda, and an equal number of tweets extracted from twitter7\nas examples of normal instances.\nThe four propaganda detection methods that they used were divided in two\ntypes:\n\u2022Attribute-based models: Logistic Regression and Support Vector Machines.\nThe features considered were word count, weighted n-grams with TF-IDF,\nand LIWC [42] word categories.\n\u2022Models based on neural networks, a Long Short-Term Memory (LSTM)\nbaseline and a modification to this baseline, which is a contribution of\nthis work that they call the LSTM or Long Short-Term Memory Regressor\n8Available at https://archive.org/details/twitter-ira\n38\n(LSTMR) Pairwise Classification Model (they designed a model that relaxes\nthe constraints of strict labeling on rankings).\nAs part of their analysis, they concluded that the best cross-domain results\nare obtained when training with news and applying those models to speeches or\ntweets, and that the cross-domain classification excluding names leads to poorer\nperformance. Their findings also suggest that exaggerations (e.g. \u201cabsolutely\u201d )\nand negative emotions (e.g. \u201clies\u201d or\u201cdevastating\u201d ) play a key role in audi-\nence manipulation. Regarding the characteristics of LIWC, words that express\nnegative emotions are typical of propaganda.\nTWEETSPIN is a collection of tweets that feature weak labels indicative of\npropaganda techniques [43]. It contains 210,392 tweets with 19 labels, referring\nto 18 propaganda techniques and 1 non-propaganda label (see Table 3.7). The\npropagandist instances were selected by retrieving tweets containing keywords re-\nlated to the propaganda techniques. Following this, they introduced MVPROP,\na transformer-based model for multi-view propaganda detection, which assimi-\nlates multi-view contextual embeddings through pairwise cross-view transformers.\nThey illustrated how enriching the input tweet text with semantic, relational, and\nknowledge views significantly enhances performance compared to other baseline\napproaches. Their experiments also confirmed the adaptability of their trained\nmodel in detecting propaganda within news articles.\nAs part of IberLEF 2023 [44], DIPROMATS was organized with the goal\nof identifying techniques to detect propagandistic tweets from governmental and\ndiplomatic entities [45]. It introduced three subtasks across two languages, Span-\nish and English: i) A binary classification task to determine whether or not a\ntweet employs propaganda techniques, ii) A multiclass, multilabel classification\ntask, where systems must ascertain, for each tweet, which of the 5 available cat-\negories ( Not propagandistic ,Appeal to Commonality ,Discrediting the Opponent ,\n39\nTable 3.7: Dataset statistics for TWEETSPIN, adapted from [43].\n#Total Propaganda Tweets 157,327\n#Total Non-Propaganda Tweets 53,165\nPropaganda technique # Tweets\nLoaded Language 18,365\nName Calling/Labeling 17,096\nReductio Ad Hitlerium 15,677\nDoubt 14,993\nAppeal To Fear/Prejudice 14,654\nWhataboutism 13,887\nRepetition 13,285\nSlogans 10,190\nAppeal To Authority 8,539\nFlag-Waving 7,675\nExaggeration, Minimization 5,416\nBlack-And-White Fallacy 4,872\nThought-terminating cliches 3,781\nBandwagon 2,547\nRed Herring 2,315\nCausal oversimplification 1,790\nStraw man 1,265\nObfuscation, Intentional Vagueness, Confusion 1,048\nLoaded Language .Appeal to Authority ) it belongs to, and iii) A fine-grained\nclassification task where systems need to identify which specific techniques are\npresent in the tweet. The distribution of the DIPROMATS dataset is shown in\nTable 3.8. As this corpus was used in this study to conduct experiments, more\ninformation about its data distribution is provided in Section 6.3.4 (Table 6.6).\n3.2.2 Propaganda Disseminated in Reddit\nBalalau and Horincar examined how propaganda influences six prominent polit-\nical forums on Reddit (see Table 3.9) that target a varied audience across two\nnations, the US and the UK [46]. They determined that the political bias of\nmedia sources serves as a significant predictor of the likelihood of propagandistic\n40\nTable 3.8: Data distribution for the English and Spanish DIPROMATS corpora.\nClass Train (ENG) Test (ENG) Train (SPA) Test (SPA)\nPropaganda 1,974 N/A 1,199 N/A\nNon-propaganda 6,434 N/A 4,921 N/A\nCountry\nChina 2,170 852 2,178 819\nEuropean Union 2,043 873 1,508 957\nRussia 2,005 955 795 596\nUSA 2,190 924 1,639 1,099\nType of tweet\nTweet 6,742 2,856 3,586 2,302\nQuoted 825 356 888 541\nRetweet 473 227 1,221 401\nReply 368 165 425 227\ncontent being used and that a smaller user community tends to disproportion-\nately disseminate such articles. Furthermore, they found that forums focused on\nless mainstream parties in a country tend to share more biased news, and that\ncultural distinctions may influence the propaganda strategies used. Additionally,\nthey noted that submissions or comments containing a higher volume of propa-\nganda are likely to garner greater user interaction, either assessed through the\nquantity of comments or through upvotes and downvotes.\nTable 3.9: Reddit dataset distribution, adapted from [46]\nSubreddit Submissions Comments\nPolitics 317K 20M\nDemocrats 9.8K 54K\nRepublican 8.2K 41K\nUKPolitics 42K 1.8M\nLabourUK 7K 58K\nTories 1.1K 12K\n41\n3.2.3 Propaganda Disseminated in Facebook\nPushing for a new modality in detection of persuasion techniques in images and\ntexts, the organizers of SemEval-2021 Task 6 [47] used a list of 22 techniques\nbased on previous propaganda research (20 of them applicable to text and 2 to\nimages) to label a collection of memes from Facebook. A total of 26 groups\ndiscussing themes such as politics, vaccines, and gender equality were crawled\nfrom 2020. The annotation step was executed in two phases: 1) Independent\nannotation of memes by annotators, and 2) Final gold labels by all annotators and\na consolidator. Their final corpus consists of 950 memes, each meme containing\nat least one persuasion technique.\n3.3 Discussion of Related Work Shortcomings\nFigure 3.1 provides a summary of the various related works that are pertinent to\nthis particular study focused on the field of computational propaganda detection.\nThis summary not only highlights the significant contributions made by various\nresearchers over time but also outlines the limitations that have been identified\nwithin these works. We aim to address these limitations through the efforts and\nfindings of our own research.\nWhen it comes to news articles, there are detection tasks aimed at document\nlevel and sentence level. The techniques used to detect propaganda on them\nmostly involve some kind of transformer-based classifier, either stand-alone or\nin an ensemble in addition to deep learning models. There exists some studies\nof propaganda on Twitter, with one using older pre-existing collections, and a\nReddit study focused on political forums from the USA and the UK. However,\nby analyzing the related work, we identified some research opportunities further\ndescribed in the following subsections.\n42\n3.3.1 Scarcity of Data and Format Differences\nThe first inclusion of propaganda in the TSHP-17 dataset shows an area of im-\nprovement in terms of considered number of propagandist sources, but also incon-\nsistencies in number of documents. For example, although their creators claim\nto have over 74k articles, their publicly distributed files only account for approx.\n39k articles. Barr\u00b4 on-Cede\u02dc no et al. elaborates on this matter, taking into account\nmore propagandist sources but also describing a more realistic number of docu-\nments [31]. Yet, the number of resources aimed specifically towards propaganda\ndetection on social media is still considerably low, not to mention the fact that\ntexts from Twitter are by their nature noisy (they are brief, contain platform-\nspecific features, and are riddled with typos and grammatical errors [48]).\n3.3.2 Manual Annotation and Distant Supervision\nAs noted in a study of related matters about political ideologies [26], a carefully\nannotated corpus by experts may end up being relatively small, so the authors\nsuggest that future work may explore semi-supervised models or active learning\ntechniques for annotating and preparing larger corpora. Every classifier needs\nquality data to make good predictions. Annotation paradigms can be organized\nin supervised, unsupervised, and alternative approaches. As part of the latter,\nthe distant supervision scheme, initially conceived for relation extraction purposes\n[49], relies on an external database to provide the labeled sources of information\nto subsequently create instances from them for training data. The labels pro-\nduced by manual-annotation efforts by experts are considered of higher quality\nin comparison to distant supervision, however, this paradigm does not suffer from\nsome of the disadvantages of hand-labeled efforts, such as being expensive, time\nconsuming, and limited in quantity.\n43\n3.3.3 Contextual Information\nThere are different perspectives in the form of contextual information that can\nbe further analyzed to unravel social patterns, and explored towards building a\nmore complete solution to detect propaganda:\n\u2022Bias levels andgeographic origins : Aside from \u201cnon-propagandist\u201d or \u201cpro-\npagandist\u201d labels, more dimensions can be associated to news sources, such\nas their bias levels (from \u201cExtreme-Left\u201d to \u201cExtreme-Right\u201d ideologies)\nand their country as the place where the news feed is established.\n\u2022Emotions : Some of the most used propaganda techniques are associated\nwith emotions, this suggests that they play an important role in manifes-\ntation of propaganda [10], [50].\n3.3.4 Concept Drift\nA prior study of computational propaganda used pre-existing Twitter datasets\n[10], however, an issue lies in the timing of the publication of said collections.\nDue to the dynamic nature of the news domain, the occurrence of concept drift\nleads to a stagnation of models trained on historical data, resulting in a decline\nin performance [51]. This element, along with manual annotation schemes, holds\nparticular importance within the realm of social media under investigation in this\nresearch, which is susceptible to quick temporal changes in topics, the introduc-\ntion of new terms with variable discriminative power, vanishing of classes and\nrise of modern fields [52].\n44\nFigure 3.1: Contributions and shortcomings of relevant related work about propaganda\ndetection.\n45\nChapter 4\nPropitter, a Corpus of Propaganda in\nTwitter\nHere, we focus on investigating the propaganda spread on Twitter by media\noutlets that have been deemed unreliable or questionable due to their promotion\nof propaganda. Thus, one of our main contributions is the development of a\nnew Twitter corpus for computational propaganda detection. This corpus, which\nwe named as Propitter , to the best of our knowledge is the largest of its kind,\ncontaining more than 385 thousand tweets from more than 240 news sources\naccounts.\nPropitter distinguishes itself from prior propaganda collections by being\nbuilt by extracting information from numerous Twitter accounts associated with\n\u201ceveryday\u201d news sources. They were chosen based on an external knowledge re-\nsource involving propaganda bias in news media. Its construction consists of data\ncollected by distant supervision (Section 4.1.1), cross-domain filtering (Section\n4.1.2), and in-domain data expansion (Section 4.1.3). These stages are further\nexplained in the next sections.\n46\n4.1 Construction Methodology\n4.1.1 Stage 1: Data collection by distant supervision\nThe construction of Propitter was inspired by QProp [31], which encompasses\nnews articles from 10 propagandist and 122 non-propagandist sources (see further\ndetails in Section 3.1.2). First, we used Media Bias/Fact Check1to address the\nimbalance in the number of sources per class. The Media Bias/Fact Check web-\nsite has assigned news sources in different \u201cBias categories\u201d , where propagandist\nsources are found within \u201cQuestionable sources\u201d . Each questionable source may\nor may not have the propaganda label as \u201cQuestionable Reasoning\u201d . Using this\ncriteria, we reviewed the news sources to identify those that tend to disseminate\npropaganda content and those that do not and then were labeled as propagandist\nornon-propagandist , respectively. For the sources having a Twitter account, we\nretrieved posted tweets using the Twitter API2. A total of 635 ktweets published\nbetween January and August of 20213were gathered from 244 distinct sources\n(the complete list of sources can be consulted in Appendix A1 and A2). Two\nfiltering criteria were applied. A tweet was discarded if it was identified as being\nwritten in other languages than English4, or if it contained at least three trending\ntopics on the date of publication5. These heuristics aim at minimizing spam [53].\nAt the end of this stage, there were a total of 545 ,997 tweets.\n1https://mediabiasfactcheck.com/\n2https://developer.twitter.com/en/docs/twitter-api\n3Propitter includes a kind of bonus partition called \u201cTrain (\u201917-\u201918)\u201d , with data collected from a\nsimilar time period as QProp , with the purpose of having in the future the opportunity to conduct cross-\ndomain analyses.\n4They were identified using Polyglot v. 16.7.4; https://polyglot.readthedocs.io/en/\nlatest/ .\n5Using Trend Calendar, https://us.trend-calendar.com/\n47\nFigure 4.1: A classifier trained on the PTC corpus (white arrow) makes predictions\nover the propagandist (red) and non-propagandist (green) tweets collected in the pre-\nvious stage. If both prediction and pseudo-label are the same (regardless of class) for\na tweet, it is considered to be reliable , ornoisy otherwise.\n4.1.2 Stage 2: Cross-domain tweets filtering\nAs expected, the collected tweets are not free of noise due to their automatic\nlabeling through distance supervision. Attempting to enhance the quality of the\ncorpus and reduce the number of noisy tweets, we implemented the filtering pro-\ncess represented in Figure 4.1. This process capitalizes on the PTC corpus [28]\n(see Section 3.1.3 for details), which is manually labeled at the sentence level\nhaving a similar length in words to tweets (on average 23 and 20 tokens, respec-\ntively). PTC was used to fine-tune a base-uncased BERT model [22] for classi-\nfying the tweets gathered in the previous step as propaganda ornon-propaganda .\nWe adopted a similar methodology to that described in [34], however, upon repli-\ncating their system, superior performance was achieved through the use of a single\nBERT model (see more details in Section 3.1.3). Considering the classes provided\nthrough distance supervision as pseudo-labels for the tweets, we compare these\nclasses against the ones obtained with the binary classifier. When both labels\ncoincide (regardless of the class) the tweet is considered as reliable , otherwise as\nnoisy .\n48\nA total of 337 ,155 tweets were judged as reliable , whereas 208 ,842 were\nflagged as noisy . Table 4.1 shows a few instances of the former, while Table 4.2\nshows some instances of the latter. It is worth noting that, in Table 4.2, the first\nthree tweets use propaganda techniques to convey their message: exaggeration ,\nname-calling , and appeal to fear , verbalized by words such as worst ,savages , and\ndread , respectively. The last four tweets include controversial topics (such as\nprice hikes , the Chinese political system , and COVID-19 ), which are very likely\ndiscussion triggers. Besides, in these examples no propaganda techniques can be\nidentified at first glance; facts are simply mentioned more objectively. Broadly\nspeaking, what the classifier seems to do is filter out certain types of (presumably)\nfalse-negative and false-positive samples.\nTable 4.1: Examples of reliable tweets at Stage 2. For these samples, the distant\nsupervision (DS) pseudo-labels and the classifier\u2019s (CLF) predictions agreed on the\nclass assignment.\nSample tweets DS label Stage 2\nCLF\nPalestine\u2019s Petty Fiefdoms: How the Palestinian Au-\nthority and Hamas are Destroying the Dream of a\nFree Palestine with Torture, Corruption and a Paral-\nlel Police State URLPropaganda Propaganda\nAfter 30 Years of Brutal Rule, Sudan\u2019s Regime is\nCrumbling Under the Weight of a New Movement\nURLPropaganda Propaganda\n#Olympics: British long jumper @USER says ath-\nletes understand its still a competition despite no fans\n#Tokyo2020 #TokyoOlympics URLNon-\npropagandaNon-\npropaganda\nDuchess of Cambridge Kate self-isolating after\nCOVID-19 contact URL URLNon-\npropagandaNon-\npropaganda\n4.1.3 Stage 3: In-domain data expansion\nIn Stage 2, a set of noisy tweets was identified, possibly mislabeled based on the\ndiscrepancy between the initial distance supervision assessment and the binary\n49\nTable 4.2: Examples of noisy tweets filtered by the classifier at Stage 2. For these\nsamples, the distant supervision (DS) pseudo-labels and the classifier\u2019s (CLF) class\nprobabilities are at maximum disagreement.\nSample tweets DS label Stage 2\nCLF\n@USER Oh my God. Two of the world\u2019 worst mass\nmurderers.Non-\npropagandaPropaganda\n\u2018Savages!\u2019: Ukraine\u2019s Black Olympian and Law-\nmaker Says He Was Verbally Attacked, Called\n\u2018Black Monkey\u2019 After He Won The Nation\u2019s Sole\nTokyo Gold Medal URLNon-\npropagandaPropaganda\nThe Taliban\u2019s stunningly swift takeover of\nAfghanistan has caused dread across much of\nthe nation, as Afghans anxiously readjust to life\nunder a militant group that repressed millions when\nlast in power. URLNon-\npropagandaPropaganda\nDollar exchange rates in Iraq URL Propaganda Non-\npropaganda\nGas prices expected to increase by up to 20 cents over\nthe summer URLPropaganda Non-\npropaganda\nThe Communist Party of China and Kenya\u2019s Ju-\nbilee Party will take practical measures to further\nstrengthen cooperation and exchanges. URL URLPropaganda Non-\npropaganda\nUAE reports 1,321 new coronavirus cases, 3 deaths\nURL URLPropaganda Non-\npropaganda\n50\nFigure 4.2: The same classifier from Stage 2 is re-trained with all reliable tweets (gray\narrow) and then makes a second round of predictions over the noisy tweets. If the new\nprediction agrees with the pseudo-label for a tweet, the sample is reconsidered into\nPropitter . At the end of this stage, Propitter is formed by joining the sets of reliable\nandreconsidered tweets.\nclassifier. Nonetheless, this mismatch can potentially be attributed to the dis-\ntinctive ways propagandist andnon-propagandist contents are expressed in tweets\nand news articles. For example, in the case of Twitter, it is very common for posts\nto contain a short sentence accompanied by hashtags and URLs. Hence, to recon-\nsider certain tweets that may have been misclassified as noisy , we conducted an\nexpansion procedure inspired by [54]. First, the reliable tweets identified in the\nprevious stage were merged with the instances of the PTC corpus. Then, using\nthese data comprising tweets and news articles\u2019 sentences, we fine-tuned another\nBERT base-uncased model. Our intuition is that this second binary classifier is\nbetter aligned with the inherent characteristics of the language used in tweets.\nAll the noisy tweets are then passed through a second classification round. The\nobtained labels are compared against the distance supervision pseudo-labels from\nStage 1 once again and those that coincide are incorporated into the Propitter\ndataset. Figure 4.2 shows a schematic representation of the in-domain data ex-\npansion procedure. This stage starts with 208 ,842noisy tweets and, after the\nsecond classification, 48 ,236 tweets were reconsidered as a second batch of reliable\ntweets.\nTable 4.3 shows instances where the predictions made by the first classifier\n51\nTable 4.3: Examples of reconsidered tweets filtered by the classifier of Stage 3.\nSample tweets Stage 2\nCLFDS label\n& Stage 3\nCLF\n@USER @USER How much is the CCP paying you\nto spread disinformation?Non-\npropagandaPropaganda\nHere are \u201c10 Reasons Why Abortion Is Evil\u201d and\nmust be opposed. We encourage you to share this.\nURL #prolife #abortion #tfp #catholicNon-\npropagandaPropaganda\nWith video. Of course they lied. They\u2019re\nDemocrats... Jason Chaffetz says it appears they even\nviolated House rules on using deceptive video. URL\n#tcot #MAGA #ImpeachmentTrialNon-\npropagandaPropaganda\nColorful toy fish, bouncy balls, and stuffed animals\nare just some of the surprises frozen into whimsical\nsculptures made by an Osaka icemaker. URL URLPropaganda Non-\npropaganda\nThe hoodie is either a company mistake or a refer-\nence to former USC President Robert Caslen\u2019s 2021\ngraduation speech where he mistakenly congratu-\nlated alumni of the \u2018University of California.\u2019 URLPropaganda Non-\npropaganda\nChina is facing a high-profile test of its commitment\nto curbing industrial pollution after steel output has\nsurged to well beyond its target of capping produc-\ntion at 2020\u2019s peak. URLPropaganda Non-\npropaganda\nFrance accuses Erdogan of \u2018provocation\u2019 over\nCyprus visit, remarks URLPropaganda Non-\npropaganda\n(which was initially trained solely on sentences extracted from news articles)\nchanged upon being trained with tweets. These tweets, in comparison to those\nin Table 4.2, exhibit distinctive features that are specific to Twitter, such as\nreferences to users, hashtags, and URLs. After being exposed to tweets containing\npropaganda the classifier could recognize patterns associated with these attributes\nof the Twitter domain. The first three tweets employ propaganda techniques such\nasreductio ad hitlerum ,name-calling ,slogans , and loaded language . On the other\nhand, the last four tweets do not exhibit any discernible technique; instead, they\ndirectly discuss certain events and appear to be grounded on factual information.\n52\nTable 4.4: Examples of discarded or non-reconsidered tweets from Stage 3.\nSample tweets Stage 2 &\n3 CLFDS label\nBig things are happening in one of the world\u2019s small-\nest capitals. The Arctic city of Nuuk in Greenland\nis poised to become the world\u2019s first certified \u201csus-\ntainable capital\u201d by the Global Sustainable Tourism\nCouncil URLPropaganda Non-\npropaganda\nThe Taliban promises women\u2019s rights and security\nunder Islamic rule, but many Afghans are desperate\nto flee. URLPropaganda Non-\npropaganda\nCatch me live on The Sons of Liberty radio show\nNow! URLNon-\npropagandaPropaganda\nUK must welcome \u2019tens of thousands of Afghan\nrefugees\u2019, urges Labour URLNon-\npropagandaPropaganda\nTable 4.5: General statistics of Propitter , showing the total number of tweets in each\npartition, as well as the portion corresponding to propaganda tweets. The \u201cBonus\u201d\ntraining partition refers to a small subset of data with timestamps similar to QProp ,\ncreated to enable and promote cross-domain experiments between collections.\nPartition Tweets Propaganda\nTrain 293,480 77,167\nMain Development 38,511 6,454\nTest 38,501 7,045\nBonus Train (\u201917-\u201918) 14,899 10,509\nTotal 385,391 101,175\nTable 4.4 shows examples of tweets that were discarded, or in other words\n\u201dnon-reconsidered\u201d, because even the predictions made by the Stage 3 classifier\nwere not in agreement with the pseudo-labels assigned by distant supervision.\nAfter the data collection, filtering, and expansion stages, the number of\ntweets in Propitter was depurated from 635 kto 385 kinstances. In order to\nestablish partitions for training, development, and testing, the tweets were ar-\nranged in chronological order. The training set consists of the 80% of the earliest\ntweets followed by 10% for validation, and the remaining 10% for testing. The\nfinal data distribution of Propitter is shown in Table 4.5.\n53\nTable 4.6: Classification baseline results on Propitter . All experiments were carried\nout on its \u201cMain\u201d partitions, reporting measures over the propaganda class.\nClassifier Precision Recall F1-score\nBERTweet 78.46 \u00b12.73 85.43 \u00b12.38 81.72 \u00b10.43\nLinear-SVM 68.05 72.68 70.29\n4.2 Propitter\u2019s Classification Results\nAddressing propaganda detection as a binary classification problem ( propaganda\nvs. non-propaganda ) is the main task that can be performed using Propitter .\nIn this sense, two baselines are proposed: a Bag-of-Words (BoW) representation\nwith a Linear SVM [55], and BERTweet [56], a pre-trained transformer-based\nmodel. The obtained results are shown in Table 4.6. Here, it is important to\nhighlight that all the experiments hereafter were performed using BERTweet for\nthree main reasons: a) this kind of classifier performs very well in a wide variety of\nNLP tasks, b) it is a pre-trained model on Twitter data, and c) it shows a better\nperformance compared to a BoW approach in the baseline results. The model\nparameters (which were chosen after multiple tuning iterations) are a batch size\nof 32, a learning rate of 2 e\u22125, an Adam optimizer, and 3 epochs. We report the\naverage of running it five times since the fine-tuning process of BERTweet is not\ndeterministic.\n4.3 Propitter\u2019s Qualitative Analysis\nWith the intention of providing additional information about Propitter , we carried\nout a qualitative analysis of it. Inspired by the analysis of prominent linguistic\nattributes in propaganda phenomenon on the TSHP-17 dataset presented in [29],\nwe calculated the same set of features on Propitter : the number of second-person\npronouns, superlative adjectives, and weak subjective words (those that might\n54\nTable 4.7: Linguistic features and their average occurrence ratio between propagandist\nandnon-propagandist tweets (second column) and news articles (third column). A\nratio above 1 means the feature is more frequent in the propaganda class. The values\nfrom the third column are borrowed from [29].\nRatio\nLexicon markers Propitter TSHP-17\n2nd person (You) 1.87 6.73\nWeak subjective words 1.40 1.13\nSuperlatives 1.00 1.17\nonly have particular subjective uses, according to [57]). The obtained results are\nshown in Table 4.7, with the relative frequencies of second-person pronouns and\nweak subjective words indicating that these lexicon markers are more associated\nwith propaganda in both articles and tweets.\nAs previously mentioned, propaganda involves the use of different tech-\nniques to achieve its purposes. Given that Propitter has been developed with a\nhybrid approach for data labeling, it is interesting to observe whether or not such\na method allows us to include samples using any propaganda technique. Table\n4.8 presents some propagandistic examples manually identified where a partic-\nular technique was used. However, it is important to mention that identifying\npropaganda techniques in tweets is beyond the scope of Propitter and that this\ninformation is included only for illustrative purposes.\n4.4 Creating PropitterX : Adding Contextual Informa-\ntion\nPropaganda manifests itself in various forms [28], which can be associated to\nfactors such as political biases [58] and emotions [3]. Intending to contribute\nto the study of propaganda from different perspectives, we extend Propitter by\nincorporating four kinds of contextual features:\n55\nTable 4.8: Sample tweets from Propitter that display the use of different propaganda\ntechniques in the collection.\nSample tweets Propaganda\ntechniques*\nEgyptians across the political spectrum are outraged by a\n\u201cpoliticized\u201d European Parliament resolution that they call a\nblatant intervention in Egypt\u2019s internal affairs, which serves\nthe interests of terrorists fighting the government of Presi-\ndent el-Sisi. URLLoaded\nlanguage\nA VERAGE JOE ? Biden Says Far-Left \u2018Doesn\u2019t Like Him\u2019\nBecause He Blocks Their \u2018Socialist Agenda\u2019 URLName call-\ning/ labeling\nErdogan Terrorists Open Fire at Other Erdogan Terror-\nistsin Al-Bab City - Video: URL #Syria #News #Politics\n#Quneitra #alBab #Aleppo #Turkey #Terrorism #Erdogan\n#alQaeda #FSA #Nusra #ISIS #HTS #NATO #RegimeChange\n#USA #RussiaRepetition\nRaise your hand if you thoroughly enjoyed watching Trump\nlose the election for the millionth time today ! (Raised Back\nof Hand Emoji Raised Back of Hand Emoji Raised Back of\nHand Emoji)Exaggeration\nor mini-\nmization\nDonald Trump and his acolytes say poor white Americans are\nvictims, but are they? URLDoubt\nSick. Kristol that is. As for the center they are simply infor-\nmation deprived. How does opening borders in the midst\nof a global pandemic, and inviting refugees from terrorist\nstates, and removing the Houthi terrorists from terrorist\nlists help anyone but our enemies?Appeal to\nfear/prejudice\nIn dealing with the COVID crisis, the public health experts\nhave failed the nation , betrayed their mission and spread con-\nfusion. So many outrages have been committed in the name\nof \u201cscience\u201d that people are rightfully distrustful. URL #tfp\n#CovidFlag-waving\nThe (Second) Horror of the Flint Water Crisis: By Walter\nBlock - Ifthe drinking water from the Flint River in Michi-\ngan looked dirty, or smelled bad, the disaster could probably\nhave been avoided. No one would have drunk the poisonous\nliquid , and roughly. . . URLCausal\noversim-\nplification\n\u201cAND WE WILL MAKE AMERICA GREAT AGAIN !\u201d\nWOW! THANK YOU PRESIDENT TRUMP!!! URLSlogans\n*Propitter has binary labels rather than multiclass technique labels.\n56\nPolitical bias Media Bias/Fact Check subjects information sources to a process\nof analysis of the news and opinions they publish6. This platform assigns\na \u201cBias Rating\u201d to each source according to the political perspective they\npromote in a variety of categories including general philosophy, economic\npolicies, and education, among others7. Based on this, we assign each\ninstance in PropitterX with its corresponding political bias, based on the\nlabels included in Appendix A3, which range from extreme-left to extreme-\nright. More details of the distribution of grouped biases are also shown in\nTable 4.9.\nTable 4.9: Bias statistics of main partitions from PropitterX corpus\nBias Train Dev Test\nLeft-wing 142,686 19,270 16,999\nRight-wing 119,823 13,619 14,805\nMisc. 30,971 5,622 6,697\nTemporal split Each instance from Twitter, nowadays known as X, has a set of\nmetadata including the date and time of its publication. This information\nallowed to chronologically organize the collection, and subsequently add a\ntemporal split attribute to the posts in the main Train partition.\nAffective information We have incorporated an additional feature which indi-\ncates the main emotion evoked by each tweet (more details in Table 4.10).\nSpecifically, we have taken into account Ekman\u2019s categorical model of emo-\ntions [59] which considers: fear,anger ,joy,sadness ,surprise ,disgust , and\nneutral . To assign the most likely emotional category related to each tweet,\n6While it is difficult to verify the veracity or accuracy of this platform with respect to the labels it\nassigns to news sources, there is a precedent of having been used in the related work consulted for the\ncreation of a corpus of propaganda articles [31]. In addition, Media Bias/Fact Check provides details on\nthe methodology and rating system they use in their own media analysis, including their own references,\navailable at https://mediabiasfactcheck.com/methodology/ .\n7https://mediabiasfactcheck.com/left-vs-right-bias-how-we-rate-the-bias-of-media-sources/\n57\nwe employed a BERT model fine-tuned8 9with a Twitter sentiment analysis\ndataset10.\nTable 4.10: Emotion statistics of main partitions from PropitterX corpus\nEmotion Train Dev Test\nAnger 47,436 5,792 5,305\nDisgust 2,279 280 217\nFear 148,123 20,436 21,885\nJoy 20,727 2,430 2,162\nNeutral 61,400 7,766 7,194\nSadness 10,127 1,446 1,336\nSurprise 3,388 361 402\nGeographic origin The vast majority of sources that were referenced have their\ncountry of origin available as informative data in the MediaBias/Fact\nCheck web resource. We compiled this information and, motivated by how\ndata is structured in related research [45], [46], we categorized the list of\ncountries into five distinct regions: America ,Asia,Europe ,Middle East ,\nandOthers . Then, we associated each instance in PropitterX with one of\nthese general geographic regions (more details are shown in Table 4.11).\nTable 4.11: Region statistics of main partitions from PropitterX corpus\nRegion Train Dev Test\nAmerica 201,210 22,755 22,433\nAsia 26,380 5,991 6,544\nEurope 18,947 4,084 3,908\nMiddle East 26,970 4,201 4,225\nOthers 19,973 1,480 1,391\n8https://huggingface.co/bhadresh-savani/bert-base-uncased-emotion\n9Achieving a 0.94 in F1-score on Emotion Dataset from Twitter.\n10https://huggingface.co/datasets/philschmid/emotion\n58\n4.5 Summary\nPropaganda is pernicious and takes advantages of the widespread use of social\nnetworks. Our research addresses the challenge of identifying propaganda on\nTwitter by employing a construction process that leverages on pre-existing re-\nsources from the news article domain to clean a collection of data gathered under\na distant supervision scheme. This allowed us to create a corpus in which we\nevaluated classification approaches as baselines. As a consequence, we discovered\nthat a state-of-the-art transformer-based classifier is, as expected, more resilient\nthan other alternatives (such as SVM with Bags-of-Words) in terms of being less\naffected by variables like temporal placement and volume of training data. Con-\nsidering these two variables, we observed that the chronological order of data\naffects more than the amount of data (volume) while training a classifier. We ex-\npect our corpus to be a useful resource in the area of computational propaganda\ndetection, since it would be the first collection of tweets in English specifically\nbuilt for this task.\n59\nChapter 5\nThe Influence of Contextual Features\nfor Propaganda Detection in Tweets\nIn order to assess the role and relevance of the contextual attributes for the\nidentification of propagandist tweets, we propose four experimental scenarios de-\nscribed in the following subsections. All of these scenarios are based on a binary\nclassification task, but each considering a different subset of tweets organized ac-\ncording to each of the contextual attributes. This allows us to shed light on their\nparticular relevance in the study of propaganda on Twitter.\n5.1 PropitterX-LR : On the Role of Political Bias\nAccording to [60], there are grounds to suspect that political affiliation plays\nan important role in determining peoples\u2019 perception of reliable or unreliable\nsources of information. In this sense, we explore if this observation can be useful\nto find out if the propaganda produced by one political position differs from that\nof another. The inclusion of political bias information in PropitterX opens the\n60\ndoor to address these inquiries within the Twitter domain. For this purpose,\nwe propose a subset of this collection referred to as \u201cPropitterX-LR\u201d to denote\nthe \u201cLeft vs Right\u201d case study. We consider the main Train ,Development , and\nTest partitions of Propitter and divide each partition into two groups: Left Wing\n(encompassing Extreme Left, Far-Left, Left, and Left-Center) and Right Wing\n(Extreme Right, Far-Right, Right, and Right-Center) according to the political\nbias associated to each instance, resulting in the partitions shown in Table 5.1.\nTable 5.1: Statistics of PropitterX-LR according to the amount of left-wing and right-\nwing tweets per partition.\nBias Train Dev Test\nProp. Non-prop. Prop. Non-prop. Prop. Non-prop.\nLeft Wing 10,831 131,855 963 18,307 885 16,114\nRight Wing 62,952 55,737 5,047 8,534 5,680 9,106\nFor experimental purposes, two binary propaganda vsnon-propaganda clas-\nsifiers were designed: a \u201cLeft Wing Classifier\u201d (LWC), trained and validated using\nsolely left-biased data, and a \u201cRight Wing Classifier\u201d (RWC), trained solely on\nright-biased data. Both classifiers were evaluated on test data with the same and\ndifferent political biases. Table 5.2 shows the obtained results, where the aver-\nage classification rate together with its corresponding standard deviation value\nare presented. Both classifiers struggle to identify propaganda from the opposing\nside in the political spectrum properly. As a result of the data distribution within\nthe partitions in Table 5.1, RWC tends to classify a greater number of tweets as\npropaganda . Given that the dominant class in the left-wing part of the test set is\nnon-propaganda , the precision of RWC is low while its recall is high. Conversely,\nLWC is inclined to classify more tweets as non-propaganda . Since the prevalent\nclass in the right-wing part of the test set is also non-propaganda , the precision\nof LWC is high while its recall is low.\nA study was conducted to further comprehend the rationale behind this\ndifference in performance through an analysis of the topics addressed by each\n61\nTable 5.2: Results of the political bias experiment. The evaluation measures were\ncalculated over the propaganda class.\nTest Data Classifier Precision Recall F1-score\nLeft Wing LWC 81.59 \u00b16.39 77.15 \u00b16.91 78.82 \u00b12.50\nRWC 26.92 \u00b14.47 76.84 \u00b16.85 39.39 \u00b13.96\nRight Wing LWC 88.88 \u00b13.10 40.81 \u00b17.93 55.31 \u00b17.29\nRWC 77.27 \u00b12.05 91.53 \u00b12.30 83.75 \u00b10.85\ngroup of biases. Employing Latent Dirichlet Allocation (LDA) [61], we analyzed\nthe propaganda tweets corresponding to each political bias, and for each case,\nwe identified its top 5 topics and their corresponding 10 most relevant words.\nFigure 5.1 shows that left-wing propaganda primarily emphasizes the pandemic\nand international matters in discussions concerning world-specific regions, such as\ntheUSandChina . Conversely, right-wing propaganda places greater emphasis on\ntopics involving Joe Biden ,abortion , and racism . Overall, the word clouds below\nillustrate the relevance of political bias for automatic propaganda detection.\nFigure 5.1: Word clouds with most prominent words from the top 5 topics detected by\nLDA in (a) left-wing propaganda and (b) right-wing propaganda.\n62\n5.2 PropitterX-TIME : On the Evolution of Trending\nTopics\nThe temporal evolution of a given phenomenon is a key challenge when addressing\na problem as a classification task under a supervised approach [62]. In the case of\npropaganda detection, changes like the emergence and disappearance of topics of\npublic interest, as well as the use of terms associated with a particular intention\ncould have a significant impact on the performance of the classifiers [63]. In this\nsense, it is imperative to explore the extent at which these changes harm the\nautomatic detection of propaganda.\nAs previously stated, the tweets in PropitterX correspond to a period of\nsix months and are ordered chronologically, with the training set having the\noldest tweets and the test set having the most recent ones. With the intention\nof assessing the role of topic changes across time in propaganda, we propose a\ndata arrangement called \u201cPropitterX-TIME\u201d . We split the training set into five\npartitions of 60 ktweets each, respecting the chronological order. The date ranges\nfor each temporal split are shown in Table 5.3.\nTable 5.3: Date ranges for each temporal split in PropitterX-TIME.\nSplit number From To\nSplit #1 1-Jan-2021 8-May-2021\nSplit #2 8-May-2021 18-Jun-2021\nSplit #3 18-Jun-2021 11-Jul-2021\nSplit #4 11-Jul-2021 27-Jul-2021\nSplit #5 27-Jul-2021 7-Aug-2021\nTest Set 13-Aug-2021 20-Aug-2021\nThe idea is to train a separate classifier for each split (10% of each partition\nis used for validation) and compare the performance of the five resulting classifiers\nafter generating predictions on the whole test set. As shown in Figure 5.2, the\nperformance of the classifiers in detecting propaganda gradually improves as the\n63\ntime span of the training data gets closer to the one of the test set.\nFigure 5.2: Classification results over the propaganda class with chronological training\nsplits.\nTo enhance our understanding of the previous results, Figure 5.3 outlines\nthe evolution of the topics across the different training splits, as well as their\ncomparison with the main topics from the test set. For example, the term trump\nholds significance in splits #1 and #2, yet its importance gradually diminishes\nfrom split #3 onwards to the test set. The terms covid andvaccine start to\nemerge from split #3, with their frequency steadily increasing up to split #5.\nTerms like military ,taliban , and afghanistan gain popularity starting at split #4,\nbefore turning the most crucial ones in the test set, temporarily overshadowing\nother subjects such as covid . In general, it is observed that the topics addressed in\nthe test data are more related to the ones in the most recent training partitions.\n64\nFigure 5.3: Word clouds with most prominent words in the top 5 topics detected by\nLDA in the propaganda from chronological splits (a) #1, (b) #2, (c) #3, (d) #4, (e) #5,\nand (f) Test set.\n5.3 PropitterX-EMO : On the Relevance of Affective In-\nformation\nSome widely recognized propaganda strategies aim to elicit an emotional response.\nThis can be observed, for instance, in the use of loaded language andslogans ,\nwhere words or expressions with emotional connotations are used to sway the\naudience\u2019s opinion [7], [28]. Thus, exploiting the role of emotions to identify pro-\npaganda is a subject that deserves to be investigated. Accordingly, we explore if a\nclassifier trained with messages that evoke emotions performs better in detecting\npropaganda compared to one trained with neutral messages. To address this, it\nis possible to employ a sub-collection denominated \u201cPropitterX-EMO\u201d , relying\non the affective information attributes delineated at the beginning of Section 4.4.\nIn particular, we applied a pre-trained BERT model1to obtain predictions of\n1https://huggingface.co/bhadresh-savani/bert-base-uncased-emotion\n65\nTable 5.4: Distribution of tweets per primary emotion evoked and class in PropitterX .\nThe percentage to the right of each emotion in the first column corresponds to its rate\nof occurrence in the training set.\nTrain Dev Test\nEmotion Propaganda Non-propaganda Propaganda Non-propaganda Propaganda Non-propaganda\nFear (64%) 35,091 113,032 3,086 17,350 3,777 18,108\nAnger (20%) 21,169 26,267 1,570 4,222 1,438 3,867\nJoy (9%) 4,111 16,616 360 2,070 360 1,802\nSadness (4%) 2,399 7,728 217 1,229 257 1,079\nSurprise (2%) 1,594 1,794 131 230 175 227\nDisgust (1%) 1,387 892 134 146 117 100\nNeutral 11,416 49,984 956 6,810 921 6,273\nthe emotions associated with each message. Table 5.4 shows the distribution of\ntweets in terms of emotional categories automatically determined.\nIn order to evaluate the relevance of emotional information in propaganda\ndetection, we designed two training settings using PropitterX-EMO :a) Consid-\nering only tweets regarding any of the 6 emotional categories taken into account\n(i.e., fear,anger ,joy,sadness ,surprise , ordisgust ), and b) Considering only neu-\ntraltweets, i.e. those in which no salient emotion was found. A total of 60 k\nneutral tweets were sampled to train the neutral classifier . Then, to have simi-\nlar training conditions, we matched that same training volume for the emotional\nclassifier , taking into account the proportion of occurrence of each emotion in the\ntraining set (both classifiers are referred to in this way, neutral andemotional ,\nhereinafter). Further details can be consulted in Appendix A4.\nTheneutral classifier was trained using 48,853 non-propaganda tweets and\n11,147 propaganda tweets. The emotional classifier , on the other hand, was\ntrained with 30,000 non-propaganda tweets and 30,000 propaganda tweets. As\ndepicted in Table 5.5, each classifier demonstrates a slightly better performance\nwhen the training and test conditions align. It is worth to highlight that despite\ntheneutral classifier being trained with only a third of the volume of propaganda\n66\ninstances compared to the emotional classifier , the performance gap between\nthem is not too wide.\nTable 5.5: Comparison of the performance of classifiers trained with: i) tweets that\nevoke a predominant emotion, and ii) neutral tweets; evaluation measures correspond\nto the propaganda class of the test set.\nTest Data Classifier Precision Recall F1-score\nEmotional Emotional 68.02\u00b14.52 91.01 \u00b12.65 77.68 \u00b12.30\nNeutral 82.55\u00b14.30 69.21 \u00b14.47 75.04 \u00b10.75\nNeutral Emotional 68.75\u00b17.94 88.12 \u00b13.98 76.75 \u00b14.39\nNeutral 78.91\u00b12.16 82.47 \u00b12.98 80.61 \u00b11.74\nFigure 5.4 illustrates the themes deliberated in the propaganda of the top\ntwo prevalent emotions in PropitterX , namely fear andanger , in comparison to\npropaganda devoid of a dominant emotion. Fear-based propaganda delves into\nmatters concerning warfare and public health ( pandemic ,vaccine , and covid ).\nAnger-provoking propaganda tackles subjects associated with racism andmigra-\ntion. Lastly, while propaganda in neutral tweets shares certain terms with the\naforementioned emotions, it presents a more uniform discourse where no par-\nticular topics emerge prominently (excluding discussions about D. Trump and\nJ. Biden in the three cases). These findings, together with the results in Table\n5.5, suggest that neutral propaganda contents cover a wide spectrum of subjects\nrather than focusing on a particular issue or trigger topic, and, therefore, that it\ncan be a good starting point for training a general propaganda classifier.\n5.4 PropitterX-GEO : On the Role of Region-Centered\nContent\nFindings suggest that the nature of propaganda may vary depending on where it is\ngenerated [46]. This hypothesis seems plausible since a prominent technique em-\nployed in propaganda, denoted as \u201cflag-waving\u201d , involves exploiting deep-rooted\n67\nFigure 5.4: Word clouds with most prominent words in the top-5 topics detected by\nLDA in propagandist tweets that exhibit a predominant emotion of (a) fear, (b) anger,\nand (c) neutral tweets (no salient emotion found).\nemotions of patriotism and nationalism to justify certain ideas or actions [28].\nTaking into account this particularity a question arises, Can propaganda from a\nparticular region of the world be effectively used to identify propagandistic posts\nproduced in a different geographic location? Attempting to address this question,\nthe\u201cPropitterX-GEO\u201d subcollection is proposed. It considers the source region\nattribute to split the data into five distinct groups: America ,2Asia,Middle East ,\nEurope , and a miscellaneous category known as Others , comprising Africa, Aus-\ntralia, and sources whose origin is unknown. The distribution of tweets per region\ninPropitterX-GEO is detailed in Table 5.6. It is worth mentioning that, unlike\nin the previous sub-collections where the train andtestpartitions were used as\nindicated in Propitter , in this case, all data were merged and then split according\nto the corresponding region of origin.\nTable 5.6: Distribution of tweets per region in the PropitterX-GEO subcollection.\nRegion Propaganda Non-propaganda Total tweets\nAmerica 64,555 181,843 246,398\nAsia 5,448 33,467 38,915\nMiddle East 8,853 26,543 35,396\nEurope 5,116 21,823 26,939\nOthers 6,694 16,150 22,844\nTotal 370,492\n2By America we refer to the continent.\n68\nTable 5.7: Results of training with one region and making predictions on the rest. The\nrows represent the regions used for training while the columns those used for testing.\nThe evaluation measure is F1over the positive (propaganda) class. The best score per\ncolumn appears boldfaced. There are no results on the main diagonal, since in each\nexperiment all available tweets from a region were used to train the corresponding\nclassifier.\nTrain Test Region\nRegion America Asia Middle East Europe Others\nAmerica \u2013 38.24 \u00b15.51 56.70\u00b12.72 47.07\u00b13.64 81.53\u00b10.68\nAsia 68.33\u00b12.75 \u2013 54.34 \u00b16.01 21.13 \u00b13.82 55.93 \u00b14.49\nMiddle East 76.34\u00b11.51 48.66\u00b13.24 \u2013 46.16 \u00b11.93 68.93 \u00b13.49\nEurope 70.11\u00b11.12 20.44 \u00b11.99 35.77 \u00b13.77 \u2013 62.23 \u00b12.49\nOthers 79.95\u00b11.27 32.45\u00b13.04 50.74 \u00b12.49 54.53\u00b13.01 \u2013\nOnce the tweets were organized according to their corresponding region,\nwe performed some binary-classification experiments to assess whether a classi-\nfication model trained using examples of propaganda from a particular region\nof the world effectively identifies and differentiates propaganda from a different\ngeographic location. Therefore, a classifier was trained with the data available\nfrom a particular region and evaluated over another one. The obtained results\nare shown in Table 5.7.\nTraining on data from America yields the most favorable detection results\nfor propaganda from Middle East andOthers . For Asia, it was better to train\nwith data from the Middle East , and for Europe training with data from Others\nperformed the best. Due to the comparable performance exhibited by classifiers\ntrained with data from America andOthers , we suspect that it is likely that the\nunspecified sources within the category Others are also located in America. On\naverage, Asia is the region where it was most difficult to detect propaganda using\noff-region training data. It is important to emphasize that some of the propaganda\ntechniques refer to aspects specific to the places where they are intended to be\napplied, such as flag-waving andslogans . Therefore, it is not surprising that there\nare differences between the propaganda spread in different regions.\n69\nFigure 5.5: Word clouds with most prominent words in the top 5 topics detected by\nLDA in the propaganda from (a) America , (b) Asia, (c) Middle East , (d) Europe , and\n(e)Others .\nAs Figure 5.5 shows it is evidently discerned that the propaganda in each\nregion references distinct entities and topics. The regions in which certain com-\nmon propaganda terms are identified are \u201c America \u201d and \u201c Others \u201d (with a high\nfrequency of references to D. Trump and J. Biden). The analysis of this figure,\nalong with the results from Table 5.7 suggest that the classifiers are linking pro-\npaganda to region-specific tokens. We hypothesize that the amount of data used\nfor training purposes did not make a significant impact on the outcomes (other-\nwise the classifier trained with data from America would have performed better\nin all experiments but this did not occur); rather, the key factor rested on the\ndiversity in the topics represented.\n70\n5.5 Summary\nIn the previous chapter, we introduced PropitterX , a new resource developed on\nthe basis of Propitter , a dataset of propaganda on Twitter. PropitterX includes\ntweets annotated for propaganda and incorporates various contextual informa-\ntion aspects such as political bias, geographical origin, emotions evoked in the\nmessages, and temporal splits. These dimensions allow us to create some sub-\ncollections or data arrangements, amplifying the potential for conducting exper-\niments that integrate propaganda with additional factors.\nSome interesting insights into the association between propaganda and var-\nious of these contextual aspects were found through initial experimentation. The\nfindings suggest that: i) Propaganda produced by sources with a left bias differs\nfrom that produced by a right bias; ii) Trending topics associated with propa-\nganda seem to evolve, impacting the performance of the capabilities for recogniz-\ning propaganda: older messages are harder to identify than the most recent ones;\niii) Neutral propaganda content covers a broader spectrum of topics than propa-\nganda anchored in particular emotions; and iv) There is variability in propaganda\nacross different geographical regions.\n71\nChapter 6\nA Contextual-Aware Approach to\nImprove Propaganda Classification\nIn the task of propaganda detection, some works have investigated the incorpora-\ntion of context. The study conducted by [64] examined emotions and sentiments\nas means of communication and social influence. These characteristics were ex-\ntracted using external models applied to tweets. For sentiments, the subcategories\nincluded Positive ,Negative , and Neutral . For emotions the categories were Anger ,\nJoy,Optimism , and Sadness . They \u201caugmented\u201d the original messages with tex-\ntual features, such as \u201cThe statement expresses optimism as emotional content.\nIts sentiment is positive. The message received 16 interactions. The country of\norigin is Russia.\u201d . Janez et al. [65] explored the posting trends observed across\ndifferent nations, such as patterns where countries that reference their own-origin\nwithin the content of their tweets tend to exhibit a higher likelihood of propagan-\ndistic behavior. They \u201cinjected\u201d this information as context to their classification\nmodel by adding the phrase \u201cThis has been written from [country].\u201d at the very\nbeginning of each tweet, so that the model could consider the geographical con-\ntext of each message during its classification process. Similarly, in our previous\n72\nresearch [66], we used the features of country of origin, tweet type, and emotion\n(determined using external models) as contextual keywords to leverage the clas-\nsifier\u2019s architecture. This same approach explained in more detail in the section\nbelow, is examined in a more comprehensive manner in this study. Specifically,\nwe assess the efficacy of the methodology in two distinct collections of tweets\nassociated with both news outlets and government entities, with the latter collec-\ntion providing an opportunity for a direct comparison of our findings with other\nclassification systems [12]. In addition, we explore the political bias of the sources\nas an extra contextual feature, and alter the volume of training data to assess the\nrelevance of contextual information in different classification scenarios. Finally,\nwe also consider the automatic prediction of contextual attributes to evaluate the\nsuitability of the proposed approach to situations in which this information is\ninitially unavailable.\n6.1 Contextual-aware Approach\nOur proposed method is based on Bidirectional Encoder Representations from\nTransformers (BERT) models [22]. In BERT, a \u201csentence\u201d refers to an arbitrary\nspan of adjacent text, and a \u201csequence\u201d indicates the input token sequence. The\nBERT model\u2019s input representation is designed to accommodate not just a sin-\ngular text sentence but also a combination of two text sequences encapsulated in\na single token sequence, where the initial token \u201c[CLS]\u201d holds the classification\nembedding and another special token, \u201c[SEP]\u201d, is employed to separate segments\nor indicate the conclusion of the sequence [67].\nMotivated by the incorporation of this auxiliary input in various other stud-\nies [67]\u2013[69], we chose to leverage it as a mean to \u201cprovide context\u201d to the sentence\npresented to the main input of text for the task of detecting propaganda. Figure\n6.1 shows the method we employed to merge a tweet\u2019s content with its set of\n73\nFigure 6.1: BERT\u2019s auxiliary input diagram with the contextual features concatenated\nto the tweet\u2019s text (adapted from [22]).\ncontextual features.\nIn this manner, we will assess the effectiveness of BERT-CA , a BERT model\ndesigned to be context-aware , which, in addition to using the tweet\u2019s text in the\nmain input, will also be enriched with contextual attributes represented as \u201cto-\nkens\u201d after the first [SEP] token in the secondary input. BERT-CA can incor-\nporate various contextual tokens, and throughout our experiments we examined\ndifferent types of context as well as combinations of these features to assess their\neffectiveness.\nTable 6.1 offers some examples of sequence compositions that include con-\ntext in the auxiliary input. In the first instance, the model is provided with\nthe background that the tweet originates from a left-wing American perspective\nand that the content incites fearby questioning the safety of vaccine administra-\ntion, particularly for pregnant women (it is important to note that the timeframe\nwhen this data was collected is related to COVID-19). In the second instance,\n74\nTable 6.1: Examples of input token sequences.\nInput Sequence\n[CLS] What are the vaccine risks for pregnant women? UW doctors have a few\nthoughts. URL [SEP] LEFT AMERICA FEAR [SEP]\n[CLS] #Crimea is a vital part of Russian civilization. It is the point of origin of\nRussian Christianity, was in Ancient Rus and Russian Empire, Soviet Russia and\nUSSR, reunited with Russia - dear to the hearts of all Russians URL [SEP] RUSSIA\n[SEP]\n[CLS] Today marks a major milestone in making Europe the first climate neutral\ncontinent in the world. With the new target to cut EU greenhouse gas emissions by\nat least 55% by 2030, we will lead the way to a cleaner planet and a green recovery.\n[SEP] JOY [SEP]\nthe tweet contains strong nationalist sentiments towards Russian territory, which\nbecomes clearer when acknowledging that this tweet originated from Russia, indi-\ncating they are expressing favorable views about themselves. The third instance\ndiscusses a significant achievement for Europe in the realm of climate change,\nadvocating for a decrease in gas emissions towards a cleaner planet. The main\nemotion identified in this tweet was joy, which aids the model in recognizing that\nit serves as propaganda infused with emotional appeal.\n6.2 Experimental settings\n6.2.1 Dataset\nTo carry out our experiments, we used the PropitterX dataset (described in\nChapter 4). It comprises a compilation of tweets originally posted by more than\n240 prominent news media accounts, labeled in a binary manner as propaganda\nandnon-propaganda . The dataset is structured into three primary sets: Train ,\nDev, and Test. In addition to the binary label of propaganda ornon-propaganda ,\nthe dataset offers the following three contextual features per tweet:\n75\n\u2022Bias: Political affiliation of the account responsible for posting the tweet.\n\u2022Region : The geographical origin of the account that published the tweet.\n\u2022Emotion : The emotional category attributed to each instance, automati-\ncally determined by a pre-trained language model [70], fine-tuned with a\nTwitter Sentiment Analysis dataset [71].\nAlong with class distributions, Table 6.2 (a combination of Tables 4.9, 4.10,\nand 4.11) shows a significant presence of tweets originating from America . There\nis also a remarkable prevalence of messages that elicit the emotion of fear, which\ncan be largely attributed to the main topics discussed on the dataset, including\nbut not limited to the COVID-19 pandemic, the dynamics of migration, and\nconflicts of war.\n6.2.2 Baseline\nA BERT-based classifier, exclusively trained by processing the tweet in the main\ninput without incorporating any additional information in the auxiliary input,\nwill serve as our baseline, hereinafter referred to as BERT-BL .\nWe used BERTweet, a large-scale language model that has been pre-trained on\n850 million English tweets [56], for both BERT-BL andBERT-CA . The model\nparameters, selected after numerous tuning iterations, consist of a batch size of\n32, a learning rate of 2 e\u22125, an Adam optimizer, a max sequence length of 250\nand a total of 3 epochs.\n76\nTable 6.2: Statistics of main partitions from PropitterX corpus\nClass Train Dev Test\nPropaganda 77,167 6,454 7,045\nNon-propaganda 216,313 32,057 31,456\nContextual Features\nRegion\nAmerica 201,210 22,755 22,433\nAsia 26,380 5,991 6,544\nEurope 18,947 4,084 3,908\nMiddle East 26,970 4,201 4,225\nOthers 19,973 1,480 1,391\nBias\nLeft-wing 142,686 19,270 16,999\nRight-wing 119,823 13,619 14,805\nMisc. 30,971 5,622 6,697\nEmotion\nAnger 47,436 5,792 5,305\nDisgust 2,279 280 217\nFear 148,123 20,436 21,885\nJoy 20,727 2,430 2,162\nNeutral 61,400 7,766 7,194\nSadness 10,127 1,446 1,336\nSurprise 3,388 361 402\n6.3 Experiments\n6.3.1 On the impact of adding context during the classification\nprocess.\nTo compare the performance of classifiers with and without contextual features\nand evaluate our method, several experiments were conducted using BERTweet,\ninitially without incorporating any form of contextual attribute, i.e., relying\nsolely on the tweet\u2019s text. Subsequently, we introduced contextual attributes\n77\nTable 6.3: F1 classification results of BERT-CA over the propaganda class adding\ndifferent contextual features. The third column shows the probability, according to the\nBayesian Wilcoxon signed-rank test, of the classifier being better than BERT-BL.\nContextual Feature Added F1-Prop \u00b1Std. Dev. Probability\nNone ( BERT-BL ) 0.8171 \u00b10.0043 -\nBias 0.8232 \u00b10.0090 0.203\nRegion 0.8532 \u00b10.0072 0.999\nEmotion 0.8213 \u00b10.0083 0.068\nBias + Region 0.8750 \u00b10.0073 0.999\nBias + Emotion 0.8343 \u00b10.0063 0.948\nRegion + Emotion 0.8473 \u00b10.0054 0.996\nBias + Region + Emotion 0.8630 \u00b10.0126 0.999\nin BERTweet\u2019s auxiliary input, testing each feature individually, followed by\ncombinations of two, and ultimately all three attributes together. For every\nclassification variant, we opted to execute five runs and then calculated the av-\nerage. These findings are presented in Table 6.3. As shown, all variants that\nincorporate context demonstrate an enhancement compared to the baseline, with\ntheRegion attribute performing the best when used alone. Nonetheless, the best\nclassification results are obtained through the use of attributes in combination,\nwith Bias + Region exhibiting the highest F1-score over the propaganda class,\nwith a relative improvement of 7.08% over BERT-BL . For the sake of clarity,\nwhen we discuss the BERT-CA model in the following analyses, we specifically\nrefer to the variant that includes all three types of context (even if it was not the\nconfiguration yielding the highest scores).\nStatistical significance test\nTo assess the significance of incorporating context within the classification frame-\nwork, we implemented a Bayesian Wilcoxon signed-rank test. This particular test\nconstitutes a non-parametric Bayesian adaptation of the Wilcoxon signed-rank\ntest, structured on the Dirichlet process, and it is recommended for the direct\n78\ncomparison of classifiers [72], [73]. Based on the collected data, the test calculates\nthe posterior probability for both the null and alternative hypotheses, giving a\nclear probability of one method outperforming the other (when evaluating two\ntreatments), which avoids the abstract interpretations often associated with fre-\nquentist tests. As evidenced by the third column of Table 6.3, according to the\nBayesian Wilcoxon signed-rank test, the likelihood that the incorporation of con-\ntextual features into the classification process yields superior scores compared to\nonly using the texts of the tweets is greater than 94% in 5 of 7 cases (the cases\nwhere the improvements are not statistically significant correspond to adding only\nbias and only emotion ).\n6.3.1.1 Fixed and new classification mistakes by BERT-CA\nAttempting to shed light on the impact and influence of incorporating contex-\ntual information for detecting propaganda, we carried out an analysis to quantify\nthe number of errors made by BERT-BL that were corrected (i.e., classified cor-\nrectly) by BERT-CA . Besides, the opposite kind of error was also analyzed. For\nthis purpose, a total of five iterations of the baseline BERT-BL classifier and\nfive iterations of the contextual-aware BERT-CA model were executed. Only\nthose instances in which all five iterations of a classifier yielded identical out-\ncomes were considered. In other terms, we omitted those occurrences in which,\nduring the five iterations, a classifier made ambiguous predictions (for example,\npredicting \u201cpropaganda\u201d in the first iteration and \u201cnon-propaganda\u201d in the fifth\niteration). Subsequently, we compared the predictions generated by both models\nconcerning the ground truth. Following this scheme, the context-aware model\nsuccessfully rectified wrong predictions made by the baseline model in a total of\n205 instances (0 .5% of the Test set). From them, 103 tweets were classified as\nnon-propaganda and 102 as propaganda , a nearly perfect balance between both\ncategories. Regarding emotional content among the 205 cases, fearandanger are\n79\nthe predominant emotions observed. This finding aligns closely with the overall\nemotional distribution of the entire collection. With respect to political bias , 58\ncorrections pertain to left-leaning tweets while 105 correspond to right-leaning\ntweets. Conversely, the contextual-aware model introduced 28 new mistakes, i.e.,\nit changed the label that the baseline model had accurately predicted. Among\nthem, the ground truth label of 17 tweets was \u201cnon-propaganda\u201d and 11 \u201cpropa-\nganda\u201d , with all propaganda tweets being of left-wing bias. One potential reason\nfor this might be that the contextual-aware classifier observed that left-leaning\nnon-propaganda was more prevalent than left-leaning propaganda in the train set\n(with a ratio of 12 to 1).\nIn Table 6.4, we offer a few examples of the corrections and new mistakes\nmade by the contextual-aware model. In row 1, there is an instance of potential\npropaganda , where the tweet uses emotional language and relies on authority (a\nmilitary veteran) to sway opinion. A correct prediction was achieved by incorpo-\nrating the detail that the tweet comes from the Middle East region, the political\norientation of the source is right, and the message elicits feelings of sadness (likely\nalluding to the veteran\u2019s condition). Row 2 also contains an example of potential\npropaganda . The phrasing, with words such as \u201cunacceptable\u201d and \u201creportedly\u201d,\nand the implicit comparison are suggestive of a biased viewpoint. When context\nabout the tweet is added to BERT-CA, the prediction is fixed. Row 3 shows an\ninstance where the core message is factual but could be framed to emphasize dis-\nruption or China\u2019s COVID policies. It depends on context and whether the URL\nleads to a biased source. By noting that it originates from a source that is neither\nleft-leaning norright-leaning , that it is based in America , and that the statement\nincited fear (likely referring to the mention of testing positive for COVID), the\ncontextual model\u2019s assessment shifted correctly to non-propaganda . In row 4,\nthe message appears to be sensationalistic, but not inherently propagandistic, as\nthe baseline model predicted. A correct prediction was achieved by taking into\n80\naccount that the tweet is left-leaning , comes from America and evokes joy. In in-\nstances where the contextual model makes mistakes, row 5 shows a message that\nuses emotionally charged language (\u201cgem,\u201d \u201cprecious\u201d) and a loaded term (\u201cre-\ndress\u201d) to persuade without full explanation, characteristic of propaganda. By\nadding context, the model found no propaganda in the message. Row 6 illustrates\nan example where the phrasing is neutral, but the underlying fear mentioned could\nbe manipulated or exaggerated. The predominant emotion identified as fear, may\nhave caused the model to mistakenly interpret it as propaganda .\n6.3.2 On the impact of adding context when using limited training\ndata\nData for propaganda detection can be scarce. Accordingly, we aimed to investi-\ngate the usefulness of contextual attributes in scenarios where there are only a few\nlabeled instances. Our intuition is that, even with restricted training resources,\ncontextual features can effectively aid in differentiating between propagandistic\nand non-propagandistic content. For that purpose, we replicated our first exper-\niment with BERT-CA incorporating all three types of context available, altering\nthe volume of data used for training the classifiers, effectively halving the training\nset up to 4 times. The findings from this experiment are illustrated in Figure 6.2,\nwhere it is evident that the value of integrating context into the classification\nprocess remains to some degree unchanged in relation to the baseline model as\nthe amount of training data diminishes. In fact, the performance of the con-\ntextual model decreases slightly less than that of the baseline model when the\ntraining data is at its lowest. For example, it is worth noting that the gap in\nF1-score between the two models widens, going from a relative difference of 4.8%\nachieved with the complete training set to 7.8% when using the smaller training\ndata volume. This indicates that, in circumstances characterized by a limited\n81\nTable 6.4: Examples of fixed and new mistakes by adding contextual features to the\nclassifier. A label of 1means propaganda , while 0means non-propaganda .\nTweet Label BERT-BL Context\nAddedBERT-CA Mistake\nA British veteran who lost\nboth his legs in an explosion\nwhile serving in Afghanistan\ndescribes the situation in the\ncountry as \u201cshameful\u201d URL1 0 Right,\nMiddle-\nEast,\nSadness1 Fixed\nCotton, an Afghan war vet,\nsaid it is unacceptable that\nUS forces are not helping\nAmericans get to the Kabul\nairport when the British and\nFrench forces reportedly are\naiding their citizens. URL1 0 Right,\nAmerica,\nFear1 Fixed\nChina\u2019s Ningbo-Zhoushan\ncontainer port, the world\u2019s\nthird-busiest, remained\npartially closed for a sixth\nday following its halt of\nall inbound and outbound\ncontainer services at its\nMeishan terminal after one\nemployee tested positive for\nthe coronavirus. URL0 1 Neutral\n(Least-\nBiased),\nAmerica,\nFear0 Fixed\nExperienced\u2019 Sloth Mom\nLunesta Gives Birth to Her\nFifth Baby at New England\nZoo URL0 1 Left,\nAmerica,\nJoy0 Fixed\nLifta must be saved not only\nbecause it is a gem of pre-\ncious natural beauty and hu-\nman architecture, but also\nbecause it is a step towards\nredress. URL1 1 Left,\nAmerica,\nAnger0 New\nThe announcement follows\nfears that Ukraine would\nban the pilgrimage for a sec-\nond year due to the COVID-\n19 pandemic URL0 0 Unknown,\nOthers,\nFear1 New\n82\nFigure 6.2: Average classification scores (F1 over the propaganda class), incorporating\ncontextual features and changing the volume of train data.\navailability of labeled training data, the incorporation of contextual attributes\ncan play a significant role in \u201cconstraining\u201d particular instances of propaganda.\nBy providing insights, such as identifying the source from which it emanates,\ncontext can consequently facilitate a better differentiation between the classes of\npropaganda andnon-propaganda .\n6.3.3 Classifying Tweets from Unknown Sources\nAccounts linked to various news media organizations, government entities, and\npolitical parties\u2014whose contextual information is often well-documented\u2014are\nprimarily responsible for the creation and dissemination of propaganda. However,\nthe accessibility and versatility of social media platforms allow for a diverse type of\n83\ncasual users to also engage in the creation and sharing of propagandist messages.\nThis situation can lead to the context surrounding these users being unclear or\nentirely unknown. We encountered the challenge of addressing situations in which\nsuch contextual attributes are absent, specifically how to navigate the scenario\nwhere a prediction about a tweet must be made without knowledge of its source of\norigin. Considering that prior experiments used a pre-trained language model to\nidentify emotions evoked by texts, we aimed to explore the feasibility of training\nmodels to recognize both the political bias within the message as well as its\ngeographical origin . The current experiment involves the use of the Train ,Dev,\nandTest partitions of the PropitterX dataset. However, rather than employing\npropaganda ornon-propaganda as the target labels for training a BERTweet\nmodel, we designated the biasattribute as the target. Additionally, in a separate\nexperiment, we applied the same concept but aimed to predict the region as the\ntarget instead. According to Table 6.5, predicting the region poses a greater\nchallenge than bias, primarily due to the fact that this attribute is categorized\ninto a larger number of classes (a total of 5), four of which are minority classes\nwhen compared to the volume of tweets originating from America . In terms of\nbias, the models were able to achieve F1-scores of 0 .75 and 0 .77 for leftandright\nbiases , respectively. Making predictions over the miscellaneous biases , a minority\nclass, proved to be more challenging.\nLeveraging these predicted attributes (hereafter referred to as calculated\ncontextual features ), we executed five classification runs using all three types of\nattributes ( bias,region , and emotion ), and other additional five runs employing\nsolely the calculated bias(as it was easier to predict accurately in contrast to re-\ngion). Consequently, we noted that incorporating all calculated features resulted\nin a decline in classification performance when compared to the baseline model.\nConversely, using the most effective available calculated attribute in isolation as\nthe only context added marginally surpassed the baseline, albeit without achiev-\n84\nTable 6.5: Classification results obtained by predicting the bias and the region of the\ntweets in the test set. \u201cSupport\u201d indicates the number of instances corresponding to\neach class.\nPrecision Recall F1-score Support\nLeft Bias 0.6418 0.9203 0.7562 16999\nRight Bias 0.8763 0.6931 0.7740 14805\nMisc. Biases 0.6663 0.2403 0.3532 6697\nAmerica 0.8536 0.9479 0.8983 22433\nAsia 0.9468 0.4080 0.5703 6544\nMiddle East 0.5187 0.8293 0.6383 4225\nEurope 0.7352 0.5448 0.6258 3908\nOthers 0.5769 0.4637 0.5141 1391\nFigure 6.3: Average classification scores (F1 over the propaganda class) obtained by\nincorporating contextual features as a secondary input.\ning statistical significance. A comparison between using the original contextual\nfeatures vs calculated ones is shown in Figure 6.3.\n85\n6.3.4 Classifying Tweets from Diplomatic Profiles and Govern-\nment Authorities\nThe studies conducted with PropitterX were aimed at detecting propaganda in\ntweets from news organizations. In order to further evaluate the effectiveness of\nour classification strategy, we carried out an experiment considering a different\ntype of propaganda, spread through accounts associated with government entities.\nIn 2023, DIPROMATS [12] introduced datasets containing propaganda\nfrom Twitter accounts of diplomats, ambassadors, and governmental entities\n(along with information about the account\u2019s country of origin andtweet type ).\nThey are collections of tweets published by Chinese, Russian, European Union,\nand United States authorities between January 2020 and March 2021. Table 6.6\nshows the distribution of the datasets for both English and Spanish (refer to [12]\nfor more information). Upon conducting a comparative analysis between Propit-\nterX andDIPROMATS , it becomes evident that the country attribute is more\nevenly distributed in DIPROMATS , and while the political bias attribute is ab-\nsent, the type of tweet serves as a new contextual feature (indicating whether the\npost was a tweet, retweet, reply, or quote). Regarding emotions (inferred via [70],\n[74]), we observe that in DIPROMATS , there is a tendency for messages issued\nby diplomats in English to lean towards a more positive tone, with joybeing the\nmost frequently evoked emotion.\nTo evaluate our approach, we concentrated on the task of binary propaganda\nidentification in both English and Spanish, using BERTweet and RoBERTuito\n[75], respectively. In both models, the same hyperparameter values described in\nSection 6.2 were applied. Table 6.7 presents the results of our methodology. Our\napproach, which incorporates context in the same manner we have delineated in\nSection 6.1, yielded the best results in Spanish by integrating the type of tweet as\ncontextual information into the auxiliary input of the RoBERTuito model, and\n86\nTable 6.6: Data distribution for the English and Spanish corpora.\nClass Train ENG Test ENG Train SPA Test SPA\nPropaganda 1,974 N/A 1,199 N/A\nNon-propaganda 6,434 N/A 4,921 N/A\nContextual Features\nCountry\nChina 2,170 852 2,178 819\nEuropean Union 2,043 873 1,508 957\nRussia 2,005 955 795 596\nUSA 2,190 924 1,639 1,099\nType of tweet\nTweet 6,742 2,856 3,586 2,302\nQuoted 825 356 888 541\nRetweet 473 227 1,221 401\nReply 368 165 425 227\nEmotion*\nAnger 2,270 760 259 90\nFear 276 72 5 4\nJoy 5,216 2,569 649 376\nLove 114 53 N/A N/A\nOthers N/A N/A 4,961 2,919\nSadness 508 141 224 66\nSurprise 24 9 22 16\n*Inferred, not available in the original dataset.\nin English by incorporating the emotion evoked by the message as a contextual\ntoken. Clearly, the incorporation of contextual attributes enhances the perfor-\nmance of our classifiers, with F1-score gains of up to 3.5 points when evaluated\nwith the Spanish dataset, and 0.96 points in the English dataset. It is significant\nto emphasize that the baseline models, devoid of context, were predominantly\nranked lower than our contextualized models.\nOur best results averaging both languages scored a 0.7953 F1-macro. For\ncomparison, we offer a summary of other approaches that have been tested in\nthe same dataset. A research team developed a hierarchical model to detect and\n87\ncharacterize propaganda techniques in text [76]. Their methodology involved fine-\ntuning a XLM-RoBERTa model using multiple datasets, achieving a F1-score of\n0.7770. The strategy employed in [77] assessed linguistic attributes and sentence\nembeddings derived from various LLMs, encompassing models tailored for En-\nglish, Spanish, and additional multilingual frameworks, resulting in a F1-score of\n0.7734. A different research team achieved a F1-score of 0.7732, implementing\ndata augmentation techniques to enhance the sample size by translating Spanish\nsamples into English and the other way around [78]. They used BETO for tweets\nwritten in Spanish and a RoBERTa-based variant of TimeLM [79] for tweets in\nEnglish, fine-tuning with a Discrepancy Correction Procedure to avoid inconsis-\ntencies in labeling.\nIn terms of propaganda detection results only in DIPROMATS-English, our\nbestcontext-aware model scored a 0.8062 F1-macro. For comparison, the strategy\nsubmitted by [65], \u201cinjecting\u201d geographical context to each message, achieved a\n0.8011 F1-macro. The approach by [64] which \u201caugmented\u201d the original messages\nwith emotions, sentiments, interactions, and countries, achieved a 0.7953 F1-\nmacro.\nThe boxplots in Figure 6.4 provide a visual representation of the comparison\nbetween our context-aware approach and other classification strategies tested by\nother studies for DIPROMATS 2023, measured in terms of F1-macro scores. Our\napproach is prominently positioned at the very limits of the upper whiskers, which\nmeans that our results are indeed part of the highest scores achieved using that\ndataset.\nIn terms of statistical significance, we implemented the Bayesian Wilcoxon\nsigned-rank test within a 5-fold cross-validation framework using the training set\nacross both languages. Despite the observation of improved classification scores\nwhen contextual information is incorporated, the statistical analysis concluded\n88\nTable 6.7: Official results obtained by our submissions in the DIPROMATS 2023\nshared task, corresponding to BERT-BL andBERT-CA .\nTask 1 Contextual Feature Added F1-score Rank\nSPANone 0.7730 10 of 18\nType of Tweet 0.8089 1 of 18\nENGNone 0.7966 6 of 30\nEmotion 0.8062 2 of 30\nA VGNone 0.7880 4 of 16\nEmotion 0.7953 1 of 16\nFigure 6.4: Box plots of the results for Task 1 of DIPROMATS 2023. The blue dots\nrepresent our best scores using contextual features in the shared task.\nthat the advantage of the contextual-aware model was not significant for English,\nbut significant in the Spanish language (see Table 6.8).\nWe speculate that the use of data in English and the source of the tweets\n(coming from official government sources and entities) generated a complex sce-\nnario, in which the contextual attributes might not have provided sufficient infor-\n89\nTable 6.8: Bayesian Signed-Rank Test results applied in DIPROMATS Spanish Train\nset. BERT-BL only takes into account tweets\u2019 text, BERT-CA uses both text and\ncontextual features.\nProbability\nBERT-BL >BERT-CA 0.199\nRegion of Practical Equivalence 0.199\nBERT-CA >BERT-BL 0.602\nmation to achieve a better distinction between propaganda andnon-propaganda\nclasses. We do not attribute this result exclusively to the source of the data (since\nstatistical significance was achieved in Spanish), but rather the fact that there\nmay be contextual differences between languages.\n6.4 Summary\nWe assessed the usefulness of incorporating contextual information for performing\npropaganda detection on tweets. We took advantage of corpora in the state-of-\nthe-art having posts annotated on the presence of propaganda which also have\nattributes regarding the context surrounding the messages like political affilia-\ntion of the source, the geographical location from which the message was posted,\nthe emotion evoked by them, and the type of tweets. These features allowed\nus to conduct experiments under diverse scenarios: without using any context,\nadding different combinations of contextual aspects, and assessing the usefulness\nof context on data scarcity. Furthermore, we evaluated the possibility of fore-\ncasting the contextual attributes considering this information is unavailable. We\nobserved that enhancing the textual content of the posts through contextual infor-\nmation improves the classification rates, with F1-score gains of up to 5.8 points in\nPropitterX , 3.1 points in DIPROMATS-Spanish and 0.9 points in DIPROMATS-\nEnglish . On the other hand, our experiments using the context-aware approach\n90\nunder scenarios in which contextual information is initially unavailable showed\nonly marginal improvements in the detection scores; these improvements did not\nreach a level of statistical significance that would validate our findings in a broader\ncontext.\n91\nChapter 7\nConclusions and Future Work\nRevisiting our Research Questions\nIn this section, we will address the research questions outlined at the beginning\nof this document.\nHow can resources derived from news articles be leveraged to identify\ncomputational propaganda on Twitter?\nIn the development of the Propitter dataset, we integrated two methodologies for\ndataset construction observed in previous propaganda-related studies: pseudo-\nlabeling through distant supervision, and manually annotated data used to refine\nand filter the data we collected. This process involved using an existing data\ncollection comprising sentences from news articles that had been manually la-\nbeled[28]. With this dataset, we trained multiple classifiers to make class predic-\ntions ( propaganda ornon-propaganda ) and to evaluate whether these predictions\naligned with the pseudo-labels generated through distant supervision. This ap-\nproach enabled us to enhance the quality of our dataset beyond what could have\nbeen achieved by relying exclusively on distant supervision. Overall, the endeavor\n92\nof detecting computational propaganda is enhanced by the development of new\nresources that encompass diverse sources and domains beyond mere news arti-\ncles. To achieve this, we have leveraged existing resources and integrated various\nlabeling methods to introduce a novel corpus ( PropitterX ).\nWhat are the differences (in terms of topics covered, emotions evoked) in\ncomputational propaganda from tweets based on its context?\nAs we remember from the conclusion of Chapter 5, some intriguing observations\nregarding the link between propaganda and various contextual elements indicate\nthat:\n\u2022Propaganda generated by sources with a left-leaning bias is distinct from\nthat created by those with a right-leaning bias, highlighting its significant\nimpact on the message content. For instance, we noted that leftist propa-\nganda predominantly concentrated on the pandemic and global issues when\naddressing specific regions of the world, like the US and China. In con-\ntrast, right-wing propaganda was centered on matters related to Joe Biden,\nabortion, and racial issues.\n\u2022Trending topics linked to propaganda appear to be dynamic, affecting the\nefficacy of the systems designed to recognize such messages. For instance, we\nnoted a shift in the frequency of the usage of terms like \u201cTrump\u201d ,\u201cCOVID\u201d ,\nand\u201cvaccine\u201d during a specific period of time. However, an event involv-\ning different terms, such as \u201cmilitary\u201d ,\u201cTaliban\u201d , and \u201cAfghanistan\u201d , can\nrise in prominence and momentarily overshadow other topics, fundamen-\ntally altering what is deemed relevant for a classifier tasked with detecting\npropaganda.\n\u2022Neutral propaganda content encompasses a wider array of subjects com-\npared to propaganda centered around specific emotions. The two most\nprominent emotions in PropitterX are fear andanger . Fear-driven pro-\n93\npaganda explores issues related to warfare and public health (including\npandemics, vaccines, and COVID). Anger-inducing propaganda addresses\ntopics linked to racism and immigration. While neutral tweets containing\npropaganda use some of the same terms as these emotions, they offer a more\nconsistent narrative where no specific subjects stand out.\n\u2022There exists a diversity of propaganda across various geographical areas.\nEach area within our dataset concentrated on distinct subjects. Notably,\nit is observed how each area addresses matters concerning politics and gov-\nernment, although the names of the places and leaders mentioned in the\ntweets vary significantly. The only regions where specific common propa-\nganda terms are recognized are \u201dAmerica\u201d and\u201dOthers\u201d (with a notable\nfrequency of mentions of D. Trump and J. Biden).\nHow can contextual information of messages be incorporated to improve\nthe effectiveness of propaganda detection in them?\nOur proposed context-aware approach is based on BERT models. The input\nrepresentation of the model is designed to accommodate not only a single text\nsentence but also a combination of two text sequences. We chose to leverage this\nto provide context to the sentence presented to the main text input for the task of\ndetecting propaganda. In this manner, the incorporation of contextual attributes\nwithin propaganda classifiers resulted in an increase in detection performance,\nwith some instances showing statistically significant improvements over baseline\nresults. Notably, the addition of contextual attributes to texts was accomplished\nwithout altering the architecture of BERT-based classifiers, which are renowned\nfor their state-of-the-art performance. This renders our method both straight-\nforward to implement and highly competitive. Analysis of data from collections\nPropitterX andDIPROMATS demonstrated that even the use of a single con-\ntextual attribute (as opposed to combinations) yields superior results compared\nto classification performed without this information.\n94\nFinal Remarks\nAs a starting point for this research endeavor, we identified a substantial gap in\nthe exploration and analysis concerning the phenomenon of propaganda as dis-\nseminated through social network platforms by various news media organizations,\nwhich was largely overlooked in previous studies. To address this, the primary\ncontribution of this work was the creation of a corpus, specifically designed for\nthe purpose of facilitating a deeper understanding of this complex issue.\nOur hypothesis (based on one of our research questions) was that contex-\ntual information of messages can be incorporated in the classification process to\nimprove the effectiveness of propaganda detection.\nIn this research, we have investigated the identification of computational\npropaganda within a social network by leveraging various contextual attributes.\nEach contextual attribute can be linked to one or multiple propaganda techniques:\n\u2022Political bias significantly impacts and reveals itself through \u201cName calling\nor labeling\u201d and\u201cSlogans\u201d .\n\u2022Geographical origin is tied to \u201cFlag-waving\u201d .\n\u2022Emotions serve as a crucial element in techniques such as \u201cLoaded Lan-\nguage\u201d and\u201cAppeal to fear/prejudice\u201d .\n\u2022One potential connection between propaganda techniques and tweet types\nlies in the importance of recognizing whether a message is aimed di-\nrectly at another account (to provoke \u201cName calling\u201d or\u201cDoubt\u201d ), merely\nretweeted/repeated ( \u201cRepetition\u201d ), or referencing other sources ( \u201cAppeal\nto authority\u201d ). These factors can be pivotal when attempting to steer the\ndirection of a discourse or argument.\n95\nOur proposal for a Contextual-Aware Approach incorporates these contex-\ntual attributes into the training and classification processes of propaganda. The\nfindings from our experiments indicate that considering all these various types\nof contextual attributes outperforms baseline strategies that ignore this addi-\ntional information, with our method being exceptionally competitive and obtain-\ning the highest scores when compared to other classification methods showcased\nat DIPROMATS 2023 workshop. Our analysis of the contextual-approach reveals\nthat integrating this contextual information is crucial for detecting propaganda,\nas it is vital to comprehend its origins, the political bias it is linked to, and the\nemotion it aims to provoke in the reader.\nScope and Limitations\n\u2022This research concentrated on enhancing the detection of computational\npropaganda disseminated on Twitter. Consequently, it is important to un-\nderstand that the conclusions we have drawn from our research may not be\nuniversally applicable or relevant to other social networks, given the unique\ncharacteristics and dynamics that each platform possesses.\n\u2022Since the tweets that have been analyzed for our research purposes are\nmostly written in the English language, this consequently means that our\noutcomes and the findings derived from them are inherently restricted to\nthis particular language.\n\u2022Due to the way we partitioned Propitter, news sources that are in the\ntraining partition are also in the development and test partitions. This\npresents a potential case of data leakage, which we mitigated by taking\ntweets from a sizable pool of news sources (244).\n\u2022Our study does encounter certain limitations, particularly regarding the\n96\nlabeling of the PropitterX dataset. As with any endeavor involving the\ncreation of a data collection, the task of labeling data is complex. We\nacknowledge that our corpus is subject to the same challenges. This lim-\nitation must be carefully considered, particularly given that the labeling\nprocess inherently involves several assumptions. In the context of our re-\nsearch, we aimed to integrate the strengths of labeling approaches observed\nin previous propaganda-related works. Our labeling process merges the ad-\nvantages of distant supervision with a filter that has been trained using\nexternally sourced instances that were manually labeled in related studies.\nConsequently, we strongly encourage anyone intending to use our data or\nreference our findings to carefully consider the conditions under which our\nresults were derived.\n\u2022Although it is true that the instances within our dataset were not individ-\nually reviewed to verify or disprove the presence of propaganda, we had\nthe opportunity to test our classification approach across multiple datasets\n(namely DIPROMATS English andSpanish ), where, to the best of our\nknowledge, manual labeling was conducted. Having tested our model on the\nDIPROMATS collection, and as we described before in Section 6.3.4, our\napproach turned out to be competitive against other classification strategies,\nsome of them even also making use of contextual attributes in a different\nway than we proposed.\nSocial Concerns\nThe significance of data management and artificial intelligence within the field of\nsocial studies is well recognized. Our research exclusively used data derived from\nnews media sources, the publications of which were publicly available during the\nperiod of data acquisition. Nevertheless, it is important to emphasize that we\n97\nassert all data, models and conclusions derived from this work are intended solely\nfor research purposes, and not for any unethical uses.\nFuture Work\n\u2022As future work, we aim to investigate additional types of contextual at-\ntributes that could enhance the classification process. Among our sugges-\ntions is the inclusion of whether messages feature multimedia elements at\nthe time they are shared on social networks, as well as the degree of en-\ngagement those messages receive.\n\u2022We intend to test our methodology with more data collections as they be-\ncome available in the future. This will also entail applying our approach to\ndata from different domains, including social networks other than Twitter.\n\u2022If additional resources become available in the future, we would be eager\nto evaluate our strategy in languages beyond English and Spanish, and\neven tailor the methodology for different classification tasks, incorporating\npertinent information for each one of them as types of \u201ccontext\u201d.\n\u2022We would like to explore the application of Large Language Models (LLMs)\nin the detection of propaganda. Given the rapid advancements in NLP\nand the increasing capabilities of LLMs to understand and generate human\nlanguage, these models present a promising tool for analyzing and identi-\nfying propagandistic content. The understanding of language, context, and\nsentiment demonstrated by LLMs could be leveraged to detect patterns\nand manipulative techniques commonly used in propaganda. This would\nnot only contribute to the growing body of research on automated con-\ntent analysis but also offer practical insights into the potential for LLMs to\n98\nserve as instruments in combating misinformation in an increasingly digital\nworld.\n99\nBibliography\n[1] G. Meikle, Social Media: Communication, Sharing and Visibility . Routledge,\n2016, ISBN : 9780415712231. [Online]. Available: https://books.google.\ncom.mx/books?id=H-XXsgEACAAJ .\n[2] P. Meel and D. K. Vishwakarma, \u201cFake news, rumor, information pollution in\nsocial media and web: A contemporary survey of state-of-the-arts, challenges\nand opportunities,\u201d Expert Systems with Applications , vol. 153, p. 112 986, 2020,\nISSN : 0957-4174. DOI:https://doi.org/10.1016/j.eswa.2019.112986 .\n[Online]. Available: https://www.sciencedirect.com/science/article/\npii/S0957417419307043 .\n[3] C. Miller, How to Detect and Analyze Propaganda ...: An Address Delivered at\nTown Hall, Monday, February 20, 1939 (A Town Hall pamphlet). Town Hall,\nIncorporated, 1939. [Online]. Available: https://books.google.com.mx/\nbooks?id=UAc4AAAAMAAJ .\n[4] N. Newman, W. Dutton, and G. Blank, \u201cSocial media in the changing ecology\nof news: The fourth and fifth estate in britain,\u201d International Journal of Internet\nScience , vol. 7, Jul. 2012.\n[5] J. Tucker, A. Guess, P. Barber \u00b4a,et al. , \u201cSocial media, political polarization, and\npolitical disinformation: A review of the scientific literature,\u201d SSRN Electronic\nJournal , Jan. 2018. DOI:10.2139/ssrn.3144139 .\n100\n[6] J. A. Tucker, Y . Theocharis, M. E. Roberts, and P. Barber \u00b4a, \u201cFrom liberation\nto turmoil: Social media and democracy,\u201d Journal of democracy , vol. 28, no. 4,\npp. 46\u201359, 2017.\n[7] G. Da San Martino, S. Cresci, A. Barr \u00b4on-Cede \u02dcno, S. Yu, R. D. Pietro, and P.\nNakov, \u201cA survey on computational propaganda detection,\u201d in IJCAI , 2020.\n[8] R. Oshikawa, J. Qian, and W. Y . Wang, \u201cA survey on natural language processing\nfor fake news detection,\u201d in Proceedings of the 12th Language Resources and\nEvaluation Conference , 2020, pp. 6086\u20136093.\n[9] A. Sardo, \u201cCategories, balancing, and fake news: The jurisprudence of the euro-\npean court of human rights,\u201d Canadian Journal of Law & Jurisprudence , vol. 33,\nno. 2, pp. 435\u2013460, 2020. DOI:10.1017/cjlj.2020.5 .\n[10] L. Wang, X. Shen, G. de Melo, and G. Weikum, \u201cCross-domain learning for\nclassifying propaganda in online contents,\u201d in Truth and Trust Online Confer-\nence, Hacks Hackers, 2020, pp. 21\u201331.\n[11] G. Bolsover and P. Howard, \u201cComputational propaganda and political big data:\nMoving toward a more critical research agenda,\u201d Big data , vol. 5, no. 4, pp. 273\u2013\n276, 2017.\n[12] Pablo Moral, Guillermo Marco, Julio Gonzalo, Jorge Carrillo-de-Albornoz, and\nIv\u00b4an Gonzalo-Verdugo, \u201cOverview of DIPROMATS 2023: Automatic detection\nand characterization of propaganda techniques in messages from diplomats and\nauthorities of world powers,\u201d Procesamiento del Lenguaje Natural , vol. 71, Sep.\n2023.\n[13] S. Marsland, Machine Learning: An Algorithmic Perspective , 2nd Ed. Chapman\nand Hall/CRC, 2014, 457 Pages, ISBN : 978-1466583283.\n[14] A. G \u00b4eron, Hands-on Machine Learning with Scikit-Learn, Keras, and Tensor-\nFlow (2019, O\u2019reilly) . O\u2019Reilly Media, 2017, ISBN : 9781492032649.\n101\n[15] D. Jurafsky and J. H. Martin, Speech and Language Processing (2nd Edition) .\nUSA: Prentice-Hall, Inc., 2009, ISBN : 0131873210.\n[16] Q. Li, H. Peng, J. Li, et al. , \u201cA survey on text classification: From traditional\nto deep learning,\u201d ACM Trans. Intell. Syst. Technol. , vol. 13, no. 2, Apr. 2022,\nISSN : 2157-6904. DOI:10.1145/3495162 . [Online]. Available: https://doi.\norg/10.1145/3495162 .\n[17] C. D. Manning, P. Raghavan, and H. Sch \u00a8utze, Introduction to Information Re-\ntrieval . Cambridge University Press, 2008, ISBN : 0521865719. [Online]. Avail-\nable: https://nlp.stanford.edu/IR-book/ .\n[18] C. Aggarwal, Machine Learning for Text . Springer International Publishing,\n2018, ISBN : 9783319735313. [Online]. Available: https://books.google.\ncom.mx/books?id=uVNSDwAAQBAJ .\n[19] B. E. Boser, I. M. Guyon, and V . N. Vapnik, \u201cA training algorithm for optimal\nmargin classifiers,\u201d in Proceedings of the fifth annual workshop on Computa-\ntional learning theory , 1992, pp. 144\u2013152.\n[20] X. Qiu, T. Sun, Y . Xu, Y . Shao, N. Dai, and X. Huang, \u201cPre-trained models for\nnatural language processing: A survey,\u201d Science China technological sciences ,\nvol. 63, no. 10, pp. 1872\u20131897, 2020.\n[21] Z. Wan, \u201cText Classification: A Perspective of Deep Learning Methods,\u201d arXiv\npreprint arXiv:2309.13761 , 2023.\n[22] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre-training of deep\nbidirectional transformers for language understanding,\u201d in Proceedings of the\n2019 Conference of the North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers) , Minneapolis, Minnesota: Association for Computational Linguistics,\nJun. 2019, pp. 4171\u20134186. DOI:10.18653/v1/N19-1423 . [Online]. Available:\nhttps://www.aclweb.org/anthology/N19-1423 .\n102\n[23] R. Dror, L. Peled-Cohen, S. Shlomov, and R. Reichart, Statistical Significance\nTesting for Natural Language Processing . Morgan & Claypool, 2020, ISBN :\n1681737957.\n[24] C. Wardle, \u201cFIRST DRAFT\u2019S Essential Guide to Understanding information\ndisorder,\u201d First Draft , no. October, p. 61, 2019. [Online]. Available: https :\n//firstdraftnews.org/wp-content/uploads/2019/10/Information%\n7B%5C_%7DDisorder%7B%5C_%7DDigital%7B%5C_%7DAW.pdf?x47711 .\n[25] M. Ginsberg, \u201c8 - propaganda art as a powerful weapon for promoting nation-\nalism, patriotism and hatred towards the enemy,\u201d in Inside the World\u2019s Major\nEast Asian Collections , ser. Chandos Information Professional Series, A. Cho,\nP. Lo, and D. K. Chiu, Eds., Chandos Publishing, 2017, pp. 85\u201395, ISBN : 978-\n0-08-102145-3. DOI:https://doi.org/10.1016/B978-0-08-102145-3.\n00008-X . [Online]. Available: https://www.sciencedirect.com/science/\narticle/pii/B978008102145300008X .\n[26] B. Sinno, B. Oviedo, K. Atwell, M. Alikhani, and J. J. Li, \u201cPolitical Ideology\nand Polarization: A Multi-dimensional Approach,\u201d in Proceedings of the 2022\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies , M. Carpuat, M.-C. de Marneffe,\nand I. V . Meza Ruiz, Eds., Seattle, United States: Association for Computational\nLinguistics, Jul. 2022, pp. 231\u2013243. DOI:10.18653/v1/2022.naacl-main.\n17. [Online]. Available: https://aclanthology.org/2022.naacl-main.\n17/.\n[27] J. Sproule, D. Culbert, G. Jowett, and K. Short, Propaganda and Democracy:\nThe American Experience of Media and Mass Persuasion (Cambridge Studies in\nthe History of Mass Communication). Cambridge University Press, 1997, ISBN :\n9780521470223. [Online]. Available: https : / / books . google . com . mx /\nbooks?id=Xv9cXHL9f18C .\n103\n[28] G. Da San Martino, S. Yu, A. Barr \u00b4on-Cede \u02dcno, R. Petrov, and P. Nakov, \u201cFine-\ngrained analysis of propaganda in news article,\u201d in Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the\n9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP) , Hong Kong, China: Association for Computational Linguistics, Nov.\n2019, pp. 5636\u20135646. DOI:10 . 18653 / v1 / D19 - 1565 . [Online]. Available:\nhttps://www.aclweb.org/anthology/D19-1565 .\n[29] H. Rashkin, E. Choi, J. Y . Jang, S. V olkova, and Y . Choi, \u201cTruth of Varying\nShades: Analyzing Language in Fake News and Political Fact-Checking,\u201d in\nProceedings of the 2017 Conference on Empirical Methods in Natural Language\nProcessing , Copenhagen, Denmark: Association for Computational Linguistics,\nSep. 2017, pp. 2931\u20132937. DOI:10.18653/v1/D17-1317 . [Online]. Available:\nhttps://www.aclweb.org/anthology/D17-1317 .\n[30] D. Graff and C. Cieri, English Gigaword LDC2003T05 ,https://catalog.\nldc.upenn.edu/LDC2003T05 , Web Download. Philadelphia: Linguistic Data\nConsortium, Jan. 2003.\n[31] A. Barr \u00b4on-Cede \u02dcno, I. Jaradat, G. Da San Martino, and P. Nakov, \u201cProppy: Orga-\nnizing the news based on their propagandistic content,\u201d Information Processing\n& Management , vol. 56, May 2019. DOI:10.1016/j.ipm.2019.03.005 .\n[32] B. Horne, S. Khedr, and S. Adali, \u201cSampling the news producers: A large news\nand feature data set for the study of the complex media landscape,\u201d in Proceed-\nings of the International AAAI Conference on Web and Social Media , vol. 12,\n2018.\n[33] G. Da San Martino, A. Barr \u00b4on-Cede \u02dcno, and P. Nakov, \u201cFindings of the NLP4IF-\n2019 shared task on fine-grained propaganda detection,\u201d in Proceedings of the\nSecond Workshop on Natural Language Processing for Internet Freedom: Cen-\nsorship, Disinformation, and Propaganda , Hong Kong, China: Association for\nComputational Linguistics, Nov. 2019, pp. 162\u2013170. DOI:10.18653/v1/D19-\n104\n5024 . [Online]. Available: https : / / www . aclweb . org / anthology / D19 -\n5024 .\n[34] A. Fadel, I. Tuffaha, and M. Al-Ayyoub, \u201cPretrained ensemble learning for fine-\ngrained propaganda detection,\u201d in Proceedings of the Second Workshop on Nat-\nural Language Processing for Internet Freedom: Censorship, Disinformation,\nand Propaganda , Hong Kong, China: Association for Computational Linguis-\ntics, Nov. 2019, pp. 139\u2013142. DOI:10.18653/v1/D19-5020 . [Online]. Avail-\nable: https://aclanthology.org/D19-5020 .\n[35] D. Cer, Y . Yang, S.-y. Kong, et al. , \u201cUniversal sentence encoder for English,\u201d in\nProceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations , Brussels, Belgium: Association for Com-\nputational Linguistics, Nov. 2018, pp. 169\u2013174. DOI:10.18653/v1/D18-2029 .\n[Online]. Available: https://aclanthology.org/D18-2029 .\n[36] G. Da San Martino, A. Barr \u00b4on-Cedeno, H. Wachsmuth, R. Petrov, and P. Nakov,\n\u201cSemeval-2020 task 11: Detection of propaganda techniques in news articles,\u201d in\nProceedings of the fourteenth workshop on semantic evaluation , 2020, pp. 1377\u2013\n1414.\n[37] G. Morio, T. Morishita, H. Ozaki, and T. Miyoshi, \u201cHitachi at SemEval-2020\ntask 11: An empirical study of pre-trained transformer family for propaganda\ndetection,\u201d in Proceedings of the Fourteenth Workshop on Semantic Evalua-\ntion, A. Herbelot, X. Zhu, A. Palmer, N. Schneider, J. May, and E. Shutova,\nEds., Barcelona (online): International Committee for Computational Linguis-\ntics, Dec. 2020, pp. 1739\u20131748. DOI:10.18653/v1/2020.semeval-1.228 .\n[Online]. Available: https://aclanthology.org/2020.semeval-1.228/ .\n[38] B. Polonijo, S. \u02c7Suman, and I. \u02c7Simac, \u201cPropaganda detection using sentiment\naware ensemble deep learning,\u201d in 2021 44th International Convention on Infor-\nmation, Communication and Electronic Technology (MIPRO) , 2021, pp. 199\u2013\n204. DOI:10.23919/MIPRO52101.2021.9596654 .\n105\n[39] T. Mikolov, K. Chen, G. Corrado, and J. Dean, \u201cEfficient estimation of word\nrepresentations in vector space,\u201d arXiv preprint arXiv:1301.3781 , 2013.\n[40] C. Hutto and E. Gilbert, \u201cVader: A parsimonious rule-based model for senti-\nment analysis of social media text,\u201d in Proceedings of the international AAAI\nconference on web and social media , vol. 8, 2014, pp. 216\u2013225.\n[41] J. Yang and J. Leskovec, \u201cPatterns of temporal variation in online media,\u201d in\nProceedings of the Fourth ACM International Conference on Web Search and\nData Mining , ser. WSDM \u201911, Hong Kong, China: Association for Comput-\ning Machinery, 2011, pp. 177\u2013186, ISBN : 9781450304931. DOI:10 . 1145 /\n1935826 . 1935863 . [Online]. Available: https : / / doi . org / 10 . 1145 /\n1935826.1935863 .\n[42] J. W. Pennebaker, M. E. Francis, and R. J. Booth, Linguistic Inquiry and Word\nCount: LIWC2001 . Lawrence Erlbaum Associates, 2001.\n[43] P. Vijayaraghavan and S. V osoughi, \u201cTWEETSPIN: Fine-grained propaganda\ndetection in social media using multi-view representations,\u201d in Proceedings of\nthe 2022 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies , M. Carpuat, M.-C. de\nMarneffe, and I. V . Meza Ruiz, Eds., Seattle, United States: Association for\nComputational Linguistics, Jul. 2022, pp. 3433\u20133448. DOI:10 . 18653 / v1 /\n2022.naacl-main.251 . [Online]. Available: https://aclanthology.org/\n2022.naacl-main.251 .\n[44] S. M. Jim \u00b4enez-Zafra, F. Rangel, and M. M.-y. G \u00b4omez, \u201cOverview of iberlef\n2023: Natural language processing challenges for spanish and other iberian lan-\nguages,\u201d in Proceedings of the Iberian Languages Evaluation Forum (IberLEF\n2023), co-located with the 39th Conference of the Spanish Society for Natural\nLanguage Processing (SEPLN 2023), CEURWS. org , 2023.\n106\n[45] P. Moral, G. Marco, J. Gonzalo, J. Carrillo-de-Albornoz, and I. Gonzalo-\nVerdugo, \u201cOverview of DIPROMATS 2023: Automatic detection and charac-\nterization of propaganda techniques in messages from diplomats and authorities\nof world powers,\u201d Procesamiento del lenguaje natural , vol. 71, pp. 397\u2013407,\n2023.\n[46] O. Balalau and R. Horincar, \u201cFrom the stage to the audience: Propaganda on\nreddit,\u201d EACL 2021 - 16th Conference of the European Chapter of the Associ-\nation for Computational Linguistics, Proceedings of the Conference , pp. 3540\u2013\n3550, 2021. DOI:10.18653/v1/2021.eacl-main.309 .\n[47] D. Dimitrov, B. B. Ali, S. Shaar, et al. , \u201cSemeval-2021 task 6: Detection of per-\nsuasion techniques in texts and images,\u201d in Proceedings of the 15th International\nWorkshop on Semantic Evaluation (SemEval-2021) , 2021, pp. 70\u201398.\n[48] I. V ogel and M. Meghana, \u201cDetecting fake news spreaders on twitter from a\nmultilingual perspective,\u201d Proceedings - 2020 IEEE 7th International Confer-\nence on Data Science and Advanced Analytics, DSAA 2020 , pp. 599\u2013606, 2020.\nDOI:10.1109/DSAA49011.2020.00084 .\n[49] M. Mintz, S. Bills, R. Snow, and D. Jurafsky, \u201cDistant supervision for relation\nextraction without labeled data,\u201d in Proceedings of the Joint Conference of the\n47th Annual Meeting of the ACL and the 4th International Joint Conference on\nNatural Language Processing of the AFNLP , Suntec, Singapore: Association\nfor Computational Linguistics, Aug. 2009, pp. 1003\u20131011. [Online]. Available:\nhttps://aclanthology.org/P09-1113 .\n[50] Y . Hua, \u201cUnderstanding BERT performance in propaganda analysis,\u201d pp. 135\u2013\n138, 2019. DOI:10.18653/v1/d19-5019 . arXiv: 1911.04525 .\n[51] S. Zhang and M. Kejriwal, \u201cConcept drift in bias and sensationalism detection:\nAn experimental study,\u201d in 2019 IEEE/ACM International Conference on Ad-\n107\nvances in Social Networks Analysis and Mining (ASONAM) , 2019, pp. 601\u2013604.\nDOI:10.1145/3341161.3343690 .\n[52] L. Rocha, F. Mour \u02dcao, A. Pereira, M. A. Gonc \u00b8alves, and W. Meira, \u201cExploiting\ntemporal contexts in text classification,\u201d in Proceedings of the 17th ACM Confer-\nence on Information and Knowledge Management , ser. CIKM \u201908, Napa Valley,\nCalifornia, USA: Association for Computing Machinery, 2008, pp. 243\u2013252,\nISBN : 9781595939913. DOI:10.1145/1458082.1458117 . [Online]. Available:\nhttps://doi.org/10.1145/1458082.1458117 .\n[53] H. Kwak, C. Lee, H. Park, and S. Moon, \u201cWhat is twitter, a social network\nor a news media?\u201d In Proceedings of the 19th International Conference on\nWorld Wide Web , ser. WWW \u201910, Raleigh, North Carolina, USA: Association\nfor Computing Machinery, 2010, pp. 591\u2013600, ISBN : 9781605587998. DOI:10.\n1145/1772690.1772751 . [Online]. Available: https://doi.org/10.1145/\n1772690.1772751 .\n[54] D. McClosky, E. Charniak, and M. Johnson, \u201cEffective Self-Training for Pars-\ning,\u201d in Proceedings of the Human Language Technology Conference of the\nNAACL, Main Conference , R. C. Moore, J. Bilmes, J. Chu-Carroll, and M.\nSanderson, Eds., New York City, USA: Association for Computational Linguis-\ntics, Jun. 2006, pp. 152\u2013159. [Online]. Available: https://aclanthology.\norg/N06-1020 .\n[55] T. Joachims, \u201cText categorization with support vector machines: Learning with\nmany relevant features,\u201d in Machine Learning: ECML-98 , C. N \u00b4edellec and C.\nRouveirol, Eds., Berlin, Heidelberg: Springer Berlin Heidelberg, 1998, pp. 137\u2013\n142, ISBN : 978-3-540-69781-7.\n[56] D. Q. Nguyen, T. Vu, and A. T. Nguyen, \u201cBERTweet: A pre-trained language\nmodel for English Tweets,\u201d in Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System Demonstrations , 2020, pp. 9\u2013\n14.\n108\n[57] T. Wilson, J. Wiebe, and P. Hoffmann, \u201cRecognizing contextual polarity in\nphrase-level sentiment analysis,\u201d in Proceedings of Human Language Technol-\nogy Conference and Conference on Empirical Methods in Natural Language\nProcessing , R. Mooney, C. Brew, L.-F. Chien, and K. Kirchhoff, Eds., Vancou-\nver, British Columbia, Canada: Association for Computational Linguistics, Oct.\n2005, pp. 347\u2013354. [Online]. Available: https://aclanthology.org/H05-\n1044 .\n[58] A. F. Cruz, G. Rocha, and H. L. Cardoso, \u201cOn document representations for\ndetection of biased news articles,\u201d in Proceedings of the 35th annual ACM sym-\nposium on applied computing , 2020, pp. 892\u2013899.\n[59] P. Ekman, \u201cUniversals and cultural differences in facial expressions of emo-\ntion.,\u201d in Nebraska symposium on motivation , University of Nebraska Press,\n1971.\n[60] R. Michael and B. Breaux, \u201cThe relationship between political affiliation and\nbeliefs about sources of \u201cfake news\u201d,\u201d Cognitive Research: Principles and Im-\nplications , vol. 6, Dec. 2021. DOI:10.1186/s41235-021-00278-1 .\n[61] D. M. Blei, A. Y . Ng, and M. I. Jordan, \u201cLatent dirichlet allocation,\u201d Journal of\nmachine Learning research , vol. 3, no. Jan, pp. 993\u20131022, 2003.\n[62] L. Rocha, F. Mour \u02dcao, A. Pereira, M. A. Gonc \u00b8alves, and W. Meira, \u201cExploiting\ntemporal contexts in text classification,\u201d in Proceedings of the 17th ACM Confer-\nence on Information and Knowledge Management , ser. CIKM \u201908, Napa Valley,\nCalifornia, USA: Association for Computing Machinery, 2008, pp. 243\u2013252,\nISBN : 9781595939913. DOI:10.1145/1458082.1458117 . [Online]. Available:\nhttps://doi.org/10.1145/1458082.1458117 .\n[63] T. Salles, L. Rocha, G. L. Pappa, F. Mour \u02dcao, W. Meira, and M. Gonc \u00b8alves,\n\u201cTemporally-aware algorithms for document classification,\u201d in Proceedings of\nthe 33rd International ACM SIGIR Conference on Research and Development\n109\nin Information Retrieval , ser. SIGIR \u201910, Geneva, Switzerland: Association for\nComputing Machinery, 2010, pp. 307\u2013314, ISBN : 9781450301534. DOI:10 .\n1145/1835449.1835502 . [Online]. Available: https://doi.org/10.1145/\n1835449.1835502 .\n[64] A. Pritzkau, \u201cInvestigating Propaganda Considering the Discursive Context of\nUtterances,\u201d in Proceedings of the Iberian Languages Evaluation Forum (Iber-\nLEF 2023) co-located with the Conference of the Spanish Society for Natu-\nral Language Processing (SEPLN 2023) , ser. CEUR Workshop Proceedings,\nvol. 3496, Ja \u00b4en, Spain: CEUR-WS.org, 2023. [Online]. Available: https://\nceur-ws.org/Vol-3496/dipromats-paper1.pdf .\n[65] F. J \u00b4a\u02dcnez-Martino and A. Barr \u00b4on-Cede \u02dcno, \u201cUnileon-UniBO at IberLEF 2023\nTask DIPROMATS: RoBERTa-based Models to Climb Up the Propaganda Tree\nin English and Spanish,\u201d in Proceedings of the Iberian Languages Evaluation\nForum (IberLEF 2023) co-located with the Conference of the Spanish Society\nfor Natural Language Processing (SEPLN 2023), Ja \u00b4en, Spain, September 26,\n2023 , ser. CEUR Workshop Proceedings, vol. 3496, Ja \u00b4en, Spain: CEUR-WS.org,\n2023. [Online]. Available: https://ceur-ws.org/Vol-3496/dipromats-\npaper3.pdf .\n[66] M. Casavantes, M. Montes-y-G \u00b4omez, D. I. H. Far \u00b4\u0131as, L. C. Gonz \u00b4alez-Gurrola,\nand A. Barr \u00b4on-Cede \u02dcno, \u201cPropaLTL at DIPROMATS: Incorporating Contextual\nFeatures with BERT\u2019s Auxiliary Input for Propaganda Detection on Tweets,\u201d\ninProceedings of the Iberian Languages Evaluation Forum (IberLEF 2023)\nco-located with the Conference of the Spanish Society for Natural Language\nProcessing (SEPLN 2023), Ja \u00b4en, Spain, September 26, 2023 , M. Montes-y-\nG\u00b4omez, F. Rangel, S. M. J. Zafra, et al. , Eds., ser. CEUR Workshop Proceed-\nings, vol. 3496, Ja \u00b4en, Spain: CEUR-WS.org, 2023. [Online]. Available: https:\n//ceur-ws.org/Vol-3496/dipromats-paper2.pdf .\n110\n[67] S. Yu, J. Su, and D. Luo, \u201cImproving bert-based text classification with auxiliary\nsentence and domain knowledge,\u201d IEEE Access , vol. 7, pp. 176 600\u2013176 612,\n2019. DOI:10.1109/ACCESS.2019.2953990 .\n[68] F. S \u00b4anchez-Vega and A. P. L \u00b4opez-Monroy, \u201cBERT\u2019s Auxiliary Sentence focused\non Word\u2019s Information for Offensiveness Detection.,\u201d in Proceedings of the\nIberian Languages Evaluation Forum (IberLEF 2021) co-located with the Con-\nference of the Spanish Society for Natural Language Processing (SEPLN 2021) ,\nser. CEUR Workshop Proceedings, vol. 2943, CEUR-WS.org, 2021. [Online].\nAvailable: https://ceur-ws.org/Vol-2943/meoffendes_paper4.pdf .\n[69] C. Sun, L. Huang, and X. Qiu, \u201cUtilizing BERT for aspect-based sentiment anal-\nysis via constructing auxiliary sentence,\u201d in Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , J.\nBurstein, C. Doran, and T. Solorio, Eds., Minneapolis, Minnesota: Association\nfor Computational Linguistics, Jun. 2019, pp. 380\u2013385. DOI:10.18653/v1/\nN19-1035 . [Online]. Available: https://aclanthology.org/N19-1035 .\n[70] Model Description,bert-base-uncased-emotion ,https://huggingface.co/\nbhadresh- savani/bert- base- uncased- emotion , [Online; accessed 30-\nMay-2023], 2018.\n[71] \u201dEmotion\u201d, Dataset Summary ,https : / / huggingface . co / datasets /\nphilschmid/emotion , [Online; accessed 30-May-2023], 2022.\n[72] A. Benavoli, F. Mangili, G. Corani, M. Zaffalon, and F. Ruggeri, \u201cA bayesian\nwilcoxon signed-rank test based on the dirichlet process,\u201d in Proceedings of the\n31st International Conference on International Conference on Machine Learn-\ning - Volume 32 , ser. ICML\u201914, Beijing, China: JMLR.org, 2014, II\u20131026\u2013II\u2013\n1034. [Online]. Available: http : / / proceedings . mlr . press / v32 /\nbenavoli14.pdf .\n111\n[73] A. Benavoli, G. Corani, J. Dem \u02c7sar, and M. Zaffalon, \u201cTime for a change: A\ntutorial for comparing multiple classifiers through bayesian analysis,\u201d The Jour-\nnal of Machine Learning Research , vol. 18, no. 1, pp. 2653\u20132688, Jan. 2017,\nISSN : 1532-4435. [Online]. Available: https://jmlr.org/papers/v18/16-\n305.html .\n[74] J. M. P \u00b4erez, J. C. Giudici, and F. Luque, pysentimiento: A Python Toolkit for\nSentiment Analysis and SocialNLP tasks , 2021. arXiv: 2106.09462 [cs.CL] .\n[75] RoBERTuito, A pre-trained language model for social media text in Spanish ,\nhttps://huggingface.co/pysentimiento/robertuito-base-uncased ,\n[Online; accessed 30-May-2023], 2022.\n[76] F.-J. Rodrigo-Gin \u00b4es, J. Carrillo-de-Albornoz, and L. Plaza, \u201cHierarchical Mod-\neling for Propaganda Detection: Leveraging Media Bias and Propaganda De-\ntection Datasets,\u201d in Proceedings of the Iberian Languages Evaluation Forum\n(IberLEF 2023) co-located with the Conference of the Spanish Society for Nat-\nural Language Processing (SEPLN 2023), Ja \u00b4en, Spain, September 26, 2023 ,\nser. CEUR Workshop Proceedings, vol. 3496, Ja \u00b4en, Spain: CEUR-WS.org, 2023.\n[Online]. Available: https : / / ceur - ws . org / Vol - 3496 / dipromats -\npaper7.pdf .\n[77] J. A. Garc \u00b4\u0131a-D\u00b4\u0131az and R. Valencia-Garc \u00b4\u0131a, \u201cUMUTeam at Dipromats 2023:\nPropaganda Detection in Spanish and English Combining Linguistic Features\nwith Contextual Sentence Embeddings,\u201d in Proceedings of the Iberian Lan-\nguages Evaluation Forum (IberLEF 2023) co-located with the Conference of the\nSpanish Society for Natural Language Processing (SEPLN 2023), Ja \u00b4en, Spain,\nSeptember 26, 2023 , ser. CEUR Workshop Proceedings, vol. 3496, Ja \u00b4en, Spain:\nCEUR-WS.org, 2023. [Online]. Available: https : / / ceur - ws . org / Vol -\n3496/dipromats-paper5.pdf .\n[78] V . Ahuir, L. F. Hurtado, F. Garc \u00b4\u0131a-Granada, and E. Sanchis, \u201cELiRF-VRAIN\nat DIPROMATS 2023: Cross-lingual Data Augmentation for Propaganda De-\n112\ntection,\u201d in Proceedings of the Iberian Languages Evaluation Forum (IberLEF\n2023) co-located with the Conference of the Spanish Society for Natural Lan-\nguage Processing (SEPLN 2023), Ja \u00b4en, Spain, September 26, 2023 , ser. CEUR\nWorkshop Proceedings, vol. 3496, Ja \u00b4en, Spain: CEUR-WS.org, 2023. [Online].\nAvailable: https://ceur-ws.org/Vol-3496/dipromats-paper6.pdf .\n[79] D. Loureiro, F. Barbieri, L. Neves, L. Espinosa Anke, and J. Camacho-collados,\n\u201cTimeLMs: Diachronic language models from Twitter,\u201d in Proceedings of the\n60th Annual Meeting of the Association for Computational Linguistics: Sys-\ntem Demonstrations , V . Basile, Z. Kozareva, and S. Stajner, Eds., Dublin, Ire-\nland: Association for Computational Linguistics, May 2022, pp. 251\u2013260. DOI:\n10 . 18653 / v1 / 2022 . acl - demo . 25 . [Online]. Available: https : / /\naclanthology.org/2022.acl-demo.25/ .\n113\nAppendices\n114\n7.A List of propagandist sources considered for Propit-\nter\u2019s construction.\nTable A1: Propagandist sources of data considered for the construction of Propitter\n(124 sources).\nSources\nActivist Mommy/Elizabeth Johnston \u2022Al Bawaba \u2022AliciaFixLuke (BB4SP) \u2022American Look-\nout\u2022American Principles Project (approject) \u2022American Thinker \u2022AmericaTFP \u2022Anti-\nEmpire \u2022Arab News \u2022Beijing Review \u2022Big League Politics \u2022Bipartisan Report \u2022Blunt\nForce Truth \u2022Breitbart \u2022Caixin \u2022Caldron Pool \u2022China Daily \u2022China Global Television\nNetwork (CGTN) \u2022Christian Action Network (Martin Mawyer) \u2022CNS News \u2022Competitive\nEnterprise Institute (ceidotorg) \u2022Conservative Daily News \u2022Conservative Free Press \u2022Con-\nservative Patriot (Co-firing-line) \u2022CWforA \u2022Daily Mail \u2022Dan Bongino \u2022DavidBartonWB\n\u2022Defiant \u2022Deplorable Kel \u2022Discover the Networks (Leftist Networks) \u2022Echo Check (The\nOther Checker) \u2022Eh Conservative \u2022envolve\u2022engpravda \u2022Europe-Israel \u2022FactCheckingTR\n\u2022FAIR (The Federation for American Immigration Reform) \u2022FocusFamily \u2022Freedom First\n\u2022Frontpage Magazine (David Horowitz) \u2022GatestoneInst \u2022Ghost.Report \u2022GOP.gov (House\nRepublicans) \u2022Granma (Cuba) \u2022Gulf News \u2022HeartlandInst \u2022I Hate the Media \u2022Indepen-\ndent Sentinel \u2022Information Liberation (Info Lib News) \u2022Institute for Historical Review \u2022JD\nRucker (NOQ Report) \u2022Judicial Watch \u2022Just The News \u2022Left Action \u2022Lew Rockwell \u2022Life\nNews\u2022Mad World News \u2022meforum \u2022Mehr News Agency \u2022Middle East Media Research In-\nstitute (MEMRI) \u2022Mondoweiss \u2022MoonBattery \u2022MoveOn \u2022National Right to Life Committee\n(NRLC) \u2022News Rescue \u2022News18 \u2022newsblaze \u2022nicolejames \u2022Nordic Monitor \u2022NowThis\nNews\u2022Occupy Democrats \u2022other98 \u2022Patriot Journal \u2022PJ Media \u2022Political Dig \u2022PragerU\n\u2022PRISource \u2022ProFamOrg \u2022Public Discourse \u2022Raheem Kassam \u2022ReadTheHornNews \u2022Red\nV oice Media \u2022redicetv \u2022remnantnews \u2022Right Side Broadcasting Network (RSBN) \u2022RT\u2022\nRudaw \u2022Ruptly \u2022Russia Insider \u2022Russian News Agency-TASS \u2022scrowder \u2022Sean Hannity\n\u2022Shafaq News \u2022Sputnik News \u2022StopSocialists \u2022Swarajya \u2022syria updates \u2022takimag \u2022Tas-\nnim News Agency \u2022Tehran Times \u2022TeleSUR \u2022The Blaze \u2022The Clover Chronicle \u2022The\nColorado Herald \u2022The D.C. Clothesline \u2022The Daily Wire \u2022The Federalist (FDRLST) \u2022The\nFree Telegraph \u2022The National (UAE) \u2022The Political Insider \u2022The RFAngle \u2022The Scoop\n\u2022The Unz Review \u2022themajalla \u2022TheBell News\u2022Tim Brown (Reformed Media/FPPTim)\n\u2022ToddStarnes.com \u2022Trending Politics \u2022Turning Point USA \u2022vdare\u2022WayneDupree.com \u2022\nWestJournalism \u2022Women are Human\n115\n7.B List of non-propagandist sources considered for\nPropitter \u2019s construction.\nTable A2: Non-propagandist sources of data considered for the construction of Propit-\nter(120 sources).\nSources\nABC News \u2022ABC11 Eyewitness News \u2022ABS-CBN News \u2022Africa News \u2022Ahram Online \u2022\nAl Arabiya \u2022Al Jazeera \u2022Al-Masdar News (AMN) \u2022Arizona Daily Star (Tucson Star) \u2022Arutz\nSheva (Israel National News) \u2022Atlanta Black Star \u2022Atlanta Jewish Times \u2022Austin American-\nStatesman \u2022Baltimore Sun \u2022Berkshire Eagle \u2022Boston Globe \u2022Boy Genius Report (BGR) \u2022\nBusiness Insider \u2022Calgary Sun \u2022CBS News \u2022Charlotte Observer \u2022Chicago Tribune \u2022Citi-\nzens for Legitimate Government (CLG News) \u2022CNN Business \u2022CNN Communications \u2022CNN\nInternational \u2022America Conservative Review \u2022Daily Hive \u2022Daily Press \u2022Daily Signal \u2022Dead-\nline Hollywood \u2022Deccan Herald \u2022Edmonton Journal \u2022Euronews \u2022Fox News (foxnews.com) \u2022\nFresno Bee \u2022Greensboro News and Record \u2022Haaretz \u2022Honolulu Star-Advertiser \u2022Huffington\nPost (HuffPost) \u2022Hurriyet Daily News \u2022IPOWER \u2022Japan Times \u2022Jewish Standard \u2022Jew-\nishNewsUK \u2022Kansas City Star \u2022Kansasdotcom \u2022KMOV \u2022KOCO News 5 \u2022Korea Herald \u2022\nKUOW NPR \u2022LA Times (Los Angeles Times) \u2022Lethbridge Herald \u2022Lexington Herald Leader\n\u2022Middle East Monitor \u2022Montreal Gazette (mtlgazette) \u2022MSN.com (MSN News) \u2022mySA \u2022\nNational Review \u2022New Republic \u2022New York Daily News \u2022Newsweek \u2022Northwest Arkansas\nDemocrat-Gazette \u2022NYJewishWeek \u2022Outside The Beltway \u2022Ozy Media \u2022Politico \u2022PressTV\n\u2022RajaPetra (Malaysia Today) \u2022Raleigh News and Observer \u2022RedState \u2022Roanoke Times \u2022\nRTDNews \u2022RTE (Radio Television of Ireland) \u2022Sacramento Bee \u2022Santa Barbara Independent\n\u2022Saudi Gazette \u2022SFGate \u2022Sky News UK \u2022St. Louis Post-Dispatch \u2022Star Tribune \u2022Stars and\nStripes \u2022Tablet Magazine \u2022Taiwan News \u2022Tampa Bay Times \u2022Texas Tribune \u2022The Bangkok\nPost\u2022The Cipher Brief \u2022The Courier-Mail (Australia) \u2022The Daily Tarheel \u2022The Day (New\nLondon) \u2022The Hartford Courant \u2022The Herald (Everett) \u2022The Hill \u2022The Jakarta Post \u2022The\nJapan News \u2022The Nation \u2022The New Humanitarian \u2022The News International \u2022The Olympian\n\u2022The Patriot-News (Pennlive.com) \u2022The Providence Journal \u2022The Santa Fe New Mexican \u2022\nThe State Newspaper \u2022The Stream \u2022The Sun \u2022The Tacoma News Tribune \u2022The Week (USA)\n\u2022Thomson Reuters Foundation \u2022Time Magazine \u2022Times Colonist \u2022Times of India \u2022Times of\nIsrael\u2022Utah Public Radio (UPR) \u2022Vancouver Sun \u2022Washington Post \u2022WGN News \u2022Windsor\nStar\n116\n7.C Bias distribution of tweets in main partitions of\nPropitterX .\nTable A3: Distribution of tweets per bias and class in main partitions of PropitterX\nBias Prop. Non-prop. Total tweets\nEXTREME LEFT 504 0 504\nFAR-LEFT 3,736 0 3,736\nLEFT 5,121 19,424 24,545\nLEFT-CENTER 3,318 146,852 150,170\nLEAST BIASED 0 23,741 23,741\nRIGHT-CENTER 10,355 53,132 63,487\nRIGHT 15,394 20,245 35,639\nFAR-RIGHT 12,104 0 12,104\nEXTREME RIGHT 35,826 0 35,826\nRIGHT-CONSPIRACY/PSEUDOSCIENCE 0 1,191 1,191\nCONSPIRACY-PSEUDOSCIENCE 0 1,209 1,209\nUNKNOWN 4,308 14,032 18,340\nTotal 370,492\n117\n7.D Training partitions with proportional sampled\nemotions in PropitterX-EMO .\nTable A4: Partition considered to train a classifier with proportional sampled emotions\ninPropitterX-EMO .\nTrain\nEmotion Propaganda Non-propaganda\nFear 19,200 19,200\nAnger 6,000 6,000\nJoy 2,700 2,700\nSadness 1,200 1,200\nSurprise 600 600\nDisgust 300 300\nNeutral 0 0\n118", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "A Multidimensional Analysis of Text for Automated Detection of Computational Propaganda in Twitter", "author": ["MEC Moreno"], "pub_year": "2025", "venue": "NA", "abstract": "The way we consume news has been transformed, with technological advancements allowing  people to easily express their views to vast audiences, including political opinions. These"}, "filled": false, "gsrank": 620, "pub_url": "https://inaoe.repositorioinstitucional.mx/jspui/handle/1009/2692", "author_id": [""], "url_scholarbib": "/scholar?hl=en&q=info:bC616e3HKdQJ:scholar.google.com/&output=cite&scirp=619&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D610%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=bC616e3HKdQJ&ei=dbWsaILdNb_SieoPzJnloAQ&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:bC616e3HKdQJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://inaoe.repositorioinstitucional.mx/jspui/bitstream/1009/2692/1/CASAVANTESMME_DCC.pdf"}}, {"title": "A missed opportunity? President Trump, the truth sandwich, and news coverage across an ideological spectrum", "year": "2021", "pdf_data": "\u00a9 Media Watch |12 (2) 177-196, 2021\nISSN 0976-0911 | E-ISSN 2249-8818\nDOI: 10.15655 /mw/2021/v12i2/160145\nA Missed Opportunity? President Trump,\nthe Truth Sandwich, and News Coverage\nAcross an Ideological Spectrum\nLinda Jean Kenix1 & Jovita Manickam2\n1University of Canterbury, New Zealand\n2University of Auckland, New Zealand\nAbstract\nMany within mainstream news media wondered aloud and in print how to cover\nPresident Trump who, they purported, frequently distorted the truth. Although\nPresident Trump is no longer in office, this research remains vitally important\nfor understanding how the press covers the office of the Presidency when the\nveracity of information is in question.  During President Trump\u2019s tenure, the\n\u2018truth sandwich,\u2019 was suggested as a technique, whereby a fact is first stated in\na news article, then a quote of the false assertion, followed by the fact again.\nThis research aimed to explore the presence of the truth sandwich in an\nideological range of news content. These findings have implications for the\nfuture coverage of Presidencies in the United States, and for governments in\nother countries, where the veracity of information is questionable.\nKeywords: Truth sandwich, President Trump, Ideology, news\nIntroduction\nThe impetus for this study was to better understand the perceived war between main-\nstream journalists and the 45th President of the United States, Donald Trump. President\nTrump repeatedly made media outlets the target of vitriolic attacks, even going so far as to\ntweet that various media outlets are the enemy of the American people. In response, many\nwithin mainstream news media wondered aloud how to cover a president who so fre-\nquently bends and even breaks, the truth. Many within the news industry believed \u201cthe\nreality-based news media (had) to stop kowtowing to the Emperor\u201d (Sullivan, 2018). Al-\nthough President Trump is no longer in office, this research is vitally important for under-\nstanding how the press covers the office of the Presidency when the veracity of information is\nin question.\nOne central concept, memorably titled the \u2018truth sandwich,\u2019 was suggested\nrepeatedly as the only appropriate way that mainstream media could cover President\nTrump (Sullivan, 2018; Illing, 2018; Memmot, 2018). The truth sandwich, coined by linguist\nGeorge Lakoff, is a recommended three-part introduction to any article in which a false\nassertion is quoted. This concept was created as a specific technique to cover President\nTrump (Sullivan, 2018). In this construct, firstly, a factual statement or truth claim is made.\nThis is then followed by a quote of a false assertion made by President Trump.\nCorrespondence to: Linda Jean Kenix, Media and Communication Department, University\nof Canterbury, Private Bag 4800 Christchurch, New Zealand 8140.\n177\n178Media Watch 12 (2)\nFinally, the truth is stated again, reaffirming the first sentence of the article. President\nTrump\u2019s false claim is therefore \u2018sandwiched\u2019 in truth (Sullivan, 2018).\nThe key purpose of this research was to examine articles for the presence of the\ntruth sandwich as this has been noted by several media critics to be essential for any press\ncoverage of President Trump. In an effort to better understand the present media environment\nsurrounding the President, this research also asked whether the slant of coverage of\nPresident Trump was, across an ideological range of outlets, positive or negative. Further,\nas data could be collected from newspapers across the political spectrum, this research\nasked whether coverage differed between conservative and liberal newspapers in an effort\nto ascertain whether positivity or negativity might have originated from a specific source.\nThis article attempted to also examine what other methods, if any, the mainstream media\nemployed in their coverage of President Trump.\nThis paper first reviews the norms of journalism to better understand the present\nchallenges to journalism as an institution, steeped in its own historical norms. This\nmanuscript will then attempt to examine journalism in the era of President Trump. It is\neasier to understand just how different contemporary journalism might be if there is a\nbetter understanding of the how the practice has evolved.\nJournalism in America\nThe Anglo-American model of journalism has lent a high level of legitimacy to journalists,\nwho have served as \u201cmoral guardians,\u201d that possess an \u201celitist claim to responsibility,\u201d\nwhich transferred a great deal of power within society (Zelizer, 2018, p. 142). The impact of\nthis can be seen today in America as journalism there continues to perform as a moral\narbiter for societal issues. The other defining characteristic of the Anglo-American model\nis its constant innovation in journalistic form. American journalism, which is often fuelled\nby a form of American exceptionalism (Zelizer, 2018, p. 142; Chalaby, 1996, p. 303), has\ntransfused innovation into the Anglo-American model of American journalism.\nAt times, American journalism has fluctuated according to its ability or failure to\nadhere to long-held occupational norms, such as objectivity, accuracy and clarity (Chalaby,\n1996; Schudson, 2001). Regardless of this perception, many within the American journalistic\nprofession have maintained that \u201cjournalists use news to achieve pragmatic aims of\ncommunity\u201d (Zelizer, 1993, p. 82). Yet, despite the longevity and ubiquity of the Anglo-\nAmerican model, and the pragmatic aims of a community focus, journalism in America has\nmore recently been accused of being too far removed from the public it claims to serve\n(Zelizer, 2018). There is a common conception among conservatives in particular that\njournalism has become \u2018elite.\u2019\nIt is important to note that news media in America, and indeed worldwide, are not\none monolithic voice. Rather, journalism is ubiquitous, dynamic and multi-directional in\napproach. However, despite the political affiliation one may have, it remains an undeniable\ntruth that the elite, powerful news media predominately shape how the public interprets\nissues and events (Sotirovic, 2000). Consequently, the public\u2019s main understanding of\nsocial issues is largely derived from a framed construction provided by the elite news\nmedia, which is intrinsically from a distinct perspective than the general population (i.e.\nAltheide, 1976; Gamson, 1992; Gitlin, 1980; Ryan, Carragee, & Schwerner, 1998; Tuchman,\n1978). Although from a distinctly elite ideology, news media create purposefully framed\nimages after a complex negotiation of contributing forces that can also be seen to be\npolitically motivated (Street, 2001). This is not a structurally deterministic process whereby\nmedia simply disseminate elite, dominant messages. Rather, \u201cthere is an interaction within\n179the media, who operate as both structures and agents, not passively disseminating dominant\nideologies (as suggested by structural accounts) but playing an active role in their creation,\nconstruction articulation and communication\u201d (Allen & Savigny, 2012, p. 280).\nActively constructed and co-created messaging coalesces to form a broader\ncultural perspective, which then provides researchers with a framework to better understand\nsimilarities across large groupings (Thompson, 1990). Thus, the inclusion or exclusion of\nparticular information, on a national scale, can have profound ideological implications.\nThis has obvious importance in a politically charged environment such as the Presidency\nof the United States. It is a common perception in the United States that news media are\nslowly losing their legitimacy in the digital era (Lischka, 2019; McDevitt & Ferrucci, 2018).\nActive audiences in America often believe that journalism is embedded with bias (Eveland\n& Shah, 2003) \u2013 whether conservative or liberal -  and journalism itself is not worthy of\npublic trust (Kohut, 2004). This lack of trust may be traced to a longstanding public\nadherence to the values of accuracy and unbiased reporting that has far superseded speed\nor serving as a watchdog (Beam, Weaver, & Brownlee, 2009), despite those in the news\nmedia widely supporting these characteristics as essential defining aspects of their\nprofession (Beam et al., 2009). Consequently, there are myriad contradictions and potential\nproblems for the stability and legitimacy of American news media - these were only further\nexacerbated by the rise of President Trump.\nJournalism in the Trump Era\nThe electoral campaign and presidency of Donald J. Trump led to seismic shifts in the news\nlandscape. During the Trump administration, journalists faced the challenge of reporting\non a president who not only subverts and restricts the traditional mechanisms by which\nthe press monitor the presidency (Zelizer, 2018, p. 146), but went further to be openly\nantagonistic towards many journalists. Although this poses obvious problems for news\ninstitutions, it also presented an opportunity for the overhaul of journalism\u2019s deeply\nentrenched norms. As Zelizer (2018) writes, coverage of President Trump \u201cmade visible the\nbubble in which journalism lives\u201d (p. 150) and the dual phenomena of Brexit and President\nTrump made a case for \u201cjournalism\u2019s necessary and immediate reset\u201d (Zelizer, 2018, p. 152).\nThis reset, were it to occur, would be set in the context of a president at war with\nthe press. Although tension between the executive office and the press is not a new\nphenomenon, President Trump\u2019s open hostility towards most mainstream news\norganisations set him apart from his predecessors (Mour\u00e3o, Thorson, Chen, & Tham,\n2018).\u00a0President\u00a0Trump\u00a0was\u00a0prone\u00a0to\u00a0incisive\u00a0Twitter\u00a0attacks\u00a0on\u00a0the\u00a0press,\u00a0frequently\nlevelling accusations of \u2018fake news\u2019 and failure at many news outlets to paint himself\nfavourably\u00a0(McVittie\u00a0\u00a0&\u00a0McKinlay,\u00a02018).\u00a0He\u00a0barred\u00a0reporters\u00a0from\u00a0press\u00a0briefings\u00a0and\nother media events who were from outlets that he deemed as \u2018fake news\u2019. President\nTrump\u00a0also\u00a0displayed\u00a0favouritism\u00a0towards\u00a0traditionally-viewed\u00a0conservative\u00a0media,\u00a0often\nappearing on the Fox cable news network and granting press credentials to unorthodox\nconservative outlets such as Infowars\u00a0(Mour\u00e3o\u00a0et\u00a0al.,\u00a02018,\u00a0p.\u00a01946).\nPresident Trump\u2019s mistrust of the press reverberated beyond news organisations.\nIn a study of news trust during the early months of the Trump administration, \u201csupport for\nTrump [was] the strongest predictor of news distrust\u201d(Mour\u00e3o et al., 2018, p. 1945) among\nthe general public. President Trump\u2019s supporters were most likely to express distrust in\nmainstream media, and gravitated towards conservative news sources, whereas those\nwho consumed mostly mainstream media were less likely to be conservative and more\nlikely to have high levels of mistrust towards the President (Mour\u00e3o et al., 2018, p. 1953).Kenix & Manickam\n180Media Watch 12 (2)\nNews media trust in general declined since President Trump\u2019s election, which is concerning\nfor the status of journalists given that \u201cnews media trust is crucial to the role that journalism\nplays in democracies\u201d (Mour\u00e3o et al., 2018, p. 1945).\nNews organisations appear hyperaware of the challenges involved in covering\nthe President. In the wake of the election some institutions issued memorandums on new\nmodes of covering President Trump (Zelizer, 2018, p. 147), while others made grand gestures\nto signal a suspension of normal relations. In an unprecedented and potentially costly\nmove, CNN made the decision not to broadcast live the first briefing of then-press secretary\nSean Spicer, citing a need to monitor what was said and then report on it later (Messina,\n2018, p. 92). The New York Times  also employed a range of verbal delegitimizing strategies\nto combat President Trump\u2019s fake news accusations and all the while asserted their own\nlegitimacy as a news organization (Lischka, 2019).  Both CNN and the New York Times  are\norganisations that President Trump frequently took aim at for disseminating negative\ncoverage about him. These news organizations had a vested interest in maintaining their\nlegitimacy in the face of his frequent attacks. Other news organisations did not take such\nmeasures, and have at times been critical of these organisations, which has led to further\nfracturing of the news environment (Zelizer, 2018). In any case, the effectiveness of these\nmeasures remains to be seen. Many of the so-called new strategies put forward to combat\nPresident Trump were employed before, to little effect. Journalists resorted to old tools such as\nobjectivity, balance and moderation (Zelizer, 2018), however, President Trump\u2019s attacks on the\nmedia continued to pepper his Twitter feed and characterise his administration.\nPresident Trump demonstrated considerable awareness of the fact that \u201cthe space\nbetween news and entertainment collapsed\u201d (Edwards, 2018, p. 40).  Many news outlets\nnoted that he carried aspects of his reality television show, The Apprentice , into the White\nHouse. Indeed, President Trump contributed to the contemporary US political system\nbecoming \u201ca form of global entertainment\u201d (Edwards, 2018, p. 27). Of course, Twitter further\nenhanced this perception. Much has been written about President Trump\u2019s use of social\nmedia, particularly his use of Twitter (Edwards, 2018; Lischka, 2019; Wahl-Jorgensen,\n2018). President Trump relied on Twitter as his primary social media outlet, with its\n\u201ccharacteristic character limit, the way sarcasm propels language on the platform and its\nspeed and trajectory\u201d (Edwards, 2018, p. 30). In broadcasting his thoughts directly through\nTwitter, President Trump bypassed the traditional gatekeeping function of mainstream\nmedia but continued to benefit from the extensive coverage of his tweets  in the mainstream\npress (Wahl-Jorgensen, 2018). Although journalists expressed frustration at President\nTrump\u2019s continual use of Twitter, they also defended their ongoing relevance in light of it\n(Edwards, 2018). Most importantly, most mainstream news outlets continued to give\nPresident Trump the coverage he desired (Zelizer 2018).\nAs the lines between news and entertainment become increasingly blurred, there\nremains questions about how exactly the mainstream media have responded to their\ncoverage of President Trump. Many argued that there needs to be a fundamental shift in\nhow the Presidency and the White House were covered. But did these changes take place?\nDid the mainstream media really shift their coverage of the President or did they simply\nsignal for a change that never occurred?\nPresident Trump and the \u2018Truth Sandwich\u2019\nGitlin (1980)  long ago defined frames as \u201cpersistent patterns of cognition, interpretation,\nand presentation, of selection, emphasis, and exclusion, by which symbol-handlers\nroutinely organise discourse\u201d (p. 7). A frame determines what is \u201crelevant\u201d (Hertog & McLeod,\n1811995, p. 4) and \u201csuggests what the issue is\u201d (Tankard Jr., Hendrickson, Silberman, Bliss, &\nGhanem, 1991). Frames (both textual and visual) are \u201corganizing principles that are socially\nshared and persistent over time, that work symbolically to meaningfully structure the\nsocial world\u201d (Reese, Gandy Jr., & Grant, 2001, p. 11).\nThe idea of framing, however, is complicated when various elite symbol handlers\norganise discourse differently. President Trump challenged what is \u2018relevant\u2019 and \u2018what the\nissue is\u2019 by contradicting prevalent news frames in the news media. Thus, the media frame\nshifted significantly within any given day. Professor Rosen, a media critic, writer, and\nprofessor of journalism at New York University, said \u201cjournalists charged with covering\n(President Trump) should suspend normal relations with the presidency of Donald Trump,\nwhich is the most significant threat to an informed public in the United States today\u201d\n(Rosen, 2018). He argued that mainstream news organisations should send their interns to\ncover the White House (and therefore refuse to lend prestige to the office) given the amount\nof untruths that have been discovered. Rosen (2018) stated clearly that the mainstream\npress must suspend normal relations in how they cover the president.\nThis sentiment has been echoed by Memmott at NPR (2015). In an article titled\n\u201cWhen there is no evidence to support a claim, we should state that\u201d the author concludes\nthat the news media have not clearly stated when something is false and that must change.\nHe argues forcefully for a \u201cno evidence\u201d frame. This sentiment is the rationale behind what\nhas been called the \u2018truth sandwich.\u2019\nAs mentioned in the Introduction, the \u2018truth sandwich\u2019 is a recommended three-\npart introduction to any article in which President Trump is quoted. This construct consists\nof a factual statement or truth claim in the first sentence, followed by a quote of a false\nstatement by President Trump, and then a reinforcement of the accurate statement made in\nthe first sentence of the article. President Trump\u2019s false claim is therefore \u2018sandwiched\u2019 in\ntruth (Sullivan, 2018). Lakoff has argued that \u201cif you negate a frame, you activate the\nframe\u201d (Illing, 2018). President Trump has used this to his advantage by saying or tweeting\nsomething that is blatantly false, knowing that news media will cover it. Although many\njournalists then go on to fact-check his claim, the fact that President Trump\u2019s framing of an\nissue is stated first means that readers are still going to remember the president\u2019s framing\n(Illing, 2018). An effective way to counteract this, according to Lakoff (Sullivan, 2019), was\nto make use of the \u2018truth sandwich\u2019 at the opening of any article on President Trump, which\nsubsequently set the tone for the rest of the article.\nAs the \u2018truth sandwich\u2019 was only recently conceptualised, there was no previous\nresearch that could be found on it\u2019s existence or lack of existence in media coverage.\nHowever, research has argued that the U.S. presidency possesses so much significant news\nvalue that journalists could naturalize presidential behaviour despite professional\njudgment (Parks, 2020). This may explain why initial untruths in the potential \u2018truth\nsandwich\u2019 coverage were so readily accepted. There has also been recent research that\ndemonstrates conducting fact checks actually increases media trust, epistemic political\nefficacy and future news intent (Pingree et. al., 2018). This strongly suggests that writing\nstories with the \u2018truth sandwich\u2019 embedded in a news story could actually be beneficial for\nthe journalistic profession. Yet, a high level of fact checking simply did not occur during\nthe Trump presidency. For almost the entirety of his Presidency, the President simply lied\nwithout much contestation from the press. This has led some journalists to opine that, in\nthe future, journalists must remember that \u201cthe first obligation of journalism is not \u2018balance,\u2019\nbut truth\u201d (Rubin, 2020).\nThere has been some pushback against the idea of the \u2018truth sandwich,\u2019 most\nnotably in a Wall Street Journal  column, in which Lakoff\u2019s arguments were said to beKenix & Manickam\n182Media Watch 12 (2)\ncondescending to audiences and that any report on the President\u2019s statements should\nbegin with his own words, not the words of the journalist covering him (Sartwell 2018).\nDespite this critique, however, the \u2018truth sandwich\u2019 stands as a widely-circulated mechanism\nby which journalists could report on President Trump. Therefore, this research examined\narticles for the presence of the \u2018truth sandwich\u2019 and explored how the mainstream media\nwere covering President Trump.\nResearch Questions\nThis paper wishes to better understand how journalists across the ideological spectrum\ncovered President Trump. Specifically, this research is aimed at understanding if a \u2018truth\nsandwich\u2019 existed in coverage of President Trump.\nRQ1 : Did the \u2018truth sandwich\u2019 exist in mainstream news articles where President Trump\nwas quoted?\nAs a central concern of many journalists was the false assertions made by President\nTrump, usually in the form of quotes, this research also sought to discern the current\npractices around quoting President Trump. This involved noting whether he was quoted in\nthe headline or lead paragraph of an article, whether or not someone in his administration\nwas quoted as speaking on his behalf, and whether any voices contradictory to President\nTrump were also quoted in a given article. Given President Trump\u2019s extensive use of Twitter\nand the media storms that often followed his tweets, quotes or mentions of his tweets were\nalso coded for, alongside other variables related to the President\u2019s Twitter usage. Therefore,\na second research question was:\nRQ2 : In what capacity was President Trump quoted?\nPresident Trump said repeatedly that he received negative coverage from the mainstream\npress and, conversely, the mainstream press said repeatedly that they treated him fairly.\nThus, the third research question was as follows:\nRQ3 : How did the mainstream press cover President Trump (more negative or positive)?\nFinally, although this was not the main focus of this research, this paper examined\nnewspapers across the political spectrum, and therefore the following research question\nwas asked:\nRQ4 : Was there a difference in the coverage of President Trump between conservative and\nliberal newspapers in the mainstream press?\nMethodology\nThis study examined four newspapers\u2019 coverage of President Trump between 20 January\n2017 and 3 September 2018 as this period of time was illustrative of the Trump presidency.\nThe process of ascertaining repeating patterns or themes from each source was guided by\nthematic analysis (Braun & Clarke, 2006), which used the research questions as a guide to\nthen develop selection criteria of codes that were used for the quantitative content analysis.\nAn initial identification of themes was made first when examining the entire dataset and\nthen those themes were continually refined after closer readings of the text. During this\nprocess, principal themes, that could not have been uncovered through segmented or\ncursory readings, began to emerge. Emerging narratives were viewed as representative of\nimplicit positions of power that worked to define those who held epistemic authority.\n183These narratives were the \u201coutcome of strategic communication decisions\u201d (Jones &\nHimelboim, 2010, p. 277) that imbue a person, place or thing with power or a lack thereof.\nAn emerging narrative was the manifestation of that positioning. Using thematic analysis,\nat this stage of the research, was important as it is not \u201cwedded to any pre-existing theoretical\nframework\u201d (Braun & Clarke, 2006, p. 81). Most significantly, thematic analysis examines\n\u201cwhether a theme is salient or relevant to a particular study is based on context and not\nnecessarily determined by quantifiable measures (e.g. frequency of occurrence)\u201d (Xu, 2020,\np. 253). This aspect of thematic analysis was very important as the final coding categories\nneeded to allow for the salience within each article.\nThe final coding categories, determined from an exhaustive thematic analysis\nguided by the research questions, enabled each of the four research questions to be\nanswered in a quantitative content analysis. Only a simple \u2018yes\u2019 or \u2018no\u2019 was in response to\nthe first research question of whether a \u2018truth sandwich\u2019 was present in the article. The\nsubsequent research questions, however, were coded through a range of variables\n(Table 2) that, again, were determined through a thematic analysis of the text.\nThe four newspapers that were in included in the final study had to meet two main\ncriteria. The first criterion was that all of the four selected newspapers had to have large\ncirculations. The circulation number of newspapers were checked from several different\nwebsites (Agility PR Solutions, 2017; IPFS, 2018; Statista, 2018). Each of the newspapers\nselected had to be well established (Peiser, 2018; Pew Research Center, 2018). The second\ncriterion was that these newspapers had to be classified as \u2018liberal\u2019 or \u2018conservative\u2019 from\nat least three amalgamated websites (Kenix & Jarvandi, 2019). The determination of those\npolitical labels came from an examination of several online sources that corroborated\nassumptions in popular culture and also the researchers of this study. These assumptions\nwere then tested against this dataset. However, the criterion for including each newspaper\nwithin a political ideology required verification from at least three different online sources\nthat were not individual opinions, but amalgamations of collective perspectives or research.\nIt is important that the online sources used were amalgamations of opinion rather than\nsimply an individual source, as these combined sites offered some level of assuredness\nsimply by way of bulked information. These amalgamated websites didn\u2019t provide absolute\naffirmation independently, but there was a heightened level of approbation when there\nwas consistency across consolidated sites \u2013 as was the case in this study. It is important\nto note that the terms \u2018liberal\u2019 and \u2018conservative\u2019 have different meanings in different parts\nof the world. However, as we used a compilation of amalgamated websites regarding the\npolitical ideology of each outlet (Table 1), we had no concern of the global differences.\nThe New York Times  and The Washington Post  were selected as liberal sources and\nThe New York Post  and The Wall Street Journal  were selected as conservative sources. These\npolitical ideologies are transmuted through mediated content in the same way that they\nare communicated through verbal communication \u2013 through words, phrases, and emphases\non particular aspects of a shared narrative and the specific framing of current events. The\nlarger public associates meaning to these political identifiers symbolically (Conover &\nFeldman, 1981) and feels the \u201crelative\u201d esteem of ideological conventions communicated\nby elite media (Schiffer, 2000).\nA form of constructed week sampling was used to select dates for article collection.\nPrevious research has found that \u201cconstructed week sampling is more efficient than simple\nrandom sampling or consecutive day sampling\u201d (Hester and Dougall, 2007, p. 811). Articles\nwere collected on a successive day of each month over a twenty-one-month period (i.e. the\nfirst Monday of the first month, the first Tuesday of the second month & so on, moving on\nto the second occurrence of each weekday at the end of the first cycle). This allowed for theKenix & Manickam\n184Media Watch 12 (2)\nexamination of articles at every point in the weekday news cycle. Articles were found\nthrough the news and journalism database, Factiva, by searching for news articles that\nmentioned President Trump in the headline or lead paragraph with the subject filter of\n\u2018domestic politics.\u2019 Any editorial content was removed from the sample, yielding a total of\n528 articles to be coded.\nThe Cohen\u2019s Kappa inter-observer reliability coefficient was utilized to provide an\nindication of the coding scheme\u2019s reliability. Intercoder reliability, as measured through\nCohen\u2019s Kappa, ranged from 87.26% to 94.38% for all of the 20 coded variables. The overall\nintercoder Cohen\u2019s Kappa was 91.21%, suggesting a highly robust coding scheme.\nFindings\nThis study employed descriptive statistics to examine variables and ascertain any\nrelationships that existed between them. Percentages, frequencies, chi-square correlations\n(2), observed counts, expected counts and adjusted residual values were all used to answer\nthe research questions. No one measure itself provided strong evidence of a particular\nfinding, but when taken cohesively, there was a suggestion of associative strength.\nRQ1 : The truth sandwich was only confirmed to be present in one article (0.2%), which\nwas in The New York Post . The first research question asked, \u2018does the \u2018truth sandwich\u2019\nexist in mainstream news articles where President Trump is quoted?\u2019 Thus, there was\nlittle to no evidence of the truth sandwich in any of the 528 sampled articles.\nRQ2 : The second research question asked, \u2018in what capacity was President Trump quoted?\u2019\nA direct quote from President Trump was present in 44.1 percent of the articles where\nmention of President Trump was in the headline or lead paragraph. The most frequent\nquote of President Trump in all of the articles was when he gave a speech or an address\n(8.7%). A direct quote from his Twitter feed was the second most popular means of quotation\n(7.8%), with press conferences and interviews accounting for 5.3 percent and 5.1 percent\nof quotes, respectively. A large percentage of articles did not quote President Trump\n(55.9%) so the numbers here have to be taken with caution as their relative sample size\nwas small. There was a relatively high percentage of articles in which someone is speaking\non behalf of President Trump or his administration (32.8 percent). Almost half of the\narticles sampled contained a quote opposing President Trump (49.1 percent). This is\nhigher than the direct quotes from President Trump (44.1 percent).\nOnly seven percent of articles contained a direct new quote of a tweet by President\nTrump (a tweet published on the day of or the day before the article) and 6.6 percent of\narticles contained a direct quote from an older tweet. Thus, Twitter did not feature as a\nmajor source in the sample. In a similar vein, new tweets by President Trump were\nmentioned, without being directly quoted, in only 1.3 percent of articles, and older tweets\nwere mentioned in 5.1 percent of articles. Finally, 6.8 percent of articles made reference\nto President Trump being a \u2018twittering\u2019 president, or something similar, and in all of these\ninstances, this was portrayed as being a negative or neutral attribute. However, the\noverwhelming majority of articles in this sample did not cite President Trump\u2019s use of\ntwitter as one of his defining characteristics (93.2 percent) despite his active use of the\nplatform. Only 5.5 percent of articles contained a reference to attacks on the press and\n5.4 percent of articles mentioned fake news.\nThe relationship between a quote of an old tweet by the President and research\nof that tweet is significant (x2 = 25.432, df = 1, p = .000) and demonstrates that President\n185Trump\u2019s tweets were more likely to be challenged or researched than would be expected\nby chance alone (adjusted residual = 5.0). However, when comparing a quote of a new\ntweet by President Trump and research of that tweet, the relationship is even more\nsignificant (x2 = 160.973, df = 1, p = .000). Therefore, when a new tweet of President\nTrump\u2019s is quoted in an article, that tweet is significantly more likely to be challenged or\nresearched than a quote of an old tweet (adjusted residual = 12.7). A significant difference\nis found when considering research of both old and new tweets by President Trump in\narticles [Figures 1 and 2].\nOnly one significant relationship was found when comparing the situations that\ngave rise to President Trump being quoted and the general view of President Trump in an\narticle (x2 = 43.699, df = 14, p = .000). It was found that when President Trump was quoted\nfrom a speech or address, the view of President Trump in the article was more likely to be\nnegative than would be expected from chance alone (adjusted residual = 3.8) [Figure 4].\nRQ3 : The third research question asked \u2018how did the mainstream press cover President\nTrump (more negative or positive)? \u2018The general view of President Trump in this sample\nwas negative (41.3 %) or neutral (53.8%) and only 4.9 percent of articles were coded as\nhaving a positive view of President Trump overall. Thus, the mainstream press covered\nPresident Trump more negatively then positively in this sample.\nThe relationship between the presence of a direct quote from President Trump\nand the general view of President Trump in an article was significant (x2 = 26.606, df = 2,\np = .000). When President Trump was quoted in an article, the general view of him was\nmore negative than would be expected by chance alone (adjusted residual = 4.6).\nConversely, when President Trump was not quoted, the article was more neutral than\nwould be expected by chance alone (adjusted residual = 5.2). Neither the presence nor the\nabsence of a direct quote from President Trump led to increased positivity in coverage.\nThe relationship between the presence of a quote opposing President Trump and\nthe general view of President Trump in an article was highly significant (x2 = 81.913,\ndf = 2, p = .000). When a quote opposing President Trump was present, the article was\nsignificantly more likely to present a negative view of President Trump than would be\nexpected by chance alone (adjusted residual = 9.0) and when there was no quote opposing\nPresident Trump, the article was considerably neutral (adjusted residual = 8.4). Again,\nboth the absence and the presence of a quote opposing President Trump did not lead to\nincreased positivity in coverage.\nThe relationship between a mention of President Trump on Twitter and the general\nview of President Trump on Twitter was the most highly significant of all relationships\nexamined (x2 = 512.292, df = 2, p = .000) [Figure 3]. The very high chi-square value indicated\nthat when President Trump\u2019s use of Twitter was mentioned in an article, the view of his\nTwitter habits in the article was significantly more likely to be negative than would be\nexpected by chance alone (adjusted residual = 17.3). His Twitter habits were also more\nlikely to be noted as neutral than would be expected by chance alone (adjusted residual\n= 14.0). In this study, President Trump\u2019s use of Twitter was never positive.\nThe relationship between the mention of attacks on the press and mentions of\nfake news was significant (x2 = 70.627, df = 1, p = .000) [Figure 5]. If an article mentioned\nan attack on the press it was more likely to also mention fake news than would be\nexpected by chance alone (adjusted residual = 8.4). The relationship between the general\nview of President Trump in the article and the general view of President Trump on Twitter\nwas also significant (x2 = 17.305, df = 4, p = .002) [Figure 6]. When the general view of\nPresident Trump in a given article was negative, the view of President Trump\u2019s Twitter use\nin that article was more likely to be negative than would be expected by chance aloneKenix & Manickam\n186Media Watch 12 (2)\n(adjusted residual = 3.3). The third research question read, \u2018How did the press cover\nPresident Trump (more negative or positive) in the sample period?\u2019 Given these findings, in the\n528 sampled articles, it was found that President Trump was predominantly covered negatively.\nRQ4 :The final research question asked if there was any difference in the coverage of\nPresident Trump between conservative and liberal newspapers. A total of 63.4 percent of\ncoded articles were from The New York Times  and The Washington Post  (categorized as\nliberal newspapers). The remaining 36.6 percent of articles were from the conservative\nnewspapers, The New York Post  and The Wall Street Journal . There was not any significance\nbetween the type of newspaper and the amount of quotes from President Trump. However,\nthe general view of President Trump was found to be highly significant (x2 = 52.799, df = 2,\np = .000) depending on if the publication was liberal or conservative. If the newspaper is\nliberal, then it was more likely to have a negative general view of President Trump than\nwould be expected by chance alone (adjusted residual = 6.7). If the newspaper is\nconservative, then it was more likely to have a positive general view of President Trump\nthan would be expected by chance alone (adjusted residual = 4.0).\nAlthough there is not any significant relationship between the ideology of the\nnewspaper and the amount of quotes from President Trump, there was a significant\nrelationship between the ideology of the newspaper and the presence of quotes that\noppose President Trump (x2 = 25.023, df = 1, p = .000). If a newspaper is liberal, then it was\nmore likely to have an opposing quote to President Trump than would be expected by\nchance alone (adjusted residual = 5.0). If a newspaper is conservative, then it was far\nmore likely to not have an opposition to President Trump than would be expected by\nchance alone (adjusted residual = 5.0).\nThere was also a significant difference in coverage between liberal and\nconservative newspapers and their mention of President Trump on Twitter (x2 = 8.557,\ndf = 1, p = .003) as well as the general view of President Trump on Twitter (x2 = 10.285, df\n= 2, p = .006). Liberal newspapers were more likely to mention President Trump on Twitter\nthan would be expected by chance alone (adjusted residual = 2.9) and they were more\nlikely to hold a negative general view of President Trump on Twitter than would be expected\nby chance alone (adjusted residual = 2.6). Therefore, given these results, this research\nfound that there was a significant difference in the coverage of President Trump between\nconservative and liberal newspapers.\nDiscussion\nAs stated earlier, this study was born out of a desire to better understand the often-\ncontentious relationship between mainstream journalists and the 45th President of the\nUnited States, Donald Trump. President Trump was openly hostile towards many individuals\nand institutions within the media, seeking to undermine their authority by any means\npossible. For their part, many within mainstream news media faced a conundrum of how\nto cover a president who so frequently bended and even broke, the truth. The \u2018truth sandwich,\u2019\na linguistic construct put forth by linguist George Lakoff, was suggested repeatedly as the\nonly appropriate response for mainstream media. The construct puts facts first, ahead of\nerroneous claims, and ends with the facts restated. This is perceived by many in the\nmainstream media to be a way in which news media can minimise the influence false\nclaims. This article attempted to understand if that truth sandwich was being utilized in\nthe mainstream press and if the coverage of President Trump was generally positive or\nnegative. As data could be collected from newspapers across the political spectrum, this\n187research also considered whether coverage differed between conservative and liberal\nnewspapers in an effort to ascertain whether positivity or negativity might originate from\na specific source. Although President Trump is no longer in office, this research is vitally\nimportant for understanding how the press covers the office of the Presidency when the\nveracity of information is in question.\nOnly minimal evidence of the truth sandwich was found in this dataset. Indeed,\nonly one confirmed instance was found from a New York Post  article in March 2018. This is\nextraordinary given the consensus within both the academic and journalistic communities\nthat there needed to be a significant change to the way in which President Trump was\nreported (Zelizer 2018; Stelter 2017; Kilby 2018; Lischka 2019). The truth sandwich appeared\nto be a plausible mechanism for effecting such a change, but it wasn\u2019t employed in any\nmeaningful way by any of the news outlets whose articles were examined in this study.\nIts absence is made even more surprising by the fact that Lakoff\u2019s recommendation was\nwidely circulated and lauded by various news outlets, such as CNN (2018), NPR (Memmott,\n2018) and The Washington Post (Sullivan, 2018). All of these commendations were from\nliberal media organisations, yet the truth sandwich was only used by the New York Post , a\nconservative newspaper. News organizations have been clearly aware of Lakoff\u2019s\nrecommendation and in some instances even advocated for its use, however, this appears\nnot to have been translated into actual practice. This suggests that the press did not want\nto engage with the work of devising a \u2018truth sandwich.\u2019\nWhat was evident in the sample was a strong negativity towards President Trump\nand a lack of direct sourcing. President Trump was not quoted in the majority of articles in\nwhich he was mentioned in the headline or lead paragraph. What seems apparent is that\nmany news organizations, rather than attempting a truth sandwich, were simply omitting\nany direct sourcing of the President in their coverage. Journalists were continuing to rely\non quotes from traditional spokespeople for the President, indicating that although President\nTrump\u2019s Twitter feed is one source of information about the Presidency, it is not the only one.\nJournalists were also clearly seeking information from individuals outside of the Presidency.\nThere is copious literature on President Trump\u2019s use of Twitter and the purported\nnews institutions\u2019 excessive coverage of his Twitter feed (Wahl-Jorgenson 2018; Edwards\n2018; Lischka 2019). However, this research suggests that although President Trump\u2019s use\nof Twitter is a marker of his presidency, journalists are still relying on quotes from more\ntraditional platforms \u2013 when they use quotes at all. It must be noted that a lack of quotes\nfrom President Trump may have been because he did not make himself available to the media.\nFurther research should ascertain whether the dearth of quoting found here is because President\nTrump withheld time to the media or the press has simply chosen not to quote him.\nThe news media in this sample also almost entirely omitted any reference to\nattacks on the press or any mention of fake news. In addition to the absence of these\nattacks, which the President reiterated on a daily basis, the general view of President\nTrump in articles was overwhelmingly negative. This negativity increased when President\nTrump was actually quoted. Conversely, when President Trump was not quoted, the article\nwas more neutral. It is interesting to note that neither the presence nor the absence of a\ndirect quote from President Trump led to increased positivity in coverage. Thus, it does not\nappear that what President Trump said actually led to more positive coverage. Rather, the\nPresident was cast negatively regardless of content.  The trend of negativity also continued\nwhen there were quotes opposing President Trump, which occurred more than quotes from\nPresident Trump himself. These opposing quotes were significantly more likely to present\na negative view of President Trump. Again, both the absence and the presence of a quote\nopposing President Trump did not lead to increased positivity in coverage.Kenix & Manickam\n188Media Watch 12 (2)\nThis is concerning, both for the President, who presumably aimed for an improved image,\nand for the press, who appeared to remain steadfastly determined to present the President\nnegatively in this sample. It may be that the President \u2018deserved\u2019 this coverage through\nactions that warranted this coverage, but the monolith of coverage found here demonstrates\nthat there was little investigations contrary to this portrayal.\nPresident Trump also appeared to have a particularly critical lens on the press.\nIf an article mentioned an attack on the press it was more likely to also mention fake news.\nThis is interesting as it suggests that criticisms of the press and accusations of fake news\nare intertwined. This is obviously dangerous for a free press. If the President of the United\nStates attacks the press and also alleges that they are \u2018fake,\u2019 then it is inevitable that public\nconfidence in the press will begin to erode. This was repeatedly demonstrated through\npublic opinion polls. There is little if anything that could be done to ebb these persecutions.\nHowever, it must be noted that the press could have certainly shifted their coverage or\nutilised the \u2018truth sandwich.\u2019\nAs Schudson has said, \u201cthe biggest challenge for journalism is to assess itself and\nto find some intellectual equilibrium\u201d (Schudson, 2019b, p. 77). The finding that there were\nsome significant differences between conservative and liberal newspapers should lead\nthe newspaper industry to pause. The fact that liberal newspapers covered President Trump\nnegatively more than one would expect from chance alone is disconcerting. It certainly\nmay be that his Presidency warrants this, but there was a fundamental difference in how\nconservative newspapers were covering President Trump. It certainly may be that\nconservative newspapers were giving President Trump a \u2018pass\u2019 and the liberal newspapers\nwere not. This paper did not look at that level of content. More research that examines the\ncontent of this coverage is needed. However, this distinction does not bode well for a well-\nfunctioning and democratic media sphere. Schudson\u2019s (2019a) advice for the media to\nassess itself desperately needs to be heeded.\nIf media critics are to be celebrated for their idea of a \u2018truth sandwich\u2019 in media\ncoverage, then such a construct needs to be in the final content. That was clearly not the\ncase here. A \u2018truth sandwich\u2019 would allow for the President to be quoted and also allow for\nan accurate record to be reported. Simply not quoting the President of the United States, as\nit appears many mainstream outlets - particularly liberal newspapers - did here, does not\nsolve any concern of fake news or attacks against the media. Nor does this method properly\nrecord the Presidency of the United States. Conservative newspapers need to be reminded\nof their role in a democracy. Namely, to ensure that the public are well-informed. This was\nnot clear from the sample. The news media absolutely need to present a fair and balanced\nperspective of President Trump - all the news media. The democracy of the United States of\nAmerica depends upon it.\nLimitations\nEven though this process is mainly quantifiable and objective, a limitation of this research\nis that some interpretation had to be present. In other words, the researcher\u2019s interpretation\nof reality played a role as the lexical terms found may not necessarily be perceived the\nsame way by everyone. Much more research is needed in this area to tease out exactly how and\nwhy the press covered President Trump in the way that they did. The sample here of content was\ntoo small to make any generalizable conclusions, although it appears there is an ideological\nbent to coverage and that there was a pre-disposition in coverage of President Trump.\nFurther, interviews with reporters and editors about the rationale for not using\nthe \u2018truth sandwich\u2019 is necessary to know the utility of this journalistic construct.\n189Table 2. Coded variables and value options\nVariables Value Options\nDirect Quote from President Trump Present, Not Present\nPlatform for President Trump Twitter, Press Conference,\nSpeech/ Address, Interview,\nOther, Not Applicable, Not\nMentioned\nPresence of President Trump quote in headline or lead paragraph Present, Not Present\nGeneral view of President Trump Positive, Negative, Neutral,\nNot Applicable\nPresence of quote for President Trump Present, Not Present\nPresence of quote opposing President Trump Present, Not Present\nPresence of truth sandwich Present, Not Present\nMention of attacks on press Present, Not Present\nMention of fake news Present, Not Present\nDirect new quote of President Trump tweet Present, Not Present\nDirect old quote of President Trump tweet Present, Not Present\nMention of new quote of President Trump tweet Present, Not Present\nMention of old quote of President Trump tweet Present, Not Present\nMention of President Trump on twitter Present, Not Present\nGeneral view of President Trump on twitter Positive, Negative, Neutral,\nNot Applicable\nChallenge\u00fd/Research of tweet Present, Not Present\nArticles centrally about president Trump Yes, No\nPresident Trump is mentioned in headline or lead paragraph Present, Not Present\nSecond platform for President Trump Twitter, Press Conference,\nSpeech/Address, Interview,\nOther, Not Applicable,\nNot Mentioned\nThird platform for President Trump Twitter, Press Conference,\nSpeech/Address, Interview,\nOther, Not Applicable,\nNot MentionedTable 1. Support for American newspaper political bias\nLiberal\nWashington Post\n\u2022 Lakeland Library: Point of View  (2016)\n\u2022 The Washington Post: Ranking the media from\nliberal to conservative, based on their\naudiences (2014)\n\u2022 AllSides: Washington Post (AllSides, 2018c)\nNew York Times\n\u2022 Freakonomics: How Biased is Your Media?\n(2012)\n\u2022 Wikipedia: The New York Times Political Stance\n(2016)\n\u2022 Media Bias/Fact Check: New York Times (2018b)Conservative\nWall Street Journal\n\u2022 Freakonomics: How Biased is Your\nMedia? (2012)\n\u2022 AllSides: Wall Street Journal (2018b)\n\u2022 Media Bias/Fact Check: Wall Street\nJournal (2018c)\nNew York Post\n\u2022 Rational Wiki: The New York Post (2016)\n\u2022 AllSides: New York Post (2018a)\n\u2022 Media Bias/Fact Check: New York Post\n(2018a)There has been much argumentation about the ideological influence on stalwart political\nnewspapers (e.g. Fox News). However, the interaction of ideology on less politically driven\nnews outlets is less clear. Further research that focused on interviews with journalists and\neditors across a broader range of news outlets could tease out these ambiguities with\ncontent analyses on data that skirts the ideological edges of newspaper content.Kenix & Manickam\n190Media Watch 12 (2)\nChi-Square Tests\nValue df Asymptomatic Exact Exact\nsignificance (2-sided) Significance\n(2-sided) (1-sided)\nPearson Chi-square 160.973* 1 .000\nContinuity correction 152.036 1 .000\nLikelihood ratio 77.605 1 .000\nFisher\u2019s exact test .000 .000\nLinear-by-linear association 160.669 1 .000\nN of valid cases 528Figure 2. Direct New Quote of Trump Tweet * Challenge/Research of Tweet Crosstabulation\nChallenge/Research of Tweet\nNo Yes Total\nDirect New Quite No Count 479 1 2 491\nof Trump Tweet Expected count 461.2 29.8 491.0\nAdjusted residual 12.7 -12.7\nYes Count 1 7 2 0 3 7\nExpected count 34.8 2.2 37.0\nAdjusted residual -12.7 12.7\nTotal Count 496 3 2 528\nExpected count 496.0 32.0 528.0Chi-Square Tests\nValue df Asymptomatic Exact Exact\nSignificance (2-sided) Significance\n(2-sided) (1-sided)\nPearson Chi-Square 25.432* 1 .000\nContinuity Correction 21.869 1 .000\nLikelihood Ratio 15.631 1 .000\nFisher\u2019s Exact Test .000 .000\nLinear-by-Linear Association 25.384 1 .000\nN of Valid Cases 528Figure 1. Direct old quote of Trump tweet * Challenge/Research of tweet crosstabulation\nChallenge/research of tweet\nNo Yes Total\nDirect Old No Count 470 2 3 493\nQuite of Expected Count 463.1 29.9 493.0\nTrump Adjusted Residual 5.0 -5.0\nTweet Yes Count 2 6 9 3 5\nExpected Count 32.9 2.1 35.0\nAdjusted Residual -5.0 5.0\nTotal Count 496 3 2 528\nExpected Count 496.0 32.0 528.0\n191Figure 3. Mention of Trump on Twitter * General view of Trump on Twitter Crosstabulation\nGeneral view of Trump on twitter\nNegative Neutral Not Total\nApplicable\nMention of No Count 0 0 492 492\nTrump on Twitter Expected Count 19.6 13.0 459.4 492.0\nAdjusted Residual -17.3 -14.0 22.6\nYes Count 2 1 1 4 1 3 6\nExpected Count 1.4 1.0 33.6 36.0\nAdjusted Residual 17.3 14.0 -22.6\nTotal Count 2 1 1 4 493 528\nExpected Count 21.0 14.0 493.0 528.0\nChi-square Tests\nValue df Asymptomatic\nsignificance\n(2-sided)\nPearson Chi-Square 512.292* 2 .000\nLikelihood Ratio 248.450 2 .000\nLinear-by-Linear Association 492.592 1 .000\nN of Valid Cases 528\nFigure 4. Platform for Trump * General View of Trump Crosstabulation\nGeneral View of Trump\nPositive Negative Neutral Total\nPlatform Twitter Count 1 2 0 2 0 4 1\nfor Trump Expected Count 2.0 16.9 22.1 41.0\nAdjusted Residual -.8 -1.0 -.7\nPress Conference Count 2 1 3 1 3 2 8\nExpected Count 1.4 11.6 15.1 28.0\nAdjusted Residual .6 .6 -.8\nSpeech/Address Count 4 3 1 1 1 4 6\nExpected Count 2.3 19.0 24.7 46.0\nAdjusted Residual 1.2 3.8 -4.3\nInterview Count 0 1 7 1 0 2 7\nExpected Count 1.3 11.1 14.5 27.0\nAdjusted Residual -1.2 2.3 -1.8\nOther Count 4 1 4 1 2 3 0\nExpected Count 1.5 12.4 16.1 30.0\nAdjusted Residual 2.2 .6 -1.6\nNot Applicable Count 1 1 9 6 188 295\nExpected Count 14.5 121.8 158.7 295.0\nAdjusted Residual -1.4 -4.6 5.2\nNot Mentioned Count 4 2 6 3 0 6 0\nExpected Count 3.0 24.8 32.3 60.0\nAdjusted Residual .7 .3 -.6\nNot Sure Count 0 1 0 1\nExpected Count .0 .4 .5 1.0\nAdjusted Residual -.2 1.2 -1.1\nTotal Count 2 6 218 284 528\nExpected Count 26.0 218.0 284.0 528.0\nChi-Square Tests\nValue df Asymptomatic\nSignificance\n(2-sided)\nPearson Chi-Square 43.699a1 4 .000\nLikelihood Ratio 44.609 1 4 .000\nLinear-by-Linear Association 8.731 1 .003\nN of Valid Cases 528\na. 9 cells (37.5%) have expected count less than 5. The minimum expected count is .05.Kenix & Manickam\n192Media Watch 12 (2)\nChi-Square Tests\nValue df Asymptomatic Exact Exact\nSignificance (2-sided) Significance\n(2-sided) (1-sided)\nPearson Chi-Square 70.627a1 .000\nContinuity Correction 62.822 1 .000\nLikelihood Ratio 32.368 1 .000\nFisher\u2019s Exact Test .000 .000\nLinear-by-Linear Association 70.494 1 .000\nN of Valid Cases 528Figure 5. Mention of attacks on press * Mention of fake news crosstabulation\nMention of fake news\nNo Yes Total\nMention of No Count 487 1 2 499\nAttacks on press Expected Count 478.2 20.8 499.0\nAdjusted Residual 8.4 -8.4\nYes Count 1 9 1 0 2 9\nExpected Count 27.8 1.2 29.0\nAdjusted Residual -8.4 8.4\nTotal Count 506 2 2 528\nExpected Count 506.0 22.0 528.0\nFigure 6. General View of Trump * General View of Trump on Twitter Crosstabulation\n                                       General view of Trump on Twitter\nNegative Neutral Not Total\napplicable\nGeneral view of Trump Positive Count 0 0 2 6 2 6\nExpected count 1.0 .7 24.3 26.0\nAdjusted residual -1.1 -.9 1.4\nNegative Count 1 6 1 0 192 218\nExpected count 8.7 5.8 203.5 218.0\nAdjusted residual 3.3 2.3 -4.1\nNeutral Count 5 4 275 284\nExpected count 11.3 7.5 265.2 284.0\nAdjusted residual -2.8 -1.9 3.4\nTotal Count 2 1 1 4 493 528\nExpected count 21.0 14.0 493.0 528.0\nChi-Square Tests\nValue df Asymptomatic\nsignificance\n(2-sided)\nPearson Chi-Square 17.305a4 .002\nLikelihood ratio 18.504 4 .001\nLinear-by-linear association 5.757 1 .016\nN of Valid cases 528\nConflict of interest : The authors declare no potential conflict of interest with respect to the\nresearch, authorship, and/or publication of this article.\nFunding : This research was funded by the College of Arts, University of Canterbury,\nNew Zealand.\n193References\nAgility PR Solutions. (2017). Top 15 U.S. Newspapers by Circulation. Retrieved from https://\nw w w. a g i l i t y p r. c o m / r es o u r c es / t o p - m e d i a - o u t l e t s / t o p - 1 5 - d a i l y - a m e r i c a n -\nnewspa pers/\nAllen, H., & Savigny, H. (2012). Selling scandal or ideology? The politics of business crime\ncoverage. European Journal of Communication, 27 (3), 278-290.\nAllSides. (2018a). New York Post. Retrieved from https://www.allsides.com/news-source/\nnew-york-post\nAllSides. (2018b). Wall Street Journal - News. Retrieved from https://www.allsides.com/\nnews-source/wall-street-journal-media-bias\nAllSides. (2018c). Washington Post - News. Retrieved from https://www.allsides.com/news-\nsource/wa shington-post-media-bias\nAltheide, D. (1976). Creating reality: How TV distorts events . Beverly Hills: Sage.\nBeam, R. A., Weaver, D., & Brownlee, B. J. (2009). Changes in professionalism of U.S. journalists\nin the turbulent twenty-first century. Journalism & Mass Communication Quarterly, 86 (2),\n277-298. doi: 10.1177 /107769900908600202\nBerkowitz, D. (2000). Doing double duty: Paradigm repair and the Princess Diana What-A-\nStory. Journalism, 1 (2), 125-143. doi: 10.1177 /146488490000100203\nBlake, A. (2014). Ranking the media from liberal to conservative, based on their audiences.\nRetrieved from https://www.washingtonpost.com/news/the-fix/wp/2014/10/21/lets-\nrank-the-media-from-liberal-to-conservative-based-on-their-audiences/\nBraun, V., & Clarke, V. (2006). Using thematic analysis in Psychology. Qualitative Research in\nPsychology, 32 (2), 77-101.\nChalaby, J. K. (1996). Journalism as an Anglo-American invention European Journal of\nCommunication, 11 (3), 303-326.\nCision. (2014). Top 10 US Daily Newspapers. Retrieved from http://www.cision.com/us/2014/\n06/top-10-us-daily-newspapers/\nCNN. (2018). How to make a \u2018truth sandwich\u2019. CNN . Retrieved from https://edition.cnn.com/\nvideos/tv/2018/06/17 /how-to-make-a-truth-sandwich-rs.cnn/video/playlists/reliable-\nsources-highlights/\nConover, P., & Feldman, S. (1981). the origins and meanings of liberal/conservative self-\nidentification. American Journal of Political Science, 25 , 617-645.\nDubner, S. (2012). How Biased is Your Media? Retrieved from http://freakonomics.com/\npodcast/how-biased-is-your-media/\nEdwards, B. T. (2018). President Trump from reality TV to Twitter, or the selfie-determination\nof nations. Arizona Quarterly: A Journal of American Literature, Culture, and Theory, 74 (3), 25-45.\nEveland, W. P., & Shah, D. (2003). The impact of individual and interpersonal factors on\nperceived news media bias. Political Psychology, 24 (1), 101-117.\nGamson, W. A. (1992). Talking politics . New York: Cambridge University Press.\nGitlin, T. (1980). The whole world is watching: Mass media in the making and unmaking of the new\nleft.  Berkley: University of California Press.\nHeider, D., McCombs, M., & Pointdexter, P. M. (2005). What the public expects of local news:\nViews on public and traditional journalism. Journalism & Mass Communication Quarterly,\n82(4), 952-967. doi: 10.1177 /107769900508200412\nHertog, J., & McLeod, D. (1995). Anarchists wreak havoc in downtown Minneapolis: A multi-\nlevel study of media coverage of radical protest. Journalism Monographs, 151 (June), 1-48.Kenix & Manickam\n194Media Watch 12 (2)\nHester, J B. and Dougall, E. (2007). The efficiency of constructed week sampling for\nconstructed week sampling for content analysis of online news.  Journalism & Mass\nCommunication Quarterly, 84 (4), 811-824.\nIlling, S (2018). How the media should respond to Trump\u2019s lies: State of the Union edition.\nRetrieved from https://www.vox.com/2018/11/15/18047360/trump-state-of-the-union-\nspeech-2019-george-lakoff\nIPFS. (2018). List of newspapers in Australia by circulation. Retrieved from https://ipfs.io/\ni p f s / Q m X o y p i z j W 3 W k n F i J n K L w H C n L 7 2 v e d x j Q k D D P 1 m X Wo 6 u c o / w i k i /\nList_of_newspapers_in_Australia_by_circulation.html\nJones, J., & Himelboim, I. (2010). Just a guy in pajamas? Framing the blogs in mainstream\nUS newspaper coverage (1999-2005). new media & society, 12 (2), 271-288. doi: 10.1177 /\n1461444809342524\nKenix, L. J., & Jarvandi, R. (2019). The Role of Ideology in the International Mainstream News\nMedia Framing of Refugees: A Comparison between Conservative and Liberal\nNewspapers in the United States, United Kingdom, and Australia. . Journal of Applied\nJournalism and Media Studies, 8.3 (November), In Press.\nKilby, A. (2018). Provoking the Citizen: Re-examining the role of TV satire in the President\nTrump era. Journalism Studies, 19 (14), 1934-1944.\nKohut, A. (2004, 11 July 2004). Media myopia: More news is not necessarily good news,  New\nYork Times .\nLakeland Libary Research Guides. (2016). Point of View. Retrieved from http://\nlibrary.lakelandcc.edu/PDFs/research/bias.pdf\nLischka, J. (2019). A badge of honour? Journalism Studies, 20 (2), 287-304.\nMcDevitt, M., & Ferrucci, P. (2018). Populism, journalism, and the limits of reflexivity: the\ncase of Donald J. President Trump. Journalism Studies, 19 (4), 512-526.\nMcVittie , C., & McKinlay, A. (2018). \u2019 Alternative facts are not facts\u2019: gaffe announcements,\nthe President Trump administration and the media. Discourse & Society , 1-16.\nMedia Bias/Fact Check. (2018a). New York Post. Retrieved from https://\nmediabiasfactcheck.com/new-york-post/\nMedia Bias/Fact Check. (2018b). New York Times. Retrieved from https://\nmediabiasfactcheck.com/new-york-times/\nMedia Bias/Fact Check. (2018c). Wall Street Journal. Retrieved from https://\nmediabiasfactcheck.com/wall-street-journal/\nMemmott, M. (2015). When There\u2019s No Evidence To Support A Claim, We Should Say That.\nRetrieved from https://www.npr.org/sections/memmos/2015/11/25/605696776/when-\nthere-s-no-evidence-to-support-a-claim-we-should-say-that\nMemmott, M. (2018). Let\u2019s put \u201ctruth sandwiches\u2019 on our menu. Retrieved from https://\nwww.npr.org/sections/memmos/2018/06/20/621753252/lets-put-truth-sandwiches-on-\nour-menu\nMessina, S. R. (2018). Airing live risks error: responsible journalism in the President Trump\nera. Journal of Media Ethics, 33 (2), 94-94.\nMour\u00e3o, R. R., Thorson, E., Chen, W., & Tham, S. M. (2018). Media repertoires and news trust\nduring the early President Trump administration. Journalism Studies, 19 (13), 1945-\n1956.\nParks, P.  (2020) The Ultimate News Value: Journalism Textbooks, the U.S. Presidency, and\nthe Normalization of Donald Trump, Journalism Studies, 21:4, 512-529, DOI: 10.1080/\n1461670X.2019.1686413\n195Peiser, J. (2018). New York Times Co. reports revenue growth as digital subscriptions rise.\nNew York Times . Retrieved from https://www.nytimes.com/2018/ 05/ 03/business/media/\nnew-york-times-earnings.html\nPew Research Center, I. (2018). Newspapers fact sheet. Retrieved from https://\nwww.journalism.org/fact-sheet/newspapers/\nPingree, R., Watson, B., Sui, M., Searles, K., Kalmoe, N. (2018). Checking facts and fighting\nback: Why journalists should defend their profession. PLoS One, 13(12), e0208600.\nhttps://doi.org/10.1371/journal.pone.0208600\nRational Wiki. (2016). The New York Post. Retrieved from http://rationalwiki.org/wiki/\nNew_York_Post\nReese, S. D., Gandy Jr., O. H., & Grant, A. E. (2001). Framing Public Life . Mahwah: Lawrence\nErlbaum.\nRosen, J. (2018). It\u2019s time for the press to suspend normal relations with the President\nTrump presidency. PressThink . Retrieved from http://pressthink.org/2018/06/its-time-\nfor-the-press-to-suspend-normal-relations-with-the-President Trump-presidency/\nRubin, J. (2020). The media should remember key lessons from the Trump era. Washington\nPost. Retrieved from:  https://global-factiva-com.ezproxy.canterbury.ac.nz/ga/\ndefault.a spx\nRyan, C., Carragee, K. M., & Schwerner, C. (1998). Media, movements, and the quest for social\njustice. Journal of Applied Communication Research, 26 , 165-181.\nSartwell, C. (2018)  \u2018Truth Sandwich\u2019? Baloney! Wall Street Journal . Retrieved from https://\nwww.wsj.com/articles/truth-sandwich-baloney-1533496472\nSchiffer, A. (2000). I\u2019m not THAT liberal: Explaining conservative democratic identification.\nPolitical Behavior, 22 (4), 293-310.\nSchudson, M. (2001). The objectivity norm in American journalism. Journalism, 2 (2), 149-170.\nSchudson, M. (2019a). Where we are and whither we are tending. Journalism, 20 (1), 77-79.\ndoi: 10.1177 /1464884918809247\nSchudson, M. (2019b). Where we are and wither we are tending. Journalism, 20 (1), 77-79.\nSotirovic, M. (2000). Effects of media use on audience framing and support for welfare. Mass\nCommunication and Society, 3 , 269-297.\nStatista. (2018). Circulation of newspapers in the United Kingdom (UK) in 2017 (in 1,000\ncopies). Retrieved from https://www.statista.com/statistics/529060/uk-newspaper-\nmarket-by-circulation/\nStreet, J. (Ed.). (2001). Mass media, politics and democracy . Basingstoke: Palgrave.\nSullivan, M. (2018). Instead of President Trump\u2019s propaganda, how about a nice \u2018truth\nsandwich. Washington Post . Retrieved from https://www.washingtonpost.com/\nlifestyle/style/instead-of-President Trumps-propaganda-how-about-a-nice-truth-\ns a n d w i c h / 2 0 1 8 / 0 6 / 1 5 / 8 0 d f 8 c 3 6 - 7 0 a f - 1 1 e 8 - b f 8 6 -\na2351b5ece99_story.html?noredirect=on&utm_term=.cc75e7cc50d0\nTankard Jr., J. W., Hendrickson, L., Silberman, J., Bliss, K., & Ghanem, S. (1991). Media Frames:\nApproaches to Conceptualization and Measurement.  Paper presented at the Association\nfor Education in Journalism and Mass Communication, Boston.\nThompson, J. B. (1990). Ideology and modern culture: Critical social theory in the era of mass\ncommunication . Cambridge: Polity Press.\nTuchman, G. (1978). Making news: A study in the construction of reality . New York: Free Press.\nWahl-Jorgensen, K. (2018). Media coverage of shifting emotional regimes: Donald President\nTrump\u2019s angry populism. Media, Culture & Society, 40 (5), 766-778.\nWikipedia. (2016). The New York Times Political Stance. Retrieved from https://\nen.wikipedia.org/wiki/The_New_York_Times#Political_stanceKenix & Manickam\n196Media Watch 12 (2)\nXu, E. (2020) A generalisable model for frame identification: towards an integrative\napproach,\u00a0 Communication Research and Practice, \u00a06:3,\u00a0245-258,\u00a0DOI:\u00a010.1080/\n22041451.2020.1759925\nZelizer, B. (1993). Has communication explained journalism? Journal of Communication, 43 (4),\n80-88. doi: 10.1111/j.1460-2466.1993.tb01307.x\nZelizer, B. (2018). Resetting journalism in the aftermath of Brexit and President Trump.\nEuropean Journal of Communication, 33 (2), 140-156.\nLinda Jean Kenix  is Head of the School of Language, Social, and Political Sciences at\nUniversity of Canterbury, New Zealand . Prof. Kenix is interested in the visual and textual\nmedia representation of marginalized groups, the reasons for, and the consequences of\nthat representation. She has been a visiting research fellow at the Oxford University,\nUniversity of Cambridge, Monash University and the University of Valencia.\nJovita Manickam  is a Masters graduate from the University of Auckland in the Media, Film\nand Television Studies Programme. She is presently a Volunteer and Event Coordinator for\nBreast Cancer Foundation New Zealand.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "A missed opportunity? President Trump, the truth sandwich, and news coverage across an ideological spectrum", "author": ["LJ Kenix", "J Manickam"], "pub_year": "2021", "venue": "Media Watch", "abstract": "Many within mainstream news media wondered aloud and in print how to cover President  Trump who, they purported, frequently distorted the truth. Although President Trump is no"}, "filled": false, "gsrank": 621, "pub_url": "https://journals.sagepub.com/doi/abs/10.15655/mw_2021_v12i2_160145", "author_id": ["bZbE-mUAAAAJ", ""], "url_scholarbib": "/scholar?hl=en&q=info:9i4xhhw7f6IJ:scholar.google.com/&output=cite&scirp=620&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D620%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=9i4xhhw7f6IJ&ei=d7WsaKCIGLXCieoP4PfQ0A8&json=", "num_citations": 6, "citedby_url": "/scholar?cites=11709142549883072246&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:9i4xhhw7f6IJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://www.mediawatchjournal.in/wp-content/uploads/2021/05/1.-A-Missed-Opportunity.pdf"}}, {"title": "Advice-Driven Learning: A Blocks-World and Fake News Detection Approach", "year": "2020", "pdf_data": "ADVICE-DRIVEN LEARNING: A BLOCKS-WORLD AND FAKE NEWS\nDETECTION APPROACH\nA Thesis\nSubmitted to the Faculty\nof\nPurdue University\nby\nNikhil Mehta\nIn Partial Ful\fllment of the\nRequirements for the Degree\nof\nMaster of Science\nDecember 2020\nPurdue University\nWest Lafayette, Indiana\nii\nTHE PURDUE UNIVERSITY GRADUATE SCHOOL\nSTATEMENT OF THESIS APPROVAL\nDr. Dan Goldwasser, Chair\nDepartment of Computer Science\nDr. Jennifer Neville\nDepartment of Computer Science\nDr. Clifton W. Bingham\nDepartment of Computer Science\nApproved by:\nDr. Kihong Park\nHead of the School Graduate Program\niii\nThis is dedicated to the most important people in my life, my family. Anil Mehta,\nArchana Mehta, Sonal Mehta, and Ambika Mehta.\niv\nACKNOWLEDGMENTS\nI would like to start by expressing deep gratitude to my advisor, Professor Dan\nGoldwasser, as without him anything presented in this work would be not be possible.\nHe was the one who \frst introduced me to the \feld of Natural Language Processing,\ngiving me an opportunity as an undergraduate to take his graduate course, despite\nit being full. He then further gave me a chance to work/research under him, which\nI continued throughout graduate school. Throughout my time working with Prof.\nGoldwasser, he has always been available and providing me with thoughtful/helpful\nideas on whatever questions I had, while allowing me the freedom to tackle the prob-\nlems I was interested in. He has been a constant support guiding me through the\nchallenges of my graduate and later part of my undergraduate career, and for that I\nwill never be able to thank him enough.\nI would also like to thank Professor Christopher Clifton and Professor Jennifer\nNeville, for agreeing to be on my thesis committee and providing me feedback to\nmake this work even stronger.\nI am also thankful to my family for constantly supporting me since I stepped foot\non Purdue's campus and before. Finally, I am thankful for my friends and the people\nI have met throughout my college career, speci\fcally Joe Chastain, Joshua Leeman,\nNaman Patwari, and Arbazz Mukadam for making my time at Purdue a lot more\nenjoyable.\nv\nTABLE OF CONTENTS\nPage\nLIST OF TABLES : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : viii\nLIST OF FIGURES : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : ix\nABBREVIATIONS : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : x\nABSTRACT : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : xi\n1 INTRODUCTION : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 1\n1.1 Advice vs. Supervised Learning : : : : : : : : : : : : : : : : : : : : : : 4\n1.2 Advice vs. Reinforcement Learning : : : : : : : : : : : : : : : : : : : : 5\n1.3 Advice vs. Active Learning : : : : : : : : : : : : : : : : : : : : : : : : : 5\n1.4 Next Chapter : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6\n2 BLOCKS WORLD : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 7\n2.1 Introduction : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 7\n2.2 Related Work : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 8\n2.3 Dataset Details : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 9\n2.4 Advice for Blocks World : : : : : : : : : : : : : : : : : : : : : : : : : : 10\n2.5 Models : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 12\n2.5.1 Baseline Model : : : : : : : : : : : : : : : : : : : : : : : : : : : 12\n2.5.2 Advice Grounding : : : : : : : : : : : : : : : : : : : : : : : : : 12\n2.5.3 End-to-End Training : : : : : : : : : : : : : : : : : : : : : : : : 14\n2.5.4 Advice Generation : : : : : : : : : : : : : : : : : : : : : : : : : 15\n2.6 Experiments : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 16\n2.6.1 Restrictive Advice : : : : : : : : : : : : : : : : : : : : : : : : : 17\n2.6.2 Corrective Advice : : : : : : : : : : : : : : : : : : : : : : : : : : 18\n2.6.3 Retry Advice : : : : : : : : : : : : : : : : : : : : : : : : : : : : 18\n2.6.4 Model Self-Advice Generation : : : : : : : : : : : : : : : : : : : 19\nvi\nPage\n2.7 Conclusion : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 21\n2.7.1 Future Work : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 21\n2.7.2 Summary : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 22\n3 FAKE NEWS DETECTION : : : : : : : : : : : : : : : : : : : : : : : : : : : 24\n3.1 Introduction : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 24\n3.2 Related Work : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 25\n3.3 Background Knowledge : : : : : : : : : : : : : : : : : : : : : : : : : : : 25\n3.3.1 BERT : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 26\n3.4 Dataset : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 27\n3.4.1 Overview : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 27\n3.4.2 Article Scraping : : : : : : : : : : : : : : : : : : : : : : : : : : : 28\n3.5 Models : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 29\n3.5.1 Graph : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 30\n3.5.2 Trust Propagation : : : : : : : : : : : : : : : : : : : : : : : : : 32\n3.5.3 BERT Fine-tuning : : : : : : : : : : : : : : : : : : : : : : : : : 33\n3.5.4 Graph Updates : : : : : : : : : : : : : : : : : : : : : : : : : : : 34\n3.5.5 Prediction : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 34\n3.5.6 Trust Propagation after Prediction : : : : : : : : : : : : : : : : 35\n3.6 Advice : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 35\n3.6.1 Types of Advice : : : : : : : : : : : : : : : : : : : : : : : : : : : 35\n3.6.2 Self-Generated Advice : : : : : : : : : : : : : : : : : : : : : : : 37\n3.7 Experiments : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 38\n3.7.1 Baseline Graph : : : : : : : : : : : : : : : : : : : : : : : : : : : 38\n3.8 Conclusion : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 38\n4 SUMMARY : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 40\n4.1 Future Work : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 41\n4.1.1 Medium-Term : : : : : : : : : : : : : : : : : : : : : : : : : : : : 41\n4.1.2 Long-Term : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 41\nvii\nREFERENCES : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 43\nviii\nLIST OF TABLES\nTable Page\n2.1 In the \frst row, given the target coordinate, the restrictive advice is pro-\nvided to place the target in the appropriate region. In the second row,\ngiven the predicted coordinate and the target coordinate, the correct cor-\nrective advice is to move down to get closer to the target. : : : : : : : : : : 11\n2.2 Results for our models compared to previous models evaluated as distance\nfrom gold prediction normalized by block length for source and target\ncoordinate prediction. : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 17\n2.3 Accuracy of model self-generated advice. : : : : : : : : : : : : : : : : : : : 19\n3.1 Distribution of factuality labels in the dataset [8]. : : : : : : : : : : : : : : 27\nix\nLIST OF FIGURES\nFigure Page\n2.1 Based on the instruction (upper sentence) the model predicts the coordi-\nnates of the block and its target location. The `x' represents an incorrect\nprediction, corrected by the provided advice (lower sentence). : : : : : : : 11\n2.2 Baseline model proposed by [23]. All our advice models are built on top\nof this.: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 12\n2.3 (a) Pre-Trained Advice Understanding Model. (b) [23] End-to-End ar-\nchitecture with our pre-trained model. World represents the board state,\nwhile o\u000bset and reference represent fully connected layers used to identify\nthe o\u000bset and reference blocks. : : : : : : : : : : : : : : : : : : : : : : : : 14\n2.4 Model for Generating Advice. : : : : : : : : : : : : : : : : : : : : : : : : : 16\n2.5 (a) The [23] model would have made a prediction (`x') close to the true\nblock (square). However, the advice region (blue) was incorrect (due to\nthe true block being close to the edge of it) and this led to a signi\fcantly\nworse prediction (circle).\n(b) In the input-speci\fc self-generated advice model, the advice region\n(blue) is centered at the incorrect coordinate prediction (`x'), leading to\nthe true source block being included and a correct prediction (circle). : : : 20\n3.1 In our graph model, we have unknown sources (pink), known sources (pur-\nple), entities (green), and articles (teal). We connect each unknown source\nto its entities and each known source to its entities. Each entity is also\nlinked to the articles about that entity. Each article in the unknown source\nis linked to its' similar articles in the known source (found by Google News\nsearch). For space, in this \fgure we show the similar articles for only one\nknown unknown source article. The edge weight for the pairs of articles is\ndetermined by the BERT [35] similarity score. The edge weight for each\narticle is the average of the edge weight for all the it's similar articles\nscores (0.25 in the graph). The edge weight for the entities is the average\nof the edge weight for all the articles, and the edge weight for the source\nis the average of all entities. This graph model allows trust to propagate\nfrom the known sources to the unknown sources. : : : : : : : : : : : : : : : 30\nx\nABBREVIATIONS\nBERT Bidirectional Encoder Representations from Transformers\nFC Fully Connected Layer\nNLP Natural Language Processing\nSOTA State of the Art\nxi\nABSTRACT\nMehta, Nikhil M.S., Purdue University, December 2020. Advice-Driven Learning: A\nBlocks-World and Fake News Detection Approach. Major Professor: Dan Gold-\nwasser.\nOver the last few years, there has been growing interest in learning models for\nvarious Natural Language Processing tasks, such as the popular blocks world domain\nand fake news detection. These works typically view these problems as single-step\nprocesses, in which a human operator gives an instruction and an automated agent\nis evaluated on its ability to execute it. In this work, we take the \frst step towards\nincreasing the bandwidth of this interaction, and suggest a protocol for including\nadvice, high-level observations about the task, which can help constrain the agents\nprediction. Advice is designed to be a short natural language sentence, provided by\nthe human operator in addition the original input for the task, that can help the\nagent improve its performance. We evaluate our advice-based approach on the blocks\nworld task and fake-news detection, and show that even simple advice can help lead to\nsigni\fcant performance improvements. To help reduce the e\u000bort involved in supplying\nthe advice, we also explore model self-generated advice which can still improve results.\n1\n1. INTRODUCTION\nOver the last few years, especially with the rise of big data, machine learning has\nbecome increasingly popular. One of the most common forms of machine learning\nis supervised learning, where a model learns how to perform a task on some input\ndata, getting supervision via labels. The goal of the model is generalization, or the\nability to perform well on unseen data. Despite it's wide use, supervised learning has\nsome issues. First, the model receives feedback only via labels, which are extremely\nchallenging to obtain. Humans have spent large amounts of money to employ other\nhumans to create large scale datasets [1, 2], but even still, there are many problems\nthat existing datasets don't address. Thus, even though models achieve good perfor-\nmance on these datasets, it doesn't mean they actually solve the task well [3], which\nmeans more money must be spent to collect a more representative dataset (which\ncould end up still being exploited by the model in some way). Second, in supervised\nlearning, once a model is trained and deployed in the real world, it is not easy to\nupdate it. Updates are typically made by receiving/collecting more data and then\n\fne-tuning the model, but this process is not trivial. Third, learning from labels\nonly is not how humans learn. Humans learn knowledge from a wide variety of ways,\nincluding interaction with other humans, which raises the question of why we train\nmachines to learn from just labels.\nInteractive learning has the potential to \fx some of these issues. In interactive\nlearning, an AI system can learn from the human through a back and forth interaction.\nThis means that the model can receive supervision even after it is trained, as humans\ncan constantly teach it to do better. Even more, since humans are involved in the\ntraining process, humans can identify problem areas for the model and attempt to\n\fx them, through various forms of input feedback, such as labels. This can help the\n2\nmodel in multiple ways, such as increasing the prediction accuracy or reducing the\ntraining time.\nOne form interactive learning is dialogue systems. Over the last few decades,\nthere has been a lot of work in Natural Language Processing to build interactive\ndialog systems to accomplish various tasks. [4] proposed the TRAINS project in the\n1990s, which was a long term e\u000bort to build a system that can use conversation\nto interact with and assist humans to solve various tasks. [5] proposed a system to\ntrain a robot interactively by teaching it various tasks while it observes a human\nperform them. More recently, [6] focuses on building a model that can accomplish\ntasks while conversing naturally with humans. Dialogue systems like these have a\ncommon theme, which is that humans provide input through various means (typing,\nspeech, a graphical user interface, etc) and the AI system translates it to text using\nan input recognizer/decoder. Then, the system must understand the text, optionally\nexecute some task, and \fnally produce some output. This process can then repeat.\nDespite all of this work in interactive dialog systems (and interactive learning),\nthere are many Natural Language Processing tasks that are still considered as single-\nstep processes, and not viewed interactively. For example, in the popular blocks world\ntask proposed by [7], a human must provide an instruction to an AI agent to move a\nblock on a grid from a source location to a target destination. In the way this task\nis traditionally solved, the agent receives the instruction, understands it, executes it,\nand makes a prediction. There is currently not an easy way for the human to interact\nthe agent to provide it some useful information that it can take advantage of to make\na better prediction.\nSimilarly, fake news detection is a task that has become very popular recently, due\nto the rise of social media. Due to this, anyone can post about anything, and false\ninformation (that is often more interesting/controversial) can be quickly spread [8].\nWhen people aim to build systems to detect fake news, they focus on designing models\nto predict whether an article/source is fake or not. However, there currently isn't an\neasy way for a human to provide knowledge they have about an article or source that\n3\nthe model can utilize to improve its performance. Thus, even this task is typically\nnot interactive.\nDespite both of these tasks being vastly di\u000berent for various reasons such as time\nproposed, domain, data used, etc., we aim to better solve them in a uniform way.\nBoth of these tasks are currently being-solved in a single-step way, where the model\nreceives some input and makes a prediction, where we aim to solve them interactively .\nBoth tasks can also be decomposed into several problems, such as determining the\nvalidity of individual statements in a potentially fake news article. We take advantage\nof this decomposability in our work.\nIn this work, our goal is to explore di\u000berent approaches for relaxing the typical\nsingle-step nature of many NLP tasks by introducing advice-driven learning (learning\nby receiving advice ). We aim to view NLP tasks as more of an interactive process, in\nwhich the human operator can observe the agents' predictions on a task and adjust\nit by providing advice , a form of online feedback. Speci\fcally, the advice consists of a\nshort natural language sentence, one that the agent can easily understand, and take\nadvantage of to solve the original task better.\nAdvice is typically provided by a human that has some knowledge of the original\ntask that the agent does not. It is an easy way for the agent to incorporate human\nfeedback or human knowledge. For example, in the case of the blocks world-task, if\nthe human knows the general region (lower left quadrant) of where the block must\nbe moved, it can provide that information as advice to the system. Fig. 2.1 shows an\nexample of this and uses the advice \\the target is in the lower left\" , to restrict the\nagents search space after observing the incorrect prediction placed the target block\nin the top half of the board.\nThe advice text is easier to understand than the original task, so that the agent\ncan take advantage of it. Advice allows a slight decomposition of the task, typically\nrestricting the solution space in some way (for example in the above blocks-world\ntask, the advice is restricting the search space of where the block must be placed to\nthe lower left). However, advice is vague enough that it only provides provides partial\n4\ninformation, since understanding the advice doesn't necessarily mean the system will\nsolve the \fnal task. Thus, the system must be built to best take advantage and learn\nfrom the advice so it can handle instances when advice is not provided.\nAdvice can also be self-generated by a model, in which case no human interaction\nis necessary. In this case, the AI system would generate and then provide itself the\nadvice. This requires building an architecture to self-generate the advice, which we\nwill discuss in later Chapters. Self-generated advice still has the properties of normal\nadvice, the only di\u000berence is that it is generated by a model, not a human.\nIn the following sections, we will compare advice-driven learning (learning by\nreceiving advice) to some traditional forms of learning, namely supervised learning,\nreinforcement learning, and active learning. Throughout the rest of this document,\nwe will show how taking advantage of advice can allow us to better solve di\u000berent\nNLP tasks, speci\fcally on the blocks-world task and fake-news detection.\n1.1 Advice vs. Supervised Learning\nIn Supervised Learning, a model is provided with a bunch of training examples\nand their labels. The model must make a prediction on the training examples and\nreceives feedback from the training labels. The goal of the model is to generalize well,\nor perform well on unseen test examples.\nAdvice-driven learning is di\u000berent from supervised learning because advice is not\nthe \fnal label. Rather, the advice text is a partial solution/hint about the task that\nenables the system to better get to the \fnal label. Typically, advice restricts the\nsolution space for the task in some way, but does not provide the answer, like is\ndone in supervised learning. However, advice can be thought as providing some sort\nof supervision for the task. Advice can also be provided before the \fnal prediction\nis made, to help the system. Due to these reasons, advice-driven learning can be\ncombined with supervised learning.\n5\n1.2 Advice vs. Reinforcement Learning\nIn Reinforcement Learning, a model aims to take a series of actions in order to\nmaximize a reward. The model receives the reward as feedback after executing the\nactions, and can take advantage of that to learn and achieve a better reward in the\nfuture.\nSimilar to the case when compared with supervised learning, advice is not the\nreward at the \fnal step, but rather a hint on how to get to the \fnal step better,\ntypically from a human that has knowledge about the process. In this way, advice\ncan also be combined with reinforcement learning.\n1.3 Advice vs. Active Learning\nActive Learning is a type of machine learning in which the model/AI agent can\nquery/ask questions to the human to obtain knowledge. Active learning is also usually\nan interactive process, as discussed in [9], where they propose a model that can learn\nby repeatedly asking questions to a human operator.\nAlthough advice text can be provided to a system that utilizes active learning\nwhen it asks for help, advice-driven learning is di\u000berent from active learning. In\nadvice-driven learning, the human observes the predictions of the model and decides\nwhat knowledge to provide the agent. This is di\u000berent from active learning where\nthe model is explicitly asking for help, usually throughout the learning process. The\nbene\fts of advice-driven learning is that the human can choose whatever information\nit wants to provide which is easier than in active learning, where the human must\nanswer the speci\fc question. The information can also be provided after the prediction\nis complete, and the model can still take advantage of it. In this way, advice-driven\nlearning can be considered a complement to active-learning, as both can be done\ntogether without providing the \fnal label to the system, and performance could still\nimprove.\n6\n1.4 Next Chapter\nIn this chapter, we have introduced advice-driven learning, which is a way for a\nhuman to easily provide feedback to an AI system. This protocol utilizes advice, a\nshort simple sentence provided by the human that the system can easily understand,\nand take advantage of to solve the original task better. Advice allows solving NLP\ntasks in a more interactive way that is also easy for humans, as they only need\nto provide information that they already have. Throughout the rest of this work,\nwe will show how taking advantage of advice when solving the blocks-world task\nand fake news detection helps performance on those tasks. We hope to show the\ngeneralizability of advice-driven learning, by showing the various ways in which it\ncan be applied to these popular traditionally single-step NLP tasks.\n7\n2. BLOCKS WORLD\n2.1 Introduction\nThe problem of constructing an arti\fcial agent capable of understanding and\nexecuting human instructions is one of the oldest long-standing AI challenges [7]. This\nproblem has numerous applications in various domains such as planning, navigation,\nand assembly. Solving this problem can also help accommodate seamless interaction\nwith personal assistants in many environments, such as o\u000eces, homes, or even out on\nthe street in public.\nOne of the earliest AI problems designed to tackle this goal of having agents\ncommunicate with humans is the blocks-world problem [7]. This task involves a\nbunch of blocks placed on a grid, and a human operator provides instructions to the\nagent of how to move the blocks. The agent must understand, and then execute the\ninstruction to solve two challenging problems: identify which block must be moved,\nand where it must be moved to. Blocks can be placed either on the grid, or on top\nof another block. However, blocks can't be moved from underneath another block.\nAlthough this task has been around for many years, solving it is still challenging.\nSuccessfully solving this task requires an advanced level of reasoning and language\nunderstanding AI systems do not currently have, which is why AI agents' performance\non this task is still worse than human performance. For this reason, we propose to\nbetter solve the traditional blocks-world problem by taking advantage of advice.\nThroughout this chapter, we will propose four novel interactive advice-based pro-\ntocols that can be applied to any robot communication architecture, ordered in terms\nof decreasing human e\u000bort. As expected, as human e\u000bort lessens, performance does\nworsen, but all protocol outperform our baselines with no advice.\n8\nWe will also explore the notion of model self-generated advice, which signi\fcantly\nreduces/eliminates human e\u000bort. In this approach, a model is trained to automatically\ngenerate advice for a given scenario, based on the assumption that it is easier to solve\nthe advice prediction sub-task rather than the \fnal coordinate prediction task. We\nvalidate this assumption by developing a neural architecture to predict the advice\nand show it can help improve the overall prediction quality, despite having no human\nassistance.\n2.2 Related Work\nHuman-robot Interaction has been a major research topic for a few decades now,\nstarting with the SHRDLU system [7]. Since then, there have been multiple systems\n[10{16] built to try and tackle this problem.\n[11] uses reinforcement learning with policy gradients to map natural language\ninstructions to a sequence of actions. Similarly, [17] also presents a planning based\nmodel to infer the most likely set of constraints from natural language instructions.\nBuilding on the prior work, [13] focus on using semantic grammars to handle more\ncomplex instructions.\nIn addition to large amounts of work on assembly and planning style natural\nlanguage instruction understanding tasks, there has been a lot of work on other forms\nof human robot interaction. [18] introduced CLEVR, a dataset where the model must\nanswer a question about a computer generated image that has a bunch of objects\nwith di\u000berent sizes, shapes, and colors. [19,20] introduced neural module networks to\ntackle this dataset. They build models that have di\u000berent modules, each speci\fcally\ndesigned to handle a part of the end-to-end task, which can then be combined together\nto achieve better performance.\n[1] introduced NLVR, a dataset to identify whether a given sentence is describing\nan image or not. They followed it up with the more challenging NLVR2 [21], which\naims to solve this same task, except this time the images are photographs rather than\n9\ncomputer generated scenes. Very recently, [22] released LXMERT, a large scale trans-\nformer model to achieve great performance on NLVR2 by using an object relationship\nencoder, a language encoder, and a cross-modality encoder trained on 5 pre-training\ntasks.\nAs discussed, there has been a lot of work in the past few decades on human\nrobot interaction, starting with the blocks-world problem. While the blocks-world\nproblem is still extremely challenging as will discuss in Section 2.3, the community\nhas also been recently interested in other tasks like NLVR2. We will show the power\nof advice-driven learning to improve performance on blocks-world, and we feel that\nadvice can be helpful on these more recent tasks as well.\n2.3 Dataset Details\nThe blocks world task has been around since the 1970s [7]. However, in this\nchapter, we focus on the Blocks World dataset proposed by [23], speci\fcally it's more\nchallenging version, where all the blocks are unlabeled. In this dataset, given a bunch\nof unlabeled blocks on a grid that look identical, a human provides an instruction to\nmove a block. The model must identify which block must be moved and where it\nmust be moved to, just like the original blocks task.\nThis version of the blocks world task is more challenging for multiple reasons, as\noriginally identi\fed by [23,24], which we summarize here. First, the blocks are referred\nto by their complex spatial information, due to the fact that they are identical. This\nmeans phrases like \\bottom right tower\" or \\below the L shape\" are common, and it is\nchallenging for AI systems to easily understand these. Second, the instructions in the\ndataset are not restricted, meaning that the vocabulary is diverse and a simple model\nsuch as a pattern-based one cannot comprehend the instructions. Third, the only label\nprovided in the dataset is that of the source and target blocks. There is no other label\ninformation provided, such as the coordinate of the reference blocks (for example in\nthe instruction \\below the tower\", the tower is the reference block). Finally, the\n10\ndataset is fairly small, with the dataset having 2,493 training, 360 development, and\n720 test examples. This is relatively small compared to the large vocabulary prevalent\nin the instructions (1,172 tokens and an average sentence length of 23.5 words). All of\nthese make the SOTA performance on this dataset well below the human performance,\nas evaluated by [23].\n2.4 Advice for Blocks World\nWe devise two types of advice for the blocks-world task, that we feel will be helpful\nto a prediction agent, given that the human provides it. Both types of advice are de-\nsigned to assist the agent by providing simpler instructions in addition to the original\ninput. Both can also be easily provided interactively in a wide variety of ways, such\nas text (spoken or typed), a remote control, or any other means of communication.\nThe \frst type of advice, restrictive advice , informs the agent about the general\nregion of the source / target coordinates, such as top left orbottom right . The regions\nare determined by dividing the grid into equally sized sections (two halves, four\nquadrants). This type of advice can be thought of as useful for restricting the search\nspace of the agent, since when it is provided, the agent knows exactly which general\nregion to search for the block in. An example of restrictive advice for a given scenario\nis shown in Figure 2.1.\nThe second type of advice, corrective advice , observes the agents predictions and\ndetermines which direction (up, down, left, right) they must be adjusted to get closer\nto the target. This type of advice is useful for narrowing down the agents prediction\nin a multi-step way.\nIn this work, we do not crowd-source these types of advice to feed into our models,\nbut rather simulate it. Our advice sentences are generated by \flling in appropriate\nregions/directions into varying sentences. For example, given an advice sentence\nplaceholder, The target is in the , and a coordinate in the lower left, we would\ngenerate the restrictive advice sentence: The target is in the lower left. We can easily\n11\nThe target is in the lower left. Locate the top-most block and \nplace it directly below the right- \nmost tower. \nx\nFig. 2.1.: Based on the instruction (upper sentence) the model predicts the coordi-\nnates of the block and its target location. The `x' represents an incorrect prediction,\ncorrected by the provided advice (lower sentence).\ndo this in our experiments, as we have access to the true coordinate and the grid\n(however we will still not tell the system what the true coordinate is, just its' gen-\neral region, like a human would when providing restrictive advice). At test time, we\nuse variations of this sentence such as: The block's region is the lower left , to avoid\nmemorization. We will later show in Table 2.2 the importance of our pre-trained\nadvice grounding models (Sec 2.5.2) in enabling sentence variability and advice un-\nderstanding ( M4vsM5). Table 2.1 shows examples of some types of advice given the\ncoordinates.\nTable 2.1.: In the \frst row, given the target coordinate, the restrictive advice is\nprovided to place the target in the appropriate region. In the second row, given the\npredicted coordinate and the target coordinate, the correct corrective advice is to\nmove down to get closer to the target.\nPredicted Target Advice\n- (-0.5, 0.5, 0.5) In the top left.\n(-0.5, 0.5, 0.9) (-0.5, 0.5, 0.5) Move down.\n12\n2.5 Models\n2.5.1 Baseline Model\nIn this subsection, we brie\ry describe the best end-to-end RNN model proposed\nby [23]. Their model takes as input the sentence and world and predicts either the\nlocation of the block to move or its \fnal location. It does this by processing the\ninstruction through a RNN, and the world through a Fully Connected layer. It then\nuses the RNN representation to make two predictions, one for the o\u000bset and one for\nthe reference block, using Fully Connected Layers. Finally, it uses the world state to\nadjust the reference prediction, adds that to the o\u000bset prediction, and uses a Fully\nConnected layer to get a \fnal output coordinate. This model can be seen visually in\nFigure 2.2, and is the one that we apply advice to.\nLSTM-RNN Instruction Offset \nReference \nWorld (x, y, z) \nFC \nFig. 2.2.: Baseline model proposed by [23]. All our advice models are built on top of\nthis.\n2.5.2 Advice Grounding\nWe pre-train a neural network model to accurately understand the advice (Fig. 2.3a).\nFor both types of advice, a LSTM-RNN [25] is used to read the advice sentence\ns=w1;w2;:::;w nand output the hidden state representations fhng. Prior to this, a\nword embedding layer is used to project the input words into high-dimension vectors\nfwig.\n13\nFor restrictive advice, the last state from the LSTM hnis fed along with a random\ncoordinate into a Fully Connected (FC) layer. The network must output a positive\nprediction if the random coordinate is in the region described by the advice sentence,\nand negative otherwise. This design allows the model to understand the meaning of\nthe advice sentence by determining if the random coordinate follows the sentence.\nOur architecture takes as input an advice sentence s=w1;w2;:::;w n, passes it\nthrough a trained embedding layer of size 100, a LSTM of size 256, and outputs the\nhidden state representations fhng. The last hidden state hnis embedded using a Fully\nConnected (FC) layer of size 100. Each axis of a random input coordinate ( x;y;z ) is\nalso passed into the network and embedded using a FC layer of size 100. These 4 FC\nlayers are summed up and passed through a \fnal FC layer Oof size 2, which is then\nfollowed by a softmax. All FC layers use the RELU activation function. We train this\nas a binary prediction problem using cross-entropy loss (shown in Equation 2.1, where\np(x) is the true distribution/labels, and q(x) is the estimated distribution/prediction.\nFor the binary case, this can be reduced and y is the true label, and p is the predicted\nvalue.), the Adam optimizer, and a learning rate of 0.001. Gradient clipping [26] with\nthreshold 5.0 is used on the LSTM parameters to avoid exploding gradients.\nL=\u0000X\n8xp(x)log(q(x)) = (ylog(p) + (1\u0000y)log(1\u0000p)) (2.1)\nFor corrective advice, the last state from the LSTM hn(size 256) is fed along\nwith a random coordinate into a FC layer (size 100), and the network must output\na coordinate that follows the advice (3-dimensional output size). For example, if the\nadvice is move the block down , the predicted coordinate must be below the random\ninput coordinate. If the advice is followed, the network receives 0 loss, otherwise a\nMSE regression loss (Equation 2.2, where there are n examples, y is the true label,\nand p is the prediction), where the ground truth is some random coordinate that\ndoes follow the advice. The model and its parameters are identical to the one for\n14\nrestrictive advice, except the \fnal FC layer Ohas size 3, and the model is trained to\noutput a coordinate that follows the advice.\nL=1\nnnX\ni=1(yi\u0000pi)2(2.2)\nThis pre-training advice grounding stage is crucial to our overall performance,\nas it allows the end-to-end models to always understand the advice provided, which\nenables them to take advantage of the advice to solve the original task better. Without\nthis stage, the end-to-end model would be confused about what the advice meant,\nwhich would further lead to more confusion when solving the original task. Thus,\nwe recommend that future systems incorporating advice add a pre-training advice\ngrounding stage, and use simple enough advice that these models can understand the\nadvice well.\nLSTM-RNN (x, y, z) \nAdvice Softmax FC \n(a)\nLSTM-RNN FC Advice \nLSTM-RNN Instruction Offset \nReference \nWorld (x, y, z) \nFC (b)\nFig. 2.3.: (a) Pre-Trained Advice Understanding Model. (b) [23] End-to-End archi-\ntecture with our pre-trained model. World represents the board state, while o\u000bset\nand reference represent fully connected layers used to identify the o\u000bset and reference\nblocks.\n2.5.3 End-to-End Training\nThe pre-trained model from Section 2.5.2 that understands various advice text is\nincorporated into the best performing End-to-End RNN architecture proposed in [23]\n(and described in Sec 2.5.1) by adding a FC layer to the pre-trained LSTM state\nhn(size 256) and summing it with the LSTM hidden state of the original model (as\nshown in Figure 2.3b). We load and freeze the best performing parameters from our\n15\npre-trained model into the relevant portion of this end-to-end architecture, and train\nit on the original task of predicting the coordinates of the source / target location,\nwith the addition of advice input. Due to our pre-training step, this end-to-end model\ncan now well understand advice, as well as solve the original task. The end-to-end\nmodel is trained and tested on the dataset splits from [23]. The hidden dimension\nsize of the new FC layer is the same as the dimension of the LSTM.\n2.5.4 Advice Generation\nAs explained in the introduction (Sec 1, Sec 2.1), advice can either be provided by\na human or self-generated. For the blocks world task, we can self-generate restrictive\nadvice. In this approach, a model is trained to automatically generate restrictive\nadvice for a given input scenario (grid and instruction), based on the assumption\nthat it is easier to predict a region containing the target coordinates rather than their\nexact location. This method allows us to improve the overall coordinate prediction\nquality, with no human assistance.\nWe use a neural network model to self-generate restrictive advice (as shown in\nFigure 2.4), passing the instruction into an embedding layer of size 256 followed by a\nLSTM of size 256, the board state into a FC layer of size 20, concatenating these into\na FC layer (size 4), and \fnally using a softmax (de\fned by Equation 2.3, where z is\nthe input into the function, and has K classes) to classify the input example into a\nregion. We train this architecture using the Adam optimizer [27] and a learning rate\nof 0.0001 and then run it on the test set, generate the appropriate advice based on the\nregion the data is classi\fed in, and use that as test advice input for the end-to-end\narchitecture from section 2.5.3.\n\u001b(z)i=ezi\nPK\nj=1ezj(2.3)\n16\nLSTM-RNN FC Instruction \nWorld FC FC Softmax \nFig. 2.4.: Model for Generating Advice.\n2.6 Experiments\nNext, we present our experiments over our four di\u000berent advice protocols, each\nwith decreasing human e\u000bort and overall performance. In each protocol, we provide\nadvice to the end-to-end model from Section 2.5.3, whether it is given by a human user\nor model self-generated. Our results, evaluated on each model's mean and median\nprediction error, are presented in Table 2.2. We always compare to the baseline [23]\nmodel (described in Sec 2.5.1), which our model is identical to besides the addition\nof advice (and we always beat), and the state-of-the-art best non-ensemble [24] archi-\ntecture. Note that [24] use an advanced neural architecture and a di\u000berent training\nprocedure (source prediction trained as classi\fcation). We hypothesize that using\nthe advice mechanism over this more complex architecture would lead to further\nimprovements, and leave it for future work.\nThe pre-trained advice grounding models from Section 2.5.2 achieve 99.99% ac-\ncuracy, and are vital, as shown by the poor performance without them ( M4vsM5).\nThese grounding models allow the end-to-end architecture to generalize to the vari-\nability in advice utterances and understand the advice fully. Our code to replicate\nthese experimental results is available at1.\n1https://github.com/hockeybro12/Improving-NaturalLanguageInteractionWithRobots-Advice\n17\nTable 2.2.: Results for our models compared to previous models evaluated as distance\nfrom gold prediction normalized by block length for source and target coordinate\nprediction.\nModel Source Target\nMedian Mean Median Mean\nM1: [23] 3.29 3.47 3.60 3.70\nM2: Our Replication of [23] 3.13 3.42 3.29 3.50\nM3: [24] { 2.21 2.78 3.07\nM4: Restrictive Advice w/o Pre-Trained Model 3.88 3.83 3.56 3.43\nM5: 4 Regions Restrictive Advice 2.23 2.21 2.18 2.19\nM6: Corrective Advice 2.76 2.94 2.72 3.06\nM7: 4 Regions Retry Advice 2.41 3.02 2.42 3.14\nM8: 2 Regions Model Self-Generated Advice 3.01 3.31 3.08 3.36\nM9: Input-Speci\fc Model Self-Generated Advice 2.87 3.12 2.99 3.26\n2.6.1 Restrictive Advice\nWhen training the end-to-end model from Section 2.5.3 (to incorporate restrictive\nadvice), we provide restrictive advice at training time for only half the examples. For\nevery epoch, a di\u000berent half set of examples (determined randomly) receive advice.\nThis mechanism gives the model a chance to learn to interpret each example with\nand without advice, so that it can handle the interactivity without over\ftting to one\nsetup. This setup also gave the best performance.\nAt test time, the advice is provided only whenever the predictions fall in the wrong\ngeneral region, just like a human would (via text as described in Section 2.4). As seen\nin Table 2.2, this model ( M5) signi\fcantly outperforms both baselines ( M1,M3). We\nnote that the performance did not improve much when advice was always provided,\nshowing that this model was able to perform well in its absence and does not rely on\nit (due to our choice not to provide advice all the time in training). In fact a human\nwould only have to provide restrictive advice for 395/720 examples, and the model\nalways follows it. We note that the performance does not improve from [23] if advice\nis only provided at train time.\n18\n2.6.2 Corrective Advice\nWe train corrective advice identically to restrictive advice from Section 2.6.1,\nexcept we train in two separate iterations. This is necessary as the model must learn\nto adjust its predictions based on the advice, which is why it is \frst trained to make\nthe normal prediction (\frst iteration), then trained to adjust the prediction (second\niteration).\nIn the \frst iteration, we train identically to [23] with no advice, but in the second\niteration corrective advice is generated based on which direction the predictions must\nbe adjusted to be more accurate. This case is simpler than restrictive advice, since\nthe human operator just has to provide the direction to adjust the predictions, rather\nthan the precise region of the coordinates. However, the performance does worsen\n(M5vsM6).\n2.6.3 Retry Advice\nIn Section 2.5.4, we introduced a model that was able to self-generate restrictive\nadvice by predicting the general region of the block coordinates given the NL instruc-\ntion and blocks world. Table 2.3 shows this model's accuracy on that task when the\nboard is split into 4 regions. As this is a hard problem with low accuracy ( A1), we\ninstead generate advice for the top 2 most con\fdent predictions (determined by the\nsoftmax scores) ( A2).\nWe now introduce a new multi-step retry advice protocol. In the \frst step, the\nmodel from Section 2.5.4 self-generates restrictive advice (using the original grid and\ninstruction from the dataset) based on the most con\fdent predicted region, which it\nuses as input in the end-to-end model. If the user believes the coordinate prediction\nbased on this advice is wrong, it can tell the model to \\retry\", and then the second\nmost likely restrictive advice will be used. Thus, the only human feedback needed now\nis telling the model to \\retry\", rather than accurate advice as before. The performance\n19\nof this ( M7) still signi\fcantly outperforms [23] and is close to the state-of-the-art [24]\non target prediction.\nIn this approach, we still use accurate advice at training time, just the self-\ngenerated advice at test time. We generate two sets of self-generated advice, one\nfor the most con\fdent region prediction, and another for the next most con\fdent one\n(determined by the softmax scores). If the general region of the coordinate predic-\ntion in the \frst iteration of running the end-to-end model (with the most con\fdent\nself-generated advice) is incorrect, the human operator will provide retry advice, and\nwe then feed in the second most con\fdent advice (using that for the \fnal prediction).\nTable 2.3.: Accuracy of model self-generated advice.\nRegions Source Target\nA1: 4 47% 40%\nA2: 4, Top 2 Con\fdence 73% 70%\nA3: 4, Input-Speci\fc 67% 62%\n2.6.4 Model Self-Advice Generation\nWe now aim to avoid any human interaction, by letting the model completely\nself-generate the advice. Accomplishing it would allow us to improve the model's\nperformance without additional human e\u000bort. We experimented with two approaches.\nIn the \frst, we generate advice as described in Section 2.6.3. However, instead of\nhaving the user ask the model to \\retry\", we treat the top 2 con\fdence regions as a\ngeneral region, and provide that as advice input as described in Section 2.6.1. In this\ncase, there is a performance improvement over [23] with no human e\u000bort required ( M8\nin Table 2.2).\nOur second approach for self-generated advice aims to improve on some of the\nshortcomings of the \frst approach. Previously, when generating the advice, we had\ndecided on four coarse-grained regions, and trained a model to classify each input\nexample into one of these regions. In many cases, the true coordinate lay close to\n20\nx\n(a)\nx (b)\nFig. 2.5.: (a) The [23] model would have made a prediction (`x') close to the true\nblock (square). However, the advice region (blue) was incorrect (due to the true block\nbeing close to the edge of it) and this led to a signi\fcantly worse prediction (circle).\n(b) In the input-speci\fc self-generated advice model, the advice region (blue) is cen-\ntered at the incorrect coordinate prediction (`x'), leading to the true source block\nbeing included and a correct prediction (circle).\nthe boundary of one of these regions, often resulting in the model predicting the\nwrong region when self-generating the advice. This incorrect prediction would lead\nto signi\fcantly worse performance (when compared to the model without advice)\nwhen running the end-to-end model from Section 2.6.1, as the advice was incorrect\n(remember that the model always follows our advice, and the true coordinate is not in\nthe advice region due to the mistake). However, if we had instead chosen our regions\nto be centered at the true coordinate of each input example, it would be less likely\nthat the model would make an incorrect region prediction (since a small error would\nstill lead to the region containing the correct coordinate). Figure 2.5 provides a visual\nexplanation of this.\nFor this reason, we now introduce input-speci\fc model self-generated advice. In\nthis case, we run the original coordinate prediction model [23] in two iterations. In the\n\frst iteration, we use the prediction to generate advice for a region (of the same size\nas in the case of 4 quadrants) centered at the predicted coordinate (see Figure 2.5b).2\nIn the second iteration, we feed in this generated advice just like Section 2.6.1. This\n2We make sure the advice region doesn't exceed the board boundaries.\n21\nmodel ( M9) achieves performance slightly worse than retry advice, and signi\fcantly\nbetter than [23], all with no human e\u000bort.3Table 2.3 shows the accuracy increase\nin predicting the advice now ( A3vsA1). It is unsurprising that this approach to\nself-generating advice performs better, as now the regions are more speci\fc to each\ncoordinate (so there is a higher probability that the true coordinate is actually in the\npredicted region - see Figure 2.5).\nWe hypothesize that the performance improvements in self-generated advice hap-\npen since it is easier to predict the general region used to generate the advice rather\nthan the speci\fc coordinates. Previously, we have also shown the bene\ft of restric-\ntive advice in improving overall coordinate prediction, so it is unsurprising that a\nhigh accuracy of advice generation leads to better overall performance. Due to this,\nwe propose that future robot communication works take advantage of predicting and\nthen using model self-generated advice in their end-to-end training procedure.\n2.7 Conclusion\n2.7.1 Future Work\nThe advice we designed and presented so far is general knowledge that a human\nis likely to have about the blocks world problem, and can provide to the agent (either\nalways, or when it sees the agent make a mistake). Speci\fcally, the human can restrict\nthe search space of the agent, tell it which direction to adjust its' predictions, or just\ntell it to retry.\nIn the future, we plan on further exploring the \frst two advice protocols to make\nthem even more interactive. For restrictive advice, we can adapt our pre-trained\nadvice understanding models to understand advice regions of various sizes. The\nhumans can then iteratively provide multiple kinds of restrictive advice, each getting\nmore speci\fc. For example the \frst advice would restrict the search space to a region\n3Note that we must re-train the model from Section 2.5.2 as there are now signi\fcantly more regions.\nThe accuracy of that model is still 99.99%, and the training procedure does not change.\n22\n1/2 of the grid, then 1/4th, then 1/8th, and so on, until the agent makes the correct\nprediction. In this case, the human is providing better and better advice (the regions\nget smaller) as the iterations go on, allowing the agent to learn how to search in a\nvariety of regions and successively narrow down the search space. This could lead to\nbetter performance as well as make the process more interactive. A similar approach\ncould also be used in the self-generated case, where the agent \frst makes a prediction\nof a broad general region, then makes a overall coordinate prediction, uses that to\nmake a prediction of a smaller general region, then another coordinate prediction,\nand so on.\nThis same multi-step approach could also be used for corrective advice, as in each\nstep the human would tell the agent which direction to adjust it's predictions to get\ncloser. The di\u000berence here compared to what we did earlier for corrective advice is\nthat we would provide advice until the agent made the correct prediction, not just\nonce.\nAlso, we could experiment with combining restrictive, corrective, and self-generated\nadvice and providing them all at once (or in succession of each other) to see if that\nimproves performance.\nFinally, it would be interesting to see how providing advice a\u000bects performance\nin the cases where advice is not provided at all. It is possible the performance can\nimprove, since the agent learns more about the blocks-world task through advice\n(for example it learns how to navigate smaller search spaces). In some preliminary\nexperiments on this, we saw some small improvements, but they were not signi\fcant\nenough to include. However, adding more interactive advice like mentioned above\ncould provide signi\fcant improvements in cases where no advice is given.\n2.7.2 Summary\nIn this chapter, we have introduced four di\u000berent interactive advice-based pro-\ntocols to improve performance on the traditional blocks-world task. Our protocols\n23\nwere proposed in order of decreasing human e\u000bort and decreasing performance, but\nall were better than our baseline [23], whom our model was identical to besides the\ninclusion of advice. The last method, model self-generated advice, shows the bene\ft\nof considering advice even when not designing an interactive protocol. Thus, we have\nshown the various ways in which advice-driven learning (learning by receiving advice)\nimproves performance on the blocks-world task. In the next chapter, we will take a\nlook at another task, fake-news detection, and see how advice can be helpful there.\n24\n3. FAKE NEWS DETECTION\n3.1 Introduction\nFake news has emerged in recent years as a major problem in society. With the\nrise of social media and smartphones, everyone is constantly connected to each other.\nFurthermore, everyone has the ability to share and post any information (whether it\nis factual or not) online, for everyone else to potentially see. This has the bene\ft of\nreal news quickly propagating to all social media users, and all of them being aware\nof what is happening in the world, but also has the downside of fake news being\nspread and users believing it. This trend is contradictory to what happened in the\npast, where people saw news through sources more challenging to produce such as\ntelevision, radio, newspapers, or speci\fc trusted online websites (such as CNN, Yahoo\nNews, etc.). However, now, since many people use social media daily, anyone can put\nout whatever they want, and if it gets enough traction, people will see it and may\nactually believe it.\nThere are numerous reasons why an organization may want to post fake news. [8]\ndiscuss some of them, which we brie\ry summarize here. Two main bene\fts from\nspreading fake news that they highlight are economic and ideology-spreading ones.\nOrganizations can bene\ft economically if they are able to successfully spread fake\nnews that brings increased tra\u000ec to their website, which allows them to generate\nrevenue through advertisements. This increased tra\u000ec is more likely to happen with\nfake news, as the news could be surprising/controversial and more people would want\nto read more about it. Organizations can also bene\ft from spreading their own beliefs\nand changing people's thoughts, such as in the case of the 2016 presidential election,\nwhere fake news could have helped Donald Trump win [28].\n25\nDue to the wide-spread nature of fake news, there have been many e\u000borts to try\nand stop it. This includes fact-checking organizations like Politifcat, Snopes, and\nFactCheck, which manually verify claims. There have also been numerous datasets\n[8, 29] released to enable AI systems to better tackle the fake news problem. While\nprogress has been made [30], fake news detection is still a challenging problem for AI\nsystems. For this reason, we propose to use advice to empower AI systems to better\ndetect fake news.\nIn this chapter, we will build a graph-based model as a baseline for fake-news\ndetection. We will then propose various types of advice that can be applied to this\nmodel, and fake-news detection in general.\n3.2 Related Work\nFake news detection has been a hot topic of research recently, partly evidenced by\nthe ACM having a special issue on it [31]. Various top conferences have also organized\ncompetitions to detect fake news, such as the FEVER [32] task. In this dataset, over\n185 thousand claims extracted by altering sentences from Wikipedia are labeled based\non evidence.\nThere have also been a few surveys [33] and overview articles [34] by various\nsources aimed to inform readers more about fake news and the science behind it.\nThere are many approaches (statement level, article level, user level) to detecting\nwhether news is fake or not, but in this work we focus on detecting whether a source\nis fake or not, based on the dataset by [8], which we describe in Section 3.4.\n3.3 Background Knowledge\nBefore we begin to tackle the problem for fake news, it is important to go over\nsome background knowledge needed for understanding our models.\n26\n3.3.1 BERT\nBERT (Bidirectional Encoder Representations from Transformers) [35] is a lan-\nguage model that has achieved state of the art (SOTA) results in many natural lan-\nguage processing tasks. Language Models in NLP are trained to estimate the relative\nlikelihood of di\u000berent phrases, and after they are trained, they can be \fnetuned to\nachieve SOTA performance on a speci\fc task/group of tasks.\nBERT takes advantage of the Transformer model as its architecture [36], which\nis an attention based model that learns relations between words in text. It uses the\nencoder part of the transformer, more details of which are described in [36].\nThe key bene\ft of BERT is that it is designed to pre-train deep bi-directional\nrepresentations from large amounts of unlabeled text. It can do this as it introduces\na novel objective that allows the model to understand the text bidirectionally. The\nobjective is a Masked Language Model objective, in which 15% of the words in the\nsentences that are fed into the model are replaced with a [MASK] token. The model\naims to predict the value of the Masked tokens, based on the context given by the\nrest of the sentence. This objective allows the model to process the each sentence\nbidirectionally rather than just left-to-right, as there is no way to cheat, since tokens\nare masked. In a traditional language modeling objective, such as predicting the\ncurrent word given the previous words, models that process the text left-to-right\ncannot process the same text right-to-left, as they would have already seen the current\nwords when reading in the reverse direction. Thus, they would be cheating.\nBERT also introduces the Next Sentence Prediction objective, in which the model\nreceives a pair of sentences and learns to predict whether or not the second sentence\nfollows from the \frst one. This binary prediction objective allows the model to better\ncapture the relationship between sentences in language.\nDue to these two novel training objectives and being trained on massive amounts\nof data, BERT can be \fnetuned to achieve high performance on many tasks [35], and\n27\nis being widely used in NLP [37{39]. We will use BERT as a key component of our\ngraph-based model.\n3.4 Dataset\n3.4.1 Overview\nIn the rest of this chapter, we will focus on the task of fake news source detection,\noriginally proposed by [8]. It is important to focus on detecting if a source is fake or\nnot, as a website that has published fake news in the past is likely to do so in the\nfuture. Thus, if there is a system that identi\fes whether a given source traditionally\nposts fake news, it could help humans decide whether or not to trust another story\nfrom that same source. While there are organizations that focus on publishing lists\nof fake news sources, these lists are tedious to compute and are easily outdated, as\nnews sources are constantly popping up. Hence, an automated system to do this is a\ngreat necessity.\nThe dataset released by [8] includes 1,066 websites for which both bias and factu-\nality labels were provided or could be easily inferred from the Media Bias/Fact Check\n(MBFC) website1. They model factuality on a 3-point scale ( Low,Mixed , and High ).\nIn this work, we do not focus on predicting bias. The distribution of the dataset is\npresented in Table 3.1. The model they proposed is a SVM based on features they\ncrawled from the websites (articles and their information, wikipedia/twitter page,\nURL, web tra\u000ec, etc.).\nTable 3.1.: Distribution of factuality labels in the dataset [8].\nFactuality Type Count\nLow 256\nMixed 268\nHigh 542\n1https://mediabiasfactcheck.com\n28\n3.4.2 Article Scraping\nAs the dataset [8] does not come with articles already, but rather just the news\nsources, we must scrape the articles from the sources. In order to scrape articles,\nwe took advantage of publicly available Python packages Scrapy2and Newspaper3.\nThese packages return articles given a website URL. In order to make sure we had\nenough data, we only considered sources where we could scrape at least 100 articles\nfrom these packages. We also make sure that our dataset is balanced, with 1/3 of\nthe labels belonging to each category. Due to this, for our preliminary experiments,\nwe were left with 66 sources. We hereby refer to all of these sources as \\unknown\nsources\", as we are unsure about their factuality.\nIn our method explained later in Section 3.5, we utilize articles from sources that\nwe trust and believe to be mostly factual. These include: cnn.com, nbcnews.com,\nfoxnews.com, news.yahoo.com, abcnews.go.com, bloomberg.com, and espn.com. We\ncall these \\known sources\", since we know they are factual. For each article from an\nunknown source in the dataset, we extract up to 25 similar articles from each known\nsource.\nIn order to \fnd the similar articles, we formulate a query string that we search\ninto Google News Search4. For each unknown source article, we extract all the entities\nfrom the article title using the Stanford NER Tagger [40], and concatenate them to\nthe query string. We also extract the date the article was published and add that\nto the query string. Finally, we append the URL of the known news source we want\nto \fnd articles from to the query string. These three steps enable the Google news\nsearch to \fnd articles with a similar topic, published at around the same date, and\npublished by the known news source. Thus, intuitively, we are hoping to \fnd articles\nfrom Google News that talk about the same events as the ones from the \\unknown\"\nsources, and we are approximating this by entities and dates. We then search this\n2https://scrapy.org\n3https://github.com/codelucas/newspaper\n4https://news.google.com\n29\nquery string into Google news, and get back the top 25 article results. For each result,\nif it has the known source URL in its' URL, we consider it to be a similar article for\nthe unknown source article from the known source. While this method is noisy, we\nhypothesize and \fnd by visual inspection that some of the 25 articles returned are\nrelated or on the same topic date as the unknown source article. This is likely due\nto the power and accuracy of Google News search. This can be done easily through\nGoogle News queries from the O\u000ecial Google API. Once the resulting articles are\nfound, we can scrape and download them like before using Scrapy and other such\nAPI's.\nWe repeat this process for all the extracted unknown source articles. Our dataset\nnow has labeled unknown sources with articles and articles from known sources that\nare similar (or identical) in topic to the unknown ones.\n3.5 Models\nBefore we apply advice to the Fake News Detection Model, we must construct a\nbaseline model, which is what we do in this section. Our model is a graph consisting\nof all the data in our dataset: unknown sources, unknown source articles, known\nsources, known source similar articles. We aim to capture trustworthiness (de\fned\nas how much we can trust a given source in terms of its factuality) of each unknown\nsource in our graph, which we do by propagating trust (either positively or negatively)\nfrom known source articles based on how similar they are to their corresponding\nunknown source articles. If an unknown source article is very similar to a known\nsource article, we trust it and consider it not to be fake. We de\fne sources that have\nhigher trustworthiness scores (more trustworthiness) to be more factual, and lower\nscoring ones to be more likely to spread fake news.\n30\n3.5.1 Graph\nFirst we de\fne how we structure our dataset into a graph. This can also be seen\nvisually in Figure 3.1\nS E\nEE A\nAA\nA\nSEA\nAA\nA0.2 \n0.3 \n0.4 0.1 0.25 \n0.25 \nFig. 3.1.: In our graph model, we have unknown sources (pink), known sources (pur-\nple), entities (green), and articles (teal). We connect each unknown source to its\nentities and each known source to its entities. Each entity is also linked to the ar-\nticles about that entity. Each article in the unknown source is linked to its' similar\narticles in the known source (found by Google News search). For space, in this \fgure\nwe show the similar articles for only one known unknown source article. The edge\nweight for the pairs of articles is determined by the BERT [35] similarity score. The\nedge weight for each article is the average of the edge weight for all the it's similar\narticles scores (0.25 in the graph). The edge weight for the entities is the average of\nthe edge weight for all the articles, and the edge weight for the source is the average\nof all entities. This graph model allows trust to propagate from the known sources\nto the unknown sources.\nIn the graph, each source (either from the unknown or known sources list) is\nincluded as a root vertex (represented as a single node, no embedding). From each\nsource root vertex, there is an edge to all of the entities (also represented as a node)\nmentioned by that source's articles (represented as a BERT embedding). Each edge\nalso has a weight, initialized to 1.0, representing that entities \\trustworthiness\" score.\n31\nAs mentioned earlier, we de\fne trustworthiness as a score de\fning how much we\ncan trust a source - sources with higher trustworthiness have higher factuality. For\nexample, if an unknown source fury.news mentions articles about Donald Trump and\nHillary Clinton, the root vertex fury.news will have edges with weight 1.0 to each of\nthese entities. We determine the entities mentioned by a source by going through\nall of its' articles' titles and extracting the unique entities using the Stanford Named\nEntity Recognition Tagger [40]. If we didn't trust fury.news at all, it would have a\ntrustworthiness score of 0, but if we trusted it a lot, it would have a score of 1.0.\nEach entity is further connected to the article it was extracted from, also initialized\nwith edge weight 1.0.\nFurthermore, each article from the unknown sources is connected to each of the\nsimilar articles that we extracted from Google News in Sec 3.4.2. We now want to\nweight these edges by how similar each unknown source article is to the known source\narticle it is linked to. We will use this similarity information later to propagate trust\nfrom known sources to unknown sources. The edge weight is determined by the BERT\ncosine similarity score between the two articles (one from the unknown source and\none similar article). This method allows us to capture how similar the articles are. To\ncompute this score, we use the BERT sentence embeddings proposed by [41], which\nare more semantically meaningful than traditional BERT embeddings. We compute\nthe embedding for each article, then compute the cosine similarity between the two\nembeddings (the default BERT-base embeddings), and \fnally use that as the edge\nweight. We use BERT as our model for this task because as discussed earlier in\nSection 3.3.1, BERT builds deep bidrectional representations for text, which can be\n\fne-tuned to achieve SOTA performance in many NLP tasks. Thus, we feel BERT\ncan well capture the content of the articles, and the similarity score between them\nwill tell us how \\similar\" each unknown source article is to each of it's paired known\nsource articles. We also discuss how we \fne-tune BERT for fake news detection in\nSection 3.5.3.\n32\n3.5.2 Trust Propagation\nOur graph now has unknown sources with their articles and entities along with\nknown sources with their articles and entities. However, it does not capture trust-\nworthiness scores for the unknown sources, so we can't determine their factuality yet.\nIn order to capture this, we must propagate trust from known sources to unknown\nsources.\nWe propagate trust back from known sources to unknown source articles to un-\nknown source entities and \fnally to unknown sources. Once this is complete, each\nunknown source will have a trustworthiness score. We trust the articles from our\npre-de\fned known sources, which are not included in the unknown sources list. We\ncompute the trustworthiness score for each unknown source article by looking at the\ntop k trusted articles that it is most similar to (based on the assumption that an\narticle that is similar to many trusted articles is likely to be trustworthy, so the ar-\nticle should get a high trustworthiness score). This similarity is captured already\nin the graph by the BERT embedding cosine similarity scores. We choose k as a\nhyperparameter (set to 4 in our most successful experiments) and do not consider all\nsimilar articles, as it is likely that not all of the 25 articles extracted from Google\nNews are talking about the same topics as the unknown source article, and we want\nto minimize the penalty articles receive due to us/Google News not being able to \fnd\nenough similar articles. We now update the edge weight between the unknown source\nentity and this article to be the average of the similarity score between the article and\nit's top k similar known source articles. Thus, we have now propagated trust from\nthe known articles to the unknown articles and back to the entities.\nFinally, we update each unkown source with the average score of all of it's entities,\nweighted by how many articles each entity had (so we do not put equal weight to\nentities with only one article as ones with a hundred articles). Now, each source has\na trustworthiness score.\n33\nIn this subsection, we have discussed how we propagated trust from known sources\nto unknown sources, by utilizing BERT based article similarity scores. In the end,\nsources that have more articles similar in content to known factual source articles will\nhave higher similarity scores, and thus more trust (represented by a high trustwor-\nthiness score).\n3.5.3 BERT Fine-tuning\nIn Section 3.3.1, we discussed how BERT can achieve SOTA performance in NLP\ntasks when it is \fne-tuned. In this subsection, we discuss how we \fne-tune our\nsentence BERT model from [41] so that it performs better on fake news detection.\nWe \fne-tune BERT-base on the task of predicting whether a news article is fake\nor not. In order to do this on large amounts of data, we take advantage of our\nexisting graph with trustworthiness scores for unknown and known sources. We sort\nthe trustworthiness scores from the graph in decreasing order. We then assume for\nthe sake of \fne-tuning that the top 20% (a hyperparameter) of unknown sources in\nour graph are to be trusted (our model says that their articles are most similar to\nthe known sources, so it's likely that they are also factual), and the bottom 20% of\nunknown sources are not trusted (or fake). In this way, we are using the initial BERT\nrepresentation which allows us to capture trust in the graph to further enhance the\nBERT representation, so we can even better capture trust in the graph. We build a\nbalanced dataset from these sources articles. The dataset has 1000 articles for high\nand low factuality sources each. We feed in these articles into BERT, and using an\nextra Fully Connected classi\fcation layer (mapping the 768 BERT dimension to 2 for\nclassi\fcation, followed by softmax), ask BERT to predict whether an article is fake\nor not. All articles from the top 20% of unknown sources are given the 0 label (for\nnot fake), and all articles from the bottom 20% of unknown sources are given the 1\nlabel (for fake). The model is trained with cross entropy loss, Adam optimizer, and a\n34\nlearning rate of 0.001. We make sure to balance the dataset (so we don't use all the\narticles from the sources) This \fne-tuned model can achieve 91% accuracy.\n3.5.4 Graph Updates\nAfter the BERT \fne-tuning step is done, we can update our graph. We can\nrecompute BERT similarity scores between pairs of articles, and then do the trust\npropagation step from Section 3.5.2 again. After that, we can update BERT again,\nand so on, until convergence.\n3.5.5 Prediction\nWe mentioned in Section 3.4.2 that our dataset is balanced, with 1/3 of the sources\nbeing each label (low, mixed, and high factuality). Thus, in order to compute the\n\fnal labels for all of our unknown sources, we just need to sort them in decreasing\norder by their trustworthiness score, and then take the top third of the sources to be\nhigh, middle third to be mixed, and bottom third to be low.\nIn a scenario where we got a new source that was not part of the dataset and\nhad to classify it, our approach would still work, with some slight modi\fcation. We\ncould classify the new source with the same label as the sources it is closest to in the\ntrustworthiness score ranking. If both the source above and below it have the same\nlabel, classi\fcation is easy and would be the same as that source. However, if both\nhave a di\u000berent label, then classi\fcation would be based on whichever source the trust\nscore score is closest to. In the case where the trust score score is equally close to\nboth neighboring sources, we would pick the label randomly with each neighboring\nsource having equal probability.\n35\n3.5.6 Trust Propagation after Prediction\nAfter making the prediction, it is possible to further propagate trust. We can\nconsider the top 5% of sources predicted as trustworthy to actually be trusted, and\nadd them to our known sources list. We then link the articles from these new known\nsources to articles from unknown sources just like we did before for the other known\nsources (using entities extracted from their titles). Then, we can compute the BERT\nembedding similarity score for these new edges, and propagate trust just like Sec-\ntion 3.5.2.\n3.6 Advice\nIn the last section, we described our baseline graph model to capture trustwor-\nthiness scores for unknown sources. In this section, we discuss how we can take\nadvantage of advice to improve the performance of that model. As in the blocks-\nworld case, we consider both human provided and self-generated advice. We aim for\nour advice to be provided interactively by the human when they choose, and it to be\nsimple enough that our model can understand and take advantage of it.\n3.6.1 Types of Advice\nThe advice we design for fake news is based on the graph and can be provided\nby a human with knowledge of the graph or certain articles. In order for a human to\nprovide advice, it \frst needs access to the graph.\nAfter building and updating the graph discussed in the previous section, the sys-\ntem makes predictions based on the trustworthiness score ranking of unknown sources.\nIn order to make it easier for the human to provide advice, the system will show the\nhuman only the part of the graph belonging to the sources that it is unsure about\n(they were close to being predicted a di\u000berent label).\n36\nThe human then o\u000bers advice to the system based on the part of the graph it is\nshown. For the purposes of this work, we consider the following types of advice:\n1. Attention advice. In this case, the human tells the model which section of the\ngraph to give either more or less importance to, based on the current trust-\nworthiness of articles/entities/sources in that section. For example, if a human\nnotices that the similar articles from the known sources in one section of the\ngraph are actually related to the unknown source article they are connected to,\nit can tell the system \\You did a good job \fnding the similar articles for article\n_\". In this case, the system will take advantage of the advice by giving more\nweight to that unknown article when computing/propagating the trustworthi-\nness score for that source.\n2. Factual advice. For this type of advice, a human with some world knowledge\nabout the entities/articles being discussed provides a factual or non factual\nstatement to the system. For example, given an article talking about President\nBush watching basketball in his o\u000ece after 9/11, something that is false, a\nhuman could provide advice such as: \\The President of the United States is\nlikely to take action after a big event like 9/11, not watch basketball\". As the\nsystem has now received advice about this article, it can take advantage by\ndecreasing the trustworthiness score of that article and giving this article more\nweight when propagating trust to the rest of the graph.\nBoth of these types of advice are provided by humans by looking at the graph\nand noticing areas where the system either did a good job or made a mistake, and\nletting it know. This is in contrast to active learning, where the system would ask\nfor help with a given article. In this case, the human is choosing which part of the\ngraph to give advice, something that is easier for them, as they can give advice based\non knowledge they already have.\nAfter advice is provided, the system can not only use it to gain a better under-\nstanding of the article and it's source, but also use it to gain better understanding\n37\nof other sources. If advice makes a source more or less trusted, this can impact the\npropagation of trust to other sources as described in Section 3.5.6. In this way, the\nadvice given to any source can also be indirectly used for other sources.\n3.6.2 Self-Generated Advice\nIn this section, we discuss how we can self-generate factual advice. To do this,\nwe take advantage of the Politifact5fact checking source. Politifact is a non-partisan\nfact-checking website that has been around since May 2007. The website lists a bunch\nof political statements and labels them on a scale of 1-5, with 1 being not true (pants\non \fre!), and 5 being trustworthy (True). We scrape all the statements (around 7000)\nand their labels from Politifact.\nWe also take advantage of a Textual Entailment model, using the one proposed\nby [42] with the same hyper parameters as them. This model was also used by [43],\nto fact-check. The Textual Entailment model can be used to determine whether a\npiece of text entails a fact or not. This approach is similar to a typical fact-checking\npipeline [32]. However, the way we get the facts through self-generated advice and\nhow we build/update the graph is unique to our work.\nWe \frst extract dates and entities mentioned in all the facts from Politifact using\nthe Stanford NER tagger as before. We then go through each article in the part of\nthe graph given to us by the system, and \fnd the related facts from Politifact based\non the articles' published date and entities in the article title. We hypothesize that\nstatements about the same entities around the same date could be related to our\narticle. We used the same hypothesis in constructing our graph \fnding the known\nsource articles for the unknown sources.\nNext, we aim to determine whether the article entails one of the statements given\non Politifact, because if it does, we can use that to either increase or decrease its\ntrustworthiness score (based on the trustworthiness of that statement). In order to\n5https://www.politifact.com\n38\ndo this, we summarize the article using [44], and then pass each sentence along with\neach candidate Politifact statement into our textual entailment model. The model\nreturns the probability that the fact is entailed by the sentence of the article, which\nwe threshold at 0.7. If an article entails a statement with probability greater than\n0.7, we increase/decrease that articles trustworthiness score proportionally based on\nthe Politifact label. For example, if an article entails a statement with very high\nfactuality, then it is likely that the article is also factual and thus its trustworthiness\nscore would be increased signi\fcantly.\nIn this way, we are noisily \fltering Politifact statements to use as factual advice\nfor our system and using those to update the trust for the articles.\n3.7 Experiments\nOur experiments are based on our graph model proposed in Section 3.5.1. Our\nexperiments so far are preliminary results.\n3.7.1 Baseline Graph\nWe use the equally balanced 66 sources we found at least 100 articles for from the\nthe dataset proposed by [8] for fake news source detection, as mentioned in Sec 3.4.2.\nWe evaluate using the label choosing scheme described in Section 3.5.5. We achieve\nan accuracy of 62% with our baseline graph model, that improves to 64% when BERT\nis \fnetuned. We can see that the performance improves when BERT is \fnetuned,\nshowing that this step helps the model better decide whether two articles are related,\nby having it focus on the key parts that make an article fake.\n3.8 Conclusion\nIn this chapter we have built a graph-based model for fake news detection that also\ntakes advantage of a state of the art language model BERT, and also applied advice\n39\nto it. We proposed two types of advice that a human could provide to the graph based\non their knowledge of the world. The advice is simple for a human to provide via\nnatural language, and powerful due to our architecture of trust propagation, allowing\nthe advice for one source to potentially impact other sources. We also showed how\none of the types of advice, factual advice, can be self-generated.\nOur next steps for this work is to conduct a study where we have humans ac-\ntually provide the advice and see how it improves performance, rather than only\nself-generating it. We hypothesize that this will work better than self-generated ad-\nvice, as each advice provided will be more relevant to each article.\n40\n4. SUMMARY\nIn this work, we have built on work previously done on interactive systems, and\nexplored di\u000berent approaches for relaxing the typical single-step nature of many NLP\ntasks by introducing advice-driven learning. We have shown how we can view NLP\ntasks as more of an interactive , process, by taking advantage of advice, short natural\nlanguage sentences that a human can provide to an AI system to help it improve its'\nperformance. Studying blocks-world and fake-news detection tasks, we have shown\nhow advice can be designed to be simple for both the human to provide and the\nsystem to understand, while still being incredibly useful. In these ways, advice has\nbeen shown to be an easy way for an AI system to incorporate human feedback and\nknowledge into its' learning process interactively.\nThe advice we discussed in this work was designed speci\fcally to be easier to\nunderstand than the original task. Speci\fcally in the blocks-world, we showed how\nour pre-trained models for understanding the advice achieved near 100% accuracy, and\nthe end-to-end models always followed the advice. These are themes we feel are critical\nfor any future advice-driven systems, as if a system cannot understand the advice, it\nwill be more challenging for it to incorporate it. As language understanding models\nget better and better, designing types of advice can get more and more complex, since\nthe models will be better equipped to understand it.\nWe also discussed self-generated advice, which is a good way to achieve perfor-\nmance even when there isn't a human to provide the advice. It is also another bene\ft\nto building a system with advice in mind, as if the human can provide it, the model\ncan have signi\fcant performance increases, but if not, it can be self-generated and\nstill be helpful.\nThe types of advice we discussed in this work can also be complementary to\nexisting forms of learning, as we discussed in the Introduction Chapter. As advice is\n41\nnot a \fnal label, but rather a partial solution/hint, advice can be provided in addition\nto supervised learning, unsupervised learning, reinforcement learning, and/or active-\nlearning. This makes advice useful as a framework that can be combined with existing\nmethods. For these reasons, we believe advice-driven learning is a useful framework\nthat other AI practitioners should utilize.\n4.1 Future Work\nIn this section, we discuss some medium-term and long-term extensions to advice\nthat can be explored in the future.\n4.1.1 Medium-Term\nIn the medium-term, our goals are to apply advice to di\u000berent NLP tasks that\nare currently being considered single-step processes. Currently, there is a lot of work\nin Machine Learning that is heavily dependent on supervision, but when designing\nadvice-based protocols, one thing to look for is the ability to decompose the \fnal task.\nIf it is possible, advice can be used to provide supervision from humans interactively\nin a focused way. Through self-generated advice, we can also aim to alleviate some\nof that e\u000bort needed by the humans.\nAs natural language processing systems like BERT [35] become more advanced,\nthe amount of decomposition needed will reduce, as the models will be able to better\nunderstand di\u000berent types of advice, and then advice can become more complex. This\nwill in turn allow us to take advantage of advice to solve even more challenging tasks.\n4.1.2 Long-Term\nIn the long term, we envision a future in which humans can seamlessly provide\nadvice to AI systems that they see on a day-to-day basis, and the systems can learn\nand constantly become better from it. Rather than in the case of active learning where\n42\na system must explicitly ask questions in order to be taught something, humans can\nobserve an AI system making a mistake / having trouble and tell it. In this way, AI\nsystems will always be improving due to human knowledge.\nAs a simple example of how this can actually be done, we look at robots. At\nPurdue University and other universities, Starship robots have become fairly popular.\nThey are tiny robots that roam the university sidewalks, delivering food and other\nsmall items from point A to point B. For example, a human can request delivery of\na food item on campus, the robot autonomously goes over to the restaurant where\nsomeone places the item in a compartment in the robot, then the robot navigates to\nthe destination, and delivers the item. One of the challenges with these robots is the\nability to cross busy streets, as sometimes their camera sensors cannot tell whether\na car is coming or not. This issue currently causes the delivery process to take a lot\nlonger than it should, as the robots can often be seen just waiting at an intersection.\nIn this situation, we envision that advice can be helpful. For example, if a human is\nwalking by, and provides the advice to the system \\Come cross with me\", the robot\ncan understand that and follow the human across the street. As it is crossing, it can\nalso examine the surroundings and learn more about the situation, so that next time\nit sees the same situation, it is more likely to cross, even when a human is not there.\nIn this case, the robot didn't ask for help, but the human just provided it through\nadvice, and the robot got better because of it. This is just one example of a real-world\nscenario where advice can be useful.\nREFERENCES\n43\nREFERENCES\n[1] A. Suhr, M. Lewis, J. Yeh, and Y. Artzi, \\A corpus of natural language for\nvisual reasoning,\" in Proceedings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Papers) , 2017, pp. 217{223.\n[2] A. Talmor, J. Herzig, N. Lourie, and J. Berant, \\Commonsenseqa: A ques-\ntion answering challenge targeting commonsense knowledge,\" arXiv preprint\narXiv:1811.00937 , 2018.\n[3] R. T. McCoy, E. Pavlick, and T. Linzen, \\Right for the wrong reasons: Di-\nagnosing syntactic heuristics in natural language inference,\" arXiv preprint\narXiv:1902.01007 , 2019.\n[4] J. F. Allen, L. K. Schubert, G. Ferguson, P. Heeman, C. H. Hwang, T. Kato,\nM. Light, N. Martin, B. Miller, M. Poesio et al. , \\The trains project: A case\nstudy in building a conversational planning agent,\" Journal of Experimental &\nTheoretical Arti\fcial Intelligence , vol. 7, no. 1, pp. 7{48, 1995.\n[5] P. E. Rybski, K. Yoon, J. Stolarz, and M. M. Veloso, \\Interactive robot task\ntraining through dialog and demonstration,\" in Proc. of the ACM/IEEE inter-\nnational conference on Human-robot interaction . ACM, 2007, pp. 49{56.\n[6] T. Wen, D. Vandyke, N. Mrk\u0014 s\u0013 \u0010c, M. Ga\u0014 s\u0013 \u0010c, L. Rojas-Barahona, P. Su, S. Ultes,\nand S. Young, \\A network-based end-to-end trainable task-oriented dialogue sys-\ntem,\" in Proc. of the Annual Meeting of the European Chapter of the Association\nfor Computational Linguistics (EACL) , vol. 1, 2017, pp. 438{449.\n[7] T. Winograd, \\Understanding natural language,\" Cognitive psychology , vol. 3,\nno. 1, pp. 1{191, 1972.\n[8] R. Baly, G. Karadzhov, D. Alexandrov, J. Glass, and P. Nakov, \\Predicting\nfactuality of reporting and bias of news media sources,\" in Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing , 2018,\npp. 3528{3539.\n[9] S. Srivastava, I. Labutov, and T. Mitchell, \\Learning to ask for conversational\nmachine learning,\" in Proceedings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP) , 2019, pp. 4155{4165.\n[10] M. MacMahon, B. Stankiewicz, and B. Kuipers, \\Walk the talk: Connecting\nlanguage, knowledge, and action in route instructions,\" 2006.\n[11] S. R. Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzilay, \\Reinforcement\nlearning for mapping instructions to actions,\" 2009.\n44\n[12] D. L. Chen and R. J. Mooney, \\Learning to interpret natural language navigation\ninstructions from observations.\" 2011.\n[13] S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G. Banerjee, S. J. Teller,\nand N. Roy, \\Understanding natural language commands for robotic navigation\nand mobile manipulation.\" 2011.\n[14] C. Matuszek, N. FitzGerald, L. Zettlemoyer, L. Bo, and D. Fox, \\A joint model\nof language and perception for grounded attribute learning,\" 2012.\n[15] J. Kim and R. J. Mooney, \\Adapting discriminative reranking to grounded lan-\nguage learning.\" 2013, pp. 218{227.\n[16] D. K. Misra, J. Langford, and Y. Artzi, \\Mapping instructions and visual obser-\nvations to actions with reinforcement learning,\" 2017.\n[17] T. M. Howard, S. Tellex, and N. Roy, \\A natural language planner interface for\nmobile manipulators,\" in 2014 IEEE International Conference on Robotics and\nAutomation (ICRA) . IEEE, 2014, pp. 6652{6659.\n[18] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. Lawrence Zitnick,\nand R. Girshick, \\Clevr: A diagnostic dataset for compositional language and el-\nementary visual reasoning,\" in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2017, pp. 2901{2910.\n[19] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein, \\Neural module networks,\"\ninProceedings of the IEEE Conference on Computer Vision and Pattern Recog-\nnition , 2016, pp. 39{48.\n[20] R. Hu, J. Andreas, M. Rohrbach, T. Darrell, and K. Saenko, \\Learning to reason:\nEnd-to-end module networks for visual question answering,\" in Proceedings of the\nIEEE International Conference on Computer Vision , 2017, pp. 804{813.\n[21] A. Suhr, S. Zhou, A. Zhang, I. Zhang, H. Bai, and Y. Artzi, \\A corpus for\nreasoning about natural language grounded in photographs,\" arXiv preprint\narXiv:1811.00491 , 2018.\n[22] H. Tan and M. Bansal, \\Lxmert: Learning cross-modality encoder representa-\ntions from transformers,\" arXiv preprint arXiv:1908.07490 , 2019.\n[23] Y. Bisk, D. Yuret, and D. Marcu, \\Natural language communication with\nrobots.\" 2016.\n[24] H. Tan and M. Bansal, \\Source-target inference models for spatial instruction\nunderstanding,\" 2018.\n[25] S. Hochreiter and J. Schmidhuber, \\Long short-term memory,\" 1997.\n[26] R. Pascanu, T. Mikolov, and Y. Bengio, \\On the di\u000eculty of training recurrent\nneural networks,\" 2013, pp. 1310{1318.\n[27] D. P. Kingma and J. Ba, \\Adam: A method for stochastic optimization,\" arXiv\npreprint arXiv:1412.6980 , 2014.\n[28] H. Allcott and M. Gentzkow, \\Social media and fake news in the 2016 election,\"\nJournal of economic perspectives , vol. 31, no. 2, pp. 211{36, 2017.\n45\n[29] W. Y. Wang, \\\\liar, liar pants on \fre\": A new benchmark dataset for fake news\ndetection,\" in Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers) , 2017, pp. 422{426.\n[30] R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi, F. Roesner, and\nY. Choi, \\Defending against neural fake news,\" arXiv preprint arXiv:1905.12616 ,\n2019.\n[31] S. Papadopoulos, K. Bontcheva, E. Jaho, M. Lupu, and C. Castillo, \\Overview\nof the special issue on trust and veracity of information in social media,\" ACM\nTransactions on Information Systems (TOIS) , vol. 34, no. 3, p. 14, 2016.\n[32] J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal, \\Fever: a large-\nscale dataset for fact extraction and veri\fcation,\" in Proceedings of the 2018\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long Papers) , 2018, pp.\n809{819.\n[33] K. Shu, A. Sliva, S. Wang, J. Tang, and H. Liu, \\Fake news detection on social\nmedia: A data mining perspective,\" ACM SIGKDD Explorations Newsletter ,\nvol. 19, no. 1, pp. 22{36, 2017.\n[34] D. M. Lazer, M. A. Baum, Y. Benkler, A. J. Berinsky, K. M. Greenhill,\nF. Menczer, M. J. Metzger, B. Nyhan, G. Pennycook, D. Rothschild et al. , \\The\nscience of fake news,\" Science , vol. 359, no. 6380, pp. 1094{1096, 2018.\n[35] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \\Bert: Pre-training of deep\nbidirectional transformers for language understanding,\" in Proceedings of the\n2019 Conference of the North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers) , 2019, pp. 4171{4186.\n[36] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n L. Kaiser, and I. Polosukhin, \\Attention is all you need,\" in Advances in neural\ninformation processing systems , 2017, pp. 5998{6008.\n[37] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, \\Language\nmodels are unsupervised multitask learners,\" OpenAI Blog , vol. 1, no. 8, 2019.\n[38] S. Reddy, D. Chen, and C. D. Manning, \\Coqa: A conversational question an-\nswering challenge,\" Transactions of the Association for Computational Linguis-\ntics, vol. 7, pp. 249{266, 2019.\n[39] J. Gao, M. Galley, L. Li et al. , \\Neural approaches to conversational ai,\" Foun-\ndations and Trends R\rin Information Retrieval , vol. 13, no. 2-3, pp. 127{298,\n2019.\n[40] J. R. Finkel, T. Grenager, and C. Manning, \\Incorporating non-local information\ninto information extraction systems by gibbs sampling,\" in Proceedings of the\n43rd annual meeting on association for computational linguistics . Association\nfor Computational Linguistics, 2005, pp. 363{370.\n[41] N. Reimers and I. Gurevych, \\Sentence-bert: Sentence embeddings using\nsiamese bert-networks,\" in Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing . Association for Computational\nLinguistics, 11 2019. [Online]. Available: http://arxiv.org/abs/1908.10084\n46\n[42] A. Parikh, O. T\u007f ackstr\u007f om, D. Das, and J. Uszkoreit, \\A decomposable attention\nmodel for natural language inference,\" in Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Processing , 2016, pp. 2249{2255.\n[43] Y. Zhang, Z. Ives, and D. Roth, \\Evidence-based trustworthiness,\" in Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics .\nFlorence, Italy: Association for Computational Linguistics, Jul. 2019, pp.\n413{423. [Online]. Available: https://www.aclweb.org/anthology/P19-1040\n[44] J. Steinberger and K. Jezek, \\Using latent semantic analysis in text summariza-\ntion and summary evaluation,\" Proc. ISIM , vol. 4, pp. 93{100, 2004.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Advice-Driven Learning: A Blocks-World and Fake News Detection Approach", "author": ["N Mehta"], "pub_year": "2020", "venue": "NA", "abstract": "Over the last few years, there has been growing interest in learning models for various  Natural Language Processing tasks, such as the popular blocks world domain and fake news"}, "filled": false, "gsrank": 624, "pub_url": "https://search.proquest.com/openview/71883b2ae850a428df322911d03239be/1?pq-origsite=gscholar&cbl=18750&diss=y", "author_id": [""], "url_scholarbib": "/scholar?hl=en&q=info:uevemgk4xVQJ:scholar.google.com/&output=cite&scirp=623&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D620%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=uevemgk4xVQJ&ei=d7WsaKCIGLXCieoP4PfQ0A8&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:uevemgk4xVQJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://hammer.purdue.edu/articles/thesis/Advice-Driven_Learning_A_Blocks-World_and_Fake_News_Detection_Approach/13356656/1/files/25742363.pdf"}}, {"title": "Ideological biases in social sharing of online information about climate change", "year": "2021", "pdf_data": "ZK[KF ZIN FZ\\OIS K\nOno{w{rtmkw ltk\u20aco\u20ac tz\u20ac{mtkw \u20acsk~tzr {q{zwtzo\ntzq{~yktt{z kl{ut mwtykto mskzro\n\\~t\ufffd\ufffdkz Q1G1Ikzz OJ\n*.Oktz [1_ok\ufffdo~. N\ufffd\ufffdow \\1W1_twwtky\ufffd\nJo|k~\ufffdyoz\ufffd {qI{y|\ufffd\ufffdo~ [mtozmo. ]zt\ufffdo~\ufffdt\ufffd\ufffd {qK\ufffdo\ufffdo~. K\ufffdo\ufffdo~. Jo\ufffd{z .]zt\ufffdon Rtzrn{y\n*\ufffdm7;4E o\ufffdo\ufffdo~1km1\ufffdv\nFl\u20act~kmt\nK\ufffd|{\ufffd\ufffd~o \ufffd{yontk m{z\ufffdoz\ufffd t\ufffdkzty|{~\ufffdkz\ufffd m{y|{zoz\ufffd {q{|tzt{z q{~yk\ufffdt{z k~{\ufffdzn mwtyk\ufffdo\nmskzro1 Vzwtzo \ufffd{mtkw yontk \ufffd\ufffdms k\ufffd\\\ufffdt\ufffd\ufffdo~. \ufffdsoq{m\ufffd\ufffd {q\ufffdst\ufffd\ufffd\ufffd\ufffdn\ufffd. |~{\ufffdtno kzk\ufffdoz\ufffdo \ufffd{\n\ufffd\ufffd\ufffdn\ufffd |\ufffdlwtm ozrkroyoz\ufffd kznntrt\ufffdkw yontk nt\ufffd\ufffdoytzk\ufffdt{z ~owk\ufffdon \ufffd{mwtyk\ufffdo mskzro1 [sk~/\ntzrkwtzv\ufffd{kz{zwtzo k~\ufffdtmwo t\ufffdkztzntmk\ufffd{~ {qyontk ozrkroyoz\ufffd1 Frr~ork\ufffdon wtzv/\ufffdsk~tzr\nq{~y\ufffd kzo\ufffd\ufffd{~v \ufffd\ufffd~\ufffdm\ufffd\ufffd~o \ufffdstms yk|\ufffd m{wwom\ufffdt\ufffdo yontk ozrkroyoz\ufffd l\ufffd\ufffdso\ufffd\ufffdo~ |{|\ufffdwk\ufffdt{z1\nNo~o \ufffdom{z\ufffd\ufffd~\ufffdm\ufffd lt|k~\ufffdt\ufffdo zo\ufffd\ufffd{~v\ufffd wtzvtzr \\\ufffdt\ufffd\ufffdo~ \ufffd\ufffdo~\ufffd \ufffd{\ufffdso\ufffdol |kro\ufffd \ufffdso\ufffd \ufffdsk~on.\n\ufffd\ufffdtzr knk\ufffdk\ufffdo\ufffd {qk||~{\ufffdtyk\ufffdow\ufffd 916ytwwt{z Kzrwt\ufffds/wkzr\ufffdkro \ufffd\ufffdoo\ufffd\ufffd l\ufffdkwy{\ufffd\ufffd 5ytwwt{z\n\ufffd\ufffdo~\ufffd n\ufffd~tzr kzo\ufffdoz\ufffdq\ufffdw \ufffdo\ufffdoz/\ufffdoov |o~t{n moz\ufffd~on {z\ufffdsokzz{\ufffdzmoyoz\ufffd {q\ufffdso][\ufffdt\ufffds/\nn~k\ufffdkw q~{y \ufffdsoWk~t\ufffd Fr~ooyoz\ufffd {zmwtyk\ufffdo mskzro1 I{yy\ufffdzt \ufffd\ufffdno\ufffdom\ufffdt{z tzntmk\ufffdo\ufffd \ufffdsk\ufffd\n\ufffdso{l\ufffdo~\ufffdon tzq{~yk\ufffdt{z/\ufffds k~tzr zo\ufffd\ufffd{~v mkzlo|k~\ufffdt\ufffdt{zon tz\ufffd{\ufffd\ufffd{\ufffdokvw\ufffd m{zzom\ufffdon\nm{y|{zoz\ufffd\ufffd. ~o|~o\ufffdoz\ufffdtzr \ufffd\ufffdl\ufffdo\ufffd\ufffd {qk~\ufffdtmwo\ufffd \ufffdsk~on l\ufffdkr~{\ufffd| {q\ufffd\ufffdo~\ufffd1 _omsk~km\ufffdo~t\ufffdo\n\ufffdso\ufffdo |k~\ufffdt\ufffdt{z\ufffd \ufffds~{\ufffdrs kzkw\ufffd\ufffdt\ufffd {q\ufffdol n{yktz\ufffd kzn\ufffdo\ufffd\ufffdm{z\ufffdoz\ufffd q~{y \ufffdsk~on k~\ufffdtmwo\ufffd. qtzn/\ntzr\ufffdsoy \ufffd{lol~{knw\ufffd no\ufffdm~tlon k\ufffdkwoq\ufffd/\ufffdtzr2oz\ufffdt~{z yoz\ufffdkwt\ufffd\ufffd r~{\ufffd| kznk~trs\ufffd/\ufffdtzr2mwt/\nyk\ufffdo \ufffdmo|\ufffdtm r~{\ufffd|1 I{~~owk\ufffdt{z kzkw\ufffd\ufffdt\ufffd \ufffds{\ufffd\ufffd k\ufffd\ufffd~tvtzr |{\ufffdt\ufffdt\ufffdo k\ufffd\ufffd{mtk\ufffdt{z lo\ufffd\ufffdooz woq\ufffd2\n~trs\ufffd |{wt\ufffdtmkw tno{w{r\ufffd kznoz\ufffdt~{zyoz \ufffdkwt\ufffd\ufffd2\ufffdmo|\ufffdtm mwtyk\ufffdo tno{w{r\ufffd ~o\ufffd|om\ufffdt\ufffdow\ufffd1 S{{vtzr\nk\ufffdtzq{~yk\ufffdt{z/\ufffds k~tzr {\ufffdo~ \ufffdtyo. \ufffdso~o t\ufffdm{z\ufffdtno~klwo \ufffd\ufffd~z{\ufffdo~ tz\ufffdsoozrkron \ufffd\ufffdo~ |{|\ufffdwk/\n\ufffdt{zkzn\ufffdsok~\ufffdtmwo\ufffd \ufffdsk\ufffdk~o\ufffdsk~on. l\ufffd\ufffd\ufffdso\ufffdol n{yktz \ufffd{\ufffd~mo\ufffd kzn|{wk~t\ufffdon zo\ufffd\ufffd{~v\n\ufffd\ufffd~\ufffdm\ufffd\ufffd~o k~o~owk\ufffdt\ufffdow\ufffd |o~\ufffdt\ufffd\ufffdoz\ufffd1 \\st\ufffd \ufffd\ufffd\ufffdn\ufffd |~{\ufffdtno\ufffd o\ufffdtnozmo \ufffdsk\ufffd{zwtzo \ufffdsk~tzr {qzo\ufffd\ufffd\nyontk m{z\ufffdoz\ufffd ~owk\ufffdon \ufffd{mwtyk\ufffdo mskzro t\ufffdl{\ufffds |{wk~t\ufffdon kzn|{wt\ufffdtmt\ufffdon. \ufffdt\ufffds ty|wtmk\ufffdt{z\ufffd\nq{~{|tzt{z n\ufffdzkytm\ufffd kzn|\ufffdlwtm nolk\ufffdo k~{\ufffdzn \ufffdst\ufffdty|{~\ufffdkz\ufffd \ufffd{mto\ufffdkw mskwwozro1\nOz\ufffd~{n\ufffdm\ufffdt{z\nOz\u20ac|t\ufffdo {q\u20acmtoz\ufffdtqtm m{z\u20acoz\u20ac\ufffd\u20ac {z\ufffdsomk\ufffd\u20aco\u20ac kzn |~tyk~\u00de oqqom\ufffd\u20ac {qmwtyk\ufffdo mskzro. t\ufffd~oyktz\u20ac\nkm{z\ufffd~{\ufffdo~\u20actkw \ufffd{|tm tz|\ufffdlwtm kzn |{wt\ufffdtmkw nt\u20acm{\ufffd~\u20aco1 [\ufffd~\ufffdo\u00de\u20ac sk\ufffdo w{zr \u20acs{\u00d0z \u20ac\ufffdl\u20ac\ufffdkz\ufffdtkw\n\ufffdk~tk\ufffdt{z tz|\ufffdlwtm lowtoq\u20ac k~{\ufffdzn mwtyk\ufffdo mskzro *q{~ o\u00f0ky|wo d4.5f+kzn \ufffdsowo\ufffdow {q|{wk~t\u20ack/\n\ufffdt{z lo\ufffd\u00d0ooz tznt\ufffdtn\ufffdkw\u20ac \u20ac\ufffd||{~\ufffdtzr kzn {||{\u20actzr km\ufffdt{z \ufffd{yt\ufffdtrk\ufffdo kz\ufffds~{|{roztm mwtyk\ufffdo\nmskzro sk\u20aclooz r~{\u00d0tzr d6f1Tontk m{\ufffdo~kro {qmwtyk\ufffdo \u20acmtozmo kzn \ufffdsoq~kyo\u20ac \ufffd\u20acon \ufffd{|~o\u20acoz\ufffd\n\ufffdsotzq{~yk\ufffdt{z mkzsk\ufffdo kzty|{~\ufffdkz\ufffd ty|km\ufffd {z|\ufffdlwtm |o~mo|\ufffdt{z\u20ac kzn \u00d0twwtzrzo\u20ac\u20ac \ufffd{\ufffdkvo\nkm\ufffdt{z d7f.|~o\u20acoz\ufffd ntqqo~oz\ufffd y{\ufffdt\ufffdk\ufffdt{z\u20ac kzn mkww\u20ac q{~km\ufffdt{z d9fkzn tzqw\ufffdozmo \ufffdsokmm\ufffd~km\u00de kzn\nPLOS ONE\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 4259k4444444444\nk4444444444\nk4444444444\nk4444444444\nk4444444444\nOPEN ACCESS\nIt\ufffdk\ufffdt{z> Ikzz \\QG._ok\ufffdo~ O[._twwtky\ufffd N\\W\n*5354+ Ono{w{rtmkw ltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo\ntzq{~yk\ufffdt{z kl{\ufffd\ufffd mwtyk\ufffdo mskzro1 WS{[ VUK\n4:*7+> o3593:9:1 s\ufffd\ufffd|\ufffd>22 n{t1{~r243146;4 2u{\ufffd~zkw1\n|{zo13593:9 :\nKnt\ufffd{~> Qk~{\ufffd\u00f8k\ufffd Qkzv{\ufffd\ufffdvt. _o\ufffd\ufffd W{yo~kztk z\n]zt\ufffdo~\ufffdt\ufffd\ufffd {q\\omsz{w{r\ufffd. WVSFU J\nZomot\ufffdon> F\ufffdr\ufffd\ufffd\ufffd 7.5353\nFmmo|\ufffdon> F|~tw 45.5354\nW\ufffdlwt\ufffdson> F|~tw 56.5354\nI{|\ufffd~trs\ufffd> \u00a95354 Ikzz o\ufffdkw1\\st\ufffdt\ufffdkz{|oz\nkmmo\ufffd\ufffd k~\ufffdtmwo nt\ufffd\ufffd~tl\ufffd\ufffdon \ufffdzno~ \ufffdso\ufffdo~y\ufffd {q\ufffdso\nI~ok\ufffdt\ufffdo I{yy{z\ufffd F\ufffd\ufffd~tl\ufffd\ufffdt{z Stmoz\ufffdo. \ufffdstms\n|o~yt\ufffd\ufffd \ufffdz~o\ufffd\ufffd~tm\ufffdo n\ufffd\ufffdo.nt\ufffd\ufffd~tl\ufffd \ufffdt{z.kzn\n~o|~{n\ufffdm\ufffdt{z tzkz\ufffdyont\ufffdy. |~{\ufffdtnon \ufffdso{~trtzkw\nk\ufffd\ufffds{~ kzn\ufffd{\ufffd~mo k~om~ont\ufffdon1\nJk\ufffdk F\ufffdktwkltwt\ufffd \ufffd[\ufffdk\ufffdoyoz\ufffd> \\soo\ufffd\ufffdo~zkw nk\ufffdk\n\ufffd\ufffdon tz\ufffdst\ufffd\ufffd\ufffd\ufffdn\ufffd *\\\ufffdt\ufffd\ufffdo~ |{\ufffd\ufffd\ufffd. zo\ufffd\ufffd k~\ufffdtmwo\ufffd+\n\ufffdk\ufffd|\ufffdlwtmw\ufffd k\ufffdktwklwo \ufffdsoz \ufffdso\ufffd\ufffd\ufffdn\ufffd \ufffdk\ufffdmk~~ton\n{\ufffd\ufffd.l\ufffd\ufffd\ufffdtzmo \ufffdso{~trtzkw nk\ufffdkt\ufffd{\ufffdzon l\ufffd\ufffdst~n\n|k~\ufffdto\ufffd \ufffdomkzz{\ufffd |\ufffdlwt\ufffds \ufffdsom{y|wo\ufffdo nk\ufffdk\ufffdo\ufffd\n\ufffdt\ufffds{\ufffd\ufffd tzq~tzrtzr \\o~y\ufffd {q]\ufffdo*\\\ufffdt\ufffd\ufffdo~+ {~\nm{|\ufffd~trs\ufffd *zo\ufffd\ufffd k~\ufffdtmwo\ufffd+1 Oz\ufffd\ufffdokn \ufffdosk\ufffdo ykno\nk\ufffdktwklwo wt\ufffd\ufffd\ufffd{q\ufffd{\ufffd~mo\ufffd *\ufffd\ufffdoo\ufffd OJ\ufffd.zo\ufffd\ufffd k~\ufffdtmwo\n]ZS\ufffd+ \ufffd{\ufffdsk\ufffdtz\ufffdo~o\ufffd\ufffdon |k~\ufffdto\ufffd mkz~o|~{n\ufffdmo {\ufffd~\n\ufffd{~v1 L\ufffdwwno\ufffdktw\ufffd {q\ufffdsonk\ufffdk\ufffdo\ufffd kznk\ufffdktwkltwt\ufffd\ufffd k~o\nrt\ufffdoz low{\ufffd1 Ltr\ufffdsk~o ~o|{\ufffdt\ufffd{~\ufffd *s\ufffd\ufffd|>22n{t1 {~r2431\n:3<72y=1qt r\ufffdsk~o146997 9=3+ sk\ufffdlooz m~ok\ufffdon\n\ufffdt\ufffds\ufffdsonk\ufffdk~o}\ufffdt~on \ufffd{~o|~{n\ufffdmo {\ufffd~~o\ufffd\ufffdw\ufffd\ufffd1\nw{zro\ufffdt\ufffd\u00de {q~o|~{n\ufffdmon yo\u20ac\u20ackro\u20ac d:f1Zomoz\ufffd \u00d0{~v sk\u20ac\u20acs{\u00d0z \ufffdsk\ufffd yontk oqqom\ufffd\u20ac \ufffdk~\u00de no|ozn/\ntzr{zo\u00f0t\u20ac\ufffdtzr |{wt\ufffdtmkw ltk\u20aco\u20ac d;f1]zno~\u20ac\ufffdkzntzr \ufffdsoyontk wkzn\u20acmk|o k~{\ufffdzn mwtyk\ufffdo mskzro\nt\u20ac{qvo\u00dety|{~\ufffdkzmo tzyk||tzr |\ufffdlwtm ozrkroyoz\ufffd \u00d0t\ufffds \ufffdsot\u20ac\u20ac\ufffdo kzn \u20ac\ufffd||{~\ufffd q{~|{wt\ufffdtmkw\nkm\ufffdt{z\u20ac \ufffd{m{zq~{z\ufffd t\ufffd1\nF\u20ac\u20aco\u20ac\u20actzr \u00d0stms |o{|wo k~oo\u00f0|{\u20acon \ufffd{\u00d0sk\ufffd tzq{~yk\ufffdt{z t\u20acq\ufffdznkyoz\ufffdkw \ufffd{kz\u00de\u20ac\ufffd\ufffdn\u00de {q\ufffdso\noqqom\ufffd\u20ac {qyontk {z|\ufffdlwtm \ufffdzno~\u20ac\ufffdkzntzr kzn {|tzt{z1 \\so nt\u20ac~\ufffd|\ufffdt\ufffdo ty|km\ufffd {q{zwtzo yontk\nsk\u20ac\ufffd~kz\u20acq{~yon \ufffdsoyontk oz\ufffdt~{zyoz\ufffd. ~kntmkww\u00de kw\ufffdo~tzr \ufffdsont\ufffdo~\u20act\ufffd\u00de {qm{z\ufffdoz\ufffd |o{|wo\nozm{\ufffdz\ufffdo~ k\u20ac\u00d0oww k\u20ac\ufffdsoo\u00f0|{\u20ac\ufffd~o |~{mo\u20ac\u20ac t\ufffd\u20acowq1 Oznt\ufffdtn\ufffdkw\u20ac k~oqkmon \u00d0t\ufffds k\u00d0tno ~kzro {q\nyontk {|\ufffdt{z\u20ac *l{\ufffds \u20ac{mtkw kzn \ufffd~knt\ufffdt{zkw+. zo\u00d0 |k\ufffd\ufffdo~z\u20ac {qo\u00f0|{\u20ac\ufffd~o *\u20acowom\ufffdon l\u00de\ufffdsoozn \ufffd\u20aco~\n{~n~t\ufffdoz l\u00de\ufffdsot~ \u20ac{mtkw zo\ufffd\u00d0{~v+ kzn tzm~ok\u20acon |~{n\ufffdm\ufffdt{z {q\ufffd\u20aco~/rozo~k\ufffdon m{z\ufffdoz\ufffd d<f1\nW~o\ufffdt{\ufffd\u20ac \u00d0{~v tz\ufffdst\u20ack~ok sk\u20acq{m\ufffd\u20acon {z\ufffdsooqqom\ufffd\u20ac {qtzmtnoz\ufffdkw o\u00f0|{\u20ac\ufffd~o {zyontk k\u00d0k~o/\nzo\u20ac\u20ac *o1r1 d=f+ kzn \ufffdsont\ufffdo~\u20act\ufffd\u00de |~o\u20acoz\ufffdon l\u00de{zwtzo ~om{yyozno~ \u20ac\u00de\u20ac\ufffdoy\u20ac *o1r1 d43f+1 [\ufffdms\noqq{~\ufffd\u20ac k~osky|o~on l\u00de\ufffdsont\ufffdo~\u20act\ufffd\u00de {q{zwtzo |wk\ufffdq{~y\u20ac. \ufffdso~k|tn |kmo {q\ufffdsot~ no\ufffdow{|yoz\ufffd\nkzn \ufffdso{lq\ufffd\u20acmk\ufffdt{z {q\ufffdsokwr{~t\ufffdsytm |~{mo\u20ac\u20aco\u20ac \ufffdso\u00de q{ww{\u00d0. kzn k\u20ack~o\u20ac\ufffdw\ufffd z{\ufffdzt\ufffdo~\u20ackw\n\ufffdzno~\u20ac\ufffdkzntzr {qo\u00f0|{\u20ac\ufffd~o oqqom\ufffd\u20ac t\u20ac|{\u20ac\u20actlwo1 _so\ufffdso~ kztznt\ufffdtn\ufffdkw t\u20acm{z\u20ac\ufffdytzr \ufffdsozo\u00d0\u20ac\n{zwtzo q~{y kworkm\u00de yontk {~rkzt\u20ack\ufffdt{z. {~|~{n\ufffdmtzr kzn m{z\u20ac\ufffdytzr tzq{~yk\ufffdt{z {z\u20ac{mtkw\nyontk. \ufffdsoq\ufffdznkyoz\ufffdkw n\u00dezkytm {qm{yy\ufffdztmk\ufffdt{z o\u00f0|{\u20ac\ufffd~o kzn tzqw\ufffdozmo t\u20ac\ufffdsk\ufffd {qzo\ufffd\u00d0{~v\nq{~yk\ufffdt{z d44f. lk\u20acon {zm~ok\ufffdt{z {qzo\u00d0 ~owk\ufffdt{z\u20acst|\u20ac lo\ufffd\u00d0ooz \ufffd\u20aco~\u20ac kzn yontk m{z\ufffdoz\ufffd l\u00dek\n\ufffdk~to\ufffd\u00de {qyokz\u20ac *\u20ac\ufffdms k\u20ac\u00d0ol/l~{\u00d0\u20actzr kzn \u20ac{mtkw tzq{~yk\ufffdt{z/\u20acsk~tzr+1 Vzwtzo yontk o\u00f0|{/\n\u20ac\ufffd~o m~ok\ufffdo\u20ac kzo\ufffd\u00d0{~v \ufffdsk\ufffd wtzv\u20ac \u20ac{\ufffd~mo\u20ac kzn m{z\u20ac\ufffdyo~\u20ac {qm{z\ufffdoz\ufffd *z{no\u20ac+ \ufffdtk\ufffdsot~ tz\ufffdo~km/\n\ufffdt{z\u20ac *onro\u20ac+. ~o}\ufffdt~tzr kzo\ufffd\u00d0{~v |o~\u20ac|om\ufffdt\ufffdo q{~t\ufffd\u20ac|~{|o~ \ufffdzno~\u20ac\ufffdkzntzr1\n\\st\u20ac \u20ac\ufffd\ufffdn\u00de kty\u20ac \ufffd{no\u20acm~tlo |k\ufffd\ufffdo~z\u20ac {q\u20acsk~tzr {zwtzo yontk m{z\ufffdoz\ufffd kl{\ufffd\ufffd mwtyk\ufffdo mskzro1\n_stwo km{y|wo\ufffdo ~om{~n {q\ufffd\u20aco~\u20ac) o\u00f0|{\u20ac\ufffd~o \ufffd{ntrt\ufffdkw m{z\ufffdoz\ufffd t\u20ac{zw\u00de |{\u20ac\u20actlwo \ufffdtkkmm\ufffd~k\ufffdo\n\ufffd~kmvtzr {q\u00d0ol l~{\u00d0\u20actzr st\u20ac\ufffd{~to\u20ac. yontk ozrkroyoz\ufffd mkzlotzqo~~on q~{y \ufffdsom{z\ufffdoz\ufffd \ufffd\u20aco~\u20ac\n\u20acsk~o {z\u20ac{mtkw yontk1 [sk~tzr k\u00d0ol k~\ufffdtmwo ~o}\ufffdt~o\u20ac km\ufffdt{z l\u00de\ufffdso\ufffd\u20aco~. kzn mk\ufffd\u20aco\u20ac t\ufffd\ufffd{k||ok~\n{z\ufffdso\u20ac{mtkw yontk qoon\u20ac {qq~tozn\u20ac kzn q{ww{\u00d0o~\u20ac. k\u20ac\u00d0oww k\u20acm{z\ufffd~tl\ufffd\ufffdtzr \ufffd{krr~ork\ufffdo \ufffd~ozn\u20ac.\n{q\ufffdoz kn\ufffdo~\ufffdt\u20acon l\u00de\u20ac{mtkw yontk |wk\ufffdq{~y\u20ac1 \\st\u20ac t\u20ac\ufffd\u20acon \ufffd{tzntmk\ufffdo k\u20actrztqtmkz\ufffdw\u00de strso~ wo\ufffdow\n{qozrkroyoz\ufffd \ufffdskz o\u00f0|{\u20ac\ufffd~o1 [{mtkw \u20acsk~tzr {qm{z\ufffdoz\ufffd tz\u20ac\ufffdkz\ufffdtk\ufffdo\u20ac k|~{y{\ufffdt{z yomskzt\u20acy.\ntzm~ok\u20actzr \ufffdso\ufffdt\u20actltwt\ufffd\u00de {qkz\u00dem{z\ufffdoz\ufffd km~{\u20ac\u20ac k\ufffd\u20aco~)\u20ac \u20ac{mtkw zo\ufffd\u00d0{~v kzn wtvow\u00de tzntmk\ufffdtzr \ufffdsk\ufffd\n\ufffdso\u20acsk~o~ kr~oo\u20ac \u00d0t\ufffds. {~k||~{\ufffdo\u20ac {q.\ufffdsom{z\ufffdoz\ufffd1 \\so\u20aco qkm\ufffd{~\u20ac. kw{zr \u00d0t\ufffds \ufffdsotzm~ok\u20actzr \ufffd\u20aco\n{q\u20ac{mtkw yontk k\u20ack\u20ac{\ufffd~mo {qzo\u00d0\u20ac d45f. yokz \ufffdsk\ufffd \u20ac\ufffd\ufffdn\u00de {qntrt\ufffdkw yontk \u20acsk~tzr mkz|~{\ufffdtno\ntz\u20actrs\ufffd\u20ac tz\ufffd{ s{\u00d0 tzq{~yk\ufffdt{z t\u20ac|~{|krk\ufffdon {zwtzo. tzmw\ufffdntzr ty|{~\ufffdkz\ufffd m{z\ufffdoy|{~k~\u00de t\u20ac\u20ac\ufffdo\u20ac\nwtvomwtyk\ufffdo mskzro1 Oz|k~\ufffdtm\ufffdwk~. _ok\ufffdo~ o\ufffdkw1d46f \u20acs{\u00d0\u20ac \ufffdsk\ufffd zo\ufffd\u00d0{~v kzkw\u00de\u20act\u20ac {q\u20ac\ufffdms |~{|/\nkrk\ufffdt{z |k\ufffd\ufffdo~z\u20ac mkz~o\ufffdokw yokztzrq\ufffdw \u20ac{mtkw \u20ac\ufffd~\ufffdm\ufffd\ufffd~o\u20ac {qzo\u00d0\u20ac ozrkroyoz\ufffd kzn m{z\u20ac\ufffdy|\ufffdt{z\nk~{\ufffdzn |{wt\ufffdtmkw o\ufffdoz\ufffd\u20ac1\nNo~o \u00d0o{|o~k\ufffdt{zkwt\u20aco {\ufffd~\u20ac\ufffd\ufffdn\u00de {q{zwtzo tzq{~yk\ufffdt{z/\u20acsk~tzr k~{\ufffdzn mwtyk\ufffdo mskzro l\u00de\no\u00f0kytztzr wtzv/\u20acsk~tzr {z\\\u00d0t\ufffd\ufffdo~1 ]\u20aco~ |{\u20ac\ufffd\u20ac *\ufffd\u00d0oo\ufffd\u20ac+ ~oqo~ozmtzr mwtyk\ufffdo mskzro kzn m{z/\n\ufffdktztzr wtzv\u20ac \ufffd{\u00d0ol m{z\ufffdoz\ufffd *]ZS\u20ac. \u00d0stms k~o{q\ufffdoz ~ozno~on tz\ufffd{ zo\u00d0\u20ac lw\ufffd~l\u20ac {~tykro\u20ac tz\n\ufffdso\\\u00d0t\ufffd\ufffdo~ mwtoz\ufffd+ \u00d0o~o m{wwom\ufffdon q~{y \ufffdso\\\u00d0t\ufffd\ufffdo~ \u20ac{mtkw yontk |wk\ufffdq{~y \ufffdtkt\ufffd\u20ac|\ufffdlwtm FWO1\n\\st\u20ac nk\ufffdk\u20aco\ufffd \u00d0k\u20ac\ufffd\u20acon \ufffd{m{z\u20ac\ufffd~\ufffdm\ufffd lt|k~\ufffdt\ufffdo zo\ufffd\u00d0{~v\u20ac wtzvtzr \ufffd\u20aco~\u20ac \ufffd{\ufffdsontrt\ufffdkw yontk \ufffdso\u00de\n\u20acsk~on1 Fzkw\u00de\u20act\u20ac {qzo\ufffd\u00d0{~v \ufffd{|{w{r\u00de qtzn\u20ac \u20ac\ufffd~{zr m{yy\ufffdzt\ufffd\u00de \u20ac\ufffd~\ufffdm\ufffd\ufffd~o. \u20ac\ufffd||woyoz\ufffdon l\u00de\nm{y|k~t\u20ac{z {q\u20ac{\ufffd~mo n{yktz\u20ac kzn \ufffdo\u00f0\ufffd\ufffdkw m{z\ufffdoz\ufffd {qk~\ufffdtmwo\u20ac \u20acsk~on \u00d0t\ufffdstz okms m{yy\ufffdzt\ufffd\u00de1\nK\u00f0|w{~k\ufffdt{z {qno\ufffdom\ufffdon m{yy\ufffdzt\ufffdto\u20ac tnoz\ufffdtqto\u20ac \u20ac\ufffd~{zr tno{w{rtmkw |{wk~t\u20ack\ufffdt{z \u00d0t\ufffdstz \ufffdso\nzo\u00d0\u20ac/\u20acsk~tzr zo\ufffd\u00d0{~v \u00d0so~o \ufffd\u20aco~\u20ac k~o\u20acor~ork\ufffdon l\u00dent\ufffdo~roz\ufffd {|tzt{z\u20ac. ~k\ufffdso~ \ufffdskz kz{zr{/\ntzr|~{mo\u20ac\u20ac. kzkw\ufffdo~zk\ufffdt\ufffdo noqtzt\ufffdt{z \u00d0stms t\u20acty|{~\ufffdkz\ufffd tz{\ufffdso~ m{z\ufffdo\u00f0\ufffd\u20ac d47f1 V\ufffdo~kww \ufffdso\n~o\u20ac\ufffdw\ufffd\u20ac tzntmk\ufffdo strsw\u00de |{wk~t\u20acon kzn |{wt\ufffdtmt\u20acon ozrkroyoz\ufffd \u00d0t\ufffds \u00d0ol m{z\ufffdoz\ufffd k~{\ufffdzn mwtyk\ufffdo\nmskzro. \u00d0t\ufffds wk~row\u00de \u20acor~ork\ufffdon kzn tno{w{rtmkww\u00de ltk\u20acon m{yy\ufffdzt\ufffdto\u20ac ~omot\ufffdtzr tzq{~yk\ufffdt{z\nq~{y ntqqo~oz\ufffd yontk \u20ac{\ufffd~mo\u20ac1 _onoy{z\u20ac\ufffd~k\ufffdo \ufffdsk\ufffd \ufffdso{l\u20aco~\ufffdon m{~~owk\ufffdt{z lo\ufffd\u00d0ooz |{wt\ufffdtmkw\n\ufffdto\u00d0\u20ac kzn mwtyk\ufffdo mskzro lowtoq\u20ac *o1r1 d6f+ o\u00f0\ufffdozn\u20ac \ufffd{{zwtzo tzq{~yk\ufffdt{z \u20ac{\ufffd~mo\u20ac kzn \ufffdsot~\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 5259L\ufffdzntzr> \\It\ufffdq\ufffdznon l\ufffdkzKW[ZI Zo\ufffdok~ms\n[\ufffd\ufffdnoz\ufffd\ufffds t|*r~kz\ufffd z\ufffdylo~ KW2T93:95 ;24.s\ufffd\ufffd|\ufffd>22\no|\ufffd~m1\ufffdv~t1{~ r2+1O_kznN_kmvz{\ufffdwonro q\ufffdzntzr\nq~{y K[ZI *r~kz\ufffd z\ufffdylo~ K[2U3455<62 4.s\ufffd\ufffd|\ufffd>22\no\ufffd~m1\ufffdv~t1{~ r2+kznUKZI *r~kz\ufffd z\ufffdylo~ UK2\nW34;76:24. s\ufffd\ufffd|\ufffd>22zo~m1\ufffdv ~t1{~r2+1 \\soq\ufffdzno~\ufffd skn\nz{~{wotz\ufffd\ufffd\ufffdn\ufffd no\ufffdtrz. nk\ufffdkm{wwom\ufffdt{z kzn\nkzkw\ufffd\ufffdt\ufffd. nomt\ufffdt{z \ufffd{|\ufffdlwt\ufffds. {~|~o|k~k \ufffdt{z{q\ufffdso\nykz\ufffd\ufffdm~t|\ufffd1\nI{y|o\ufffdtzr tz\ufffdo~o\ufffd\ufffd\ufffd >\\sok\ufffd\ufffds{~\ufffd sk\ufffdo nomwk~on\n\ufffdsk\ufffdz{m{y|o\ufffdtzr tz\ufffdo~o\ufffd\ufffd\ufffd o\ufffdt\ufffd\ufffd1\n\u20acsk~on ~okno~\u20acst|\u20ac1 L{~\ufffdsoqt~\u20ac\ufffd \ufffdtyo. \u00d0o\ufffd~kmv zo\ufffd\u00d0{~v \u20ac\ufffd~\ufffdm\ufffd\ufffd~o {\ufffdo~ k;/\u00d0oov |o~t{n. tzmw\ufffdn/\ntzrknt\u20ac~\ufffd|\ufffdt\ufffdo yontk o\ufffdoz\ufffd *\ufffdso kzz{\ufffdzmoyoz\ufffd {q\ufffdso\u00d0t\ufffdsn~k\u00d0kw {q\ufffdso][F q~{y \ufffdsoWk~t\u20ac\nFr~ooyoz\ufffd {zmwtyk\ufffdo mskzro+. \ufffd{\u20acs{\u00d0 \ufffdso|o~\u20act\u20ac\ufffdozmo {q\ufffdso{l\u20aco~\ufffdon |{wk~t\u20ack\ufffdt{z kzn |{wt\ufffdt/\nmt\u20ack\ufffdt{z {qyontk \u20acsk~tzr ~owk\ufffdon \ufffd{mwtyk\ufffdo mskzro {\ufffdo~ \ufffdtyo kzn \u00d0t\ufffds \ufffdk~\u00detzr lkmvr~{\ufffdzn\nwo\ufffdow\u20ac {q|\ufffdlwtm tz\ufffdo~o\u20ac\ufffd1\n[{mtkw yontk nk\ufffdk sk\u20aclooz \ufffd\u20acon \ufffd{\u20ac\ufffd\ufffdn\u00de \u20aco\ufffdo~kw k\u20ac|om\ufffd\u20ac {q|\ufffdlwtm {|tzt{z k~{\ufffdzn mwtyk\ufffdo\nmskzro. tzmw\ufffdntzr k\ufffd\ufffdt\ufffd\ufffdno\u20ac \ufffd{\u00d0k~n\u20ac kzn ozrkroyoz\ufffd \u00d0t\ufffds mwtyk\ufffdo mskzro yt\ufffdtrk\ufffdt{z \u20ac\ufffd~k\ufffdorto\u20ac\nd49f. yontk q~kytzr {q\ufffdsowokvon \ufffdIwtyk\ufffdork\ufffdo\ufffd oyktw\u20ac {zb{\ufffd\\\ufffdlo d9f.kzn \ufffdso\u20ac|~okntzr {q\nmkww\u20ac q{~m{wwom\ufffdt\ufffdo km\ufffdt{z k\ufffd\ufffdsoIVW49 m{zqo~ozmo d4:f1 Uo\ufffd\u00d0{~v\u20ac k~okztz\ufffd\ufffdt\ufffdt\ufffdo ~o|~o\u20acoz\ufffdk/\n\ufffdt{z. kzn m{yo \u00d0t\ufffds ks{\u20ac\ufffd {qkzkw\u00de\ufffdtmkw \ufffd{{w\u20ac \ufffd{\ufffdzno~\u20ac\ufffdkzn \ufffdso\u20acsk|o {q{zwtzo nt\u20acm\ufffd\u20ac\u20act{z\u20ac\nk~{\ufffdzn mwtyk\ufffdo mskzro? q{~o\u00f0ky|wo Kwro\u20acoy o\ufffdkw1d4;f o\u00f0|w{~o \ufffdsozo\ufffd\u00d0{~v {qs\u00de|o~wtzv\u20ac\nlo\ufffd\u00d0ooz lw{r\u20ac. \u00d0stwo _twwtky\u20ac o\ufffdkw1d4<f \u20ac\ufffd\ufffdn\u00de \ufffdso\u20ac\ufffd~\ufffdm\ufffd\ufffd~o {qq{ww{\u00d0o~. ~o\ufffd\u00d0oo\ufffd kzn yoz\ufffdt{z\nzo\ufffd\u00d0{~v\u20ac {z\\\u00d0t\ufffd\ufffdo~1 Ozl{\ufffds {q\ufffdso\u20aco \u20ac\ufffd\ufffdnto\u20ac. \ufffd\u20aco~ m{yy\ufffdzt\ufffdto\u20ac ykztqo\u20ac\ufffd k\u20acnoz\u20acow\u00de tz\ufffdo~m{z/\nzom\ufffdon mw\ufffd\u20ac\ufffdo~\u20ac \u00d0t\ufffds \u20actytwk~ msk~km\ufffdo~t\u20ac\ufffdtm\u20ac1 \\so\u20aco m{yy\ufffdzt\ufffdto\u20ac k~ostrsw\u00de |{wk~t\u20acon. \u20ac\ufffdms \ufffdsk\ufffd\nokms m{yy\ufffdzt\ufffd\u00de t\u20ac\u00d0oww no\u20acm~tlon l\u00dek\u20actzrwo \ufffdto\u00d0|{tz\ufffd. \u00d0t\ufffds qo\u00d0y{no~k\ufffdo \ufffd{tmo\u20ac1 [tytwk~ |k\ufffd/\n\ufffdo~z\u20ac sk\ufffdo kw\u20ac{ looz {l\u20aco~\ufffdon q{~{zwtzo |{wt\ufffdtmkw nt\u20acm{\ufffd~\u20aco *o1r1 d4=\u02d857f+1 \\st\u20ac |k\ufffd\ufffdo~z {q{|tz/\nt{z|{wk~t\u20ack\ufffdt{z kzn \u20acor~ork\ufffdt{z sk\u20acty|{~\ufffdkz\ufffd ty|wtmk\ufffdt{z\u20ac q{~{|tzt{z mskzro kzn \ufffdso\nwtvowts{{n {qrw{lkw m{z\u20acoz\u20ac\ufffd\u20ac d59f1 W{wk~t\u20ack\ufffdt{z tz{zwtzo \u20ac{mtkw yontk t\u20acy{\u20ac\ufffd q~o}\ufffdoz\ufffdw\u00de \u20ac\ufffd\ufffdn/\ntontz\ufffdso|{wt\ufffdtmkw \u20ac|so~o. o\u20ac|omtkww\u00de q{~\ufffd\u00d0{/|k~\ufffd\u00de |{wt\ufffdtmkw \u20ac\u00de\u20ac\ufffdoy\u20ac \u00d0t\ufffds kztno{w{rtmkw \u20ac|wt\ufffd\nkw{zr kwoq\ufffd/~trs\ufffd k\u00f0t\u20ac1 N{\u00d0o\ufffdo~. \ufffdso|soz{yoz{z kw\u20ac{ o\u00f0\ufffdozn\u20ac \ufffd{\ufffdsom{y|o\ufffdtzr {|tzt{z\u20ac\nk~{\ufffdzn mwtyk\ufffdo mskzro. \u00d0stms k~o{q\ufffdoz \u20acty|wtqton k\u20acknolk\ufffdo lo\ufffd\u00d0ooz oz\ufffdt~{zyoz\ufffdkwt\u20ac\ufffd\u20ac *\u20ac\ufffd|/\n|{~\ufffdtzr \ufffdso\u20acmtoz\ufffdtqtm m{z\u20acoz\u20ac\ufffd\u20ac kzn |~{y{\ufffdtzr km\ufffdt{z+ kzn \u20acmo|\ufffdtm\u20ac *n{\ufffdl\ufffdtzr {~{||{\u20actzr \ufffdso\nm{z\u20acoz\u20ac\ufffd\u20ac kzn zoon q{~km\ufffdt{z+. z{\ufffd\u00d0t\ufffds\u20ac\ufffdkzntzr \ufffdsont\ufffdo~\u20act\ufffd\u00de {q\ufffdto\u00d0|{tz\ufffd\u20ac kzn ~o|~o\u20acoz\ufffdk/\n\ufffdt{z\u20ac {q\ufffdst\u20acm{y|wo\u00f0 t\u20ac\u20ac\ufffdo d5:f1 \\so\u20aco |~o\ufffdt{\ufffd\u20ac zo\ufffd\u00d0{~v/lk\u20acon \u20ac\ufffd\ufffdnto\u20ac rozo~kww\u00de \ufffd~ok\ufffd nk\ufffdk\u20aco\ufffd\u20ac\nk\u20ac\u20actzrwo \u20aczk|\u20acs{\ufffd\u20ac. kw{zr \u00d0t\ufffds \ufffdsoty|wtmt\ufffd k\u20ac\u20ac\ufffdy|\ufffdt{z \ufffdsk\ufffd \ufffdso|soz{yozk \ufffdzno~ \u20ac\ufffd\ufffdn\u00de \ufffdk~to\u20ac\n\u20acw{\u00d0w\u00de1 \\so tz\ufffdo~\ufffdkw\u20ac \u20ac\ufffd\ufffdnton ~kzro q~{y y{z\ufffds\u20ac \ufffd{\u00deok~\u20ac. l\ufffd\ufffdl\u00dems{{\u20actzr \u20ac\ufffdms k\ufffdtyo\u20acmkwo t\ufffdt\u20ac\n|{\u20ac\u20actlwo \ufffd{{\ufffdo~w{{v \ufffdsomskzro\u20ac \ufffdsk\ufffd \u20ac{mtkw zo\ufffd\u00d0{~v\u20ac mkzo\u00f0|o~tozmo tz\u20acs{~\ufffdo~ |o~t{n\u20ac1\n\\so |k\ufffd\ufffdo~z {q|{wk~t\u20ack\ufffdt{z tzl{\ufffds |{wt\ufffdtmkw kzn mwtyk\ufffdo mskzro m{z\ufffdo\u00f0\ufffd\u20ac t\u20ac{q\ufffdoz k\u20ac\u20ac{mtk\ufffdon\n\u00d0t\ufffds \ufffdsoo\u00f0t\u20ac\ufffdozmo {qoms{ mskylo~\u20ac tz\ufffdso\u20ac{mtkw yontk om{\u20ac\u00de\u20ac\ufffdoy. \u00d0so~ol\u00de \ufffd\u20aco~\u20ac ms{{\u20aco \ufffd{\nk\u20ac\u20ac{mtk\ufffdo \u00d0t\ufffds |o{|wo kzn zo\u00d0\u20ac/yontk \u20ac{\ufffd~mo\u20ac \u00d0stms m{zq{~y \ufffd{kzn ~otzq{~mo \ufffdsot~ o\u00f0t\u20ac\ufffdtzr\nlowtoq\u20ac d4<f1 Kms{ mskylo~\u20ac sk\ufffdo looz |~{|{\u20acon \ufffd{m{z\ufffd~tl\ufffd\ufffdo \ufffd{\ufffdso\u20ac|~okntzr {qyt\u20actzq{~yk/\n\ufffdt{z d5;f. |{wt\ufffdtmkw zo\ufffd\u00d0{~v\u20ac {qoz\ufffdt~{zyoz\ufffdkw km\ufffd{~\u20ac d5<f. o\u00f0|{\u20ac\ufffd~o \ufffd{|{wt\ufffdtmkw tzq{~yk\ufffdt{z {z\n\u20ac{mtkw yontk d54f. kzn {zwtzo m{z\ufffdoz\ufffd k~{\ufffdzn mwtyk\ufffdo mskzro *o1r1 d4;. 4<.5=f+1 Oz\ufffdst\u20ac\u00d0{~v \u00d0o\nq{m\ufffd\u20ac {z\ufffdso\u20ac\ufffd~\ufffdm\ufffd\ufffd~kw |soz{yoz{z {qoms{ mskylo~\u20ac l\ufffd\ufffd{\ufffdso~ \u20acms{wk~\u20ac sk\ufffdo w{{von k\ufffds{\u00d0\n\ufffdso\u00de k~owtzvon \ufffd{|\u20ac\u00dems{w{rtmkw |~{mo\u20ac\u20aco\u20ac \u20ac\ufffdms k\u20acm{zqt~yk\ufffdt{z ltk\u20ac *o1r1 d63f+1\nW~o\ufffdt{\ufffd\u20ac \u20ac\ufffd\ufffdnto\u20ac {qtzq{~yk\ufffdt{z/\u20acsk~tzr k~{\ufffdzn mwtyk\ufffdo mskzro sk\ufffdo q{m\ufffd\u20acon {z\ufffdso|~{yt/\nzozmo {qntqqo~oz\ufffd \u20ac{\ufffd~mo\u20ac1 Uo\u00d0ykz d64f kzkw\u00de\u20acon \ufffdso\ufffd\u00d0oo\ufffd\u20ac kzn tzq{~yk\ufffdt{z \u20ac{\ufffd~mo\u20ac \u20acsk~on\nkw{zr\u20actno \ufffdso~owok\u20aco {q\ufffdsoOWII FZ9 _M4 ~o|{~\ufffd. qtzntzr kq{m\ufffd\u20ac {z\ufffdso|\ufffdlwtm ozrkroyoz\ufffd\n\u00d0t\ufffds \u20acmtozmo. kzn kn{ytzkzmo {qyktz\u20ac\ufffd~oky yontk \u20ac{\ufffd~mo\u20ac1 [oro~lo~r kzn Gozzo\ufffd\ufffd d4:f\no\u00f0kytzon \ufffdsol~okvn{\u00d0z {qntqqo~oz\ufffd wtzv \u20ac{\ufffd~mo\u20ac \ufffd\u20acon kw{zr\u20actno mkww\u20ac \ufffd{m{wwom\ufffdt\ufffdo km\ufffdt{z k\ufffd\n\ufffdsoIVW49 m{zqo~ozmo1 Rt~twozv{ kzn [\ufffdo|msozv{\ufffdk d65f \u20ac\ufffd\ufffdnton \ufffdso]ZS\u20ac \u20acsk~on {z\\\u00d0t\ufffd\ufffdo~\n{\ufffdo~ \ufffdsom{\ufffd~\u20aco {q{zo\u00deok~ tzqt\ufffdontqqo~oz\ufffd wkzr\ufffdkro\u20ac. qtzntzr \ufffdsk\ufffd l\u00dem{\ufffdz\ufffd~\u00de. \ufffdso][n{yt/\nzk\ufffdon \ufffd{\ufffdkw \ufffd\u00d0oo\ufffd m{\ufffdz\ufffd\u20ac. kzn kyt\u00f0 {q\ufffd~knt\ufffdt{zkw yontk. km\ufffdt\ufffdt\u20ac\ufffd kzn \u20acmo|\ufffdtm \u20act\ufffdo\u20ac \u00d0o~o \u20acsk~on1\nW{wk~t\u20ack\ufffdt{z t\u20ackm{yy{z {l\u20aco~\ufffdk\ufffdt{z tz\ufffdsomwtyk\ufffdo mskzro nolk\ufffdo1 U{\ufffdklwo \u20ac\ufffd\ufffdnto\u20ac \u20ac\ufffdms k\u20ac\nJ\ufffdzwk| o\ufffdkw1d6fsk\ufffdo \u20acs{\u00d0z \ufffdsk\ufffd \ufffdso|{wk~t\u20ack\ufffdt{z oqqom\ufffd tzmwtyk\ufffdo mskzro {|tzt{z sk\u20acr~{\u00d0z\nlo\ufffd\u00d0ooz 4==; kzn 534:1 Fzty|km\ufffd {q|{wk~t\u20ack\ufffdt{z mkzlo{l\u20aco~\ufffdon tz\ufffdsoq~kyo\u20ac \ufffd\u20acon \ufffd{nt\u20ac/\nm\ufffd\u20ac\u20ac mwtyk\ufffdo mskzro. \u20ac\ufffdms k\u20acl\u00deQkzr kzn Nk~\ufffd d66f. \u00d0s{ kzkw\u00de\u20acon \ufffdso\ufffdsoyo\u20ac |~o\u20acoz\ufffd tz\\\u00d0t\ufffd/\n\ufffdo~)\u20ac mwtyk\ufffdo mskzro nolk\ufffdo km~{\u20ac\u20ac \ufffd\u00d0{\u00deok~\u20ac1 \\so\u00de q{\ufffdzn ntqqo~ozmo\u20ac tz\ufffdso\ufffdo~ytz{w{r\u00de \ufffd\u20acon\nl\u00de{||{\u20actzr r~{\ufffd|\u20ac. \u00d0t\ufffds Zo|\ufffdlwtmkz/wokztzr \u20ac\ufffdk\ufffdo\u20ac tz\ufffdso][\ufffd\u20actzrglobalwarming tz\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 6259\n|~oqo~ozmo \ufffd{climatechange. kzn {q\ufffdoz \ufffd\u20actzr s{k\u00f0 q~kyo\u20ac \ufffd{mk\u20ac\ufffd n{\ufffdl\ufffd {z\ufffdso\u20acmtoz\ufffdtqtm m{z/\n\u20acoz\u20ac\ufffd\u20ac1 V)Uotww o\ufffdkw1d67f q{\ufffdzn \ufffdsk\ufffd \ufffdst\u20ac\ufffd~ozn o\u00f0\ufffdoznon \ufffd{\ufffdsoyontk m{\ufffdo~kro {q\ufffdso|\ufffdlwtmk/\n\ufffdt{z {q\ufffdsoOWII FZ9 \u00d0{~vtzr r~{\ufffd| ~o|{~\ufffd\u20ac1 G\u00de\u20ac\ufffd\ufffdn\u00detzr \ufffdsoq~kyo\u20ac \ufffd\u20acon tzzo\u00d0\u20ac|k|o~ kzn\n\ufffdowo\ufffdt\u20act{z l~{knmk\u20ac\ufffd\u20ac. \ufffdso\u00de q{\ufffdzn mwok~ |~oqo~ozmo\u20ac q{~ntqqo~oz\ufffd q~kyo\u20ac ky{zr\u20ac\ufffd \ufffdso\ufffdk~t{\ufffd\u20ac\nzo\u00d0\u20ac {~rkzt\u20ack\ufffdt{z\u20ac1 [{yo \u00d0{~v sk\u20acw{{von k\ufffdm{\ufffdz\ufffdo~tzr \ufffdsor~{\u00d0tzr wo\ufffdow\u20ac {q|{wk~t\u20ack\ufffdt{z.\ntzmw\ufffdntzr cskzr o\ufffdkw1d;f\u00d0s{ \u20ac\ufffd\ufffdn\u00de \ufffdsooqqom\ufffd {qmwk~tq\u00detzr yo\u20ac\u20ackro\u20ac {zkmm\ufffd~km\u00de {\ufffdo~ \ufffdso|o~/\nmot\ufffdon wo\ufffdow\u20ac {qm{z\u20acoz\u20ac\ufffd\u20ac ky{zr mwtyk\ufffdo \u20acmtoz\ufffdt\u20ac\ufffd\u20ac1 Fy{zr \ufffdsot~ o\u00f0|o~tyoz\ufffdkw r~{\ufffd|. o\u00f0|{/\n\u20ac\ufffd~o \ufffd{\ufffdsomwk~tq\u00detzr yo\u20ac\u20ackro wokn \ufffd{y{~o \ufffdztq{~y kmm\ufffd~km\u00de k~{\ufffdzn \ufffdso\u20acmtoz\ufffdtqtm m{z\u20acoz\u20ac\ufffd\u20ac\n\ufffds~{\ufffdrs r~ok\ufffdo~ ty|km\ufffd tz\ufffdsok~ok\u20ac {qw{\u00d0o~ lk\u20acowtzo lowtoq1\n\\so oqqom\ufffd\u20ac {q|{wk~t\u20ack\ufffdt{z n{z{\ufffdkw\u00d0k\u00de\u20ac ykztqo\u20ac\ufffd o}\ufffdkww\u00de {zokms ozn {q\ufffdso{|tzt{z \u20ac|om/\n\ufffd~\ufffdy1 [ms\ufffdwn\ufffd o\ufffdkw1d69f m{y|k~on \ufffdso\ufffd\u20ackro {q\ufffdso\ufffdo~y\u20acclimatechange kznglobalwarming\nkm~{\u20ac\u20ac \ufffdso\u00d0ol\u20act\ufffdo\u20ac {qk\u20aco~to\u20ac {q\ufffdstzv \ufffdkzv\u20ac1 Ztrs\ufffd/\u00d0tzr \ufffdstzv \ufffdkzv\u20ac \u00d0o~o y{~o wtvow\u00de \ufffd{\ufffd\u20aco\nglobalwarming. \u00d0t\ufffds \ufffdso{||{\u20act\ufffdo \ufffd~ozn {l\u20aco~\ufffdon tzwoq\ufffd/\u00d0tzr \ufffdstzv \ufffdkzv\u20ac1 \\so\u20aco qtzntzr\u20ac {q\nm{z\ufffdoz\ufffd ntqqo~ozmo\u20ac q~{y |{wk~t\u20acon \u20ac{\ufffd~mo\u20ac o\u00f0\ufffdozn\u20ac lo\u00de{zn \ufffdsomwtyk\ufffdo mskzro nolk\ufffdo1 L\ufffd~\ufffdso~\nkzkw\u00de\u20act\u20ac {qmwtyk\ufffdo \u20acmo|\ufffdtm {~rkzt\u20ack\ufffdt{z\u20ac kzn \ufffdsot~ q\ufffdzntzr \u20ac{\ufffd~mo\u20ac l\u00deLk~~oww d6:f q{\ufffdzn \u20acsk~on\n\u20ac{\ufffd~mo\u20ac {qq\ufffdzntzr km~{\u20ac\u20ac ykz\u00de {q\ufffdsoy1 Go\u00de{zn \ufffdso\ufffd{|tm {qmwtyk\ufffdo mskzro. L~oow{z o\ufffdkw1\nd6;f |~o\u20acoz\ufffd kz{\ufffdo~\ufffdto\u00d0 {qs{\u00d0 \ufffdsontqqo~oz\ufffd {zwtzo km\ufffdt\ufffdt\u20acy \u20ac\ufffd~k\ufffdorto\u20ac {qwoq\ufffd/ kzn ~trs\ufffd/\u00d0tzr\nr~{\ufffd|\u20ac ykztqo\u20ac\ufffd ntqqo~oz\ufffd \ufffd\u00de|o\u20ac {qm{z\ufffdoz\ufffd kzn k\ufffdntozmo\u20ac1 \\so\u00de strswtrs\ufffd kvo\u00dentqqo~ozmo tz\n\ufffdso|o~mot\ufffdon \u20ac\ufffd~k\ufffdorto\u20ac {q\ufffdsontqqo~oz\ufffd r~{\ufffd|\u20ac1 Soq\ufffd/\u00d0tzr r~{\ufffd|\u20ac \ufffdk~ro\ufffd \ufffdsk\u20acs\ufffdkr km\ufffdt\ufffdt\u20acy\ufffd\nwokntzr \ufffd{\u20ac{mtkw |~{y{\ufffdt{z {qy{\ufffdoyoz\ufffd\u20ac \u00d0so~ok\u20ac ~trs\ufffd/\u00d0tzr r~{\ufffd|\u20ac ozrkro y\ufffdms y{~o\n~okntw\u00de \u00d0t\ufffds \u20ac\u00dey|k\ufffdso\ufffdtm yontk {~rkzt\u20ack\ufffdt{z\u20ac \ufffd{|~{y{\ufffdo \ufffdsot~ yo\u20ac\u20ackro\u20ac kzn r{kw\u20ac1 L~oow{z\no\ufffdkw1kw\u20ac{ ~om{rzt\u20aco k\u20actytwk~ \ufffd~ozn tzr~{\ufffd| m{so~ozmo \u00d0so~o ~trs\ufffd/\u00d0tzr r~{\ufffd|\u20ac k~o\ufffdtrs\ufffdo~\n\u00d0soz m{y|k~on \ufffd{woq\ufffd/\u00d0tzr r~{\ufffd|\u20ac q{~yon q~{y kw{{\u20aco m{kwt\ufffdt{z {qy\ufffdw\ufffdt|wo t\u20ac\u20ac\ufffdo/won r~{\ufffd|\u20ac1\nI{z\u20actno~tzr \ufffdst\u20acm{{~ntzk\ufffdon q\ufffdzntzr {qy\ufffdw\ufffdt|wo r~{\ufffd|\u20ac kzn ozrkroyoz\ufffd \u00d0t\ufffds yontk {~rkzt/\n\u20ack\ufffdt{z\u20ac. t\ufffdt\u20ac\ufffd{loo\u00f0|om\ufffdon \ufffdsk\ufffd mwtyk\ufffdo \u20acmo|\ufffdtm yo\u20ac\u20ackrtzr t\u20acwtvow\u00de \ufffd{loy{~o m{z\u20act\u20ac\ufffdoz\ufffd \ufffdskz\nm{y|o\ufffdtzr oz\ufffdt~{zyoz\ufffdkwt\u20ac\ufffd tzq{~yk\ufffdt{z1\nFw{zr \u00d0t\ufffds mwtyk\ufffdo mskzro. \u20ac\ufffd\ufffdnto\u20ac {qtzq{~yk\ufffdt{z/\u20acsk~tzr {z\u20ac{mtkw yontk sk\ufffdo oyl~kmon\nknt\ufffdo~\u20act\ufffd\u00de {qntqqo~oz\ufffd \ufffd{|tm\u20ac? [\ufffdk~lt~n d6<f \u20ac\ufffd\ufffdnton \ufffdso\u20ac|~okn {qyt\u20actzq{~yk\ufffdt{z k~{\ufffdzn yk\u20ac\u20ac\n\u20acs{{\ufffdtzr o\ufffdoz\ufffd\u20ac tz\ufffdso]zt\ufffdon [\ufffdk\ufffdo\u20ac. qtzntzr kmw\ufffd\u20ac\ufffdo~ {qkw\ufffdo~zk\ufffdt\ufffdo zo\u00d0\u20ac \u20act\ufffdo\u20ac \u20aco|k~k\ufffdo q~{y\nyktz\u20ac\ufffd~oky yontk \u20ac{\ufffd~mo\u20ac1 F~tq o\ufffdkw1d6=f kzn Jow^tmk~t{ o\ufffdkw1d5;f \u20ac\ufffd\ufffdn\u00de \ufffdso\u20ac|~okntzr\nlosk\ufffdt{\ufffd~ {q~\ufffdy{\ufffd~\u20ac kzn yt\u20actzq{~yk\ufffdt{z {z\\\u00d0t\ufffd\ufffdo~ kzn Lkmol{{v ~o\u20ac|om\ufffdt\ufffdow\u00de kzn mwk\u20ac\u20actq\u00de\n{l\u20aco~\ufffdon \ufffd~ozn\u20ac1 [msytn\ufffd o\ufffdkw1d73f kzkw\u00de\u20aco kzo\ufffd\u00d0{~v {qzo\u00d0\u20ac/~owk\ufffdon |kro\u20ac {zLkmol{{v.\n\u00d0so~o \ufffd\u00d0{|kro\u20ac k~om{zzom\ufffdon tq\ufffdso\u00de sk\ufffdo |{\u20ac\ufffd\u20ac \ufffdsk\ufffd k~owtvon {~m{yyoz\ufffdon {zl\u00de\ufffdso\u20ackyo\n\ufffd\u20aco~1 \\sot~ mw\ufffd\u20ac\ufffdo~ kzkw\u00de\u20act\u20ac ~o\ufffdokw\u20ac kstrsw\u00de |{wk~t\u20acon \u20ac\ufffd~\ufffdm\ufffd\ufffd~o. k\u20ac\u20acooz tzkz\ufffdylo~ {q{\ufffdso~\nm{z\ufffdo\u00f0\ufffd\u20ac *o1r1 d4=f+1 _twwtky\u20ac o\ufffdkw1d74f kw\u20ac{ \u20ac\ufffd\ufffdn\u00de \ufffdsom{yy\ufffdzt\ufffd\u00de \u20ac\ufffd~\ufffdm\ufffd\ufffd~o {q|{wt\ufffdtmkw zo\u00d0\u20ac/\n\u20acsk~tzr \ufffdtk\\\u00d0t\ufffd\ufffdo~. qtzntzr m{yy\ufffdzt\ufffdto\u20ac msk~km\ufffdo~t\u20acon l\u00del{\ufffds ro{r~k|stmkw kzn |{wt\ufffdtmkw qkm/\n\ufffd{~\u20ac1 _ok\ufffdo~ o\ufffdkw1d46f o\u00f0kytzon tzq{~yk\ufffdt{z/\u20acsk~tzr {z\\\u00d0t\ufffd\ufffdo~ n\ufffd~tzr \ufffdso]RMozo~kw Kwom/\n\ufffdt{z tz5349. \u20acs{\u00d0tzr \u20ac\ufffd~{zr m{yy\ufffdzt\ufffd\u00de \u20ac\ufffd~\ufffdm\ufffd\ufffd~o o\u00f0|wktzon l\u00detno{w{rtmkw. ro{r~k|stmkw kzn\n\ufffd{|tmkw |~oqo~ozmo\u20ac1 Kkms {q\ufffdso\u20aco \u20ac\ufffd\ufffdnto\u20ac \u20acs{\ufffdwn lom{z\u20actno~on tz\ufffdsom{z\ufffdo\u00f0\ufffd {q\ufffdso\ufffd\u00de|tmkw\n\u20acsk~o~ kzn \ufffdsotzq{~yk\ufffdt{z \ufffdso\u00de k~oo\u00f0|{\u20acon \ufffd{1U{\ufffd kww\ufffd\u20aco~\u20ac {z\u20ac{mtkw yontk k~oo\u00f0|{\u20acon \ufffd{\n\ufffdso\u20ackyo tzq{~yk\ufffdt{z. kzn \ufffd\u00de|tmkww\u00de \ufffdso\u00de k~oo\u00f0|{\u20acon \ufffd{tzq{~yk\ufffdt{z tz\u00d0stms \ufffdso\u00de sk\ufffdo\nkw~okn\u00de \u20acs{\u00d0z kztz\ufffdo~o\u20ac\ufffd \ufffds~{\ufffdrs \ufffdsot~ {\u00d0z nomt\u20act{z\u20ac {q\u00d0stms \ufffd\u20aco~\u20ac \ufffd{q{ww{\u00d0. kw{zr \u00d0t\ufffds\nkwr{~t\ufffdsytm qtw\ufffdo~tzr oqqom\ufffd\u20ac1 [sk~tzr tzq{~yk\ufffdt{z ~o}\ufffdt~o\u20ac km\ufffdt{z {z\ufffdso\ufffd\u20aco~)\u20ac |k~\ufffd kzn k\u20ac\u20ac\ufffdms\n\ufffds{\u20aco \u00d0s{ ms{{\u20aco \ufffd{\u20acsk~o k~owtvow\u00de |k~\ufffd {qky{~o strsw\u00de tz\ufffdo\u20ac\ufffdon \u20ac\ufffdl\u20aco\ufffd {q\ufffdso\ufffd\u20aco~\u20ac o\u00f0|{\u20acon\n\ufffd{kmo~\ufffdktz |tomo {qtzq{~yk\ufffdt{z1\n\\so ~o\u20ac\ufffd{q\ufffdso|k|o~ |~{moon\u20ac k\u20acq{ww{\u00d0\u20ac1 [om\ufffdt{z To\ufffds{n\u20ac no\ufffdktw\u20ac \ufffdsoyo\ufffds{n {qnk\ufffdk m{wwom/\n\ufffdt{z kzn |~o|k~k\ufffdt{z. kw{zr \u00d0t\ufffds \ufffdso\ufffdomszt}\ufffdo\u20ac \ufffd\u20acon \ufffd{m{z\u20ac\ufffd~\ufffdm\ufffd kzn kzkw\u00de\u20aco \ufffdsotzq{~yk/\n\ufffdt{z/\u20acsk~tzr zo\ufffd\u00d0{~v\u20ac1 V\ufffd~ yktz ~o\u20ac\ufffdw\ufffd\u20ac q{ww{\u00d0 tz[om\ufffdt{z Zo\u20ac\ufffdw\ufffd\u20ac. tzmw\ufffdntzr zo\ufffd\u00d0{~v kzkw\u00de\u20act\u20ac\nkzn msk~km\ufffdo~t\u20ack\ufffdt{z {qm{yy\ufffdzt\ufffd\u00de \u20ac\ufffd~\ufffdm\ufffd\ufffd~o\u20ac. kzn qtzkww\u00de [om\ufffdt{z Jt\u20acm\ufffd\u20ac\u20act{z |~{\ufffdtno\u20ac k\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 7259\n\ufffds{~{\ufffdrs nt\u20acm\ufffd\u20ac\u20act{z {q\ufffdsoyktz qtzntzr\u20ac {q\ufffdst\u20ac\u20ac\ufffd\ufffdn\u00de kzn |wkmo\u20ac \ufffdsoy tz\ufffdso\u00d0tno~ m{z\ufffdo\u00f0\ufffd {q\nyontk oqqom\ufffd\u20ac {z\ufffdsomwtyk\ufffdo mskzro nolk\ufffdo1 Fnnt\ufffdt{zkw nk\ufffdk kzn \ufffdt\u20ac\ufffdkwt\u20ack\ufffdt{z\u20ac mkzloq{\ufffdzn tz\n\ufffdso[4Ltwo1\nTo\ufffds{n\ufffd\n\\\u00d0oo\ufffd m{wwom\ufffdt{z kzn |~o/|~{mo\u20ac\u20actzr\n\\st\u20ac kzkw\u00de\u20act\u20ac \ufffd\u20aco\u20ac \u20aco\ufffdoz \u00d0oov\u20ac {q\\\u00d0t\ufffd\ufffdo~ nk\ufffdk m{wwom\ufffdon q~{y \ufffdso[\ufffd~okytzr FWO d75f1 \\\u00d0oo\ufffd\u20ac\nm{z\ufffdktztzr \ufffdso\u20ac\ufffd~tzr\u20acclimatechange {~globalwarming \u00d0o~o m{wwom\ufffdon lo\ufffd\u00d0ooz 534;/39/43\nkzn 534;/3:/5;. rt\ufffdtzr kztzt\ufffdtkw nk\ufffdk\u20aco\ufffd {q9.653. 733\ufffd\u00d0oo\ufffd\u20ac l\u00de4.=;9. 9=6\ufffd\u20aco~\u20ac1 Oz\u20ac|om\ufffdt{z\n\u20ac\ufffdrro\u20ac\ufffd\u20ac \ufffdsk\ufffd y{\u20ac\ufffd {q\ufffdsom{z\ufffdoz\ufffd m{yo\u20ac q~{y \ufffdso][kzn \ufffdso]R1\n\\so m{wwom\ufffdt{z |o~t{n mk|\ufffd\ufffd~o\u20ac kvo\u00deo\ufffdoz\ufffd tz\ufffdso\ufffdzq{wntzr mwtyk\ufffdo mskzro zk~~k\ufffdt\ufffdo. \ufffdso\nkzz{\ufffdzmoyoz\ufffd {z4\u20ac\ufffdQ\ufffdzo 534; l\u00de\ufffdsoz/][ W~o\u20actnoz\ufffd J{zkwn \\~\ufffdy| \ufffdsk\ufffd \ufffdso][F \u00d0{\ufffdwn\n\u00d0t\ufffdsn~k\u00d0 q~{y \ufffdsoWk~t\u20ac Fr~ooyoz\ufffd {zmwtyk\ufffdo mskzro yt\ufffdtrk\ufffdt{z1 \\st\u20ac o\ufffdoz\ufffd mk\ufffd\u20acon kwk~ro\n\u20ac|tvo {qkm\ufffdt\ufffdt\ufffd\u00de {z\\\u00d0t\ufffd\ufffdo~ k~{\ufffdzn \ufffdso\ufffd{|tm {qmwtyk\ufffdo mskzro tww\ufffd\u20ac\ufffd~k\ufffdon l\u00de[4Ltrtz[4Ltwo1\n\\{\u20ac\ufffd\ufffdn\u00de \ufffdst\u20aco\ufffdoz\ufffd tz\ufffdsom{z\ufffdo\u00f0\ufffd {qkw{zro~ |o~t{n {q\ufffdz{~ykw\ufffd km\ufffdt\ufffdt\ufffd\u00de. \ufffdsom{wwom\ufffdt{z \u20ac|kz\u20ac\n\u20aco\ufffdoz \u00d0oov\u20ac moz\ufffd~on {z\ufffdso\u00d0oov {qr~ok\ufffdo\u20ac\ufffd km\ufffdt\ufffdt\ufffd\u00de1 _o\u20aco|k~k\ufffdo \ufffdsonk\ufffdk\u20aco\ufffd tz\ufffd{ \u20aco\ufffdoz {zo/\n\u00d0oov tz\ufffdo~\ufffdkw\u20ac \ufffd{kmm{\ufffdz\ufffd q{~\ufffdso|{\ufffdoz\ufffdtkw \u00d0oovw\u00de |o~t{ntmt\ufffd\u00de tz\u20ac{mtkw yontk \ufffd\u20ackro kzn rt\ufffdo\u20ac\n\u20ac\ufffdqqtmtoz\ufffd \u20acky|wtzr noz\u20act\ufffd\u00de q{~~{l\ufffd\u20ac\ufffd zo\ufffd\u00d0{~v kzkw\u00de\u20aco\u20ac1\n\\st\u20ac \u20ac\ufffd\ufffdn\u00de q{m\ufffd\u20aco\u20ac {z\ufffdsontrt\ufffdkw yontk ozrkroyoz\ufffd kzn \u20acsk~tzr losk\ufffdt{\ufffd~ {q\\\u00d0t\ufffd\ufffdo~ \ufffd\u20aco~\u20ac\nnt\u20acm\ufffd\u20ac\u20actzr mwtyk\ufffdo mskzro1 \\{mk|\ufffd\ufffd~o \ufffdst\u20aclosk\ufffdt{\ufffd~. \ufffdsonk\ufffdk\u20aco\ufffd t\u20acqtw\ufffdo~on \ufffd{~oy{\ufffdo kww\n~o\ufffd\u00d0oo\ufffd\u20ac *\u00d0so~o k\ufffd\u20aco~ ~o|{\u20ac\ufffd\u20ac kz{~trtzkw \ufffd\u00d0oo\ufffd+ kzn }\ufffd{\ufffdo\u20ac *\u00d0so~o k\ufffd\u20aco~ ~o|{\u20ac\ufffd\u20ac kz{~trtzkw\n\ufffd\u00d0oo\ufffd \u00d0t\ufffds \ufffdsot~ {\u00d0z m{yyoz\ufffdk~\u00de |~o|oznon+1 \\so |\ufffd~|{\u20aco {q\ufffdst\u20acqtw\ufffdo~ t\u20ac\ufffd{q{m\ufffd\u20ac {z{~trtzkw\n\ufffd\u00d0oo\ufffd\u20ac. \u00d0stms k~ok\u20ac\u20ac\ufffdyon \ufffd{lo\ufffdso\u20ac\ufffd~{zro\u20ac\ufffd k\ufffdktwklwo yok\u20ac\ufffd~o {q\ufffd\u20aco~ ozrkroyoz\ufffd \u00d0t\ufffds m{z/\n\ufffdoz\ufffd? \ufffdsow{\u00d0oqq{~\ufffd m{\u20ac\ufffd {q~o\ufffd\u00d0oo\ufffdtzr kzn }\ufffd{\ufffdtzr yokz\u20ac \ufffdsk\ufffd \ufffd\u20aco~ ozrkroyoz\ufffd mkzz{\ufffd lo\ntzqo~~on k\u20acmwok~w\u00de q~{y \ufffdso\u20aco \ufffd\u00d0oo\ufffd \ufffd\u00de|o\u20ac1 \\so ~oyktztzr \ufffd\u00d0oo\ufffd\u20ac k~oq\ufffd~\ufffdso~ qtw\ufffdo~on \ufffd{~o\ufffdktz\n{zw\u00de \ufffds{\u20aco \u00d0stms m{z\ufffdktz kzoylonnon wtzv *]ZS+ \ufffd{\u00d0ol m{z\ufffdoz\ufffd1 \\so\u20aco wtzv\u20ac k~o\ufffdsontrt\ufffdkw\nyontk t\ufffdoy\u20ac \u20acsk~on l\u00de\ufffdso\ufffd\u00d0oo\ufffd k\ufffd\ufffds{~. tzmw\ufffdntzr zo\u00d0\u20ac k~\ufffdtmwo\u20ac. lw{r |{\u20ac\ufffd\u20ac. \ufffdtno{\u20ac kzn {\ufffdso~\nm{z\ufffdoz\ufffd1 [\ufffdms \ufffd\u00d0oo\ufffd\u20ac mkzlom{y|{\u20acon l\u00deykz\ufffdkw tz\u20aco~\ufffdt{z {qkwtzv {~mwtmvtzr \ufffdsoj\u20acsk~o) l\ufffd\ufffd/\n\ufffd{z{q\ufffdoz |~o\u20acoz\ufffdon kw{zr\u20actno {zwtzo zo\u00d0\u20ac m{z\ufffdoz\ufffd1 F||wtmk\ufffdt{z {qkww\ufffdsoqtw\ufffdo~\u20ac wok\ufffdo\u20ac knk\ufffdk\u20aco\ufffd\n{q9=5. <63\ufffd\u00d0oo\ufffd\u20ac m{z\ufffdktztzr ]ZS\u20ac l\u00de4=9. 467\ufffd\u20aco~\u20ac. l~{voz n{\u00d0z {\ufffdo~ \ufffdso;/\u00d0oov \u20ac\ufffd\ufffdn\u00de\n|o~t{n tz\\klwo 41T{\u20ac\ufffd z{\ufffdklw\u00de. \ufffdsomoz\ufffd~o |{tz\ufffd {q\ufffdsonk\ufffdk\u20aco\ufffd tz\u00d0oov 7tzmw\ufffdno\u20ac \ufffdso][tz\ufffdoz/\n\ufffdt{z \ufffd{\u00d0t\ufffdsn~k\u00d0 q~{y \ufffdsoWk~t\u20ac Fr~ooyoz\ufffd. |~{n\ufffdmtzr kn~kyk\ufffdtm \u20ac\ufffd~ro tz\\\u00d0t\ufffd\ufffdo~ km\ufffdt\ufffdt\ufffd\u00de1\n]ZS \ufffdkwtnk\ufffdt{z\nK\ufffdo~\u00de ]ZS q{\ufffdzn tz\ufffdso\ufffd\u00d0oo\ufffd nk\ufffdk\u20aco\ufffd \u00d0k\u20ac\ufffdkwtnk\ufffdon tzJomoylo~ 534; \ufffd{oz\u20ac\ufffd~o \ufffdsk\ufffd t\ufffd\u00d0k\u20ack\n\u00d0{~vtzr wtzv \ufffd{kztnoz\ufffdtqtklwo t\ufffdoy {q{zwtzo m{z\ufffdoz\ufffd1 \\st\u20ac \ufffdkwtnk\ufffdt{z \u20ac\ufffdo| \u00d0k\u20aczomo\u20ac\u20ack~\u00de q{~k\n\\klwo 41_oovw\u00de \ufffd\u00d0oo\ufffd\u20ac kzn \ufffdzt}\ufffdo \ufffd\u20aco~\u20ac tz{\ufffd~ qtw\ufffdo~on nk\ufffdk\u20aco\ufffd1 _oov 7.tz\u00d0stms \ufffdsoWk~t\u20ac Fr~ooyoz\ufffd kzz{\ufffdz mo/\nyoz\ufffd \u00d0k\u20acykno. tzmw\ufffdno\u20ac ykz\u00de y{~o \ufffd\u00d0oo\ufffd\u20ac \ufffdskz kz\u00de{\ufffdso~ \u00d0oov. kmm{\ufffdz\ufffdtzr q{~69& {qkww\ufffdso\ufffd\u00d0oo\ufffd\u20ac \u20ac\ufffd\ufffdnton1\n_oov V~trtzkw \\\u00d0oo\ufffd\u20ac ]zt}\ufffdo \ufffd\u20aco~\u20ac\n4 95.;6; 5;.7==\n5 :3.=55 64.;:=\n6 ;<.696 6<.:<6\n7 53=. :6; =:.93:\n9 :;.567 6:.:53\n: :9.57< 67.:59\n; 9<.:== 5<.96:\ns\ufffd\ufffd|\ufffd>22n {t1{~r243146;42u {\ufffd~zkw1|{z o13593:9:1\ufffd33 4\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 9259\nz\ufffdylo~ {q~ok\u20ac{z\u20ac1 Lt~\u20ac\ufffdw\u00de. okms ]ZS y\ufffd\u20ac\ufffd lokmmo\u20ac\u20actlwo \ufffd{kww{\u00d0 \u20ac\ufffdl\u20aco}\ufffdoz\ufffd kzkw\u00de\u20act\u20ac {qm{z/\n\ufffdoz\ufffd1 [om{znw\u00de. \ufffd\u00de|{r~k|stmkw o~~{~\u20ac l\u00de\ufffd\u00d0oo\ufffd k\ufffd\ufffds{~\u20ac k~o\u20ac{yo\ufffdtyo\u20ac tzm{~~om\ufffdw\u00de tz\ufffdo~|~o\ufffdon l\u00de\n\ufffdso\\\u00d0t\ufffd\ufffdo~ FWO k\u20ac]ZS\u20ac. \u20ac\ufffdms k\u20ac|o~t{n\u20ac q{ww{\u00d0on l\u00dekw|skz\ufffdyo~tm msk~km\ufffdo~\u20ac *hello.world+1\n\\st~nw\u00de. \ufffdso\ufffdkwtnk\ufffdt{z |~{mo\u20ac\u20ac skznwo\u20ac \u20acs{~\ufffdozon ]ZS\u20ac. \u00d0stms ~o|~o\u20acoz\ufffd \ufffdsoyku{~t\ufffd\u00de {q]ZS\u20ac\ntz\ufffd\u00d0oo\ufffd\u20ac1 Tkz\u00de \\\u00d0t\ufffd\ufffdo~ \ufffd\u20aco~\u20ac ykvo \ufffd\u20aco{q\ufffdst~n/|k~\ufffd\u00de ]ZS \u20acs{~\ufffdozo~\u20ac. \u20aco~\ufffdtmo\u20ac \u00d0stms m~ok\ufffdo k\n~ont~om\ufffd \ufffd{krt\ufffdoz w{zr ]ZS q~{y k\u20acs{~\ufffdo~ \ufffdo~\u20act{z \ufffdsk\ufffd mkz\ufffdsoz lo\ufffd\u20acon \ufffd{~on\ufffdmo \ufffdsomsk~km/\n\ufffdo~\u20aczoonon \ufffd{oylon ]ZS tzk\ufffd\u00d0oo\ufffd *\ufffdst\u20ac t\u20ac\ufffdzzomo\u20ac\u20ack~\u00de \u20actzmo \\\u00d0t\ufffd\ufffdo~ tzmw\ufffdno\u20ac t\ufffd\u20ac{\u00d0z \u20acs{~\ufffd/\noztzr \u20aco~\ufffdtmo \ufffdsk\ufffd ~on\ufffdmo\u20ac kz\u00de]ZS \ufffd{56msk~km\ufffdo~\u20ac+1 ]ZS \u20acs{~\ufffdozo~\u20ac k~okw\u20ac{ \u20ac{yo\ufffdtyo\u20ac \ufffd\u20acon\n\ufffd{m{zmokw \ufffdso\ufffdk~ro\ufffd ]ZS q~{y kz\u00de{zo \u00d0s{ yk\u00de |{\ufffdoz\ufffdtkww\u00de mwtmv {zt\ufffd1\\so\u20aco \u20acs{~\ufffd wtzv\u20ac y\ufffd\u20ac\ufffd\nlo\ufffd~kmon \ufffd{\ufffdsot~ no\u20ac\ufffdtzk\ufffdt{z l{\ufffds \ufffd{oz\u20ac\ufffd~o \ufffdso\u00de |{tz\ufffd \ufffdw\ufffdtyk\ufffdow\u00de \ufffd{k\ufffdkwtn ~o\u20ac{\ufffd~mo. kzn kw\u20ac{\n\ufffd{lo\ufffd~ok\ufffdon kw{zr\u20actno {\ufffdso~ wtzv\u20ac \ufffd{\ufffdso\u20ackyo no\u20ac\ufffdtzk\ufffdt{z1\nKkms ]ZS t\u20ac~o\u20ac{w\ufffdon tznt\ufffdtn\ufffdkww\u00de. yk~vtzr k\u20ac\ufffdkwtn {zw\u00de \ufffds{\u20aco \ufffdsk\ufffd ~o\ufffd\ufffd~z N\\\\W \u20ac\ufffdk\ufffd\ufffd\u20ac\nm{no 5aa tyyontk\ufffdow\u00de {~\ufffds~{\ufffdrs k\u20acykww z\ufffdylo~ {q|o~ykzoz\ufffd ~ont~om\ufffd\u20ac *634%634%444\n%5aa \u20ac\ufffdk\ufffd\ufffd\u20ac m{no\u20ac+1 \\so 5aa \u20ac\ufffdk\ufffd\ufffd\u20ac m{no\u20ac tzntmk\ufffdo \ufffdk~t{\ufffd\u20ac \u20ac\ufffdmmo\u20ac\u20acq\ufffdw {\ufffd\ufffdm{yo\u20ac \ufffd{kzN\\\\W\n~o}\ufffdo\u20ac\ufffd1\nTkz\u00de \ufffdkwtn ]ZS\u20ac tzmw\ufffdnon y{ntqto~\u20ac tz\u20aco~\ufffdon q{~\ufffd~kmvtzr kzn yo\ufffdknk\ufffdk |\ufffd~|{\u20aco\u20ac. \u00d0t\ufffds{\ufffd\ufffd\nkw\ufffdo~tzr \ufffdsono\u20ac\ufffdtzk\ufffdt{z |kro1 \\st\u20ac mkzwokn \ufffd{ykz\u00de ]ZS\u20ac |{tz\ufffdtzr \ufffd{\ufffdso\u20ackyo ~o\u20ac{\ufffd~mo. \u20ac{\ufffd{\nytztyt\u20aco \ufffdso|{\u20ac\u20actltwt\ufffd\u00de {qm{z\u20actno~tzr \ufffdso\u20aco k\u20acnt\u20ac\ufffdtzm\ufffd ]ZS\u20ac. kww\u20acmsoyo\u20ac kzn }\ufffdo~to\u20ac k~o\n~oy{\ufffdon q~{y \ufffdso~o\u20ac{w\ufffdon ]ZS\u20ac1 F\u20acykww z\ufffdylo~ {qn{yktz\u20ac \u00d0o~o kn\ufffdo~\u20acow\u00de kqqom\ufffdon l\u00de\ufffdst\u20ac1\nb{\ufffd\\\ufffdlo kzn {\ufffdso~ M{{rwo \u20aco~\ufffdtmo\u20ac \u00d0o~o \ufffdsoy{\u20ac\ufffd |~{ytzoz\ufffd. wk~row\u00de n\ufffdo\ufffd{\ufffdso\u20ac\ufffd~\ufffdm\ufffd\ufffd~o {q\n\ufffdsot~ wtzv\u20ac1 F\u20ack~o\u20ac\ufffdw\ufffd. {\ufffd~|~{mo\u20ac\u20ac n{o\u20ac z{\ufffdnt\u20ac\ufffdtzr\ufffdt\u20acs lo\ufffd\u00d0ooz ntqqo~oz\ufffd b{\ufffd\\\ufffdlo \ufffdtno{\u20ac\n\u00d0t\ufffdstz \ufffdsozo\ufffd\u00d0{~v\u20ac1\n\\so \ufffdkwtnk\ufffdt{z |~{mo\u20ac\u20ac ~o\u20ac\ufffdw\ufffd\u20ac tzkyk||tzr lo\ufffd\u00d0ooz okms ~k\u00d0]ZS *k\u20acrt\ufffdoz tzk\ufffd\u00d0oo\ufffd+\nkzn \ufffdso\ufffdkwtnk\ufffdon qtzkw no\u20ac\ufffdtzk\ufffdt{z kq\ufffdo~ kz\u00dekmmo|\ufffdklwo ~ont~om\ufffd |k\ufffds\u20ac1 Fz\u00de \ufffd\u00d0oo\ufffd m{z\ufffdktztzr k\ufffd\nwok\u20ac\ufffd {zo\ufffdz\ufffdkwtnk\ufffdon ]ZS t\u20ac~oy{\ufffdon q~{y \ufffdsonk\ufffdk\u20aco\ufffd1 Ltzkww\u00de. kww\ufffd\u20aco~\u20ac \u00d0s{ \ufffd\u00d0oo\ufffdon y{~o\n\ufffdskz 93\ufffdtyo\u20ac \u00d0t\ufffdstz krt\ufffdoz \u00d0oov k~o~oy{\ufffdon? \ufffdst\u20ac\ufffds~o\u20acs{wn t\u20acq{\ufffdzn \ufffd{lok\u20acoz\u20actlwo wtyt\ufffd \ufffd{\nyt\ufffdtrk\ufffdo \ufffdsoty|km\ufffd {qk\ufffd\ufffd{yk\ufffdon kmm{\ufffdz\ufffd\u20ac. o\u20ac|omtkww\u00de zo\u00d0\u20ac krr~ork\ufffd{~ kmm{\ufffdz\ufffd\u20ac1 Fm~{\u20ac\u20ac \ufffdso\n\u20aco\ufffdoz \u00d0oov\u20ac 5;4\ufffdzt}\ufffdo kmm{\ufffdz\ufffd\u20ac \u00d0o~o kqqom\ufffdon. wokntzr \ufffd{\ufffdso~oy{\ufffdkw {q::.<=5\ufffd\u00d0oo\ufffd\u20ac\n*k||~{\u00f0tyk\ufffdow\u00de 4&{q\ufffdso\ufffd{\ufffdkw nk\ufffdk\u20aco\ufffd+1 \\st\u20ac woq\ufffd579. 77:\ufffd\u00d0oo\ufffd\u20ac l\u00de446. 497\ufffd\u20aco~\u20ac \u20acsk~tzr 97.\n7:5nt\u20ac\ufffdtzm\ufffd. \ufffdkwtnk\ufffdon. ]ZS\u20ac km~{\u20ac\u20ac kww{q\ufffdso\u20aco\ufffdoz {zo/\u00d0oov \u00d0tzn{\u00d0\u20ac1\nUo\ufffd\u00d0{~v m{z\u20ac\ufffd~\ufffdm\ufffdt{z\nGt|k~\ufffdt\ufffdo zo\ufffd\u00d0{~v\u20ac. m{z\ufffdktztzr \ufffd\u00d0{z{no mwk\u20ac\u20aco\u20ac ~o|~o\u20acoz\ufffdtzr \ufffd\u20aco~\u20ac kzn ]ZS\u20ac. k~om{z\u20ac\ufffd~\ufffdm\ufffdon\nl\u00dem~ok\ufffdtzr kzonroi%j\u00d0sozo\ufffdo~ \ufffd\u20aco~i\u20acsk~o\u20ac ]ZSj.tww\ufffd\u20ac\ufffd~k\ufffdon l\u00deLtr41T\ufffdw\ufffdt|wo \u20acsk~o\u20ac {q\n]ZSjl\u00de\ufffd\u20aco~itzm~oyoz\ufffd \ufffdsoonro \u00d0otrs\ufffd \u20ac{\ufffdsk\ufffd \ufffdsoqtzkw lt|k~\ufffdt\ufffdo onro \u00d0otrs\ufffdw*i.j+~o|~o/\n\u20acoz\ufffd\u20ac \ufffdsoz\ufffdylo~ {q\ufffdtyo\u20ac \ufffd\u20aco~i\u20acsk~on ]ZSj1\nUo\u00f0\ufffd k\ufffdzt|k~\ufffdt\ufffdo |~{uom\ufffdt{z t\u20ac|~{n\ufffdmon q~{y \ufffdsolt|k~\ufffdt\ufffdo zo\ufffd\u00d0{~v. \u00d0so~ol\u00de kzo\ufffd\u00d0{~v {q\n{zw\u00de ]ZS z{no\u20ac t\u20acm~ok\ufffdon. \u00d0so~o onro\u20ac ozm{no \ufffdsoz\ufffdylo~ {q\ufffd\u20aco~\u20ac \ufffdsk\ufffd \u20acsk~on \ufffdsom{zzom\ufffdon\n|kt~ {q]ZS\u20ac1 \\so zo\ufffd\u00d0{~v m{z\u20ac\ufffd~\ufffdm\ufffdt{z |~{mo\u20ac\u20ac t\u20actww\ufffd\u20ac\ufffd~k\ufffdon tzLtr41\\so ]ZS\u20ac \u20acsk~on l\u00dek\n\u20actzrwo \ufffd\u20aco~ *Ltr 4k+q{~y kq\ufffdww\u00de m{zzom\ufffdon mwt}\ufffdo {qz{no\u20ac ~o|~o\u20acoz\ufffdtzr \ufffdso\u20acsk~tzr |k\ufffd\ufffdo~z q{~\n\ufffdsk\ufffd \ufffd\u20aco~ *Ltr 4l+1 \\so \u00d0s{wo \ufffdzt|k~\ufffdt\ufffdo zo\ufffd\u00d0{~v t\u20ac\ufffdsoz m{z\u20ac\ufffd~\ufffdm\ufffdon l\u00dem{y|{\u20actzr \ufffdsomwt}\ufffdo\u20ac\nq{~kww\ufffd\u20aco~\u20ac *Ltr 4m+1\\so |~{uom\ufffdt{z kww{\u00d0\u20ac \ufffdso\ufffd\u20aco{qoqqtmtoz\ufffd \ufffdzt|k~\ufffdt\ufffdo zo\ufffd\u00d0{~v kzkw\u00de\u20act\u20ac kwr{/\n~t\ufffdsy\u20ac kzn q{m\ufffd\u20aco\u20ac \ufffdsokzkw\u00de\u20act\u20ac {z\ufffdso~owk\ufffdt{z\u20acst|\u20ac lo\ufffd\u00d0ooz ]ZS\u20ac1 \\so \ufffdzt|k~\ufffdt\ufffdo zo\ufffd\u00d0{~v {q\n]ZS\u20ac ~o|~o\u20acoz\ufffd\u20ac \ufffdsom{wwom\ufffdt\ufffdo |k\ufffd\ufffdo~z {q\u20acsk~tzr {zwtzo m{z\ufffdoz\ufffd {q\ufffdso\ufffdk~ro\ufffdon \\\u00d0t\ufffd\ufffdo~ \ufffd\u20aco~\n|{|\ufffdwk\ufffdt{z1 [\ufffdk\ufffdt\u20ac\ufffdtm\u20ac q{~\ufffdso\u20aco\ufffdoz {zo/\u00d0oov zo\ufffd\u00d0{~v\u20ac m~ok\ufffdon k~ort\ufffdoz tz[4\\klwo tz[4Ltwo1\n_o~o\u20ac\ufffd~tm\ufffd {\ufffd~kzkw\u00de\u20aco\u20ac \ufffd{\ufffdsortkz\ufffd m{y|{zoz\ufffd \ufffd{k\ufffd{tn \ufffdsot\u20ac\u20ac\ufffdo {qzo\ufffd\u00d0{~v q~kryoz\ufffdk/\n\ufffdt{z kzn wtyt\ufffd t\ufffd\u20acty|km\ufffd {zm{yy\ufffdzt\ufffd\u00de no\ufffdom\ufffdt{z1 Ozkwwmk\u20aco\u20ac \ufffdsortkz\ufffd m{y|{zoz\ufffd m{z\u20act\u20ac\ufffd\u20ac {q\nk~{\ufffdzn \ufffd\u00d0{\ufffdst~n\u20ac {qkww]ZS z{no\u20ac tz\ufffdsozo\ufffd\u00d0{~v1 Vzo ty|{~\ufffdkz\ufffd no\u20actrz nomt\u20act{z ykno tz\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 :259\n\ufffdst\u20ac|~{mo\u20ac\u20ac t\u20acs{\u00d0 \ufffd\u20aco~ \u20acsk~o\u20ac k~oozm{non tz\ufffd{ \ufffdzt|k~\ufffdt\ufffdo onro \u00d0otrs\ufffd? ntqqo~oz\ufffd \u00d0otrs\ufffdtzr\n\u20acmsoyo\u20ac \u20actrztqtmkz\ufffdw\u00de ty|km\ufffd \ufffdso|o~q{~ykzmo {qm{yy\ufffdzt\ufffd\u00de no\ufffdom\ufffdt{z kwr{~t\ufffdsy\u20ac1 ]\u20aco~ m{z/\n\ufffd~tl\ufffd\ufffdt{z\u20ac \ufffd{\ufffdzt|k~\ufffdt\ufffdo onro \u00d0otrs\ufffd\u20ac k~o\u20acmkwon l\u00de\ufffdsoqkm\ufffd{~ 42*ki\u22124+.\u00d0so~okit\u20ac\ufffdso\ufffd{\ufffdkw\nz\ufffdylo~ {qk~\ufffdtmwo \u20acsk~o\u20ac l\u00de\ufffd\u20aco~i*tzmw\ufffdntzr ~o|ok\ufffdon wtzv\u20ac+. \u20ac\ufffdms \ufffdsk\ufffd \ufffdso\ufffd{\ufffdkw \ufffdzt|k~\ufffdt\ufffdo onro\n\u00d0otrs\ufffd m{z\ufffd~tl\ufffd\ufffdt{z l\u00deokms \ufffd\u20aco~ t\u20acki25*k\u20ac|~{|{\u20acon tzUo\u00d0ykz d76f+1 \\st\u20ac s\u00de|o~l{wtm\n\u00d0otrs\ufffdtzr \u20acmsoyo \u00d0k\u20acq{\ufffdzn \ufffd{kww{\u00d0 ~{l\ufffd\u20ac\ufffd ~om{\ufffdo~\u00de {qm{yy\ufffdzt\ufffd\u00de \u20ac\ufffd~\ufffdm\ufffd\ufffd~o\u20ac kq\ufffdo~ |~{uom/\n\ufffdt{z d77f1 _t\ufffds{\ufffd\ufffd \ufffdst\u20ac\u00d0otrs\ufffdtzr qkm\ufffd{~. k\ufffd\u20aco~)\u20ac onro \u00d0otrs\ufffd m{z\ufffd~tl\ufffd\ufffdt{z \ufffd{\ufffdso|~{uom\ufffdon zo\ufffd/\n\u00d0{~v t\u20ac}\ufffdkn~k\ufffdtm tz\ufffdsoz\ufffdylo~ {q]ZS\u20ac \u20acsk~on. \u20actzmo okms \ufffd\u20aco~ m~ok\ufffdo\u20acki*ki\u22124+25 onro\u20ac tz\n\ufffdso|~{uom\ufffdt{z. mk\ufffd\u20actzr \ufffd\u20aco~\u20ac \u00d0s{ \u20acsk~o ykz\u00de k~\ufffdtmwo\u20ac \ufffd{}\ufffdtmvw\u00de n{ytzk\ufffdo \ufffdsozo\ufffd\u00d0{~v kzn\n\u20ac\ufffdl\u20aco}\ufffdoz\ufffd kzkw\u00de\u20act\u20ac1\nI{y|\ufffd\ufffdtzr \ufffdso|~{uom\ufffdt{z ~o}\ufffdt~o\u20ac vz{\u00d0wonro {q\ufffdsoltknukmozm\u00de yk\ufffd~t\u00f0B{q\ufffdsortkz\ufffd\nm{y|{zoz\ufffd. \u00d0so~o \ufffdso~{\u00d0\u20ac ~o|~o\u20acoz\ufffd \ufffd\u20aco~\u20ac kzn \ufffdsom{w\ufffdyz\u20ac ~o|~o\u20acoz\ufffd ]ZS\u20ac? kzn kntkr{zkw\nyk\ufffd~t\u00f0D\u20ac\ufffdms \ufffdsk\ufffdDiiB42*ki\u22124+1Mt\ufffdoz \ufffdso\u20aco yk\ufffd~tmo\u20ac. \ufffdsoknukmozm\u00de yk\ufffd~t\u00f0 q{~\ufffdso\u00d0otrs\ufffdon\n\ufffd\u20aco~ |~{uom\ufffdt{z PBBTDBkzn sozmo \ufffdso\u00d0otrs\ufffd {qkzonro lo\ufffd\u00d0ooz ]ZS\u20acikznjtz\ufffdso|~{uom/\n\ufffdt{z\nw\u0085iCj\u0086\u0088d\nu9Usersw\u0085uCi\u0086w\u0085uCj\u0086\ndeg\u0085u\u0086\u00007B \u00854\u0086\nLtr41[msoyk\ufffdtm ntkr~ky {q\ufffdsolt|k~\ufffdt\ufffdo zo\ufffd\u00d0{~v m{z\u20ac\ufffd~ \ufffdm\ufffdt{z kzn \ufffdzt|k~\ufffdt\ufffdo |~{uom\ufffdt{z1 Kkms \ufffd\u20aco~ t\u20acm{zzom\ufffdon \ufffd{\ufffdso]ZS\u20ac \ufffdso\u00de \u20acsk~o tz\n\ufffdso\u20ac\ufffd\ufffdn\u00de \u00d0oov1 \\so \ufffdzt|k~\ufffdt\ufffdo |~{uom\ufffdt{z m~ok\ufffdo\u20ac onro\u20ac lo\ufffd\u00d0ooz \ufffd\u00d0{]ZS\u20ac \u00d0sozo\ufffdo ~\ufffdso\u00de k~o\u20acsk~on l\u00de\ufffdso\u20ackyo |o~\u20ac{z1 T\ufffdw\ufffdt|wo onro\u20ac tz\ufffdso\n|~{uom\ufffdt{ ztzntmk\ufffdo \ufffdsk\ufffd y\ufffdw\ufffdt|wo \ufffd\u20aco~\u20ac sk\ufffdo \u20acsk~on \ufffdso|kt~ {q]ZS\u20ac. kzn \ufffdst\u20actzq{~yk\ufffd t{zt\u20ac\ufffd~kmvon l\u00deonro \u00d0otrs\ufffd\u20ac1 F\ufffd\u20aco~)\u20ac onro m{z\ufffd~tl \ufffd\ufffdt{z\n\ufffd{\ufffdso|~{uom\ufffdt{ ztzm~ok\u20aco\u20ac }\ufffdkn~k\ufffdtm kww\u00de\u00d0t\ufffds \ufffdsoz\ufffdylo~ {q]ZS\u20ac \ufffdso\u00de \u20acsk~o. |{\ufffdoz\ufffdtkww\u00de wokntzr \ufffd{kn{ytzkzmo {qstrsw\u00de km\ufffdt\ufffdo \ufffd\u20aco~\u20ac tz\ufffdso\n\ufffdzt|k~ \ufffdt\ufffdo|~{uom\ufffdt{ z.q{~o\u00f0ky|wo ]\u20aco~ 6*~on+ tz\ufffdso|~{uom\ufffdt{ z?\ufffdst\u20act\u20acskznwon l\u00deks\u00de|o~l{wtm \u00d0otrs\ufffd tzr\u20acmsoyo *\u20acoo [om\ufffdt{z Uo\ufffd\u00d0{~v\nm{z\u20ac\ufffd~\ufffd m\ufffdt{z+1\ns\ufffd\ufffd|\ufffd>22 n{t1{~r243146;4 2u{\ufffd~zkw1|{zo1 3593:9:1r334\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 ;259\nI{yy\ufffdzt\ufffd\u00de no\ufffdom\ufffdt{z\nI{yy\ufffdzt\ufffd\u00de no\ufffdom\ufffdt{z \u00d0k\u20ac\ufffd\u20acon \ufffd{qtzn mw\ufffd\u20ac\ufffdo~\u20ac {qnoz\u20acow\u00de tz\ufffdo~m{zzom\ufffdon z{no\u20ac tz\ufffdsozo\ufffd/\n\u00d0{~v. \u00d0stms tz\ufffdst\u20acm{z\ufffdo\u00f0\ufffd ~o|~o\u20acoz\ufffd \u20aco\ufffd\u20ac{q]ZS\u20ac \u00d0stms \u00d0o~o \u20acooz l\u00dekr~{\ufffd| {q\u20actytwk~ \\\u00d0t\ufffd/\n\ufffdo~\ufffd\u20aco~\u20ac1 I{yy\ufffdzt\ufffd\u00de no\ufffdom\ufffdt{z d79f t\u20ackyokz\u20ac {qkwr{~t\ufffdsytmkww\u00de tnoz\ufffdtq\u00detzr \u20ac\ufffdms mw\ufffd\u20ac\ufffdo~\u20ac tz\nkrt\ufffdoz \ufffdzt|k~\ufffdt\ufffdo zo\ufffd\u00d0{~v1 \\st\u20ac \u20ac\ufffd\ufffdn\u00de \ufffd\u20acon kr~oon\u00de kwr{~t\ufffdsy |~{|{\u20acon l\u00deIwk\ufffd\u20aco\ufffd o\ufffdkw1d7:f\n\ufffdsk\ufffd |k~\ufffdt\ufffdt{z\u20ac z{no\u20ac tz\ufffd{ m{yy\ufffdzt\ufffdto\u20ac \u20ac\ufffdms \ufffdsk\ufffd \ufffdsoy{n\ufffdwk~t\ufffd\u00de {q\ufffdso|k~\ufffdt\ufffdt{z t\u20ac{|\ufffdtyt\u20acon?\ny{n\ufffdwk~t\ufffd\u00de yok\u20ac\ufffd~o\u20ac \ufffdso|~{|{~\ufffdt{z {qonro\u20ac \u00d0t\ufffdstz m{yy\ufffdzt\ufffdto\u20ac ~owk\ufffdt\ufffdo \ufffd{\ufffdso|~{|{~\ufffdt{z {q\nonro\u20ac lo\ufffd\u00d0ooz m{yy\ufffdzt\ufffdto\u20ac1 Oqkm{yy\ufffdzt\ufffd\u00de k\u20ac\u20actrzyoz\ufffd t\u20ac\u20actrztqtmkz\ufffdw\u00de lo\ufffd\ufffdo~ \ufffdskz ~kzn{y. k\ny{n\ufffdwk~t\ufffd\u00de \u20acm{~o lo\ufffd\u00d0ooz 316kzn 31;t\u20ac\ufffd\u00de|tmkww\u00de {l\u20aco~\ufffdon d79f1 T{n\ufffdwk~t\ufffd\u00de \u20acm{~o\u20ac q{~okms\nzo\ufffd\u00d0{~v k~ort\ufffdoz tz[4\\klwo tz[4Ltwo1\nI{z\ufffdoz\ufffd kzkw\u00de\u20act\u20ac\nWkro m{z\ufffdoz\ufffd \u00d0k\u20acm{wwom\ufffdon q{~okms \ufffdkwtnk\ufffdon ]ZS \ufffd\u20actzr Jtqql{\ufffd d7;f. kz{zwtzo \u20aco~\ufffdtmo q{~\no\u00f0\ufffd~km\ufffdtzr \ufffdsom{z\u20ac\ufffdt\ufffd\ufffdoz\ufffd |k~\ufffd\u20ac {qkzN\\TS n{m\ufffdyoz\ufffd kzn |~o\u20acoz\ufffdtzr \ufffdsoy tzkzok\u20actw\u00de kzk/\nw\u00de\u20acon q{~yk\ufffd1 Oz\u20ac{yo mk\u20aco\u20ac. Jtqql{\ufffd {zw\u00de tnoz\ufffdtqto\u20ac \ufffdso\ufffdt\ufffdwo kzn mkzz{\ufffd k\ufffd\ufffd{yk\ufffdtmkww\u00de no\ufffdom\ufffd\n\ufffdsom{z\ufffdoz\ufffd {qk\u00d0ol|kro1 \\{yt\ufffdtrk\ufffdo \ufffdsoty|km\ufffd {q\ufffdst\u20ac. \ufffdso\ufffdt\ufffdwo t\u20ac\ufffd\u20acon k\u20ac\ufffdso|kro m{z\ufffdoz\ufffd\nq{~\u20ac\ufffdms ]ZS\u20ac1 \\st\u20ac \u20ac\ufffdl\u20ac\ufffdt\ufffd\ufffd\ufffdt{z kqqom\ufffdon wo\u20ac\u20ac\ufffdskz 9&{q\ufffdso]ZS\u20ac tzokms \u00d0oov. o\u00f0mo|\ufffd _oov\n7\u00d0stms ~o}\ufffdt~on \u20ac\ufffdl\u20ac\ufffdt\ufffd\ufffd\ufffdt{z tz91=& {q\ufffdso]ZS\u20ac1 Fnnt\ufffdt{zkww\u00de. k\u20acykww z\ufffdylo~ {qn{yktz\u20ac\n\u00d0o~o tzm{y|k\ufffdtlwo \u00d0t\ufffds \ufffdsoJtqql{\ufffd FWO1 \\st\u20ac kmm{\ufffdz\ufffd\u20ac q{~k~{\ufffdzn 7&{qkww]ZS\u20ac tz\ufffdsortkz\ufffd\nm{y|{zoz\ufffd\u20ac {qkz\u00de\u00d0oov kzn y{\u20ac\ufffdw\u00de k~t\u20aco n\ufffdo\ufffd{|kro q{~yk\ufffd\ufffdtzr {~\ufffdtyo{\ufffd\ufffd t\u20ac\u20ac\ufffdo\u20ac1\nVzwtzo m{z\ufffdoz\ufffd \u00d0k\u20ackzkw\u00de\u20acon }\ufffdkz\ufffdt\ufffdk\ufffdt\ufffdow\u00de l\u00demkwm\ufffdwk\ufffdtzr \ufffdo~y q~o}\ufffdozm\u00de/tz\ufffdo~\u20aco n{m\ufffd/\nyoz\ufffd q~o}\ufffdozm\u00de *\\L/OJL+ \u20acm{~o\u20ac. \ufffd~ok\ufffdtzr okms ]ZS k\u20ackn{m\ufffdyoz\ufffd kzn \ufffdso\u20aco\ufffd{qkww]ZS\u20ac tz\n\ufffdso\u00d0s{wo zo\ufffd\u00d0{~v k\u20ac\ufffdsom{~|\ufffd\u20ac1 \\L/OJL kzkw\u00de\u20aco\u20ac kty \ufffd{tnoz\ufffdtq\u00de nt\u20ac\ufffdtzm\ufffdt\ufffdo {~ty|{~\ufffdkz\ufffd\n\ufffd{voz\u20ac *\ufffd\u20ac\ufffdkww\u00de \u00d0{~n\u20ac+ tzkn{m\ufffdyoz\ufffd. lk\u20acon {z\ufffdsot~ q~o}\ufffdozm\u00de tz\ufffdson{m\ufffdyoz\ufffd ~owk\ufffdt\ufffdo \ufffd{\n\ufffdsot~ q~o}\ufffdozm\u00de km~{\u20ac\u20ac \ufffdsom{~|\ufffd\u20ac1 \\s~oo vtzn\u20ac {q\ufffd{voz \u00d0o~o \u20ac\ufffd\ufffdnton tz\ufffdst\u20ac\u00d0k\u00de. tz\ufffds~oo \u20aco|k/\n~k\ufffdo kzkw\u00de\u20aco\u20ac> \u00d0ol n{yktz\u20ac. |kro m{z\ufffdoz\ufffd \ufffdztr~ky\u20ac. kzn |kro m{z\ufffdoz\ufffd ltr~ky\u20ac1 _ol n{yktz\u20ac\n\u00d0o~o o\u00f0\ufffd~km\ufffdon q{~okms ]ZS \ufffd{|o~yt\ufffd kzkw\u00de\u20act\u20ac {q\ufffdso\u20ac{\ufffd~mo\u20ac {qm{z\ufffdoz\ufffd \u20acsk~on {z\\\u00d0t\ufffd\ufffdo~1\n_{~n\u20ac kzn ltr~ky\u20ac *\ufffd\u00d0{/\u00d0{~n |s~k\u20aco\u20ac+ \u00d0o~o o\u00f0\ufffd~km\ufffdon q~{y \ufffdso\u00d0ol |kro\u20ac k\u20ac\u20ac{mtk\ufffdon \u00d0t\ufffds\nokms ]ZS \ufffd{kww{\u00d0 wk~ro/\u20acmkwo kzkw\u00de\u20act\u20ac {q\ufffdso\ufffd{|tm\u20ac \u00d0t\ufffdstz \ufffdsom{z\ufffdoz\ufffd1 Goq{~o mkwm\ufffdwk\ufffdtzr \ufffdso\n\\L/OJL \ufffdom\ufffd{~\u20ac q{~|kro m{z\ufffdoz\ufffd. \ufffdso[z{\u00d0lkww \u20ac\ufffdoyyo~ d7<f \u00d0k\u20ack||wton \ufffd{okms m{z\ufffdoz\ufffd\n\ufffd{voz1 Kkms \u20ac\ufffdoyyon \ufffd{voz t\u20acyk||on lkmv \ufffd{\ufffdsoy{\u20ac\ufffd m{yy{z \ufffd{voz \ufffdsk\ufffd yk|\u20ac \ufffd{t\ufffdq{~\nok\u20aco {q~okntzr. o1r1tqqt\u20acso~. qt\u20acson kzn qt\u20acstzr kwwk||ok~ {zmo kzn qt\u20acso\u20ac k||ok~\u20ac \ufffd\u00d0tmo \ufffdsoz\n\ufffdsoqtzkw ~o|~o\u20acoz\ufffdk\ufffdt{z {q\ufffdso\u20aco \ufffd{voz\u20ac t\u20acqt\u20acso\u20ac1\n\\L/OJL \u20acm{~o \ufffdom\ufffd{~\u20ac q{~okms ]ZS k~omkwm\ufffdwk\ufffdon kzn ~o|~o\u20acoz\ufffd \ufffdsoq~o}\ufffdozm\u00de {q\ufffd{voz\u20ac tz\nkn{m\ufffdyoz\ufffd ~owk\ufffdt\ufffdo \ufffd{\ufffdsot~ q~o}\ufffdozm\u00de km~{\u20ac\u20ac \ufffdsom{~|\ufffd\u20ac1 L{~\ufffd{vozttzn{m\ufffdyoz\ufffd d\u00d0osk\ufffdo\nTF-IDF\u0085tCd\u0086\u0088tf\u0085tCd\u0086log7\u0087n\n7\u0087df\u0085t\u0086\ufffd\ufffd\n\u00877\ufffd \ufffd\nC \u00855\u0086\n\u00d0so~ont\u20ac\ufffdsoz\ufffdylo~ {qn{m\ufffdyoz\ufffd\u20ac tz\ufffdsom{~|\ufffd\u20ac.tf*t.d+m{\ufffdz\ufffd\u20ac \ufffdsoq~o}\ufffdozm\u00de {qttzdkzn\ndf*t+ m{\ufffdz\ufffd\u20ac \ufffdsoz\ufffdylo~ {qn{m\ufffdyoz\ufffd\u20ac tz\ufffdsom{~|\ufffd\u20ac \u00d0stms m{z\ufffdktzt1\\{msk~km\ufffdo~t\u20aco okms\nm{yy\ufffdzt\ufffd\u00de q{\ufffdzn tz\ufffdso\u20acsk~tzr zo\ufffd\u00d0{~v. \ufffdso\\L/OJL \ufffdom\ufffd{~\u20ac q{~okms ]ZS tzkm{yy\ufffdzt\ufffd\u00de\n\u00d0o~o \u20ac\ufffdyyon \ufffd{{l\ufffdktz krr~ork\ufffdo \u20acm{~o\u20ac1 Oz\ufffdso\u20aco m{yy\ufffdzt\ufffd\u00de/wo\ufffdow m{y|k~t\u20ac{z\u20ac. kstrs\n\\L/OJL \u20acm{~o q{~km{yy\ufffdzt\ufffd\u00de yokz\u20ac \ufffdsk\ufffd \ufffdsort\ufffdoz \ufffd{voz k||ok~\u20ac y{~o q~o}\ufffdoz\ufffdw\u00de tz\ufffdso\n]ZS\u20ac tz\ufffdst\u20acm{yy\ufffdzt\ufffd\u00de. \u00d0soz m{y|k~on \ufffd{{\ufffdso~ m{yy\ufffdzt\ufffdto\u20ac1 Ozokms mk\u20aco. \ufffd{voz\u20ac \u00d0stms\n{mm\ufffd~ tzqo\u00d0o~ \ufffdskz 93]ZS\u20ac. {~y{~o \ufffdskz 93& {qkww]ZS\u20ac tzkrt\ufffdoz \u00d0oov k~o~ouom\ufffdon. \u20ac\ufffdms\n\ufffdsk\ufffd \ufffdo~\u00de m{yy{z {~\ufffdo~\u00de ~k~o \ufffd{voz\u20ac k~o{yt\ufffd\ufffdon1 _okw\u20ac{ ~oy{\ufffdon k\u20aco\ufffd{qm{yy{z \u20ac\ufffd{|/\n\u00d0{~n\u20ac tzmw\ufffdntzr trump.paris kznagreement1 Oz|~tzmt|wo. n/r~ky\u20ac mkzlo\u20ac\ufffd\ufffdnton \u00d0t\ufffds kz\u00de\n\ufffdkw\ufffdo {qn.l\ufffd\ufffdlo\u00de{znnB5\ufffd{voz q~o}\ufffdozm\u00de t\u20acrozo~kww\u00de \ufffd{{w{\u00d0\ufffd{lo\ufffd\u20acoq\ufffdw1\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 <259\nOno{w{rtmkw m{ntzr {q\u20ac{\ufffd~mo n{yktz\u20ac\n\\{o\u00f0kytzo tno{w{rtmkw |{\u20act\ufffdt{ztzr {q|{|\ufffdwk~ \u20ac{\ufffd~mo n{yktz\u20ac kw{zr k\u00f0o\u20ac {q|{wt\ufffdtmkw wokztzr\n*q~{y woq\ufffd2|~{r~o\u20ac\u20act\ufffdo \ufffd{~trs\ufffd2m{z\u20aco~\ufffdk\ufffdt\ufffdo+ kzn mwtyk\ufffdo \u20acmo|\ufffdtmt\u20acy *q~{y oz\ufffdt~{zyoz\ufffdkwt\u20ac\ufffd \ufffd{\n\u20acmo|\ufffdtm+. :5{q\ufffdso;9y{\u20ac\ufffd m{yy{zw\u00de \u20acsk~on \u20ac{\ufffd~mo n{yktz\u20ac km~{\u20ac\u20ac \ufffdsooz\ufffdt~o \u20ac\ufffd\ufffdn\u00de |o~t{n\n\u00d0o~o ykz\ufffdkww\u00de m{non q{~tno{w{rtmkw ltk\u20ac *wt\u20ac\ufffdon tz[7\\klwo tz[4Ltwo+1 Ono{w{r\u00de o\u00f0|~o\u20ac\u20acon tz\nk~\ufffdtmwo\u20ac q~{y okms n{yktz \u00d0k\u20acr~knon {zk\ufffds~oo/|{tz\ufffd \u20acmkwo q{~|{wt\ufffdtmkw {|tzt{z *Soq\ufffd/Uo\ufffd/\n\ufffd~kw/Ztrs\ufffd+ kzn mwtyk\ufffdo mskzro {|tzt{z *Kz\ufffdt~{zyoz\ufffdkwt\u20ac\ufffd/Uo\ufffd\ufffd~kw/ [mo|\ufffdtm+. \u00d0t\ufffds kzknnt\ufffdt{zkw\nz\ufffdww ~k\ufffdtzr *]zmwok~+ knnon \ufffd{l{\ufffds \u20acmkwo\u20ac q{~mk\u20aco\u20ac \u00d0so~o z{mwok~ tno{w{r\u00de \u00d0k\u20ac\u20acooz1\nF\ufffdoky {q\u20act\u00f0s\ufffdykz m{no~\u20ac \ufffd\u20acon \ufffdst\u20ac\u20acmkwo \ufffd{tzno|oznoz\ufffdw\u00de \u20acm{~o \ufffdo\u00f0\ufffdo\u00f0\ufffd~km\ufffd\u20ac {qk~\ufffdtmwo\u20ac2\nm{z\ufffdoz\ufffd q~{y \ufffdsoy{\u20ac\ufffd m{yy{zw\u00de \u20acsk~on n{yktz\u20ac1 L{~\ufffdso;9n{yktz\u20ac \u00d0t\ufffds \ufffdsoy{\u20ac\ufffd \u20acsk~o\u20ac\nkm~{\u20ac\u20ac \ufffdso\u20aco\ufffdoz/\u00d0oov |o~t{n. \u00d0o\u20acky|won \ufffdso|kro m{z\ufffdoz\ufffd q{~qt\ufffdoy{\u20ac\ufffd \u20acsk~on k~\ufffdtmwo\u20ac *\ufffd\u20actzr\nJtqql{\ufffd \ufffd{o\u00f0\ufffd~km\ufffd mwokz \ufffdo\u00f0\ufffd. k\u20acno\u20acm~tlon kl{\ufffdo+1 Oqqo\u00d0o~ \ufffdskz qt\ufffdok~\ufffdtmwo\u20ac q~{y k|{|\ufffdwk~\nn{yktz \u00d0o~o k\ufffdktwklwo tz\ufffdsonk\ufffdk\u20aco\ufffd. \ufffdsoz kww\u00d0o~o tzmw\ufffdnon1 Oz\u20ac{yo mk\u20aco\u20ac. \ufffdso|kro q{~yk\ufffd\u20ac\n\u00d0o~o tzm{y|k\ufffdtlwo \u00d0t\ufffds \ufffdsoJtqql{\ufffd FWO. ~o\ufffd\ufffd~ztzr z{m{z\ufffdoz\ufffd q{~43n{yktz\u20ac *\u20acoo [7\\klwo tz\n[4Ltwoq{~\ufffdsoo\u00f0mw\ufffdnon n{yktz\u20ac+1 _oknnt\ufffdt{zkww\u00de o\u00f0mw\ufffdnon \ufffdso\u20ac{mtkw yontk \u20act\ufffdo\u20acTwitter.\nWordpress kznReddit? \ufffdso\u20aco n{z{\ufffdsk\ufffdo ont\ufffd{~tkw m{z\ufffd~{w kzn \ufffdso~oq{~o wkmv k\ufffdztqton tno{w{rt/\nmkw|{\u20act\ufffdt{z1 \\st\u20ac woq\ufffdk\u20aco\ufffd{q:5n{yktz\u20ac \ufffd{lom{non1 Kkms o\u00f0\ufffd~km\ufffd m{z\u20act\u20ac\ufffdon {q\ufffd|\ufffd{\ufffds~oo\nm{y|wo\ufffdo |k~kr~k|s\u20ac *{qk\ufffdwok\u20ac\ufffd 63\u00d0{~n\u20ac+ q~{y \ufffdsowtzvon \u00d0ol |kro \ufffdo\u00f0\ufffd1 \\{wtyt\ufffd kz\u00de\u20ac\ufffdluom/\n\ufffdt\ufffdt\ufffd\u00de k~t\u20actzr q~{y \ufffdsom{no~\u20ac) |o~\u20ac{zkw |o~\u20ac|om\ufffdt\ufffdo\u20ac. okms o\u00f0\ufffd~km\ufffd \u00d0k\u20ackz{z\u00deyt\u20acon *t1o1\u20ac{\ufffd~mo\nn{yktz kzn k\ufffd\ufffds{~ tzq{~yk\ufffdt{z \u00d0o~o ~oy{\ufffdon+ \u00d0soz |~o\u20acoz\ufffdon q{~m{ntzr1\nI{no~\u20ac \u00d0o~o |~{\ufffdtnon \u00d0t\ufffds \ufffdsoq{ww{\u00d0tzr noqtzt\ufffdt{z\u20ac \ufffd{sow| ykvo \ufffdsot~ k\u20ac\u20aco\u20ac\u20acyoz\ufffd\u20ac>\n\u02ddLeft> Fwoq\ufffd/\u00d0tzr \u20ac\ufffdkzmo mkzlomsk~km\ufffdo~t\u20acon l\u00de\ufffdso|~{y{\ufffdt{z {q\u20ac\ufffdk\ufffdo lozoqt\ufffd\u20ac kzn \u20aco~\ufffdtmo\u20ac.\n|\ufffdlwtm tz\ufffdo\u20ac\ufffdyoz\ufffd tzkzn ~or\ufffdwk\ufffdt{z {q|~t\ufffdk\ufffdo l\ufffd\u20actzo\u20ac\u20aco\u20ac. tzm~ok\u20acon \ufffdk\u00f0k\ufffdt{z {qm{~|{~k\ufffdt{z\u20ac\nkzn strs ok~zo~\u20ac. kzn \u20ac\ufffd||{~\ufffd q{~\u00d0{~vo~\u20ac kzn \ufffd~kno \ufffdzt{z\u20ac1\n\u02ddRight> F~trs\ufffd/\u00d0tzr \u20ac\ufffdkzmo mkzlomsk~km\ufffdo~t\u20acon l\u00de|~{y{\ufffdtzr w{\u00d0\ufffdk\u00f0k\ufffdt{z kzn ytztyt\u20actzr\n\ufffdsotz\ufffdo~qo~ozmo {qr{\ufffdo~zyoz\ufffd tz|o~\u20ac{zkw kzn l\ufffd\u20actzo\u20ac\u20ac wt\ufffdo\u20ac1 W\ufffdlwtm tz\ufffdo\u20ac\ufffdyoz\ufffd t\u20acytzt/\nyt\u20acon. tzqk\ufffd{\ufffd~ {qkww{\u00d0tzr yk~vo\ufffd q{~mo\u20ac \ufffd{m{z\ufffd~{w r~{\u00d0\ufffds kzn |~{\ufffdt\u20act{z {q\u20aco~\ufffdtmo\u20ac1\n\u02ddEnvironmentalist> Fzoz\ufffdt~{zyoz\ufffdkwt\u20ac\ufffd \u20ac\ufffdkzmo \u20ac\ufffd||{~\ufffd\u20ac \ufffdso\u20acmtoz\ufffdtqtm m{z\u20acoz\u20ac\ufffd\u20ac {zkz\ufffds~{/\n|{roztm mwtyk\ufffdo mskzro kzn |~{y{\ufffdo\u20ac tyyontk\ufffdo km\ufffdt{z l\u00der{\ufffdo~zyoz\ufffd\u20ac kzn tznt\ufffdtn\ufffdkw\u20ac \ufffd{\nyt\ufffdtrk\ufffdo \ufffdsoq\ufffd\ufffd\ufffd~o ty|km\ufffd\u20ac1\n\u02ddSceptic> Fmwtyk\ufffdo \u20acmo|\ufffdtm \u20ac\ufffdkzmo {||{\u20aco\u20ac \ufffdso\u20acmtoz\ufffdtqtm m{z\u20acoz\u20ac\ufffd\u20ac {zkz\ufffds~{|{roztm mwtyk\ufffdo\nmskzro1 [\ufffdms {||{\u20act\ufffdt{z \ufffdk~to\u20ac q~{y }\ufffdo\u20ac\ufffdt{ztzr \ufffdsoo\u00f0t\u20ac\ufffdozmo {~mk\ufffd\u20aco\u20ac {qmwtyk\ufffdo mskzro.\n\ufffd{{||{\u20actzr oqq{~\ufffd\u20ac \ufffd{yt\ufffdtrk\ufffdo t\ufffd\u20acty|km\ufffd\u20ac1\nKkms m{no~ k\u20ac\u20actrzon k\u20acm{~o \ufffd{okms k~\ufffdtmwo o\u00f0\ufffd~km\ufffd tzno|oznoz\ufffdw\u00de1 \\sk\ufffd t\u20ac.\u20act\u00f0m{no~\u20ac rozo~/\nk\ufffdon \ufffd|\ufffd{qt\ufffdo\u20acm{~o\u20ac q{~okms {q:5n{yktz\u20ac *tz|~km\ufffdtmo. \ufffdst\u20acky{\ufffdz\ufffdon \ufffd{4:=< \u20acm{~o\u20ac {\ufffd\ufffd{qk\nyk\u00f0ty\ufffdy {q4<:3. \u00d0t\ufffds okms n{yktz ~omot\ufffdtzr \ufffd|\ufffd{63\u20acm{~o\u20ac+1 Kkms k~\ufffdtmwo \u20acm{~o t\u20ackztz\ufffdoro~\nnoz{\ufffdtzr \ufffdsowo\ufffdow {qtno{w{rtmkw ltk\u20ac \ufffdsom{no~ {l\u20aco~\ufffdo\u20ac tz\ufffdsk\ufffd k~\ufffdtmwo */42324 q{~woq\ufffd2zo\ufffd\ufffd~kw2\n~trs\ufffd {~oz\ufffdt~{zyoz\ufffdkwt\u20ac\ufffd2zo\ufffd\ufffd~kw2\u20acmo| \ufffdtm.tz{~no~. \u00d0t\ufffds \ufffdzmwok~ \u20acm{~o\u20ac trz{~on? \u20acoo[6\n\\klwo tz[4Ltwo+1 \\so k~\ufffdtmwo \u20acm{~o\u20ac k\u20ac\u20actrzon l\u00deokms m{no~ \u00d0o~o \ufffdsoz k\ufffdo~kron q{~okms n{yktz\n\ufffd{no\ufffdo~ytzo kz{\ufffdo~kww n{yktz \u20acm{~o q{~\ufffdsk\ufffd m{no~1 Kkms n{yktz \u20acm{~o t\u20ack~okw/\ufffdkw\ufffdon z\ufffdy/\nlo~tz\ufffdso~kzro d\u22124. 4f?\ufffdso~o k~o:\u20acm{~o\u20ac q{~okms n{yktz. {zoq~{y okms m{no~1\n[tzmo \ufffdsok\u20ac\u20actrzyoz\ufffd {qtno{w{rtmkw ltk\u20ac t\u20ac\u20ac{yo\u00d0sk\ufffd \u20ac\ufffdluom\ufffdt\ufffdo. \u00d0o|o~q{~yon kzknu\ufffd\u20ac\ufffd/\nyoz\ufffd \ufffd{\ufffdson{yktz \u20acm{~o\u20ac k\u20ac\u20actrzon l\u00deokms m{no~1 \\st\u20ac knu\ufffd\u20ac\ufffdyoz\ufffd z{~ykwt\u20acon okms n{yktz\n\u20acm{~o l\u00de\u20ac\ufffdl\ufffd~km\ufffdtzr \ufffdsoyokz n{yktz \u20acm{~o q{~\ufffdsk\ufffd m{no~ km~{\u20ac\u20ac kww:5n{yktz\u20ac. \ufffdsoz nt\ufffdtntzr\nl\u00de\ufffdso\u20ac\ufffdkznk~n no\ufffdtk\ufffdt{z1 \\st\u20ac |~{mo\u20ac\u20ac ~o|~o\u20acoz\ufffd\u20ac n{yktz \u20acm{~o\u20ac k\u20acz/\u20acm{~o\u20ac kzn z{~ykwt\u20acon\nq{~\u20ac\ufffdluom\ufffdt\ufffdo ltk\u20ac {qtznt\ufffdtn\ufffdkw m{no~\u20ac1 Ltzkww\u00de. \u00d0ok\ufffdo~kro \ufffdsoz{~ykwt\u20acon n{yktz \u20acm{~o\u20ac\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 =259\nkm~{\u20ac\u20ac kwwm{no~\u20ac \ufffd{qtzn k\u20actzrwo n{yktz \u20acm{~o1 \\st\u20ac |~{mo\u20ac\u20ac \u00d0k\u20ack||wton q{~l{\ufffds |{wt\ufffdtmkw kzn\nmwtyk\ufffdo mskzro tno{w{r\u00de1 [tzmo u\ufffdnryoz\ufffd {qmwtyk\ufffdo mskzro tno{w{r\u00de ~owto\u20ac {z\u20ac{yo vz{\u00d0wonro\n{qmwtyk\ufffdo mskzro *o1r1 \ufffdso\u20acmtoz\ufffdtqtm m{z\u20acoz\u20ac\ufffd\u20ac+ \ufffdsom{no~\u20ac \u00d0o~o ~om~\ufffdt\ufffdon q~{y \ufffdso|{\u20ac\ufffdr~kn\ufffd/\nk\ufffdo~o\u20acok~ms m{yy\ufffdzt\ufffd\u00de tz\u20acmtozmo\u20ac k\ufffd\ufffdso]zt\ufffdo~\u20act\ufffd\u00de {qK\u00f0o\ufffdo~1\nL\ufffdww ~o\u20ac\ufffdw\ufffd\u20ac q{~\ufffdsom{ntzr {qokms n{yktz k~o|~o\u20acoz\ufffdon tz[5Ltrtz[4Ltwo1 \\{\ufffdo\u20ac\ufffd\ufffdso~owt/\nkltwt\ufffd\u00de {q{\ufffd~m{ntzr o\u00f0o~mt\u20aco \u00d0om{y|k~o {\ufffd~r~kno\u20ac \ufffd{\ufffdson{yktz k\u20ac\u20aco\u20ac\u20acyoz\ufffd\u20ac l\u00deTontk\nGtk\u20ac2Lkm\ufffd Isomv d7=f *TGLI+. kqkm\ufffd/msomvtzr kzn yontk ltk\u20ac \u20act\ufffdo1 _o\ufffd~kz\u20acwk\ufffdo \ufffdsoTGLI ~k\ufffd/\ntzr\u20ac \ufffd{{\ufffd~{\u00d0z \u20acmkwo kzn k||w\u00de \ufffdso\u20ackyoz/\u20acm{~o z{~ykwt\u20ack\ufffdt{z. \ufffdsoz mkwm\ufffdwk\ufffdon \ufffdsom{~~owk\ufffdt{z\n{q\ufffdso|{wt\ufffdtmkw ltk\u20ac \u20acm{~o\u20ac l\u00de{\ufffd~m{no~\u20ac krktz\u20ac\ufffd \ufffds{\u20aco q~{y TGLI q{~\ufffds{\u20aco n{yktz\u20ac \u00d0stms\nsk\ufffdo looz ~k\ufffdon l\u00deTGLI1 _oqtzn \u20ac\ufffd~{zr |{\u20act\ufffdt\ufffdo m{~~owk\ufffdt{z *Wok~\u20ac{z)\u20ac rB31<.pD43\u2212:.\nnB7=+noy{z\u20ac\ufffd~k\ufffdtzr \ufffdsk\ufffd \ufffdso|{wt\ufffdtmkw m{ntzr |~{mo\u20ac\u20ac t\u20acm{z\u20act\u20ac\ufffdoz\ufffd \u00d0t\ufffds \ufffdsoo\u00f0|o~\ufffd k\u20ac\u20aco\u20ac\u20ac/\nyoz\ufffd\u20ac |~{\ufffdtno l\u00deTGLI1 U{o}\ufffdt\ufffdkwoz\ufffd o\u00f0\ufffdo~zkw k\u20ac\u20aco\u20ac\u20acyoz\ufffd\u20ac k~ok\ufffdktwklwo q{~mwtyk\ufffdo mskzro\ntno{w{r\u00de. l\ufffd\ufffd\ufffdso~{l\ufffd\u20ac\ufffd |o~q{~ykzmo {q{\ufffd~m{ntzr |~{mo\u20ac\u20ac q{~|{wt\ufffdtmkw ltk\u20ac rt\ufffdo\u20ac m{zqtnozmo\ntz\ufffdsoyo\ufffds{n{w{r\u00de1 [ooLtr5kkzn 5lq{~\ufffdsok\ufffdo~kro ~o\u20ac\ufffdw\ufffd\u20ac km~{\u20ac\u20ac n{yktz\u20ac kzn m{no~\u20ac q{~\n|{wt\ufffdtmkw kzn mwtyk\ufffdo mskzro ltk\u20ac ~o\u20ac|om\ufffdt\ufffdow\u00de1\nI{y|k~t\u20ac{z\u20ac {\ufffdo~ \ufffdtyo\n\\{yok\u20ac\ufffd~o \ufffdsomskzro tz|k\ufffd\ufffdo~z\u20ac {q\u20acsk~tzr mwtyk\ufffdo yontk {\ufffdo~ \ufffdtyo. \ufffdso|kt~\u00d0t\u20aco \u20actytwk~t\ufffd\u00de\n{q\ufffdso\u20aco\ufffd\u20ac{q\ufffd\u20aco~\u20ac. k~\ufffdtmwo\u20ac *]ZS\u20ac+ kzn \u20ac{\ufffd~mo n{yktz\u20ac \u00d0k\u20acmkwm\ufffdwk\ufffdon q{~\ufffdso\u20aco\ufffdoz \u00d0oov\u20ac tz\n\ufffdso\u20ac\ufffd\ufffdn\u00de |o~t{n1 Fzk\u20ac\u00deyyo\ufffd~tm \u20actytwk~t\ufffd\u00de yok\u20ac\ufffd~o t\u20acnoqtzon \ufffd{rt\ufffdo kztzntmk\ufffdt{z {q|o~\u20act\u20ac/\n\ufffdozmo lo\ufffd\u00d0ooz \u00d0oov\u20ac kzn tnoz\ufffdtq\u00de tzqw\ufffd\u00f0o\u20ac {qzo\u00d0 |k~\ufffdtmt|kz\ufffd\u20ac {~m{z\ufffdoz\ufffd1 \\st\u20ac o\u00f0|~o\u20ac\u20act{z\nm{y|k~o\u20ac \u00d0oov\u20acAkznBl\u00de\ufffdsoq~km\ufffdt{z {q\ufffdso\ufffd\u20aco~\u20ac.u.wtzv\u20ac.l.{~n{yktz\u20ac. d.\ufffdsk\ufffd k||ok~on tz\n\u00d0oovA\ufffdsk\ufffd kw\u20ac{ k||ok~on tz\u00d0oovB1Oz{~no~ \ufffd{kmm{\ufffdz\ufffd q{~\ufffdso~o|ok\ufffdon \ufffd\u20ackro m{yy{z tz\n{zwtzo \u20ac{mtkw zo\ufffd\u00d0{~v\u20ac. kyok\u20ac\ufffd~o {qs{\u00d0 ykz\u00de \ufffdtyo\u20ac k\ufffd\u20aco~. ]ZS {~n{yktz k||ok~\u20ac tzokms\n{q\ufffdso\ufffd\u00d0{\u00d0oov\u20ac t\u20actzmw\ufffdnon l\u00de\ufffd\u20actzr y\ufffdw\ufffdt\u20aco\ufffd\u20ac q{~u.lkznd1\nSu\nACB\u0088yuAiuBy\nyuAyCSl\nACB\u0088ylAilBy\nylAyCSd\nACB\u0088ydAidBy\nydAyB\u00856\u0086\n^kw\ufffdo\u20ac {q\ufffdso\u20actytwk~t\ufffd\u00de yok\u20ac\ufffd~o qkww\u00d0t\ufffdstz \ufffdso~kzro d3.4f1^kw\ufffdo\u20ac k||~{kmstzr 3\u20actrztq\u00de\n\ufffdsk\ufffd \ufffdo~\u00de qo\u00d0{q\ufffdso\ufffd\u20aco~\u20ac *~o\u20ac|om\ufffdt\ufffdow\u00de ]ZS\u20ac. n{yktz\u20ac+ tz\u00d0oovAk~okw\u20ac{ |~o\u20acoz\ufffd tz\u00d0oovB.\n\u00d0so~ok\u20ac \ufffdkw\ufffdo\u20ac k||~{kmstzr 4\u20actrztq\u00de \ufffdsk\ufffd zok~w\u00de kww{q\ufffdso\ufffd\u20aco~\u20ac *~o\u20ac|om\ufffdt\ufffdow\u00de ]ZS\u20ac. n{yktz\u20ac+\ntz\u00d0oovAk~okw\u20ac{ |~o\u20acoz\ufffd tz\u00d0oovB1U{\ufffdo \ufffdsk\ufffd l\u00deno\u20actrzSA.B=\u0088SB.Aq{~A=\u0088Btzrozo~kw1\nZo\ufffd\ufffdw\ufffd\ufffd\n\\st\u20ac \u20acom\ufffdt{z nt\ufffdtno\u20ac {\ufffd~~o\u20ac\ufffdw\ufffd\u20ac tz\ufffd{ \ufffds~oo yktz qtzntzr\u20ac1 Oz\ufffdsoqt~\u20ac\ufffd |k~\ufffd \u00d0oq{m\ufffd\u20ac {z_oov 7\n{q\ufffdso\u20ac\ufffd\ufffdn\u00de |o~t{n. tz\u00d0stms \ufffdso][\u00d0t\ufffdsn~k\u00d0kw q~{y \ufffdsoWk~t\u20ac Fr~ooyoz\ufffd \u00d0k\u20ackzz{\ufffdzmon. \ufffd{\nnoy{z\u20ac\ufffd~k\ufffdo \ufffdsol~{kn tno{w{rtmkw |{wk~t\u20ack\ufffdt{z {l\u20aco~\ufffdon tz\ufffdsotzq{~yk\ufffdt{z/\u20acsk~tzr zo\ufffd\u00d0{~v\u20ac1\n[om{znw\u00de. \u00d0omsk~km\ufffdo~t\u20aco \ufffdso\u20ac\ufffdl/m{yy\ufffdzt\ufffdto\u20ac \ufffdsk\ufffd ykvo \ufffd|\ufffdsozo\ufffd\u00d0{~v. \u20acs{\u00d0tzr \ufffdsk\ufffd \u20aco\ufffd/\no~kwwtzvon woq\ufffd/\u00d0tzr2oz\ufffdt~{zyoz\ufffd kwt\u20ac\ufffd m{yy\ufffdzt\ufffdto\u20ac m{/o\u00f0t\u20ac\ufffd \u00d0t\ufffds k\u20actzrwo ~trs\ufffd/\u00d0tzr2\u20acmo|\ufffdtm\nm{yy\ufffdzt\ufffd\u00de1 Ltzkww\u00de. \u00d0ow{{v k\ufffdkww\u20aco\ufffdoz \u00d0oov\u20ac tz\ufffdso\u20ac\ufffd\ufffdn\u00de |o~t{n \ufffd{o\u00f0|w{~o s{\u00d0 zo\ufffd\u00d0{~v\n\u20ac\ufffd~\ufffdm\ufffd\ufffd~o kzn |{wk~t\u20ack\ufffdt{z mskzro\u20ac {\ufffdo~ \ufffdtyo1\nIwtyk\ufffdo yontk \u20acsk~tzr t\u20ac|{wk~t\u20acon kzn |{wt\ufffdtmt\u20acon\n_olortz l\u00demsk~km\ufffdo~t\u20actzr \ufffdsotzq{~yk\ufffdt{z/\u20acsk~tzr zo\ufffd\u00d0{~v n\ufffd~tzr \ufffdsomoz\ufffd~kw \u00d0oov {q\ufffdso\n\u20ac\ufffd\ufffdn\u00de |o~t{n. _oov 7.tz\u00d0stms \ufffdso][\u00d0t\ufffdsn~k\u00d0kw q~{y \ufffdsoWk~t\u20ac Fr~ooyoz\ufffd \u00d0k\u20ackzz{\ufffdzmon1\nJ\ufffd~tzr \ufffdst\u20ac\u00d0oov. ;.7=:]ZS\u20ac \u00d0o~o \u20acsk~on l\u00de75.446\\\u00d0t\ufffd\ufffdo~ \ufffd\u20aco~\u20ac1 Fq\ufffdo~ |~{uom\ufffdt{z \ufffdst\u20ac|~{/\nn\ufffdmo\u20ac k\ufffdzt|k~\ufffdt\ufffdo zo\ufffd\u00d0{~v {q;.7=:]ZS\u20ac m{zzom\ufffdon l\u00de43;. 637onro\u20ac tzntmk\ufffdtzr \u00d0stms |kt~\u20ac\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 43259\n{q]ZS\u20ac \u00d0o~o m{/\u20acsk~on1 Ltr6\u20acs{\u00d0\u20ac \ufffdso]ZS m{/\u20acsk~o zo\ufffd\u00d0{~v {q\ufffdsoqt\ufffdowk~ro\u20ac\ufffd m{yy\ufffdzt/\n\ufffdto\u20acl\u00de\ufffd{\ufffdkw \u20acsk~o m{\ufffdz\ufffd1 \\so wk\u00de{\ufffd\ufffd t\u20acno\ufffdo~ytzon l\u00de\ufffdsoL{~moF\ufffdwk\u20ac5 kwr{~t\ufffdsy d93f. \u00d0stms\nr~{\ufffd|\u20ac noz\u20acow\u00de m{zzom\ufffdon z{no\u20ac \ufffd{ro\ufffdso~? \ufffdt\u20ac\ufffdkw tz\u20ac|om\ufffdt{z \u20acs{\u00d0\u20ac kmwok~ |k~\ufffdt\ufffdt{z tz\ufffd{ \ufffd\u00d0{\nwk~ro mw\ufffd\u20ac\ufffdo~\u20ac1 Fwr{~t\ufffdsytm m{yy\ufffdzt\ufffd\u00de no\ufffdom\ufffdt{z ~o\ufffdokw\u20ac q\ufffd~\ufffdso~ |k~\ufffdt\ufffdt{ztzr \u00d0t\ufffdstz \ufffdsowk~ro~\nmw\ufffd\u20ac\ufffdo~ {q\ufffdsozo\ufffd\u00d0{~v. tww\ufffd\u20ac\ufffd~k\ufffdon l\u00de\ufffdsontqqo~oz\ufffd z{no m{w{\ufffd~\u20ac1 G\u00dem{z\u20actno~tzr \ufffdsom{z\ufffd~k\u20ac\ufffdtzr\n\u20ac\ufffd~\ufffdm\ufffd\ufffd~o\u20ac q{\ufffdzn l\u00de\ufffdsom{yy\ufffdzt\ufffd\u00de no\ufffdom\ufffdt{z kzn zo\ufffd\u00d0{~v wk\u00de{\ufffd\ufffd kwr{~t\ufffdsy\u20ac. \ufffdso~o t\u20aco\ufffdtnozmo\n{qy\ufffdw\ufffdt|wo wk\u00deo~\u20ac {q\u20ac\ufffd~\ufffdm\ufffd\ufffd~o \u00d0t\ufffdstz \ufffdsozo\ufffd\u00d0{~v1 U{\ufffdo \ufffdsk\ufffd \ufffdsowk~ro \u00deoww{\u00d0 z{no \u20aco|k~k\ufffdon\nLtr51F\ufffdo~kro n{yktz tno{w{rt mkw|{\u20act\ufffdt{ z\u20ack\u20ac\u20actrzon l\u00deokms m{no~ kzn \ufffdso{\ufffdo~kww k\ufffdo~kro km~{\u20ac\u20ac kwwm{no~\u20ac1 ^kw\ufffdo\u20ac \u20acs{\ufffdwn lo\ufffds{\ufffdrs\ufffd\n{qk\u20ac\u20ac\ufffdkznk~n no\ufffdtk\ufffdt{z\u20ac q~{y \ufffdsoyokz1 ^o~\ufffdtmkw lk~\u20ac tzntmk\ufffdo\u00b1{zo\u20ac\ufffdkznk~n no\ufffdtk\ufffdt{z km~{\u20ac\u20ac \ufffdsom{no~\u20ac1\ns\ufffd\ufffd|\ufffd>22n{ t1{~r243146;42u {\ufffd~zkw1|{z o13593:9:1r3 35\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 44259\nq~{y \ufffdso\ufffd\u00d0{yktz mw\ufffd\u20ac\ufffdo~\u20ac t\u20acyoutube.com/watch1 \\st\u20ac z{no t\u20acntqqtm\ufffdw\ufffd \ufffd{tz\ufffdo~|~o\ufffd k\u20act\ufffdkrr~o/\nrk\ufffdo\u20ac ykz\u00de b{\ufffd\\\ufffdlo \ufffdtno{ wtzv\u20ac. \u20acooytzrw\u00de q~{y kww\u20actno\u20ac {q\ufffdsonolk\ufffdo1 _omkzz{\ufffd \ufffdo~tq\u00de \ufffdst\u20ac\n\u00d0t\ufffds {\ufffd~\ufffdo\u00f0\ufffd/lk\u20acon m{z\ufffdoz\ufffd kzkw\u00de\u20act\u20ac1\n\\so~o t\u20ack\u20ac\ufffd~{zr m{~~owk\ufffdt{z lo\ufffd\u00d0ooz {|tzt{z\u20ac kl{\ufffd\ufffd mwtyk\ufffdo mskzro kzn |{wt\ufffdtmkw tno{w{r\u00de\no\u00f0|~o\u20ac\u20acon tz\u20acsk~on m{z\ufffdoz\ufffd1 Ltr7|w{\ufffd\u20ac \ufffdso|{\u20act\ufffdt{z {q\ufffdso:5\u00d0ol n{yktz\u20ac {zk\u00f0o\u20ac {qmwtyk\ufffdo\nltk\u20ac kzn |{wt\ufffdtmkw ltk\u20ac. lk\u20acon {zm{z\ufffdoz\ufffd {qk~\ufffdtmwo\u20ac m{non l\u00de\ufffdso|kzow1 Soq\ufffd/~trs\ufffd |{wt\ufffdtmkw tno{w/\n{r\u00dekzn oz\ufffdt~{zyoz\ufffdkwt\u20ac\ufffd/\u20ac mo|\ufffdtm mwtyk\ufffdo {|tzt{z\u20ac k~o\ufffdo~\u00de \u20ac\ufffd~{zrw\u00de m{~~owk\ufffdon *Wok~\u20ac{z)\u20ac\nrB31<:+ kzn \ufffdo~\u00de qo\u00d0n{yktz\u20ac k||ok~ tz\ufffdso~trs\ufffd/\u00d0tzr2oz\ufffdt~{zyoz\ufffdk wt\u20ac\ufffdkzn woq\ufffd/\u00d0tzr2\u20acmo|/\n\ufffdtm}\ufffdkn~kz\ufffd\u20ac1\nTk||tzr tno{w{rto\u20ac2{|tzt{z\u20ac k\u20ac\u20ac{mtk\ufffdon \u00d0t\ufffds \u00d0ol n{yktz\u20ac {z\ufffd{ \ufffdsotzq{~yk\ufffdt{z/\u20acsk~tzr\nzo\ufffd\u00d0{~v \u20ac\ufffd~\ufffdm\ufffd\ufffd~o \u20acs{\u00d0\u20ac kzk\u20ac\u20ac{mtk\ufffdt{z lo\ufffd\u00d0ooz zo\ufffd\u00d0{~v |{\u20act\ufffdt{z kzn \ufffdto\u00d0|{tz\ufffd1 Ltr9\u20acs{\u00d0\u20ac\n\ufffdsozo\ufffd\u00d0{~v ntkr~ky q~{y Ltr6\u00d0t\ufffds ]ZS/z{no m{w{\ufffd~\u20ac kw\ufffdo~on \ufffd{\u20acs{\u00d0 ltk\u20aco\u20ac tz\ufffdso|{wt\ufffdtmkw2\nmwtyk\ufffdo {|tzt{z\u20ac o\u00f0|~o\u20ac\u20acon l\u00de\ufffdsot~ \u00d0ol n{yktz\u20ac1 Ltr9km{w{\ufffd~\u20ac \ufffdsoz{no\u20ac \ufffd{strswtrs\ufffd \ufffdso\nwoq\ufffd/~trs\ufffd |{wt\ufffdtmkw ltk\u20ac1 \\so woq\ufffd/skzn mw\ufffd\u20ac\ufffdo~ m{z\ufffdktz\u20ac |~on{ytzkz\ufffdw\u00de woq\ufffd/\u00d0tzr \u20act\ufffdo\u20ac1 \\so ~trs\ufffd/\nskzn mw\ufffd\u20ac\ufffdo~ sk\u20ackstrs m{zmoz\ufffd~k\ufffdt{z {q~trs\ufffd/\u00d0tzr \u20act\ufffdo\u20ac. l\ufffd\ufffdkw\u20ac{ sk\u20acykz\u00de \ufffdzm{non \u20act\ufffdo\u20ac1 F\n\u20actytwk~ |k\ufffd\ufffdo~z mkzlo\u20acooz tzLtr9l.\u00d0stms m{w{\ufffd~\u20ac z{no\u20ac l\u00demwtyk\ufffdo mskzro ltk\u20ac. \u00d0t\ufffds \ufffdso\nwoq\ufffd/skzn mw\ufffd\u20ac\ufffdo~ |~on{ytzkz\ufffdw\u00de oz\ufffdt~{zyoz\ufffdkwt\u20ac\ufffd kzn \ufffdso~trs\ufffd/skzn mw\ufffd\u20ac\ufffdo~ m{z\ufffdktztzr y{\u20ac\ufffd\nLtr61Ozq{~yk\ufffd t{z/\u20acsk~tzr zo\ufffd\u00d0{~v\u20ac k~o|{wk~t\u20acon1 Ww{\ufffd \u20acs{\u00d0\u20ac \ufffdso]ZS m{/\u20acsk~tz rzo\ufffd\u00d0{~v q{~\ufffdso\u00d0oov tz\u00d0stms \ufffdso][\u00d0t\ufffdsn~k\u00d0kw\nq~{y \ufffdsoWk~t\u20ac Fr~ooyoz\ufffd \u00d0k\u20ackzz{\ufffdzmon *_oov 7{q\ufffdso\u20ac\ufffd\ufffdn\u00de |o~t{n+1 \\so qt\ufffdowk~ro\u20ac\ufffd m{yy\ufffdzt\ufffdt o\u20acl\u00de\ufffd{\ufffdkw \u20acsk~o m{\ufffdz\ufffd k~ont\u20ac|wk\u00deon\n*:;1:=& {q;.7=:z{no\u20ac+1 I{yy\ufffdzt \ufffdto\u20ac4\u22129k~om{w{\ufffd~on lw\ufffdo. \u00deoww{\u00d0. r~ooz. ~onkzn |\ufffd~|wo ~o\u20ac|om\ufffdt\ufffdow\u00de kzn z{no \u20act\u00feot\u20ac|~{|{~\ufffdt {zkw\n\ufffd{\ufffdso\u20ac}\ufffdk~o ~{{\ufffd {q\ufffd{\ufffdkw \u20acsk~o m{\ufffdz\ufffd1 U{no |wkmoyoz\ufffd \ufffd\u20aco\u20ac kq{~mo/nt ~om\ufffdon kwr{~t\ufffdsy d93f \u00d0stms r~{\ufffd|\u20ac noz\u20acow\u00de m{zzom\ufffdon z{no\u20ac\n\ufffd{ro\ufffdso~? \ufffdst\u20acwk\u00de{\ufffd\ufffd strswtrs\ufffd\u20ac \ufffd\u00d0{wk~ro mw\ufffd\u20ac\ufffdo~\u20ac. \u00d0t\ufffds q{\ufffd~ m{yy\ufffdzt\ufffdt o\u20ac{z\ufffdsowoq\ufffdkzn k\u20actzrwo m{yy\ufffdzt\ufffd\u00de {z\ufffdso~trs\ufffd1\ns\ufffd\ufffd|\ufffd>22n{t1{ ~r243146;42u {\ufffd~zkw1|{zo 13593:9:1r336\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 45259\n{q\ufffdso\u20acmo|\ufffdtm n{yktz\u20ac1 \\kvoz \ufffd{ro\ufffdso~. \ufffdso\u20aco qtzntzr\u20ac noy{z\u20ac\ufffd~k\ufffdo \u20actrztqtmkz\ufffd \u20ac\ufffd~ozr\ufffds {q\n|{wk~t\u20ack\ufffdt{z kzn |{wt\ufffdtmt\u20ack\ufffdt{z tztzq{~yk\ufffdt{z/\u20acsk~tzr kl{\ufffd\ufffd mwtyk\ufffdo mskzro {z\\\u00d0t\ufffd\ufffdo~. \u00d0t\ufffds\n\ufffd\u00d0{wk~ro mw\ufffd\u20ac\ufffdo~\u20ac {q\ufffd\u20aco~\u20ac kzn tzq{~yk\ufffdt{z \u20ac{\ufffd~mo\u20ac. l~{knw\u00de msk~km\ufffdo~t\u20acon k\u20ackwoq\ufffd/\u00d0tzr2oz\ufffdt/\n~{zyoz\ufffdkwt\u20ac\ufffd r~{\ufffd| kzn k~trs\ufffd/\u00d0tzr2\u20acmo|\ufffdtm r~{\ufffd|1\nIsk~km\ufffdo~t\u20ack\ufffdt{z {qtzq{~yk\ufffdt{z/\u20acsk~tzr m{yy\ufffdzt\ufffdto\u20ac\n\\so yktz \u20ac{\ufffd~mo n{yktz\u20ac kzn tzntmk\ufffdt\ufffdo m{z\ufffdoz\ufffd {qyontk k~\ufffdtmwo\u20ac \u20acsk~on \u00d0t\ufffdstz \ufffdsoqt\ufffdowk~r/\no\u20ac\ufffdm{yy\ufffdzt\ufffdto\u20ac tz_oov 7mkzlo\u20acooz tzLtr:.tz\u00d0stms \ufffdso~kntt {q\ufffdsomt~mwo\u20ac \u20acs{\u00d0 \ufffdso~owk/\n\ufffdt\ufffdo\u20act\u00feo\u20ac {q\ufffdso\u20aco yku{~ m{yy\ufffdzt\ufffdto\u20ac1 No~o I{yy\ufffdzt\ufffdto\u20ac 4.6.7kzn 9k~om{yy\ufffdzt\ufffdto\u20ac\n\u00d0t\ufffdstz \ufffdsowoq\ufffd/\u00d0tzr2oz\ufffdt~{zyoz\ufffdkwt \u20ac\ufffdmw\ufffd\u20ac\ufffdo~ tz\ufffdsotzq{~yk\ufffdt{z/\u20acsk~tzr zo\ufffd\u00d0{~v. \u00d0so~ok\u20ac\nI{yy\ufffdzt\ufffd\u00de 5t\u20ac\ufffdso\u20actzrwo m{yy\ufffdzt\ufffd\u00de tz\ufffdso~trs\ufffd/\u00d0tzr2\u20acmo|\ufffdtm mw\ufffd\u20ac\ufffdo~1 Ltr:\ufffd\u20aco\u20ac k\\L/OJL\n\u00d0otrs\ufffdtzr \u20acmsoyo q{~okms \ufffd{voz. \u20ac\ufffdms \ufffdsk\ufffd \ufffdso|~{ytzoz\ufffd \ufffd{voz\u20ac k~o\ufffds{\u20aco \ufffdsk\ufffd k~omsk~km\ufffdo~/\nt\u20ac\ufffdtm {qk|k~\ufffdtm\ufffdwk~ m{yy\ufffdzt\ufffd\u00de \u00d0soz m{y|k~on \ufffd{\ufffdsozo\ufffd\u00d0{~v k\u20ack\u00d0s{wo1 [{\ufffd~mo n{yktz\u20ac k~o\n\u20acs{\u00d0z tzLtr:kkzn m{z\ufffdoz\ufffd t\u20ac\u20acs{\u00d0z tzLtr:l*\ufffdztr~ky\u20ac+ kzn Ltr:m*ltr~ky\u20ac+. \ufffd{kww{\u00d0 k\nmsk~km\ufffdo~t\u20ack\ufffdt{z {q\ufffdsol~{kn \ufffdsoyo\u20ac tzokms m{yy\ufffdzt\ufffd\u00de tz\ufffdo~y\u20ac {qro{r~k|stm q{m\ufffd\u20ac. |{wt\ufffdtmkw\n{~mwtyk\ufffdo \u20acmtozmo ltk\u20aco\u20ac kzn vo\u00de\u20ac\ufffdluom\ufffd\u20ac {qtz\ufffdo~o\u20ac\ufffd1 \\klwo 5\u20ac\ufffdyyk~t\u20aco\u20ac \ufffdso\u20aco m{yy\ufffdzt\ufffdto\u20ac\n\ufffd\u20actzr \ufffdsonk\ufffdk q~{y Ltr:1\nS{{vtzr k\ufffd\u20ac{\ufffd~mo n{yktz\u20ac *Ltr :k+.ntqqo~oz\ufffd ro{r~k|stm kzn |{wt\ufffdtmkw ltk\u20aco\u20ac mkzlo\ntzqo~~on lk\u20acon {zok~wto~ kzkw\u00de\u20act\u20ac {qn{yktz tno{w{r\u00de1 I{yy\ufffdzt\ufffd\u00de 5qok\ufffd\ufffd~o\u20ac |~on{ytzkz\ufffdw\u00de\n~trs\ufffd/\u00d0tzr \u20ac{\ufffd~mo\u20ac. \u00d0so~ok\u20ac I{yy\ufffdzt\ufffdto\u20ac 4\u20196/9m{z\ufffdktz m{z\ufffdoz\ufffd q~{y woq\ufffd/\u00d0tzr \u20ac{\ufffd~mo\u20ac1\nLtr71Iwtyk\ufffdo yontk m{z\ufffdoz\ufffd t\u20ac|{wt\ufffdtmt\u20acon1 Tokz |{wt\ufffdtmk wtno{w{r\u00de *woq\ufffd/\ufffd{/~trs\ufffd+ kzn mwtyk\ufffdo {|tzt{z\n*oz\ufffdt~{zy oz\ufffdkwt\u20ac\ufffd/\ufffd{/ \u20acmo|\ufffdtm+ o\u00f0|~o\u20ac\u20acon tzm{z\ufffdoz\ufffd q~{y \ufffdso:5m{non \u00d0ol n{yktz\u20ac {\ufffdo~ \ufffdso\u20act\u00f0m{no~\u20ac *\u20acoo Ono{w{rtmkw\nm{ntzr {q\u20ac{\ufffd~mo n{yktz\u20ac+ 1W{tz\ufffd \u20act\u00feot\u20ac|~{|{~\ufffdt{zk w\ufffd{\ufffdso\u20ac}\ufffdk~o ~{{\ufffd {q\ufffd{\ufffdkw \u20acsk~o m{\ufffdz\ufffd kzn wtzo\u20ac tzntmk\ufffdo\u00b1{zo\n\u20ac\ufffdkznk~n no\ufffdtk\ufffdt{z1 Sklow\u20ac k~o\u20acs{\u00d0z q{~43y{\u20ac\ufffd q~o}\ufffdoz\ufffdw\u00de \u20acsk~on n{yktz\u20ac tz\ufffdsom{non wt\u20ac\ufffd1\ns\ufffd\ufffd|\ufffd>22n {t1{~r243146;42u {\ufffd~zkw1|{z o13593:9:1r3 37\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 46259\nLtr91Uo\ufffd\u00d0{~v mw\ufffd\u20ac\ufffdo~\u20ac k~otno{w{rt mkww\u00de ltk\u20acon1 \\so \ufffd\u00d0{wk~ro mw\ufffd\u20ac\ufffdo~\u20ac \u00d0t\ufffdstz \ufffdso]ZS m{/\u20acsk~tz rzo\ufffd\u00d0{~v q{~_oov\n7\u20acs{\u00d0z \u00d0t\ufffds ]ZS\u20ac m{w{\ufffd~on l\u00de>*k+\ufffdsok\ufffdo~kro |{wt\ufffdtmkw ltk\u20ac {q\ufffdsot~ \u20ac{\ufffd~mo n{yktz? kzn *l+\ufffdsok\ufffdo~kro mwtyk\ufffdo\nmskzro ltk\u20ac {q\ufffdsot~ \u20ac{\ufffd~mo n{yktz1 Zon noz{\ufffdo\u20ac woq\ufffd/\u00d0tzr n{yktz\u20ac. lw\ufffdo noz{\ufffdo\u20ac ~trs\ufffd/\u00d0tzr n{yktz\u20ac. r~ooz noz{\ufffdo\u20ac\noz\ufffdt~{zy oz\ufffdkwt\u20ac\ufffd n{yktz\u20ac. {~kzro noz{\ufffdo\u20ac \u20acmo|\ufffdtm n{yktz\u20ac1 _st\ufffdo noz{\ufffdo\u20ac kz\u00den{yktz m{non k\u20aczo\ufffd\ufffd~kw kzn n{yktz\u20ac\nz{\ufffdm{non k~otzr~k\u00de1\ns\ufffd\ufffd|\ufffd>22n {t1{~r243146;42u {\ufffd~zkw1|{z o13593:9:1r3 39\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 47259\n\\o~y\u20ac \u20ac\ufffdms k\u20acdecision kznwithdraw tzokms {q\ufffdsom{yy\ufffdzt\ufffdto\u20ac \u20acs{\u00d0 \ufffdsk\ufffd \ufffdsoWk~t\u20ac Fr~oo/\nyoz\ufffd kzz{\ufffdzmoyoz\ufffd t\u20ackyku{~ \ufffd{|tm {qm{z\ufffdo~\u20ack\ufffdt{z n\ufffd~tzr \ufffdst\u20ac\u20aco\ufffdoz/nk\u00de |o~t{n. o\ufffdoz \u20ac|kz/\nztzr \ufffdsotno{w{rtmkw2ro{r~k|stm nt\ufffdt\u20act{z\u20ac tww\ufffd\u20ac\ufffd~k\ufffdon l\u00deLtr:k1\\so woq\ufffd/\u00d0tzr m{yy\ufffdzt\ufffdto\u20ac k~o\n~ok\u20ac{zklw\u00de \u20actytwk~ \ufffd{okms {\ufffdso~. \u00d0stwo \ufffdso~trs\ufffd/\u00d0tzr m{yy\ufffdzt\ufffd\u00de t\u20ac\ufffdzt}\ufffdo tzt\ufffd\u20acyoz\ufffdt{z {q\n|~o\ufffdt{\ufffd\u20ac ][|{wt\ufffdtmkw qtr\ufffd~o\u20ac \u20ac\ufffdms k\u20acObama kznGore kzn \ufffd\u20aco{q\u20acmtoz\ufffdtqtm \ufffdo~ytz{w{r\u00de1 \\so\nltr~ky\u20ac y{\u20ac\ufffdw\u00de m{zqt~y \ufffdsoqtzntzr\u20ac tzLtr:l.l\ufffd\ufffdkw\u20ac{ strswtrs\ufffd ntqqo~ozmo\u20ac tz\ufffdo~ytz{w{r\u00de.\no1r1r~ok\ufffdo~ |~{ytzozmo {qglobalwarming tz~trs\ufffd/\u00d0tzr I{yy\ufffdzt\ufffd\u00de 51Ltr:kzn \\klwo 5noy/\n{z\u20ac\ufffd~k\ufffdo \ufffdsk\ufffd \ufffdso~o t\u20ac\ufffdk~tk\ufffdt{z tz\ufffdsoro{r~k|stm \u20acm{|o {q\ufffdsom{yy\ufffdzt\ufffdto\u20ac1 \\so y{\u20ac\ufffd k||k~oz\ufffd\nm{z\ufffd~k\u20ac\ufffd o\u00f0t\u20ac\ufffd\u20ac lo\ufffd\u00d0ooz I{yy\ufffdzt\ufffd\u00de 4.\u00d0stms sok\ufffdtw\u00de qok\ufffd\ufffd~o\u20ac ]Rzo\u00d0\u20ac \u20ac{\ufffd~mo\u20ac. kzn I{yy\ufffd/\nzt\ufffdto\u20ac 5kzn 6.\u00d0stms tzmw\ufffdno y{\u20ac\ufffdw\u00de ][\u20ac{\ufffd~mo\u20ac1 \\so nt\u20ac\ufffd~tl\ufffd\ufffdt{z {q\u00d0{~n\u20ac kzn ltr~ky\u20ac tzLtr\n:lkzn :m\u20acs{\u00d0 \u20ac{yo \ufffd{|tmkw ntqqo~ozmo\u20ac lo\ufffd\u00d0ooz \ufffdsoq{\ufffd~ m{yy\ufffdzt\ufffdto\u20ac tz\ufffdsowoq\ufffd/\u00d0tzr mw\ufffd\u20ac\ufffdo~1\nI{yy\ufffdzt\ufffdto\u20ac 6kzn 7tzmw\ufffdno y{~o \ufffdo~y\u20ac ~owk\ufffdon \ufffd{\ufffdsom{z\u20aco}\ufffdozmo\u20ac {q\ufffdsonomt\u20act{z q{~\ufffdso\nAmerican |o{|wo \u00d0so~ok\u20ac I{yy\ufffdzt\ufffd\u00de 4t\u20acyktzw\u00de m{zmo~zon \u00d0t\ufffds \ufffdsotz\ufffdo~zk\ufffdt{zkw |{wt\ufffdtmkw\nLtr:1[{\ufffd~mo n{yktz\u20ac kzn m{z\ufffdoz\ufffd {qyontk k~\ufffdtmwo\u20ac \u20acsk~on \u00d0t\ufffdstz \ufffdsoqt\ufffdo wk~ro\u20ac\ufffd m{yy\ufffdzt \ufffdto\u20ac tz\ufffdsotzq{~yk\ufffdt{z /\u20acsk~tzr zo\ufffd\u00d0{~v1\n\\{voz\u20ac tzokms |w{\ufffd k~o\u00d0otrs\ufffd on*\ufffd\u20actzr \\L/OJL+ \ufffd{ykvo nt\u20ac\ufffdtzm\ufffdt\ufffdo \ufffd{voz\u20ac |~{ytzoz\ufffd1 It~mwo \u20act\u00feot\u20ac\u20acmkwon \ufffd{tzntmk\ufffdo \ufffdso\ufffd{\ufffdkw z\ufffdylo~ {q\n]ZS \u20acsk~o\u20ac \u00d0t\ufffdstz \ufffdsom{yy\ufffdzt\ufffd\u00de1 \\o~y\u20ac m{w{\ufffd~on lwkmv k~o\ufffdsostrso\u20ac\ufffd \u00d0otrs\ufffdon \ufffdo~y\u20ac ~o}\ufffdt~on \ufffd{~okms 49& {q\ufffdso\ufffd{\ufffdkw \u00d0otrs\ufffd tzk\nm{yy\ufffdz t\ufffd\u00de1L{~\ufffdt\u20ac\ufffdkw mwk~t\ufffd\u00de. okms \u20ac\ufffdoyyon \ufffd{voz t\u20ac~o|~o\u20acoz\ufffdon l\u00de\ufffdsoy{\u20ac\ufffd m{yy{z \ufffd{voz \ufffdsk\ufffd yk|\u20ac \ufffd{t\ufffd*{~|kt~ {q\ufffd{voz\u20ac q{~ltr~ky\u20ac+1\ns\ufffd\ufffd|\ufffd>22n{ t1{~r243146;42u {\ufffd~zkw1|{zo 13593:9:1r3 3:\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 49259\n~kytqtmk\ufffdt{z\u20ac1 \\so\u20aco qtzntzr\u20ac \u20ac\ufffdrro\u20ac\ufffd r~ok\ufffdo~ q{m\ufffd\u20ac kzn m{so~ozmo ky{zr\u20ac\ufffd \ufffdso~trs\ufffd/\u00d0tzr kzn\nmwtyk\ufffdo/\u20acmo|\ufffdtm q~kyo\u20ac kzn \u20ac\ufffd||{~\ufffd \ufffdsoqtzntzr\u20ac {qd6;f. \u00d0t\ufffds r~ok\ufffdo~ q~kryoz\ufffdk\ufffdt{z ky{zr\u20ac\ufffd\n\ufffdsowoq\ufffd/\u00d0tzr kzn oz\ufffdt~{zyoz\ufffdkwt\u20ac\ufffd q~kyo\u20ac? \ufffdst\u20acyk\u00de |k~\ufffdw\u00de ~oqwom\ufffd \ufffdsowk~ro~ \u20act\u00feo{q\ufffdsowoq\ufffd/\n\u00d0tzr2oz\ufffdt~{zyoz\ufffdkwt\u20ac\ufffd mw\ufffd\u20ac\ufffdo~ tz\ufffdsotzq{~yk\ufffdt{z/\u20acsk~tzr zo\ufffd\u00d0{~v. \u00d0stms |o~yt\ufffd\u20ac r~ok\ufffdo~\ntz\ufffdo~zkw ntqqo~oz\ufffdtk\ufffdt{z1\nI{z\u20act\u20ac\ufffdozm\u00de {qzo\ufffd\u00d0{~v \u20ac\ufffd~\ufffdm\ufffd\ufffd~o {\ufffdo~ \ufffdtyo\n\\{\ufffdzno~\u20ac\ufffdkzn \ufffdsom{z\u20act\u20ac\ufffdozm\u00de {q\ufffdso|{wk~t\u20acon tzq{~yk\ufffdt{z/\u20acsk~tzr |~{mo\u20ac\u20ac {\ufffdo~ \ufffdtyo. \ufffdso\nzo\ufffd\u00d0{~v \u20ac\ufffd~\ufffdm\ufffd\ufffd~o \u00d0k\u20aco\u00f0kytzon kw{zr \u00d0t\ufffds \u20actytwk~t\ufffd\u00de2|o~\u20act\u20ac\ufffdozmo {q\ufffdso\u20aco\ufffd\u20ac{q\ufffd\u20aco~\u20ac. \u20ac{\ufffd~mo\nn{yktz\u20ac kzn \u20acsk~on ]ZS\u20ac *k~\ufffdtmwo\u20ac+ km~{\u20ac\u20ac kww\u00d0oov\u20ac tz\ufffdso\u20ac\ufffd\ufffdn\u00de |o~t{n1 [tytwk~t\ufffd\u00de \u20acm{~o\u20ac k~o\n~o|{~\ufffdon tz\\klwo 61Ltr;|~o\u20acoz\ufffd\u20ac \ufffdsozo\ufffd\u00d0{~v ntkr~ky\u20ac {q\ufffdso~oyktztzr \u20act\u00f0\u00d0oov\u20ac tz{\ufffd~\n\u20ac\ufffd\ufffdn\u00de |o~t{n *_oov\u20ac 4\u22126\u20199\u2212;+1\nOzokms \u00d0oov. k\u20actytwk~ nt\ufffdt\u20act{z tz\ufffd{ \ufffd\u00d0{mw\ufffd\u20ac\ufffdo~\u20ac t\u20ac{l\u20aco~\ufffdon tz\ufffdsotzq{~yk\ufffdt{z/\u20acsk~tzr\nzo\ufffd\u00d0{~v. kw\ufffds{\ufffdrs \ufffdso\u20ac|omtqtm m{y|{\u20act\ufffdt{z {q\ufffdso\ufffd\u20aco~\u20ac kzn \u20acsk~on k~\ufffdtmwo\u20ac \ufffdsk\ufffd q{~y \ufffdsozo\ufffd/\n\u00d0{~v mskzro\u20ac \u20ac\ufffdl\u20ac\ufffdkz\ufffdtkww\u00de {\ufffdo~ \ufffdtyo1 \\so z\ufffdylo~ {qm{yy\ufffdzt\ufffdto\u20ac tz\ufffdsowoq\ufffd/skzn kzn ~trs\ufffd/\nskzn mw\ufffd\u20ac\ufffdo~\u20ac \ufffdk~to\u20ac {\ufffdo~ \ufffdso\u00d0oov\u20ac kzn \ufffdso~owk\ufffdt\ufffdo \u20act\u00feo\u20ac {q\ufffdsontqqo~oz\ufffd m{yy\ufffdzt\ufffdto\u20ac kw\u20ac{\nmskzro1 N{\u00d0o\ufffdo~. okms \u00d0oov ~o\ufffdokw\u20ac \ufffdso\u20ackyo l~{kn |k\ufffd\ufffdo~z {qkwk~ro~ woq\ufffd/\u00d0tzr2oz\ufffdt~{zyoz/\n\ufffdkwt\u20ac\ufffd mw\ufffd\u20ac\ufffdo~ \u20ac|wt\ufffd tz\ufffd{ \u20aco\ufffdo~kw \u20acykwwo~ \u20ac\ufffdl/m{yy\ufffdzt\ufffdto\u20ac. \u00d0t\ufffds k\u20acykwwo~ ~trs\ufffd/\u00d0tzr2\u20acmo|\ufffdtm mw\ufffd\u20ac/\n\ufffdo~.\u20acs{\u00d0tzr \ufffdsk\ufffd \ufffdst\u20ac|k\ufffd\ufffdo~z t\u20acz{\ufffdkzk~\ufffdoqkm\ufffd {q\ufffdsotzm~ok\u20acon km\ufffdt\ufffdt\ufffd\u00de n\ufffd~tzr _oov 71\\kvoz\n\ufffd{ro\ufffdso~ \u00d0t\ufffds Ltr6.\ufffdso\u20aco ~o\u20ac\ufffdw\ufffd\u20ac noy{z\u20ac\ufffd~k\ufffdo \ufffdsk\ufffd \ufffdso|k\ufffd\ufffdo~z {qzo\ufffd\u00d0{~v nt\ufffdt\u20act{z |o~\u20act\u20ac\ufffd\u20ac {\ufffdo~\n\ufffdtyo. \u20ac|kzztzr y\ufffdw\ufffdt|wo \u00d0oov\u20ac {qjz{~ykw) km\ufffdt\ufffdt\ufffd\u00de kzn {zoo\u00f0mo|\ufffdt{zkw \u00d0oov {qstrs yontk\nkm\ufffdt\ufffdt\ufffd\u00de1\nI{z\u20actno~tzr \ufffdsotz\ufffdo~/\u00d0oov \u20actytwk~t\ufffd\u00de lo\ufffd\u00d0ooz \ufffd\u20aco~ |{|\ufffdwk\ufffdt{z\u20ac kzn \ufffdso\u20ac{\ufffd~mo n{yktz\u20ac\nkzn ]ZS\u20ac \ufffdso\u00de \u20acsk~on. \\klwo 6\u20acs{\u00d0\u20ac \ufffdsotz\ufffdo~/\u00d0oov \u20actytwk~t\ufffd\u00de {q\ufffdzt}\ufffdo \ufffd\u20aco~\u20ac. ]ZS\u20ac kzn \u00d0ol/\n\u20act\ufffdon{yktz \u20acsk~o\u20ac. kw{zr \u00d0t\ufffds \ufffdsoytzty\ufffdy. yokz. yk\u00f0ty\ufffdy kzn \u20ac\ufffdkznk~n no\ufffdtk\ufffdt{z {q|kt~/\n\u00d0t\u20aco \u20actytwk~t\ufffd\u00de yok\u20ac\ufffd~o\u20ac1 \\so \u20acsk~on k~\ufffdtmwo\u20ac tz\\klwo 6l\u20acs{\u00d0 \ufffdsow{\u00d0o\u20ac\ufffd \u20actytwk~t\ufffd\u00de lo\ufffd\u00d0ooz\n\u00d0oov\u20ac. \u00d0t\ufffds k\u20acwtrs\ufffdw\u00de strso~ \u20actytwk~t\ufffd\u00de lo\ufffd\u00d0ooz knukmoz\ufffd \u00d0oov\u20ac1 Oz\ufffd\ufffdt\ufffdt\ufffdow\u00de. \ufffdso\u20ac{\ufffd~mo n{yktz\u20ac\\klwo 51Isk~km\ufffdo~t \u20ack\ufffdt{z {q\ufffdsoqt\ufffdo wk~ro\u20ac\ufffd m{yy\ufffdzt\ufffdto\u20ac tz_oov 71\nI{yy\ufffdz t\ufffd\u00de4\\st\u20ac t\u20ac\ufffdsowk~ro\u20ac\ufffd m{yy\ufffdzt\ufffd \u00dekzn t\u20acn{ytzk\ufffdon l\u00deyktz\u20ac\ufffd~o kyyontk {\ufffd\ufffdwo\ufffd\u20ac. \u20ac\ufffdms k\u20acTheGuardian\nkznTheIndependent. y{\u20ac\ufffdw\u00de q~{y \ufffdso]R1 \\st\u20ac m{yy\ufffdzt\ufffd \u00detzmw\ufffdno\u20ac yktz\u20ac\ufffd ~oky zo\u00d0\u20ac ~o|{~\ufffdtzr\nkzn nt\u20acm\ufffd\u20ac\u20aco\u20ac \ufffdsoWk~t\u20ac Fr~ooyoz\ufffd {zmwtyk\ufffdo mskzro kwy{\u20ac\ufffd o\u00f0mw\ufffd\u20act\ufffdow\u00de. q{m\ufffd\u20actzr {z\ufffdso\nm{z\u20aco}\ufffdo zmo\u20ac {qW~o\u20actnoz\ufffd \\~\ufffdy|)\u20ac nomt\u20act{z kzn kz\u00detz\ufffdo~zk\ufffdt{z kw~o\u20ac|{z \u20aco\u20ac1\nI{yy\ufffdz t\ufffd\u00de5\\st\u20ac m{yy\ufffdzt\ufffd \u00detzmw\ufffdno\u20ac ykz\u00de ~trs\ufffd/\u00d0tzr \u20ac{\ufffd~mo\u20ac. tzmw\ufffdntzr kw\ufffdo~zk\ufffdt\ufffdo zo\u00d0\u20ac \u20act\ufffdo\u20ac \u20ac\ufffdms k\u20ac\nBreitbart kznTheDailyCaller1 [{mtkw yontk \u20act\ufffdo\u20ac k~okw\u20ac{ |~{ytzoz\ufffd tz\ufffdst\u20acm{yy\ufffdz t\ufffd\u00dek\u20ac\nFacebook. Twitter. kznGab kwwk||ok~. \u20ac\ufffdrro\u20ac\ufffd tzr\ufffdsk\ufffd \ufffdst\u20acr~{\ufffd| mk|\ufffd\ufffd~o\u20ac k\ufffd\ufffdoy|\ufffd\u20ac \ufffd{\ufffd\u20aco\\\u00d0t\ufffd\ufffdo~ \ufffd{\n~o/\u20acsk~o m{z\ufffdoz\ufffd q~{y {\ufffdso~ \u20ac{mtkw |wk\ufffdq{~y \u20ac1[{yo o\u20ac\ufffdklwt\u20acs onyontk {\ufffd\ufffdwo\ufffd\u20ac \u20ac\ufffdms k\u20acTheDailyMail\nkznFoxNews k~o|~o\u20acoz\ufffd. l\ufffd\ufffdk~owo\u20ac\u20acq{m\ufffd\u20acon tz\ufffdst\u20acm{yy\ufffdz t\ufffd\u00de\ufffdskz kw\ufffdo~zk\ufffdt\ufffdo zo\u00d0\u20ac \u20act\ufffdo\u20ac1 \\st\u20ac\nm{yy\ufffdzt\ufffd \u00dekw\u20ac{ nt\u20acm\ufffd\u20ac\u20aco\u20ac \ufffdsoWk~t\u20ac Fr~ooyoz\ufffd nomt\u20act{z ykno l\u00deW~o\u20actnoz\ufffd \\~\ufffdy|. k\u20ac\u00d0oww k\u20ac\nmo~\ufffdktz k\u20ac|om\ufffd\u20ac {qmwtyk\ufffdo \u20acmtozmo1 \\st\u20ac t\u20ac\ufffdso{zw\u00de m{yy\ufffdz t\ufffd\u00de\ufffd{q{m\ufffd\u20ac {zq{~yo~ ][|{wt\ufffdtmtkz\u20ac\nObama kznGore kzn \ufffdso|s~k\u20acoglobalwarming n{ytzk\ufffdo\u20ac \ufffdsoltr~ky mw{\ufffdn *Ltr :m+1\nI{yy\ufffdz t\ufffd\u00de6\\st\u20ac m{yy\ufffdzt\ufffd \u00dem{z\u20act\u20ac\ufffd\u20ac {qykz\u00de yktz\u20ac\ufffd~o kyn{yktz\u20ac ~o\ufffdokwtzr k][q{m\ufffd\u20ac1 \\so y{\u20ac\ufffd |~{ytzoz\ufffd\nn{yktz\u20ac so~o k~oo\u20ac\ufffdklwt\u20acson yktz\u20ac\ufffd~o kyyontk \u20ac{\ufffd~mo\u20ac \u20ac\ufffdms k\u20ac\ufffdsoWashington Post kznNewYork\nTimes1 F\u20ac\u00d0t\ufffds I{yy\ufffdzt\ufffd \u00de4.\ufffdsoWk~t\u20ac Fr~ooyoz\ufffd {zmwtyk\ufffdo mskzro t\u20ackvo\u00de\ufffd{|tm {qtz\ufffdo~o\u20ac\ufffd1\nI{yy\ufffdz t\ufffd\u00de7\\st\u20ac m{yy\ufffdzt\ufffd \u00detzmw\ufffdno\u20ac kz\ufffdylo~ {qkw\ufffdo~zk\ufffdt\ufffdo kzn \u20acykwwo~ zo\u00d0\u20ac yontk n{yktz\u20ac \u00d0t\ufffds ky{\u20ac\ufffdw\u00de\nwoq\ufffd/\u00d0tzr ltk\u20ac. \u20ac\ufffdms k\u20acDailyKoskznMotherJones. ky{zr\u20ac\ufffd o\u20ac\ufffdklwt\u20acs onyktz\u20ac\ufffd ~oky zo\u00d0\u20ac \u20ac{\ufffd~mo\u20ac1\n\\so m{z\ufffdoz\ufffd so~o t\u20ac\u20actytwk~ \ufffd{\ufffdsk\ufffd {qI{yy\ufffdzt\ufffd \u00de4kzn I{yy\ufffdzt\ufffd \u00de6.l\ufffd\ufffdkw\u20ac{ ~oqo~ozmo\u20ac \ufffdsoz ][\nKz\ufffdt~{zyoz \ufffdkwW~{\ufffdom\ufffdt{z Frozm\u00de Fnytzt\u20ac\ufffd~k \ufffd{~[m{\ufffd\ufffd W~\ufffdt\ufffd\ufffd kzn Ttmstrkz m{zr~o\u20ac\u20acy kz\\ty\n_kwlo~r q{~\ufffdsot~ m{yyoz\ufffd\u20ac k~{\ufffdzn mwtyk\ufffdo mskzro1\nI{yy\ufffdz t\ufffd\u00de9\\st\u20ac m{yy\ufffdzt\ufffd \u00det\u20acm{y|~t\u20acon {qkyt\u00f0 {q\u20ac{mtkw yontk. zo\u00d0\u20ac kzn m{yyoz\ufffdk ~\u00de\u20act\ufffdo\u20ac1 Frktz. \ufffdsoWk~t\u20ac\nFr~ooyoz\ufffd nomt\u20act{z t\u20ackq{m\ufffd\u20ac. \u00d0t\ufffds knnt\ufffdt{zkw q~kytzr k~{\ufffdzn rw{lkw m{z\u20aco}\ufffdoz mo\u20ackzn {|tzt{z/\n|tomo\u20ac {z\ufffdsonomt\u20act{z1\ns\ufffd\ufffd|\ufffd>22n {t1{~r243146;42u {\ufffd~zkw1|{z o13593:9:1\ufffd33 5\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 4:259\nq~{y \u00d0stms m{z\ufffdoz\ufffd \u00d0k\u20ac\u20acsk~on \u20acs{\u00d0 strso~ \u20actytwk~t\ufffd\u00de lo\ufffd\u00d0ooz \u00d0oov\u20ac tz\\klwo 6m1]\u20aco~ |{|\ufffd/\nwk\ufffdt{z\u20ac \u20acs{\u00d0 wtyt\ufffdon |o~\u20act\u20ac\ufffdozmo lo\ufffd\u00d0ooz \u00d0oov\u20ac. \u20ac\ufffdyyk~t\u20acon l\u00de\\klwo 6k1\\so {\ufffdo~kww |k\ufffd\ufffdo~z\nt\u20ac\ufffdsk\ufffd. q~{y \u00d0oov \ufffd{\u00d0oov. kwtyt\ufffdon |~{|{~\ufffdt{z {q\ufffdso\ufffd\u20aco~ |{|\ufffdwk\ufffdt{z kzn \u20aco\ufffd{q\u20ac{\ufffd~mo\nn{yktz\u20ac |o~\u20act\u20ac\ufffd. \u00d0t\ufffds y\ufffdms w{\u00d0o~ |o~\u20act\u20ac\ufffdozmo tz\ufffdso\u20aco\ufffd\u20ac{qk~\ufffdtmwo\u20ac \ufffdsk\ufffd k~o\u20acsk~on1 \\st\u20ac t\u20ackz\ntz\ufffdo~o\u20ac\ufffdtzr qtzntzr \u00d0t\ufffds ~o\u20ac|om\ufffd \ufffd{\ufffdsostrs m{z\u20act\u20ac\ufffdozm\u00de tz\ufffdso\ufffd\u00d0{/mw\ufffd\u20ac\ufffdo~ zo\ufffd\u00d0{~v \u20ac\ufffd~\ufffdm\ufffd\ufffd~o\n\ufffdsk\ufffd t\u20ac~owtklw\u00de {l\u20aco~\ufffdon o\ufffdo~\u00de \u00d0oov1 \\so wkmv {q|o~\u20act\u20ac\ufffdozmo tz\u20acsk~on ]ZS\u20ac yk\u00de loo\u00f0|wktzon\nl\u00de\ufffdsorozo~kw \ufffd{wk\ufffdtwt\ufffd\u00de {qzo\u00d0\u20ac yontk. \u00d0so~o k~\ufffdtmwo\u20ac \ufffd\u00de|tmkww\u00de sk\ufffdo k\u20acs{~\ufffd wtqo\ufffdtyo *o1r1 5/6\nnk\u00de\u20ac \ufffdt\u20actltwt\ufffd\u00de tz{zwtzo \u20acsk~tzr d46f+1 T{no~k\ufffdo |o~\u20act\u20ac\ufffdozmo {q\ufffd\u20aco~\u20ac kzn \u20ac{\ufffd~mo\u20ac lo\ufffd\u00d0ooz\n\u00d0oov\u20ac |o~sk|\u20ac \u20ac\ufffdrro\u20ac\ufffd\u20ac kzkm\ufffdt\ufffdo m{~o r~{\ufffd| \u00d0s{ k~o|~o\u20acoz\ufffd okms \u00d0oov. \u00d0t\ufffds k\u00d0tno~ r~{\ufffd|\n\u00d0s{ k||ok~ wo\u20ac\u20acq~o}\ufffdoz\ufffdw\u00de1\nFyk~von ntqqo~ozmo mkzlo{l\u20aco~\ufffdon tz\ufffdso\ufffd\u00de|tmkw \u20actytwk~t\ufffd\u00de \u20acm{~o\u20ac q{~_oov 7kzn q{~\n{\ufffdso~ \u00d0oov\u20ac1 S{\u00d0o~ \u20actytwk~t\ufffd\u00de \u20acm{~o\u20ac \u00d0o~o {l\u20aco~\ufffdon q{~\ufffd\u20aco~\u20ac. ]ZS\u20ac kzn n{yktz\u20ac q~{y _oov 7\nk||ok~tzr tz{\ufffdso~ \u00d0oov\u20ac. \u00d0stwo m{z\ufffdo~\u20acow\u00de. strso~ \u20actytwk~t\ufffd\u00de \u00d0k\u20ac{l\u20aco~\ufffdon q{~\ufffd\u20aco~\u20ac. ]ZS\u20ac\nkzn n{yktz\u20ac q~{y \ufffdso{\ufffdso~ \u20act\u00f0\u00d0oov\u20ac k||ok~tzr tz_oov 71\\so z\ufffdylo~ {qzo\u00d0 \ufffd\u20aco~\u20ac. ]ZS\u20ac\nkzn n{yktz\u20ac tz_oov 7kw\u20ac{ \u20acs{\u00d0\u20ac k\u20ac\ufffdk~v m{z\ufffd~k\u20ac\ufffd \u00d0t\ufffds {\ufffdso~ \u00d0oov\u20ac> ;314& {q\ufffd\u20aco~\u20ac. <513& {q\n]ZS\u20ac kzn 7;1=& {qn{yktz\u20ac k~o\ufffdzt}\ufffdo \ufffd{_oov 7kzn z{\ufffd|~o\u20acoz\ufffd tzkz\u00de{\ufffdso~ \u00d0oov1 \\st\u20ac t\u20ac\n\u20ac\ufffd~{zr o\ufffdtnozmo \ufffdsk\ufffd \ufffdsoo\ufffdoz\ufffd\u20ac {q_oov 7k||ok~ \ufffd{sk\ufffdo \u20ac|\ufffd~~on kztzqw\ufffd\u00f0 {ql{\ufffds \u20ac{mtkw\nyontk |k~\ufffdtmt|kz\ufffd\u20ac kzn ntrt\ufffdkw yontk \u20ac{\ufffd~mo\u20ac *\ufffd\u20aco~\u20ac> yokz 7=16&. ytz1 791:&. yk\u00f01 9415&.\\klwo 61[tytwk~t\ufffd\u00de \u20acm{~o\u20ac q{~\ufffd\u20aco~\u20ac. ]ZS\u20ac kzn n{yktz\u20ac lo\ufffd\u00d0ooz okms {q\ufffdso\u20aco\ufffdoz \u00d0oov\u20ac mkwm\ufffdwk\ufffdon \ufffd\u20actzr K}*6+1[tytwk~t\ufffd\u00de t\u20acnt~om\ufffdt{z kw1\\so \u20actytwk~t\ufffd \u00dert\ufffdoz tzmoww*7.\n4+t\u20ac\ufffdso|~{|{~\ufffdt {z{q\ufffdso\ufffd\u20aco~\u20ac2]ZS\u20ac 2n{yktz\u20ac tz_oov 7kw\u20ac{ tz\u20acooz _oov 41I{{w \u20acskno\u20ac tzntmk\ufffdo \ufffdkw\ufffdo\u20ac \u20acykwwo~ \ufffdskz \ufffdsoyokz \u00d0stwo \u00d0k~y \u20acskno\u20ac tzntmk\ufffdo \ufffdkw\ufffdo\u20ac\nr~ok\ufffdo~ \ufffdskz \ufffdsoyokz1\n_oov 4 5 6 7 9 : ;\n4 315= 3166 316= 3159 3159 3155\n5 3157 3163 3169 3155 3157 3154 ytz1*S+ B313<7\n6 314= 3154 316< 3153 314= 314< yokz1*S+ B3156:\n7 313< 313= 3149 3144 3143 313= yk\u00f01*S+ B316<=\n9 314< 3153 3159 316: 3157 3154 \u20ac\ufffdno\ufffd1*S+ B313;<\n: 314= 3155 315: 3169 315: 3159\n; 3153 3156 315< 316: 315; 3163\n*k+F\u20ac\u00deyyo\ufffd~ tm{\ufffdo~wk| {q\ufffd\u20aco~\u20ac1\n_oov 4 5 6 7 9 : ;\n4 3146 313< 3144 313: 3139 3137\n5 3143 3147 3143 3139 3139 3137 ytz1*S+ B31355\n6 3139 3143 314: 3139 3137 3136 yokz1*S+ B313;<\n7 3135 3136 313: 313: 3136 3135 yk\u00f01*S+ B314=6\n9 3137 3139 313: 314= 314: 313; \u20ac\ufffdno\ufffd1*S+ B3137;\n: 3137 3139 313: 3143 314; 314:\n; 3137 3137 3139 313= 313= 314=\n*l+F\u20ac\u00deyyo\ufffd ~tm{\ufffdo~wk| {q]ZS\u20ac1\n_oov 4 5 6 7 9 : ;\n4 3199 319= 31;5 319< 3197 3194\n5 3177 319< 31;9 317< 3199 317< ytz1*S+ B31499\n6 3167 3174 31;: 3175 3175 316= yokz1*S+ B317=:\n7 3149 3153 315= 3156 3155 314< yk\u00f01*S+ B31;:;\n9 3174 3176 3196 31;: 3195 317: \u20ac\ufffdno\ufffd1*S+ B3149<\n: 3174 3195 319: 31;: 319: 3196\n; 317: 3197 31:5 31;; 319= 31:6\n*m+F\u20ac\u00deyyo\ufffd~ tm{\ufffdo~wk| {qn{yktz\u20ac1\ns\ufffd\ufffd|\ufffd>22n{ t1{~r243146;42u {\ufffd~zkw1|{zo 13593:9:1\ufffd336\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 4;259\nLtr;1I{z\u20act\u20ac\ufffdoz\ufffd zo\ufffd\u00d0{~v \u20ac\ufffd~\ufffdm\ufffd\ufffd~o {\ufffdo~ \ufffdtyo1 Uo\ufffd\u00d0{~v ntkr~ky\u20ac {q\ufffdso\ufffd{|qt\ufffdom{yy\ufffdzt\ufffdt o\u20ackm~{\u20ac\u20ac \ufffdso\u20act\u00f0~oyktztzr \u00d0oov\u20ac1 Kkms qtr\ufffd~o\nt\u20ac{~toz\ufffdon \u20ac\ufffdms \ufffdsk\ufffd \ufffdsowoq\ufffd/\u00d0tzr mw\ufffd\u20ac\ufffdo~ t\u20ac{z\ufffdsowoq\ufffdkzn \ufffdso~trs\ufffd/\u00d0tzr mw\ufffd\u20ac\ufffdo~ t\u20ac{z\ufffdso~trs\ufffd1 Ozokms mk\u20aco z{no m{w{\ufffd~ \u20actrztqto\u20ac m{yy\ufffdz t\ufffd\u00de\nyoylo ~\u20acst| kzn \u20act\u00feot\u20ac|~{|{~\ufffdt{zkw \ufffd{\ufffdso\u20ac}\ufffdk~o ~{{\ufffd {q\ufffd{\ufffdkw \u20acsk~o m{\ufffdz\ufffd1 I{yy\ufffdzt \ufffdto\u20ack~owklowwon 4\u22129tznom~ok\u20actzr {~no~ {q\u20act\u00feo. kzn\nm{w{~on lw\ufffdo. \u00deoww{\u00d0. r~ooz. ~onkzn |\ufffd~|wo ~o\u20ac|om\ufffdt\ufffdow\u00de1 U{no |wkmoyoz\ufffd t\u20acno\ufffdo~ytzon l\u00de\ufffdsoW\u00de\ufffds{z ty|woyoz \ufffdk\ufffdt{z {q\ufffdsoL{~moF\ufffd wk\u20ac5\nkwr{~t\ufffd syd93f1\ns\ufffd\ufffd|\ufffd>22 n{t1{~r243146;4 2u{\ufffd~zkw1|{zo1 3593:9:1r33;\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 4<259\n\u03c3B5143&? ]ZS\u20ac> yokz ;316&. ytz1 ::1:&. yk\u00f01 ;615&.\u03c3B:197&? n{yktz\u20ac> yokz 5=1<&.\nytz1 5;16&. yk\u00f01 6419&.\u03c3B7154&+1 _stwo \u00d0ol n{yktz\u20ac o\u00f0stlt\ufffd \ufffdsoy{\u20ac\ufffd \u20ac\ufffdkltwt\ufffd\u00de lo\ufffd\u00d0ooz\n\u00d0oov\u20ac. ]ZS\u20ac o\u00f0stlt\ufffd \ufffdsowok\u20ac\ufffd \u20ac\ufffdkltwt\ufffd\u00de kzn \ufffd\u20aco~\u20ac qkwwlo\ufffd\u00d0ooz \ufffdso\u20aco \ufffd\u00d0{o\u00f0\ufffd~oyo\u20ac1\n\\{m{zqt~y \ufffdsk\ufffd \ufffdso|o~\u20act\u20ac\ufffdoz\ufffd \ufffd\u00d0{/mw\ufffd\u20ac\ufffdo~ zo\ufffd\u00d0{~v \u20ac\ufffd~\ufffdm\ufffd\ufffd~o\u20ac \u20acooz tzLtr;k~o|{wk~t\u20acon\nkw{zr \ufffdso\u20ackyo woq\ufffd/~trs\ufffd |{wt\ufffdtmkw kzn oz\ufffdt~{zyoz\ufffdkwt\u20ac\ufffd/\u20acmo|\ufffdtm tno{w{rtmkw k\u00f0o\u20ac k\u20ac\ufffdsk\ufffd \u20acooz q{~\n_oov 7*\u20acoo Ltr9+.\ufffdson{yktz ltk\u20ac m{ntzr\u20ac \u00d0o~o k||wton \ufffd{\ufffdsozo\ufffd\u00d0{~v\u20ac m~ok\ufffdon q{~_oov\u20ac 4/\n6kzn 9/;*[:kzn [;Ltr\u20ac tz[4Ltwo+1 L\ufffd~\ufffdso~y{~o. k\ufffdo~kro ltk\u20aco\u20ac \u00d0o~o mkwm\ufffdwk\ufffdon q{~kwwm{y/\ny\ufffdzt\ufffdto\u20ac tzokms \u00d0oov tzLtr<1\\~ozn\u20ac tz|{wk~t\u20ack\ufffdt{z {\ufffdo~ \ufffdtyo k~o\u20acs{\u00d0z k\u20aczo\ufffd\u00d0{~v k\ufffdo~/\nkro\u20ac q{~|{wt\ufffdtmkw kzn mwtyk\ufffdo mskzro ltk\u20aco\u20ac. kw{zr\u20actno \ufffdsom{yy\ufffdzt\ufffd\u00de k\ufffdo~kro\u20ac. tzLtr<1Oz\ny{\u20ac\ufffd \u00d0oov\u20ac. \ufffdso\u00d0s{wo/zo\ufffd\u00d0{~v k\ufffdo~kro \u20acs{\u00d0\u20ac ytwn woq\ufffd/\u00d0tzr kzn oz\ufffdt~{zyoz\ufffdkwt\u20ac\ufffd ltk\u20ac1\n_oov 9\u20acs{\u00d0\u20ac *q{~ \ufffdso{zw\u00de \ufffdtyo km~{\u20ac\u20ac \ufffdso\u20aco\ufffdoz/\u00d0oov |o~t{n+ kwk~ro zo\ufffd\ufffd~kw m{yy\ufffdzt\ufffd\u00de1\n[\ufffd||{~\ufffdtzr \ufffdso\ufffdt\u20ac\ufffdkw o\ufffdtnozmo \u20acooz tz[:kzn [;Ltr\u20ac tz[4Ltwo. kzn \ufffdsom{yy\ufffdzt\ufffd\u00de/wo\ufffdow ltk\u20ac\n\u20acm{~o\u20ac tzLtr<.\ufffdso\u20aco qtzntzr\u20ac \u20acs{\u00d0 \ufffdsk\ufffd \ufffdso|{wk~t\u20acon zo\ufffd\u00d0{~v \u20ac\ufffd~\ufffdm\ufffd\ufffd~o {l\u20aco~\ufffdon tz_oov 7t\u20ac\n|o~\u20act\u20ac\ufffdoz\ufffd kzn t\u20acz{\ufffdkzk~\ufffdoqkm\ufffd {q\ufffdsotzm~ok\u20aco tzkm\ufffdt\ufffdt\ufffd\u00de tz\ufffdsomwtyk\ufffdo mskzro m{z\ufffdo~\u20ack\ufffdt{z\nno\u20ac|t\ufffdo \ufffdso\ufffd\ufffd~z{\ufffdo~ tz\ufffd\u20aco~\u20ac. \u20ac{\ufffd~mo n{yktz\u20ac kzn \u20acsk~on k~\ufffdtmwo\u20ac1\nJt\ufffdm\ufffd\ufffd\ufffdt{z\n\\st\u20ac |k|o~ |~o\u20acoz\ufffd\u20ac kzkzkw\u00de\u20act\u20ac {qntrt\ufffdkw yontk \u20acsk~tzr losk\ufffdt{\ufffd~ k~{\ufffdzn \ufffdsom{z\ufffdo\u20ac\ufffdon t\u20ac\u20ac\ufffdo\n{qmwtyk\ufffdo mskzro1 V\ufffd~ kzkw\u00de\u20act\u20ac w{{v\u20ac k\ufffd\ufffdsozo\ufffd\u00d0{~v \u20ac\ufffd~\ufffdm\ufffd\ufffd~o q{~yon l\u00de\ufffd\u20aco~\u20ac \u20acsk~tzr \u00d0ol\nk~\ufffdtmwo\u20ac ~owk\ufffdon \ufffd{mwtyk\ufffdo mskzro. m{yltztzr zo\ufffd\u00d0{~v kzkw\u00de\u20act\u20ac \u00d0t\ufffds m{y|\ufffd\ufffdk\ufffdt{zkw kzn s\ufffdykz\n\ufffdo\u00f0\ufffdkzkw\u00de\u20aco\u20ac \ufffd{tnoz\ufffdtq\u00de kzn msk~km\ufffdo~t\u20aco m{yy\ufffdzt\ufffdto\u20ac {q\ufffd\u20aco~\u20ac kzn \ufffdsok~\ufffdtmwo\u20ac2\u20ac{\ufffd~mo\u20ac \ufffdso\u00de\nLtr<1So\ufffdow\u20ac {q|{wt\ufffdtmkw kzn mwtyk\ufffdo mskzro ltk\u20ac {\ufffdo~ \ufffdsom{\ufffd~\u20aco {q\ufffdso\u20aco\ufffdoz \u00d0oov \u20ac\ufffd\ufffdn\u00de |o~t{n1 \\so\u20aco k~oyok\u20ac\ufffd~on k\u20ac\ufffdso\nyokz m{non ltk\u20ac {qn{yktz\u20ac \u00d0otrs\ufffdon l\u00de\ufffd{\ufffdkw \u20acsk~o\u20ac *\u20acoo Ono{w{rtm kwm{ntzr {q\u20ac{\ufffd~mo n{yktz\u20ac+ 1Gtk\u20ac tzokms {q\ufffdsoqt\ufffdowk~ro\u20ac\ufffd\nm{yy\ufffdz t\ufffdto\u20ac k~o~o|~o\u20acoz\ufffdo nl\u00de\ufffdso\u20acmk\ufffd\ufffdo~ |{tz\ufffd\u20ac tzokms \u00d0oov. kzn \ufffdsoltk\u20ac km~{\u20ac\u20ac \ufffdso\u00d0s{wo zo\ufffd\u00d0{~v t\u20acrt\ufffdoz l\u00de\ufffdsowtzo\u20ac1 \\so\nm{w{\ufffd~ {q\ufffdsom{yy\ufffdzt\ufffd\u00de |{tz\ufffd\u20ac t\u20acm{z\u20act\u20ac\ufffdoz\ufffd \u00d0t\ufffds {\ufffdso~ qtr\ufffd~o\u20ac1 Ozy{\u20ac\ufffd \u00d0oov\u20ac. \ufffdsok\ufffdo~kro zo\ufffd\u00d0{~v ltk\u20ac t\u20acwoq\ufffd{qmoz\ufffd~o kzn\ny{~o oz\ufffdt~{zyoz \ufffdkwt\u20ac\ufffd \ufffdskz \u20acmo|\ufffdtm1 \\st\u20ac \ufffd~ozn m{z\ufffdtz\ufffdo\u20ac \ufffd{\ufffdsotznt\ufffdtn\ufffdkw m{yy\ufffdz t\ufffdto\u20ac. \u00d0t\ufffds \ufffdsoyku{~t\ufffd\u00de lotzr woq\ufffd/\u00d0tzr kzn\noz\ufffdt~{zy oz\ufffdkwt\u20ac\ufffd1\ns\ufffd\ufffd|\ufffd>22 n{t1{~r243146;4 2u{\ufffd~zkw1|{zo1 3593:9:1r33<\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 4=259\n\u20acsk~o1 \\so kty t\u20ac\ufffd{\ufffdzno~\u20ac\ufffdkzn s{\u00d0 |o{|wo ozrkro \u00d0t\ufffds. kzn \u20acsk~o {zwtzo yontk m{z\ufffdoz\ufffd\nkl{\ufffd\ufffd mwtyk\ufffdo mskzro {z\u20ac{mtkw yontk *\u20ac|omtqtmkww\u00de \\\u00d0t\ufffd\ufffdo~+1\n_osk\ufffdo q{\ufffdzn \ufffdsk\ufffd ky{zr\u20ac\ufffd \ufffdsom{yy\ufffdzt\ufffdto\u20ac {q\u20acsk~on ]ZS\u20ac. ~trs\ufffd/\u00d0tzr kzn mwtyk\ufffdo\n\u20acmo|\ufffdtm \ufffdto\u00d0\u20ac k~o\u20ac\ufffd~{zrw\u00de m{~~owk\ufffdon. k\u20ack~owoq\ufffd/\u00d0tzr kzn oz\ufffdt~{zyoz\ufffdkwt\u20ac\ufffd \ufffdto\u00d0\u20ac1 \\st\u20ac m{~~owk/\n\ufffdt{z sk\u20aclooz {l\u20aco~\ufffdon tztznt\ufffdtn\ufffdkw\u20ac loq{~o \ufffd\u20actzr \u20ac\ufffd~\ufffdo\u00de/lk\u20acon yo\ufffds{n\u20ac *o1r1 d6.94.95f+1 V\ufffd~\n\u20ac\ufffd\ufffdn\u00de \u20acs{\u00d0\u20ac \ufffdsk\ufffd \ufffdsok\u20ac\u20ac{mtk\ufffdt{z o\u00f0\ufffdozn\u20ac \ufffd{yontk {\ufffd\ufffdwo\ufffd\u20ac. \u20ac|omtqtmkww\u00de \ufffd{\ufffdsom{z\ufffdoz\ufffd |~{n\ufffdmon\nl\u00dekwk~ro \u20aco\ufffd{q{zwtzo zo\u00d0\u20ac |~{\ufffdtno~\u20ac *Ltr 7+.kzn t\u20ackyku{~ qok\ufffd\ufffd~o {qs{\u00d0 {zwtzo m{z\ufffdoz\ufffd t\u20ac\n\u20acsk~on1 Zomoz\ufffd \u00d0{~v l\u00deN{~z\u20aco\u00de o\ufffdkw1d95f \u20ac\ufffdrro\u20ac\ufffd\u20ac \ufffdsk\ufffd \ufffdso\u20acmo|\ufffdtmt\u20acy/m{z\u20aco~\ufffdk\ufffdt\u20acy wtzv yk\u00de\nlo\u20ac\ufffd~{zro\u20ac\ufffd tz\ufffdso][1\\st\u20ac qtzntzr t\u20ac\u20ac\ufffd||{~\ufffdon so~o. \u00d0so~o \u00d0o{l\u20aco~\ufffdo \ufffd\u00d0{wk~ro m{yy\ufffdzt/\n\ufffdto\u20aclk\u20acon yktzw\u00de {z][\u20ac{\ufffd~mo\u20ac? {zoy{~o woq\ufffd/\u00d0tzr kzn oz\ufffdt~{zyoz\ufffdkwt\u20ac\ufffd. \ufffdso{\ufffdso~ y{~o\n~trs\ufffd/\u00d0tzr kzn \u20acmo|\ufffdtm1 Fw\ufffds{\ufffdrs \u00d0ontnz{\ufffdo\u00f0kytzo \u00d0so~o \ufffdso\ufffd\u20aco~\u20ac tz{\ufffd~nk\ufffdk\u20aco\ufffd \u00d0o~o\nw{mk\ufffdon ro{r~k|stmkww\u00de. \ufffdson{ytzkzmo {q][\u20ac{\ufffd~mo\u20ac tz\ufffd\u00d0{{q\ufffdsom{yy\ufffdzt\ufffdto\u20ac. kzn {q]R\n\u20ac{\ufffd~mo\u20ac tz{zo{q\ufffdsom{yy\ufffdzt\ufffdto\u20ac. \u20ac\ufffdrro\u20ac\ufffd\u20ac \ufffdsk\ufffd \ufffdso\u20aco m{yy\ufffdzt\ufffdto\u20ac k~owk~row\u00de q{~yon k~{\ufffdzn\n\ufffd\u20aco~\u20ac q~{y \ufffds{\u20aco m{\ufffdz\ufffd~to\u20ac1 V\ufffd~ {\ufffdo~kww q{m\ufffd\u20ac {zKzrwt\u20acs/wkzr\ufffdkro \ufffd\u00d0oo\ufffd\u20ac kzn yontk m{z\ufffdoz\ufffd.\n|w\ufffd\u20ac wk~ro z\ufffdylo~\u20ac {q\\\u00d0t\ufffd\ufffdo~ \ufffd\u20aco~\u20ac tz\ufffdso\u20aco m{\ufffdz\ufffd~to\u20ac *zok~w\u00de 49& {q{zwtzo ][kn\ufffdw\ufffd\u20ac \ufffd\u20acon\n\\\u00d0t\ufffd\ufffdo~ tz534; d45f+. k~om{z\u20act\u20ac\ufffdoz\ufffd \u00d0t\ufffds \ufffdst\u20acm{zuom\ufffd\ufffd~o1\nOz\ufffdst\u20ac\u20ac\ufffd\ufffdn\u00de \u00d0oqtzn \ufffd\u00d0{wo\ufffdow\u20ac {qm{yy\ufffdzt\ufffd\u00de \u20ac\ufffd~\ufffdm\ufffd\ufffd~o1 F\ufffd\ufffdsostrso\u20ac\ufffd wo\ufffdow. \ufffdso~o t\u20ac|{wk~t/\n\u20ack\ufffdt{z kzn \u20acor~ork\ufffdt{z \u00d0t\ufffds kwk~ro woq\ufffd/\u00d0tzr2oz\ufffdt~{zyoz\ufffdkwt\u20ac\ufffd r~{\ufffd|tzr kzn k\u20acykwwo~ ~trs\ufffd/\n\u00d0tzr2\u20acmo|\ufffdtm r~{\ufffd|tzr mwok~w\u00de \ufffdt\u20actlwo tz\ufffdso\u20acsk~tzr zo\ufffd\u00d0{~v kq\ufffdo~ \ufffdsok||wtmk\ufffdt{z {q\ufffdsoq{~mo/\nnt~om\ufffdon wk\u00de{\ufffd\ufffd kwr{~t\ufffdsy1 _t\ufffdstz \ufffdsowoq\ufffd/\u00d0tzr2oz\ufffdt~{zyoz\ufffd kwt\u20ac\ufffd r~{\ufffd|. kwr{~t\ufffdsytm m{yy\ufffd/\nzt\ufffd\u00de no\ufffdom\ufffdt{z qtzn\u20ac *\ufffd\u00de|tmkww\u00de+ q{\ufffd~ \u20acykwwo~ r~{\ufffd|tzr\u20ac. msk~km\ufffdo~t\u20acon l\u00deq\ufffd~\ufffdso~ kzkw\u00de\u20act\u20ac k\u20ac\nkl{\ufffdo1 O\ufffdt\u20actz\ufffdo~o\u20ac\ufffdtzr \ufffd{z{\ufffdo \ufffdsk\ufffd \ufffdso~trs\ufffd/\u00d0tzr2mwtyk\ufffdo \u20acmo|\ufffdtm r~{\ufffd| t\u20acy{~o noz\u20acow\u00de m{z/\nzom\ufffdon tz\ufffdo~zkww\u00de \ufffdskz \ufffdsowoq\ufffd/\u00d0tzr2oz\ufffdt~{zyoz\ufffdkwt\u20ac\ufffd r~{\ufffd| *[\ufffd\ufffdnoz\ufffd)\u20ac t/\ufffdo\u20ac\ufffd k||wton \ufffd{k\ufffdo~kro\nmw\ufffd\u20ac\ufffdo~tzr m{oqqtmtoz\ufffd\u20ac {qkwwwoq\ufffd/\u00d0tzr kzn ~trs\ufffd/\u00d0tzr m{yy\ufffdzt\ufffdto\u20ac tzLtr\u20ac 6kzn ;.\u00d0t\ufffds mw\ufffd\u20ac/\n\ufffdo~tzr m{oqqtmtoz\ufffd\u20ac mkwm\ufffdwk\ufffdon tzMo|st \ufffd\u20actzr Sk\ufffdk|\u00de)\u20ac kwr{~t\ufffdsy d96f.pB3134=+1 \\st\u20ac qtzntzr\noms{o\u20ac \ufffdso{l\u20aco~\ufffdk\ufffdt{z l\u00deI{wwo{zt o\ufffdkw1d97f \ufffdsk\ufffd tz\ufffdso][.km\ufffdt\ufffdo Zo|\ufffdlwtmkz\u20ac o\u00f0stlt\ufffd r~ok\ufffdo~\nwo\ufffdow\u20ac {qs{y{|stw\u00de \ufffdskz Joy{m~k\ufffd\u20ac tz\ufffdsot~ |k\ufffd\ufffdo~z\u20ac {qtz\ufffdo~km\ufffdt{z {z\\\u00d0t\ufffd\ufffdo~1\n\\so \ufffdoy|{~kw kzkw\u00de\u20act\u20ac |~o\u20acoz\ufffdon kl{\ufffdo rt\ufffdo\u20ac m{zqtnozmo \ufffdsk\ufffd \ufffdso|{wk~t\u20acon zo\ufffd\u00d0{~v \u20ac\ufffd~\ufffdm/\n\ufffd\ufffd~o t\u20ac~{l\ufffd\u20ac\ufffd {\ufffdo~ \ufffdtyo. no\u20ac|t\ufffdo \ufffd\ufffd~z{\ufffdo~ tz\ufffdso\ufffd\u20aco~ |{|\ufffdwk\ufffdt{z\u20ac. \u20aco\ufffd\u20ac{q\u20acsk~on k~\ufffdtmwo\u20ac kzn\nzo\u00d0\u20ac \u20ac{\ufffd~mo\u20ac1 _o\u20ac\ufffd\ufffdnton \u20actytwk~t\ufffd\u00de lo\ufffd\u00d0ooz \ufffdso\u20aco\ufffd\u20ac{q\ufffd\u20aco~\u20ac. ]ZS\u20ac kzn n{yktz\u20ac km~{\u20ac\u20ac \ufffdso\n\u20aco\ufffdoz \u00d0oov\u20ac. k\u20ac\u00d0oww k\u20ac\ufffdso|~{|{~\ufffdt{z {q\ufffd\u20aco~\u20ac. ]ZS\u20ac kzn n{yktz\u20ac k||ok~tzr {zw\u00de tz\ufffdso\no\u00f0mo|\ufffdt{zkw _oov 7*tz\u00d0stms mwtyk\ufffdo mskzro \u00d0k\u20ac\ufffdso\u20ac\ufffdluom\ufffd {qkyktz\u20ac\ufffd~oky zo\u00d0\u20ac o\ufffdoz\ufffd\n\u00d0soz W~o\u20actnoz\ufffd \\~\ufffdy| kzz{\ufffdzmon \ufffdso][\u00d0t\ufffdsn~k\u00d0kw q~{y \ufffdsoWk~t\u20ac Fr~ooyoz\ufffd+1 \\st\u20ac yku{~\no\ufffdoz\ufffd \u20ac\ufffdl\u20ac\ufffdkz\ufffdtkww\u00de tzm~ok\u20acon \ufffdso\ufffd{w\ufffdyo {q\u20ac{mtkw yontk yo\u20ac\u20ackro\u20ac ~owk\ufffdon \ufffd{mwtyk\ufffdo mskzro\nkzn n~o\u00d0 tzkztzm~ok\u20acon z\ufffdylo~ {qzo\u00d0 \ufffd\u20aco~\u20ac. \u00d0s{ \u20acsk~on kr~ok\ufffdo~ z\ufffdylo~ {qk~\ufffdtmwo\u20ac q~{y k\nr~ok\ufffdo~ z\ufffdylo~ {q{zwtzo tzq{~yk\ufffdt{z \u20ac{\ufffd~mo\u20ac1 \\s\ufffd\u20ac \u00d0o{l\u20aco~\ufffdon zo\ufffd\u00d0{~v \u20ac\ufffd~\ufffdm\ufffd\ufffd~o kzn m{y/\n|{\u20act\ufffdt{z tzl{\ufffds jz{~ykw {|o~k\ufffdt{z) k\u20ac\u00d0oww k\u20actzkz\ufffdz\ufffd\u20ac\ufffdkw \u20ac\ufffdk\ufffdo {qstrs km\ufffdt\ufffdt\ufffd\u00de. \u20acs{\u00d0tzr \ufffdso\n\ufffd\u00de|tmkw wo\ufffdow {q\u00d0oov/\ufffd{/\u00d0oov \ufffd{wk\ufffdtwt\ufffd\u00de k\u20ac\u00d0oww k\u20ac\ufffdso\u20ac\ufffdl\u20ac\ufffdkz\ufffdtkw mskzro \ufffdzno~ \ufffdsotzqw\ufffdozmo {q\n\ufffdsont\u20ac~\ufffd|\ufffdt\ufffdo yktz\u20ac\ufffd~oky zo\u00d0\u20ac o\ufffdoz\ufffd1 F\ufffdkww\ufffdtyo\u20ac. tzmw\ufffdntzr \ufffdsont\u20ac~\ufffd|\ufffdt\ufffdo o\ufffdoz\ufffd. \ufffdso|{wk~/\nt\u20acon zo\ufffd\u00d0{~v \u20ac\ufffd~\ufffdm\ufffd\ufffd~o ~oyktz\u20ac \u20ac\ufffd~{zr kzn mwok~w\u00de \ufffdt\u20actlwo. no\u20ac|t\ufffdo \u20ac\ufffdl\u20ac\ufffdkz\ufffdtkw mskzro\u20ac tzt\ufffd\u20ac\nm{z\u20ac\ufffdt\ufffd\ufffdoz\ufffd |k~\ufffd\u20ac *\ufffd\u20aco~\u20ac kzn k~\ufffdtmwo\u20ac+1 F\u20ac\u20ac\ufffdms. \u00d0om{zmw\ufffdno \ufffdsk\ufffd \ufffdsol~{kn \ufffd{|{w{rtmkw qok\ufffd\ufffd~o\u20ac\n\ufffdt\u20actlwo tzLtr\u20ac 6kzn ;k~o\u20ac\ufffdklwo {\ufffdo~ \ufffdso\u20aco\ufffdoz/\u00d0oov n\ufffd~k\ufffdt{z {q{\ufffd~nk\ufffdk\u20aco\ufffd1 [tytwk~w\u00de. \u00d0o\nqtzn \ufffdsk\ufffd \ufffdsok\u20ac\u20ac{mtk\ufffdt{z {qwoq\ufffd/\u00d0tzr2oz\ufffdt~{zyoz\ufffdkwt\u20ac\ufffd \ufffdto\u00d0\u20ac. kzn {q~trs\ufffd/\u00d0tzr2\u20acmo|\ufffdtm \ufffdto\u00d0\u20ac.\nk~o|o~\u20act\u20ac\ufffdoz\ufffd. k\u20ac\u20acs{\u00d0z tzLtr9kkzn 9l.[:kzn [;Ltr\u20ac tz[4Ltwo1 L\ufffd\ufffd\ufffd~o \u20ac\ufffd\ufffdn\u00de yk\u00de qtzn t\ufffd\nq~\ufffdt\ufffdq\ufffdw \ufffd{o\u00f0kytzo \u00d0so\ufffdso~ \u20ac\ufffdms \u20ac\ufffdkltwt\ufffd\u00de o\u00f0t\u20ac\ufffd\u20ac q{~{\ufffdso~ {zwtzo zo\ufffd\u00d0{~v\u20ac {~{\ufffdso~ |{wt\ufffdtmt\u20acon\nnt\u20acm\ufffd\u20ac\u20act{z\u20ac1\n\\so \u20actytwk~t\ufffd\u00de \u20ac\ufffdk\ufffdt\u20ac\ufffdtm\u20ac tz\\klwo 6\u20ac\ufffdrro\u20ac\ufffd krozo~kw \ufffd~ozn q{~k\u20acykww |~{|{~\ufffdt{z {q\ufffdso\n\ufffd\u20aco~\u20ac. ]ZS\u20ac kzn n{yktz\u20ac \ufffd{|o~\u20act\u20ac\ufffd km~{\u20ac\u20ac \u00d0oov\u20ac \u00d0stwo {\ufffdso~\u20ac k||ok~ {zw\u00de \u20ac|{~kntmkww\u00de1 Oz\ufffdso\n]ZS mk\u20aco. \ufffdsom{y|k~k\ufffdt\ufffdow\u00de w{\u00d0wo\ufffdow\u20ac {q\u20actytwk~t\ufffd\u00de k~oz{\ufffd\u20ac\ufffd~|~t\u20actzr k\u20aczo\u00d0\u20ac k~\ufffdtmwo\u20ac k~o\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 53259\n}\ufffdtmvw\u00de \u20ac\ufffd|o~\u20aconon l\u00dezo\u00d0 qkm\ufffd\u20ac kzn |o~\u20ac|om\ufffdt\ufffdo\u20ac1 Fy{zr \ufffdso\ufffd\u20aco~\u20ac kzn n{yktz\u20ac. \ufffdsoo\ufffdtnozmo\n\u20ac\ufffdrro\u20ac\ufffd\u20ac \ufffdsozo\ufffd\u00d0{~v sk\u20ack\u20ac\ufffdklwo m{~o \u00d0t\ufffds kz\ufffdz\u20ac\ufffdklwo |o~t|so~\u00de1 V\ufffdo~ \ufffdso\u20aco\ufffdoz \u00d0oov\u20ac \u20ac\ufffd\ufffdn/\nton.kz\ufffdylo~ {q\u00d0ol\u20act\ufffdo\u20ac o\u20ac\ufffdklwt\u20acs \ufffdsoy\u20acow\ufffdo\u20ac k\u20acm~t\ufffdtmkw \ufffd{\ufffdsoqw{\u00d0 {qtzq{~yk\ufffdt{z k~{\ufffdzn mwt/\nyk\ufffdo mskzro. ot\ufffdso~ k\u20aczo\u00d0\u20ac \u20ac{\ufffd~mo\u20ac *\u20ac\ufffdms k\u20acTheGuardian {~Breitbart+ {~k\u20acm{zn\ufffdt\ufffd\u20ac q{~\n|o~\u20ac{zkw {|tzt{z\u20ac *\u20ac\ufffdms k\u20acTwitter kznWordpress+1 Tkz\u00de {\ufffdso~ \u20act\ufffdo\u20ac k~o|o~t|so~kw \ufffd{\ufffdso\nzo\u00d0\u20ac/\u20acsk~tzr zo\ufffd\u00d0{~v {qmwtyk\ufffdo mskzro. k||ok~tzr \u20ac|{~kntmkww\u00de \u00d0t\ufffds kw{\u00d0o~ q~o}\ufffdozm\u00de {q\n\ufffd\u20ackro1 \\st\u20ac |k|o~ sk\u20acnowtlo~k\ufffdow\u00de k\ufffd{tnon \u20ac\ufffd\ufffdn\u00detzr \ufffd\u20aco~\u20ac k\u20actznt\ufffdtn\ufffdkw\u20ac. l\ufffd\ufffdt\ufffd\u20acooy\u20ac ~ok\u20ac{z/\nklwo \ufffd{o\u00f0|om\ufffd km{~o r~{\ufffd| {qm{yyt\ufffd\ufffdon kzn \u20ac\ufffd~{zrw\u00de tz\ufffdo~o\u20ac\ufffdon |o{|wo \u00d0s{ ~or\ufffdwk~w\u00de \u20acsk~o\ntzq{~yk\ufffdt{z kl{\ufffd\ufffd mwtyk\ufffdo mskzro. \u00d0t\ufffds {\ufffdso~\u20ac \u00d0s{ m{z\ufffd~tl\ufffd\ufffdo wo\u20ac\u20acq~o}\ufffdoz\ufffdw\u00de {~{zw\u00de \u00d0soz\ny{\ufffdt\ufffdk\ufffdon l\u00deo\u00f0\ufffdo~zkw qkm\ufffd{~\u20ac1 I{z\u20actno~tzr \ufffdst\u20ac\ufffd\u20aco~ losk\ufffdt{\ufffd~ tz\ufffdsom{z\ufffdo\u00f0\ufffd {q\ufffdso|o~\u20act\u20ac\ufffdoz\ufffd\nzo\ufffd\u00d0{~v \u20ac\ufffd~\ufffdm\ufffd\ufffd~o \u00d0osk\ufffdo {l\u20aco~\ufffdon. t\ufffdk||ok~\u20ac \ufffdsk\ufffd \ufffdso~o\ufffdokwon |{wk~t\u20ack\ufffdt{z t\u20ackqok\ufffd\ufffd~o {q\ufffdso\n\u20ac\u00de\u20ac\ufffdoy k\u20ack\u00d0s{wo kzn z{\ufffdmk\ufffd\u20acon l\u00de\u20ac|omtqtm o\ufffdoz\ufffd\u20ac {~\ufffd\u20aco~\u20ac1\n\\st\u20ac k~\ufffdtmwo tww\ufffdytzk\ufffdo\u20ac \u20aco\ufffdo~kw zo\u00d0 ntyoz\u20act{z\u20ac {q\ufffdsoyontk nolk\ufffdo k~{\ufffdzn oz\ufffdt~{zyoz\ufffdkw\n|{wt\ufffdtm\u20ac kzn mwtyk\ufffdo mskzro1 \\so qtzntzr\u20ac m{y|woyoz\ufffd |~o\ufffdt{\ufffd\u20ac \u20ac\ufffd\ufffdnto\u20ac \ufffdsk\ufffd sk\ufffdo \u20acs{\u00d0z mwt/\nyk\ufffdo/~owk\ufffdon oms{ mskylo~\u20ac o\u00f0t\u20ac\ufffd tznt~om\ufffd \ufffd\u20aco~/\ufffd\u20aco~ tz\ufffdo~km\ufffdt{z\u20ac *o1r1 d4<. 5;f+ l\u00de\u20acs{\u00d0tzr\n\ufffdsk\ufffd \u20actytwk~ \u20ac\ufffd~\ufffdm\ufffd\ufffd~o\u20ac kw\u20ac{ msk~km\ufffdo~t\u20aco |k\ufffd\ufffdo~z\u20ac {qtzq{~yk\ufffdt{z/\u20acsk~tzr1 \\so \\\u00d0t\ufffd\ufffdo~ yo\u20ac\u20ackro\u20ac\n\ufffdsk\ufffd q{~y \ufffdso\u20acsk~tzr zo\ufffd\u00d0{~v \u20ac\ufffd\ufffdnton so~o ~oqwom\ufffd kyomskzt\u20acy {qzo\u00d0\u20ac |~{y{\ufffdt{z kzn km\ufffdt\ufffdo\nk\ufffd\ufffdoy|\ufffd\u20ac \ufffd{tzq{~y {\ufffdso~\u20ac. \u00d0stms t\u20acz{\ufffdkw\u00d0k\u00de\u20ac \ufffdsomk\u20aco tznt~om\ufffd |o~\u20ac{zkw tz\ufffdo~km\ufffdt{z\u20ac1 _okw\u20ac{\nmk|\ufffd\ufffd~o k\ufffdt\ufffdkw knnt\ufffdt{zkw owoyoz\ufffd l\u00deo\u00f0|w{~tzr \ufffdsozo\ufffd\u00d0{~v \ufffd{|{w{r\u00de {\ufffdo~ \ufffdtyo kzn noy{z/\n\u20ac\ufffd~k\ufffdtzr \ufffdsk\ufffd \ufffdso|{wk~t\u20acon \u20ac\ufffd~\ufffdm\ufffd\ufffd~o |o~\u20act\u20ac\ufffd\u20ac. o\ufffdoz \u00d0soz \ufffdso~o t\u20acknt\u20ac~\ufffd|\ufffdt\ufffdo yktz\u20ac\ufffd~oky zo\u00d0\u20ac\no\ufffdoz\ufffd1 Ozknnt\ufffdt{z. \u00d0osk\ufffdo \u20acs{\u00d0z k\u20ac\ufffd~{zr m{~~owk\ufffdt{z lo\ufffd\u00d0ooz |{wt\ufffdtmkw kzn mwtyk\ufffdo/~owk\ufffdon\ntno{w{rtmkw ltk\u20aco\u20ac tzzo\u00d0\u20ac |~{n\ufffdm\ufffdt{z kzn m{z\u20ac\ufffdy|\ufffdt{z. \u00d0t\ufffds k\u20ac\u20ac{mtk\ufffdt{z\u20ac lo\ufffd\u00d0ooz woq\ufffd/\u00d0tzr2\noz\ufffdt~{zyoz\ufffdkw kzn ~trs\ufffd/\u00d0tzr2\u20acmo|\ufffdtm |{\u20act\ufffdt{z\u20ac1 \\st\u20ac \u00d0tnoz\u20ac \ufffdso\u20acm{|o {q|~o\ufffdt{\ufffd\u20ac \u20ac\ufffd\ufffdnto\u20ac {q\nyktz\u20ac\ufffd~oky zo\u00d0\u20ac yontk *o1r1 d94f+ \ufffd{tzmw\ufffdno \ufffdsotzm~ok\u20actzrw\u00de ty|{~\ufffdkz\ufffd {zwtzo zo\u00d0\u20ac yontk1\n_soz w{{von k\ufffdtzt\u20ac{wk\ufffdt{z. \u20aco\ufffdo~kw n{yktz\u20ac k||ok~ \ufffd{lom{non \u00d0t\ufffds y{~o {qkwoq\ufffd/\u00d0tzr.\noz\ufffdt~{zyoz\ufffdkwt\u20ac\ufffd ltk\u20ac \ufffdskz \u00d0{\ufffdwn loo\u00f0|om\ufffdon l\u00den{yktz vz{\u00d0wonro o\u00f0|o~\ufffd\u20ac1 \\\u00de|tmkww\u00de zo\ufffd\ufffd~kw\n\u20act\ufffdo\u20ac. \u20ac\ufffdms k\u20acGw{{ylo~r. k~o~o|{~\ufffdon k\u20acwoq\ufffd/\u00d0tzr kzn oz\ufffdt~{zyoz\ufffdkwt\u20ac\ufffd |~tyk~tw\u00de \ufffds~{\ufffdrs \ufffdso\nwoz\u20aco\u20ac \ufffdso\u00de \ufffd\u20aco\ufffd{m{\ufffdo~ \ufffdso\u20ac|omtqtm \ufffd{|tm {qmwtyk\ufffdo mskzro1 V\ufffdso~ n{yktz\u20ac. \u20ac\ufffdms k\u20ac\ufffdsoJktw\u00de\nIkwwo~. yk\u00de sk\ufffdo looz |{{~w\u00de ~o|~o\u20acoz\ufffdon l\u00de\ufffdsom{z\ufffdoz\ufffd \u20acky|wtzr |~{mo\u20ac\u20ac k\u20acqkm\ufffd\ufffdkw kzn \u20acmtoz/\n\ufffdtqtm o\u00f0\ufffd~km\ufffd\u20ac \ufffdkvoz \u00d0t\ufffds{\ufffd\ufffd \u20ac\ufffd~~{\ufffdzntzr mskwwozrtzr m{z\ufffdo\u00f0\ufffd \u00d0o~o m{z\u20actno~on k\u20ac\u20ac\ufffd||{~\ufffdtzr\n\ufffdso\u20acmtoz\ufffdtqtm m{z\u20acoz\u20ac\ufffd\u20ac1 \\st\u20ac \ufffdr{{n qkt\ufffds\ufffd {z\ufffdso|k~\ufffd {q\ufffdsom{no~\u20ac ~o\ufffd\ufffd~z\u20ac k|k~\ufffdtm\ufffdwk~w\u00de o~~{/\nzo{\ufffd\u20ac ~o\u20ac|{z\u20aco q{~_k\ufffd\ufffd\u20ac ]|_t\ufffds \\sk\ufffd. k|~{ytzoz\ufffd lw{r tz\ufffdso\u20acmo|\ufffdtmkw m{yy\ufffdzt\ufffd\u00de1 Tkz\u00de\n{q\ufffdsot~ k~\ufffdtmwo\u20ac |~o\u20acoz\ufffd }\ufffd{\ufffdk\ufffdt{z\u20ac q~{y |\ufffdlwtm qtr\ufffd~o\u20ac kzn \u20acmtoz\ufffdtqtm |k|o~\u20ac tzknnt\ufffdt{z \ufffd{\ufffdsot~\n{\u00d0z m{yyoz\ufffdk~\u00de \ufffdsk\ufffd q~o}\ufffdoz\ufffdw\u00de mskwwozro\u20ac \ufffdsoq~kytzr. \u00d0stms k~om{z\ufffdo\u00f0\ufffd\ufffdkw mw\ufffdo\u20ac \ufffdsk\ufffd yk\u00de\nlo\ufffdzk\ufffdktwklwo \ufffd{\ufffdsom{no~\u20ac1 \\so wk\u20ac\ufffdo\u00f0|wkzk\ufffdt{z q{~|{\u20ac\u20actlwo yt\u20acmwk\u20ac\u20actqtmk\ufffdt{z {qmo~\ufffdktz\nn{yktz\u20ac tnoz\ufffdtqton tz\ufffdst\u20aco\u00f0o~mt\u20aco t\u20ac\ufffdso~o|{\u20ac\ufffdtzr {qm{z\ufffdoz\ufffd q~{y {\ufffdso~ \u20ac{\ufffd~mo\u20ac1 Oz\ufffdo\u20ac\ufffdtrk\ufffdt{z\n{q\ufffdsok~\ufffdtmwo\u20ac m{non q~{y L{\u00f0 Uo\u00d0\u20ac q{\ufffdzn \ufffdsk\ufffd \ufffds~oo {q\ufffdsoqt\ufffdok~\ufffdtmwo\u20ac \u00d0o~o q~{y F\u20ac\u20ac{mtk\ufffdon\nW~o\u20ac\u20ac \u20ac{\ufffd~mo\u20ac. \u00d0stms yk\u00de |~o\u20acoz\ufffd kntqqo~oz\ufffd ont\ufffd{~tkw ltk\u20ac \ufffd{L{\u00f0 Uo\u00d0\u20ac {~trtzkw m{z\ufffdoz\ufffd1 \\so\nms{tmo {q\u00d0stms m{z\ufffdoz\ufffd \ufffd{~o|{\u20ac\ufffd q~{y {\ufffdso~ \u20ac{\ufffd~mo\u20ac t\u20ackzty|{~\ufffdkz\ufffd ont\ufffd{~tkw nomt\u20act{z kzn k\u20ac\n\u20ac\ufffdms \u00d0olowto\ufffdo \ufffdst\u20actzmw\ufffd\u20act{z sk\u20acz{\ufffdkn\ufffdo~\u20acow\u00de kqqom\ufffdon {\ufffd~qtzntzr\u20ac1\nL\ufffd\ufffd\ufffd~o \u00d0{~v tz\ufffdst\u20ack~ok m{\ufffdwn q\ufffd~\ufffdso~ o\u00f0kytzo \ufffdso\ufffdto\u20aclo\ufffd\u00d0ooz |{wt\ufffdtmkw kzn oz\ufffdt~{zyoz\ufffdkw\n{|tzt{z\u20ac1 \\st\u20ac \u20ac\ufffd\ufffdn\u00de \ufffd\u20acon s\ufffdykz m{no~\u20ac \ufffd{no\ufffdom\ufffd ltk\u20aco\u20ac q~{y k~\ufffdtmwo m{z\ufffdoz\ufffd1 _stwo \ufffdst\u20ac\nk||~{kms \u00d0k\u20ac\u20ac\ufffdmmo\u20ac\u20acq\ufffdw. t\ufffdmk~~to\u20ac \u20ac\ufffdl\u20ac\ufffdkz\ufffdtkw m{\u20ac\ufffd\u20ac \u00d0stms ykvo t\ufffdsk~n \ufffd{{|o~k\ufffdo q{~wk~ro\nnk\ufffdk\u20aco\ufffd\u20ac1 O\ufffdm{\ufffdwn lok~r\ufffdon \ufffdsk\ufffd {\ufffd~\ufffd\u20aco{q\u20ac\ufffdluom\ufffdt\ufffdo s\ufffdykz r~kntzr t\u20ackwtyt\ufffdk\ufffdt{z tz{\ufffd~\nkzkw\u00de\u20act\u20ac. l\ufffd\ufffdt\ufffdt\u20ackzomo\u20ac\u20ack~\u00de m{y|~{yt\u20aco rt\ufffdoz \ufffdsotzso~oz\ufffdw\u00de \u20ac\ufffdluom\ufffdt\ufffdo zk\ufffd\ufffd~o {q|{wt\ufffdtmkw\nkzn oz\ufffdt~{zyoz\ufffdkw lowtoq\u20ac kzn \ufffdsom\ufffd~~oz\ufffd wkmv {q{luom\ufffdt\ufffdo \ufffd{{w\u20ac q{~kzkw\u00de\u20actzr \u20ac\ufffdms m{y|wo\u00f0\ntz\ufffdo~|~o\ufffdk\ufffdt{z\u20ac tzwk~ro }\ufffdkz\ufffdt\ufffdto\u20ac {qnk\ufffdk1 Fzk\ufffd\ufffd{yk\ufffdon mwk\u20ac\u20actqto~ q{~\ufffdso\u20aco ltk\u20aco\u20ac \u00d0{\ufffdwn\n~o}\ufffdt~o \u20actrztqtmkz\ufffd \u00d0{~v q{~t\ufffd\u20acm~ok\ufffdt{z *\ufffd\u20actzr klwozn {qykmstzo wok~ztzr kzn zk\ufffd\ufffd~kw wkz/\nr\ufffdkro |~{mo\u20ac\u20actzr+. l\ufffd\ufffd\u00d0{\ufffdwn \u20ac\ufffd||{~\ufffd q\ufffd\ufffd\ufffd~o wk~ro/\u20acmkwo \u20ac\ufffd\ufffdnto\u20ac. kzomo\u20ac\u20ack~\u00de \u20ac\ufffdo| rt\ufffdoz \ufffdso\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 54259\no\ufffdo~/tzm~ok\u20actzr \ufffd{w\ufffdyo {q{zwtzo m{z\ufffdoz\ufffd1 Fz{\ufffdso~ ~o\u20acok~ms }\ufffdo\u20ac\ufffdt{z m{zmo~z\u20ac \u00d0so\ufffdso~ \ufffdso\n|k\ufffd\ufffdo~z\u20ac {l\u20aco~\ufffdon {z\\\u00d0t\ufffd\ufffdo~ o\u00f0\ufffdozn \ufffd{{\ufffdso~ {zwtzo |wk\ufffdq{~y\u20ac1 _o\u20ac\ufffd\ufffdnton \\\u00d0t\ufffd\ufffdo~ n\ufffdo\ufffd{t\ufffd\u20ac\n|~{ytzoz\ufffd ~{wo k\u20ackyokz\u20ac q{~\ufffd\u20aco~\u20ac \ufffd{qtzn zo\u00d0\u20ac \u20ac\ufffd{~to\u20ac d99f kzn t\ufffd\u20acq~o}\ufffdoz\ufffd \ufffd\u20ackro k\u20ack|wk\ufffd/\nq{~y q{~wt\ufffdow\u00de |{wt\ufffdtmkw nolk\ufffdo\u20ac tz\u00d0stms \u20ac\ufffd~kzro~\u20ac mkztz\ufffdo~km\ufffd1 Oq\u20ac\ufffdt\ufffdklwo nk\ufffdk\u20aco\ufffd\u20ac m{\ufffdwn lo\n{l\ufffdktzon. t\ufffd\u00d0{\ufffdwn lo|{\u20ac\u20actlwo \ufffd{k||w\u00de \u20actytwk~ yo\ufffds{n\u20ac \ufffd{{\ufffdso~ |{|\ufffdwk~ \u20act\ufffdo\u20ac \u20ac\ufffdms k\u20acLkmol{{v\nkzn Zonnt\ufffd? s{\u00d0o\ufffdo~. |~t\ufffdkm\u00de ~o\u20ac\ufffd~tm\ufffdt{z\u20ac |~o\ufffdoz\ufffd ~o\u20acok~ms {zykz\u00de \u20ac{mtkw yontk |wk\ufffdq{~y\u20ac1\nLtzkww\u00de. {zoq\ufffd~\ufffdso~ }\ufffdo\u20ac\ufffdt{z \ufffdsk\ufffd t\u20acz{\ufffdknn~o\u20ac\u20acon so~o t\u20acn{\u00d0z\u20ac\ufffd~oky o\u00f0|{\u20ac\ufffd~o \ufffd{\u20acsk~on m{z/\n\ufffdoz\ufffd. \ufffdsk\ufffd t\u20ac.\u00d0s{ \u20acoo\u20ac \ufffdsok~\ufffdtmwo\u20ac \ufffdsk\ufffd k~o\u20acsk~on l\u00de\\\u00d0t\ufffd\ufffdo~ \ufffd\u20aco~\u20acD O\ufffdt\u20ac|~{lklwo \ufffdsk\ufffd \ufffdso\ufffd\u20aco~\u20ac\nmk|\ufffd\ufffd~on tz{\ufffd~nk\ufffdk\u20aco\ufffd. \ufffdsk\ufffd \u20acowom\ufffd kzn \u20acsk~o {zwtzo k~\ufffdtmwo\u20ac kl{\ufffd\ufffd mwtyk\ufffdo mskzro \ufffds~{\ufffdrs\n\\\u00d0t\ufffd\ufffdo~. km\ufffdk\u20acj{|tzt{z wokno~\u20ac) *q{ww{\u00d0tzr \ufffdsow{zr/o\u20ac\ufffdklwt\u20acson \ufffd\u00d0{/\u20ac\ufffdo| qw{\u00d0 y{now {qRk\ufffd\u00fe\nkzn Sk\u00fek~\u20acqown d9:f+. w{mk\ufffdtzr zo\u00d0 tzq{~yk\ufffdt{z kl{\ufffd\ufffd \ufffdso\ufffd{|tm kzn nt\u20ac\u20acoytzk\ufffdtzr t\ufffd\ufffd{\ufffdsot~\nq{ww{\u00d0o~\u20ac1 Tok\u20ac\ufffd~tzr \ufffdso\ufffd{w\ufffdyo {qn{\u00d0z\u20ac\ufffd~oky \ufffdto\u00d0\u20ac kzn ~o\ufffd\u00d0oo\ufffd ~k\ufffdo\u20ac \u00d0{\ufffdwn kz\u20ac\u00d0o~ ty|{~/\n\ufffdkz\ufffd }\ufffdo\u20ac\ufffdt{z\u20ac kl{\ufffd\ufffd \u00d0stms yo\u20ac\u20ackro\u20ac k~oy{~o oqqom\ufffdt\ufffdo \u00d0soz \u20acsk~on {z\u20ac{mtkw yontk1 Vzo vo\u00de\n}\ufffdo\u20ac\ufffdt{z t\u20ac\ufffdsoo\u00f0\ufffdoz\ufffd \ufffd{\u00d0stms |{wk~t\u20ack\ufffdt{z o\u00f0t\u20ac\ufffd\u20ac tz\ufffdst\u20ac\u20acom{znk~\u00de m{z\u20ac\ufffdy|\ufffdt{z {qmwtyk\ufffdo\nmskzro yontk. {~\u00d0so\ufffdso~ zo\ufffd\u00d0{~v oqqom\ufffd\u20ac yt\ufffdtrk\ufffdo |{wk~t\u20ack\ufffdt{z l\u00demk|\ufffd\ufffd~tzr \ufffdto\u00d0\u20ac q~{y l{\ufffds\n\u20actno\u20ac1\nFz\u00de rk\ufffdso~tzr {qnk\ufffdk q~{y {zwtzo \u20ac{mtkw zo\ufffd\u00d0{~v\u20ac ~o}\ufffdt~o\u20ac mk~oq\ufffdw m{z\u20actno~k\ufffdt{z {q\ufffdso\nltk\u20aco\u20ac \ufffdsk\ufffd t\ufffdyk\u00de tz\ufffd~{n\ufffdmo1 \\so \ufffd\u20aco~\u20ac {q{zwtzo |wk\ufffdq{~y\u20ac k~okntqqo~oz\ufffd nt\u20ac\ufffd~tl\ufffd\ufffdt{z {q|o{|wo\n\ufffdskz \ufffdsorozo~kw |{|\ufffdwk\ufffdt{z> \ufffdso\u00de \ufffdozn \ufffd{lo\u00de{\ufffdzro~. y{~o \u00d0okw\ufffds\u00de kzn lo\ufffd\ufffdo~ on\ufffdmk\ufffdon d9;f1\nV\ufffd~ \u20acky|wtzr {zvo\u00de\u00d0{~n\u20ac yokz\u20ac \ufffdsk\ufffd \u00d0o\u00d0twwmk|\ufffd\ufffd~o \ufffd\u20aco~\u20ac \u00d0s{ k~oy{~o ozrkron \u00d0t\ufffds \ufffdso\n\ufffd{|tm {qmwtyk\ufffdo mskzro. kzn y{~o{\ufffdo~ k~o\u20actrztqtmkz\ufffdw\u00de tz\ufffdo\u20ac\ufffdon oz{\ufffdrs \ufffd{\u20acsk~o tzq{~yk\ufffdt{z\n\u00d0t\ufffds \ufffdsot~ q{ww{\u00d0o~\u20ac1 F\u20ac\u20ac\ufffdms \u00d0ok~om{z\u20actno~tzr kstrsw\u00de/y{\ufffdt\ufffdk\ufffdon \u20acky|wo {qkmo~\ufffdktz |k~\ufffd {q\n\u20ac{mto\ufffd\u00de. l\ufffd\ufffd\ufffdst\u20act\u20ac\u00d0sk\ufffd \u00d0twwlo\ufffdt\u20actlwo \ufffd{ykz\u00de \ufffd\u20aco~\u20ac \u00d0soz \ufffdso\u00de \ufffdt\u20act\ufffd \\\u00d0t\ufffd\ufffdo~1 \\st\u20ac \u20acky|wtzr\nkw\u20ac{ o\u00f0|wktz\u20ac \ufffdsontqqo~ozmo q~{y Mw{lkw _k~ytzr)\u20ac [t\u00f0Fyo~tmk\u20ac d4f\ufffdsk\ufffd \u00d0o{l\u20aco~\ufffdo. k\u20ac{zw\u00de\n\ufffdso\ufffdkwk~yon\ufffd kzn \ufffdnt\u20acyt\u20ac\u20act\ufffdo\ufffd \ufffd\u00de|o\u20ac k~owtvow\u00de \ufffd{lotz\ufffdo\u20ac\ufffdon oz{\ufffdrs \ufffd{lomk|\ufffd\ufffd~on1 \\so \u00d0{~v\n{q_twwtky\u20ac o\ufffdkw1d4<f \u20ac\ufffd||{~\ufffd \ufffdst\u20ack\u20ac\ufffdsot~ m{ntzr {q\\\u00d0t\ufffd\ufffdo~ \ufffd\u20aco~\u20ac q{\ufffdzn {zw\u00de \ufffdsoo\u00f0\ufffd~oyo\noz\ufffdt~{zyoz\ufffdkwt\u20ac\ufffd\u20ac kzn \u20acmo|\ufffdtm\u20ac1 V\ufffd~ vo\u00de\u00d0{~n\u20ac \u00d0o~o ms{\u20acoz \ufffd{mk|\ufffd\ufffd~o k\u20acy\ufffdms {q\ufffdsoKzrwt\u20acs\nwkzr\ufffdkro m{y|{zoz\ufffd\u20ac {q\ufffdsomwtyk\ufffdo mskzro m{z\ufffdo~\u20ack\ufffdt{z {zwtzo. l\ufffd\ufffd\ufffdso{l\u20aco~\ufffdon \u20ac\ufffd~\ufffdm\ufffd\ufffd~o\u20ac\nyk\u00de \ufffdk~\u00de tz{\ufffdso~ wkzr\ufffdkro\u20ac1 \\so\u20aco qkm\ufffd{~\u20ac \u20acs{\ufffdwn lom{z\u20actno~on tzkz\u00dekzkw\u00de\u20act\u20ac {q\u20ac{mtkw\nyontk nk\ufffdk l\ufffd\ufffd\u00d0on{z{\ufffdlowto\ufffdo \ufffdsk\ufffd \ufffdso\u00de kqqom\ufffd \ufffdso\u20ac\ufffd~ozr\ufffds {q{\ufffd~qtzntzr\u20ac tzkz\u00de\u00d0k\u00de1\n\\st\u20ac \u00d0{~v noy{z\u20ac\ufffd~k\ufffdo\u20ac \ufffdsk\ufffd yontk m{yy\ufffdztmk\ufffdt{z {qtzq{~yk\ufffdt{z k~{\ufffdzn mwtyk\ufffdo mskzro\nqkmo\u20ac ykz\u00de mskwwozro\u20ac tz\ufffdsokro{q\u20ac{mtkw yontk1 ]\u20aco~\u20ac ~o\ufffd\ufffd~z \ufffd{\ufffdso\u20ackyo \ufffd~\ufffd\u20ac\ufffdon \u20ac{\ufffd~mo\u20ac q{~\ntzq{~yk\ufffdt{z o\ufffdoz \u00d0soz |~o\u20acoz\ufffdon \u00d0t\ufffds zo\u00d0 m{z\ufffdo\u00f0\ufffd\u20ac. kzn kz\u00dek\ufffd\ufffdoy|\ufffd\u20ac \ufffd{k\ufffd\ufffd~km\ufffd zo\u00d0 ~okno~/\n\u20acst|\u20ac zoon \ufffd{m{z\u20actno~ \ufffdst\u20aclosk\ufffdt{\ufffd~1 ]zno~\u20ac\ufffdkzntzr s{\u00d0 \ufffdso\u20aco \ufffd~\ufffd\u20ac\ufffdon \ufffdto\u20acq{~y \u00d0twwlovo\u00de\n\ufffd{m{ylk\ufffdtzr \ufffdso\u20ac|~okn {qyt\u20actzq{~yk\ufffdt{z \ufffdsk\ufffd m\ufffd~~oz\ufffdw\u00de mskwwozro\u20ac {zwtzo \u20ac{mtkw zo\ufffd\u00d0{~v\u20ac\nkzn \ufffdt\ufffdkw q{~|~{y{\ufffdtzr km\ufffdt{z {zmwtyk\ufffdo mskzro kzn {\ufffdso~ \u20ac{mto\ufffdkw mskwwozro\u20ac1\n[\ufffd||{~\ufffdtzr tzq{~yk\ufffdt{z\n[4Ltwo1\n*WJL+\nF\ufffd\ufffds{~ I{z\ufffd~tl\ufffd\ufffdt{z\ufffd\nI{zmo|\ufffd\ufffdkwt\u00fek\ufffdt{z> \\~t\u20ac\ufffdkz Q1G1Ikzz. Oktz [1_ok\ufffdo~1\nJk\ufffdk m\ufffd~k\ufffdt{z> \\~t\u20ac\ufffdkz Q1G1Ikzz. Oktz [1_ok\ufffdo~1\nL{~ykw kzkw\u00de\u20act\u20ac> \\~t\u20ac\ufffdkz Q1G1Ikzz. Oktz [1_ok\ufffdo~. N\u00de\u00d0ow \\1W1_twwtky\u20ac1\nL\ufffdzntzr km}\ufffdt\u20act\ufffdt{z> N\u00de\u00d0ow \\1W1_twwtky\u20ac1\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 55259\nOz\ufffdo\u20ac\ufffdtrk\ufffdt{z> \\~t\u20ac\ufffdkz Q1G1Ikzz. Oktz [1_ok\ufffdo~. N\u00de\u00d0ow \\1W1_twwtky\u20ac1\nTo\ufffds{n{w{r\u00de> \\~t\u20ac\ufffdkz Q1G1Ikzz. Oktz [1_ok\ufffdo~. N\u00de\u00d0ow \\1W1_twwtky\u20ac1\n[{q\ufffd\u00d0k~o> \\~t\u20ac\ufffdkz Q1G1Ikzz. Oktz [1_ok\ufffdo~1\n[\ufffd|o~\ufffdt\u20act{z> Oktz [1_ok\ufffdo~. N\u00de\u00d0ow \\1W1_twwtky\u20ac1\n^t\u20ac\ufffdkwt\u00fek\ufffdt{z> \\~t\u20ac\ufffdkz Q1G1Ikzz. Oktz [1_ok\ufffdo~1\n_~t\ufffdtzr \u02d8{~trtzkw n~kq\ufffd> \\~t\u20ac\ufffdkz Q1G1Ikzz1\n_~t\ufffdtzr \u02d8~o\ufffdto\u00d0 \u2019ont\ufffdtzr> \\~t\u20ac\ufffdkz Q1G1Ikzz. Oktz [1_ok\ufffdo~. N\u00de\u00d0ow \\1W1_twwtky\u20ac1\nZoqo~ozmo\ufffd\n41 Sot\ufffdo~{\ufffdt \ufffd\ufffdF.Tktlkms K.Z{\ufffdo~/Z oz{\ufffdq I.Lotzlo~ rM.N{\ufffdo W1Mw{lkw _k~ytzr )\ufffd[t\ufffdFyo~tmk\ufffd> [o|/\n\ufffdoylo~ 5345? 53461\n51 To\ufffdkr Q.L\ufffd\u00c6ms\ufffdwtz \\.[msk\u00c6qo~T[1 Mw{lkw \ufffdk~ytzr)\ufffd qt\ufffdoMo~ykz\ufffd\ufffd >F\ufffd\ufffd|{w{r\ufffd {qMo~ykz\ufffd )\ufffdto\ufffd\ufffd {z\nmwtyk\ufffdo mskzro kzn|k\ufffd\ufffdo~z\ufffd {qyontk \ufffd\ufffdokzntzq{~yk\ufffdt {z1W\ufffdlwtm ]zno~\ufffd\ufffdkz ntzr {q[mtozmo1 534;? 5:\n*7+>767\u02d879 41s\ufffd\ufffd|\ufffd>22n{ t1{~r243144;; 23=:6::5 9499=599< WTOJ> 5:47547<\n61 J\ufffdzwk| ZK.TmI~tr s\ufffdFT. bk~{\ufffds QN1\\so W{wt\ufffdtmkw Jt\ufffdtno {zIwtyk\ufffdo Iskzro> Wk~\ufffdt\ufffdkz W{wk~t\ufffdk\ufffd t{z\n_tnoz\ufffd tz\ufffdso]1[1 Kz\ufffdt~{zyoz\ufffd >[mtozmo kznW{wtm\ufffd q{~[\ufffd\ufffd\ufffdktzk lwoJo\ufffdow{|yoz\ufffd1 534:? 9<*9+>7\u02d856 1\n71 Lownykz S.Nk~\ufffd W[.Sot\ufffdo~{ \ufffdt\ufffd\ufffd F.Tktlkms K.Z{\ufffdo~/Zoz{\ufffd qI1J{N{\ufffd\ufffdtwo Tontk Wo~mo|\ufffd t{z\ufffd Sokn \ufffd{\nFm\ufffdt{zD \\so Z{wo {qN{\ufffd\ufffdtwo Tontk Wo~mo|\ufffdt{z\ufffd .W{wt\ufffdtmkw Kqqtmkm\ufffd. kznOno{w{r\ufffd tzW~ontm\ufffd tzrIwtyk\ufffdo\nIskzro Fm\ufffdt\ufffdt\ufffdy1 I{yy\ufffdz tmk\ufffdt{z Zo\ufffdok~ms1 534;? 77*<+>43== \u02d844571 s\ufffd\ufffd|\ufffd>22n{t1{~ r243144; ;2\n33=6:935479 :9=47\n91 W{~\ufffdo~ FQ.Noww\ufffd\ufffdoz O1Oz\ufffdo\ufffd\ufffd trk\ufffdtzr Wk~\ufffdtmt|k\ufffd{~\ufffd J\ufffdzkyt m\ufffd\\s~{\ufffdrs [{mtkw Tontk ]\ufffdtzr kT\ufffdw\ufffdtno\ufffdo~ yt/\nzkz\ufffd \ufffdL~kyo\ufffd F||~{km s>\\so Ik\ufffdo {qIwtyk\ufffd ork\ufffdo {zb{\ufffd\\\ufffdlo1 Q{\ufffd~zkw {qI{y|\ufffd\ufffdo~/ Tontk\ufffdon I{y/\ny\ufffdztmk\ufffdt{z 15347? 4=*7+>435 7\u02d843741 s\ufffd\ufffd|\ufffd>22n{t1{~ r2431444 42umm71453: 9\n:1 I{zz{~ W.Nk~~t\ufffd K.M\ufffd\ufffd [.Lo~zkzn{ Q.[skzv JG.R\ufffd~\ufffd \\.o\ufffdkw1Oz\ufffdo~|o~\ufffd {zkw m{yy\ufffdztmk\ufffd t{zkl{\ufffd\ufffd mwt/\nyk\ufffdo mskzro> s{\ufffd yo\ufffd\ufffdkro\ufffd mskzro \ufffdsoz m{yy\ufffdztm k\ufffdon \ufffds~{\ufffdrs \ufffdty\ufffdwk\ufffdon {zwtzo \ufffd{mtkw zo\ufffd\ufffd{~v\ufffd1\nIwtyk\ufffdtm Iskzro1 534:? 46:*6+>7:6 \u02d87;:1 s\ufffd\ufffd|\ufffd>22n{t1{~ r2431433 ;2\ufffd439<7 /34:/4:76/\ufffd\n;1 cskzr G.\ufffdkzno~Stznoz [.Ttwnozlo~ro~ T.Tk~w{z QZ.N{\ufffdo WJ.Sot\ufffdo~{\ufffdt \ufffd\ufffdF1K\ufffd|o~tyo z\ufffdkwoqqom\ufffd\ufffd\n{qmwtyk\ufffdo yo\ufffd\ufffdkro\ufffd \ufffdk~\ufffd ro{r~k|stm kww\ufffd1 Uk\ufffd\ufffd~o Iwtyk\ufffdo Iskzro1 534<? <*9+>6;3\u02d86 ;71s\ufffd\ufffd|\ufffd>2 2n{t1{~r2\n431436<2 \ufffd7499</34</ 3455/3\n<1 ^kwvozl\ufffd~r W.Wo\ufffdo~ Q1I{yy Zo\ufffdok~m s\u0152^to\ufffd\ufffd q~{y K\ufffd~{|o\ufffd Lt\ufffdo Iskwwozr o\ufffdq{~\ufffdsoL\ufffd\ufffd\ufffd~o {qTontk/\nKqqom\ufffd\ufffd Zo\ufffdok~ms1 Oz\ufffdo~zk\ufffdt{zkw Q{\ufffd~zkw {qI{yy\ufffdz tmk\ufffdt{z1 5346? ;*3+1\n=1 Loo\ufffdoww Q\\1Froznk [o\ufffd\ufffdtzr \ufffds~{\ufffdrs [{mtkw Tontk> \\so Oy|{~\ufffdkzmo {qOzmtnoz\ufffdkw Uo\ufffd\ufffd K\ufffd|{\ufffd\ufffd~o kzn\n[{mtkw Ltw\ufffdo~tzr tz\ufffdsoJtrt\ufffdkw K~k1 W{wt\ufffdtmkw Zo\ufffdok~ms Y\ufffdk~\ufffdo~w\ufffd 1534<? ;4*5+>7<5\u02d8 7=71 s\ufffd\ufffd|\ufffd>22n{ t1{~r2431\n44;;243:9= 45=4;;77<=9\n431 T{\u00c6wwo~Q.\\~twwtzr J.Nowlo~r o~U.\ufffdkzK\ufffdG1J{z{\ufffdlwkyo t\ufffd{z\ufffdsokwr{~t\ufffdsy> kzoy|t~tmkw k\ufffd\ufffdo\ufffd\ufffdyoz \ufffd{q\ny\ufffdw\ufffdt|wo ~om{yyozno ~\ufffd\ufffd\ufffd\ufffdoy\ufffd kzn\ufffdsot~ ty|km\ufffd {zm{z\ufffdoz\ufffd nt\ufffdo~\ufffdt\ufffd\ufffd1 Ozq{~yk\ufffdt{ z.I{yy\ufffdz tmk\ufffdt{z \u2019\n[{mto\ufffd\ufffd1 534<? 54*;+>=9=\u02d8 =;;1 s\ufffd\ufffd|\ufffd>22n{t1{~ r243143<32 46:=44<a153 4<147773;:\n441 Ik\ufffd\ufffdoww\ufffd T1I{yy\ufffdz tmk\ufffdt{z. W{\ufffdo~ kznI{\ufffdz\ufffdo~/|{\ufffd o~tz\ufffdsoUo\ufffd\ufffd{~v [{mto\ufffd\ufffd1 Oz\ufffdo~zk\ufffdt{zkw Q{\ufffd~zkw {q\nI{yy\ufffdz tmk\ufffdt{z1 533;? 4*4+1\n451 [sok~o~ K.M{\ufffd\ufffdq~ton Q1Uo\ufffd\ufffd ]\ufffdo Fm~{\ufffd\ufffd [{mtkw Tontk Wwk\ufffdq{~y \ufffd534;1 Wo\ufffd Zo\ufffdok~ms Ioz\ufffdo~? 534;1\nF\ufffdktwklwo q~{y> s\ufffd\ufffd|>22\ufffd\ufffd\ufffd1 u{\ufffd~zkwt\ufffdy1{~r 2534;23= 23;2zo\ufffd\ufffd/\ufffd\ufffd o/km~{\ufffd\ufffd/\ufffd{ mtkw/yontk/|w k\ufffdq{~y\ufffd/53 4;21\n461 _ok\ufffdo~ O[._twwtky\ufffd N.It{~{tkz \ufffdO.Qk\ufffdz\ufffd S.I{kz \\.Gkzn\ufffdmmt [1I{yy\ufffdz t\ufffdto\ufffd {q{zwtzo zo\ufffd\ufffd o\ufffd|{/\n\ufffd\ufffd~o n\ufffd~tzr \ufffdso]RMozo~kw Kwom\ufffdt{z 53491 Vzwtzo [{mtkw Uo\ufffd\ufffd{~v\ufffd kznTontk1 534=? 43/44>4<\u02d863 1\ns\ufffd\ufffd|\ufffd>22n{t1{~ r2431434 :2u1{\ufffdzoy153 4=139133 4\n471 JtTkrrt{ W.K\ufffdkz\ufffd Q.G~\ufffd\ufffd{z G1Nk\ufffdo Fyo~tmkz )\ufffd[{mtkw F\ufffd\ufffdt\ufffd\ufffdno\ufffd Gom{yo T{~o W{wk~t\ufffdonD Fyo~tmkz\nQ{\ufffd~zkw {q[{mt{w{ r\ufffd14==:? 435*6+>:=3 \u02d8;991 s\ufffd\ufffd|\ufffd>22n{t1{~ r243143< :2563==9\n491 Lo~zkzno\ufffd T.Wtmm{w{ S[M. Tk\ufffdzk~n J._t||{{ T.Totwt I.Fwkzt N1\\kwvtzr Iwtyk\ufffd oIskzro \ufffdtk[{mtkw\nTontk> I{yy\ufffdztm k\ufffdt{z. Kzrkroy oz\ufffdkznGosk\ufffdt{\ufffd~1 Oz>W~{moontzr\ufffd {q\ufffdso<\ufffdsFIT I{zqo~oz mo{z\n_ol [mtozmo? 534:1 |1<9\u02d8=71 F\ufffdktwklwo q~{y> s\ufffd\ufffd|>22n{t1k my1{~r24314 47925=3 <46415=3<4:; 1\n4:1 [oro~lo~ rF.Gozzo\ufffd\ufffd _S1 [{mtkw Tontk kzn\ufffdsoV~rkzt\ufffdk\ufffdt{ z{qI{wwom\ufffdt\ufffdo Fm\ufffdt{z> ]\ufffdtzr \\\ufffdt\ufffd\ufffdo~ \ufffd{\nK\ufffd|w{~o \ufffdsoKm{w{rt o\ufffd{q\\\ufffd{ Iwtyk\ufffdo Iskzro W~{\ufffdo\ufffd\ufffd\ufffd1 \\so I{yy\ufffdz tmk\ufffdt{z Zo\ufffdto\ufffd1 5344? 47*6+>4=;\u02d8\n5491 s\ufffd\ufffd|\ufffd>22n{ t1{~r243143<3 243;4775 41534419=;593\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 56259\n4;1 Kwro\ufffdoy J.[\ufffdo\ufffdvkw S.Jtkv{|{ \ufffdw{\ufffd U1[\ufffd~\ufffdm\ufffd\ufffd~o kznI{z\ufffdoz\ufffd {q\ufffdsoJt\ufffdm{\ufffd~ \ufffdo{zIwtyk\ufffdo Iskzro tz\n\ufffdsoGw{r{\ufffd| so~o> \\so GtrWtm\ufffd\ufffd~o1 Kz\ufffdt~{zy oz\ufffdkw I{yy\ufffdz tmk\ufffdt{z1 5349? =*5+>4:=\u02d84 <<1s\ufffd\ufffd|\ufffd>22n{t1{~ r2\n43143<32 4;9573651534 71=<696 :\n4<1 _twwtky\ufffd N\\W. TmT\ufffd~~k\ufffd QZ.R\ufffd~\ufffd \\.Skylo~\ufffd LN1Uo\ufffd\ufffd{~ vkzkw\ufffd\ufffdt\ufffd ~o\ufffdokw\ufffd {|oz q{~\ufffdy\ufffd kznoms{\nmskylo~\ufffd tz\ufffd{mtkw yontk nt\ufffdm\ufffd\ufffd\ufffdt {z\ufffd{qmwtyk\ufffdo mskzro1 Mw{lkw Kz\ufffdt~{zyoz\ufffd kwIskzro1 5349? 65>45:\u02d8\n46<1 s\ufffd\ufffd|\ufffd>22n{ t1{~r2431434: 2u1rw{oz\ufffdmsk 15349136 133:\n4=1 I{z{\ufffdo~ TJ. M{zm \ufffdkw\ufffdo\ufffd G.Lwkyytzt F.Tozm\ufffdo ~L1Wk~\ufffdt\ufffdkz k\ufffd\ufffdyyo\ufffd~ to\ufffdtz{zwtzo |{wt\ufffdtmkw km\ufffdt\ufffdt\ufffd\ufffd 1\nKWQ Jk\ufffdk [mtozmo1 5345? 4*4+>:1 s\ufffd\ufffd|\ufffd>22n{t1{ ~r243144732o| un\ufffd:\n531 I{z{\ufffdo~ T.Zk\ufffdvto\ufffdtm\ufffd Q.L~kzmt\ufffdm {T.M{zm \ufffdkw\ufffdo\ufffd G.Tozm\ufffdo ~L.Lwkyytzt F1W{wt\ufffdtmkw W{wk~t\ufffdk\ufffdt{z {z\n\\\ufffdt\ufffd\ufffdo~1 Oz>Oz\ufffdo~zk \ufffdt{zkw FFFO I{zqo~ozmo {z_ol kzn[{mtkw Tontk? 53441 |1<=\u02d8=:1 F\ufffdktwklwo q~{y>\ns\ufffd\ufffd|\ufffd>22\ufffd\ufffd\ufffd 1kkkt1{~r2{m \ufffd2tzno\ufffd1|s| 2OI_[T2OI _[T442|k |o~2\ufffdto\ufffd25< 7;1\n541 _oov\ufffd GK.Skzo J[.Rty JN. Soo[[.R\ufffdkv U1Ozmtnoz\ufffdkw K\ufffd|{\ufffd\ufffd~o .[owom\ufffdt\ufffdo K\ufffd|{\ufffd\ufffd~o .kznW{wt\ufffdtmk w\nOzq{~yk\ufffdt{ z[sk~tzr> Oz\ufffdor~k\ufffdtzr Vzwtzo K\ufffd|{\ufffd\ufffd~o Wk\ufffd\ufffdo~z\ufffd kznK\ufffd|~o\ufffd\ufffdt{ z{z[{mtkw Tontk1 Q{\ufffd~zkw {q\nI{y|\ufffd\ufffdo~/ Tontk\ufffdon I{yy\ufffdztm k\ufffdt{z1 534;? 55*:+>6:6\u02d8 6;=1 s\ufffd\ufffd|\ufffd>22n{t1{~ r24314444 2umm71454==\n551 Is{t Q.SooQR1Oz\ufffdo\ufffd\ufffdtrk\ufffdtzr \ufffdsooqqom\ufffd\ufffd {qzo\ufffd\ufffd \ufffdsk~tzr kzn|{wt\ufffdtmkw tz\ufffdo~o\ufffd\ufffd {z\ufffd{mtkw yontk zo\ufffd\ufffd{~v\nso\ufffdo~{rozot \ufffd\ufffd1I{y|\ufffd\ufffdo~\ufffd tzN\ufffdyk zGosk\ufffdt{~1 5349? 77>59<\u02d8 5::1 s\ufffd\ufffd|\ufffd>22n{t1{~ r2431434 :2u1msl1534 71\n44135=\n561 Gkv\ufffds\ufffd K.To\ufffd\ufffdtzr [.Fnkytm S1K\ufffd|{\ufffd\ufffd~o \ufffd{tno{w{rtmk ww\ufffdnt\ufffdo~\ufffdo zo\ufffd\ufffd kzn{|tzt{z {zLkmol{{v1 [mt/\nozmo1 53491 s\ufffd\ufffd|\ufffd>22n{t1{~ r2431445 :2\ufffdmtozmo1kkk 44:3 WTOJ> 59=96<53\n571 Fnkytm SF.Mwkzmo U1\\so W{wt\ufffdtmkw Gw{r{\ufffd| so~o kzn\ufffdso5337 ]1[1 Kwom\ufffdt{z> Jt\ufffdtnon \\so\ufffd Gw{r1 Oz>\nW~{moontzr\ufffd {q\ufffdso6~nOz\ufffdo~zk\ufffdt{zkw _{~v\ufffds{| {zStzv Jt\ufffdm{\ufffdo~\ufffd 1StzvRJJ)391 FIT? 53391 |16:\u02d8761\nF\ufffdktwklwo q~{y> s\ufffd\ufffd|>22n{t1k my1{~r24314 4792446 75;4144675;; 1\n591 [\ufffdz\ufffd\ufffdotz IZ1 Zo|\ufffdlwtm1m{y 5131W~tzmo\ufffd{ z]zt\ufffdo~\ufffdt\ufffd\ufffd W~o\ufffd\ufffd? 533;1 F\ufffdktwklwo q~{y> s\ufffd\ufffd|>22\ufffd\ufffd\ufffd1 u\ufffd\ufffd{~1{~r2\n\ufffd\ufffdklwo2u1m\ufffd\ufffd; \ufffdl\ufffd\ufffd1\n5:1 V)Uotww [Q.N\ufffdwyo T1Fztm{ztm k||~{kms q{~~o|~o\ufffdoz\ufffd tzrmwtyk\ufffdo mskzro1 Mw{lkw Kz\ufffdt~{zy oz\ufffdkw\nIskzro1 533=? 4=*7+>735\u02d8 7431 s\ufffd\ufffd|\ufffd>22n{t1{~ r2431434:2u1 rw{oz\ufffdmsk 1533=13;1337\n5;1 Jow^tmk~t{ T.Go\ufffd\ufffdt F.c{ww{ L.Wo\ufffd~{zt L.[mkwk F.Ikwnk~owwt M.o\ufffdkw1\\so \ufffd|~okntzr {qyt\ufffdtzq{~yk\ufffd t{z\n{zwtzo1 W~{moontzr\ufffd {q\ufffdsoUk\ufffdt{zkw Fmknoy\ufffd {q[mtozmo\ufffd1 534:? 446*6+>997 \u02d899=1 s\ufffd\ufffd|\ufffd>22n{t1 {~r2431\n43;62|zk\ufffd 1494;774446\n5<1 Qk\ufffdz\ufffd S._krrwo Q.Lt\ufffdso~ JZ1 Fzoy|t~tmkw o\ufffdkytzk\ufffd t{z{qoms{ mskylo~\ufffd tz][mwtyk\ufffdo |{wtm\ufffd zo\ufffd/\n\ufffd{~v\ufffd1 Uk\ufffd\ufffd~o Iwtyk\ufffdo Iskzro1 5349? 9>;<5\u02d8; <:1s\ufffd\ufffd|\ufffd>22n{t1{~ r2431436 <2zmwtyk\ufffdo5 :::\n5=1 Nk\u00c6\ufffd\ufffd\ufffdwo~ \\1Nok\ufffdtzr \ufffd|\ufffdsonolk\ufffdoD Tok\ufffd\ufffd~tz rq~kryoz\ufffdk \ufffdt{zkzn|{wk~t\ufffdk\ufffdt{z tzkMo~ykz mwtyk\ufffdo\nmskzro s\ufffd|o~wtzv zo\ufffd\ufffd{~v1 [{mtkw Uo\ufffd\ufffd{~ v\ufffd1534<? 97>636\u02d86461 s\ufffd\ufffd|\ufffd>22n{t1{~ r2431434:2u1 \ufffd{mzo\ufffd1534; 1\n431335\n631 Jow^tmk~t{ T.[mkwk F.Ikwnk~o wwtM.[\ufffdkzwo\ufffd NK.Y\ufffdk\ufffd\ufffd~{mt{m mst_1T{nowtz rm{zqt~yk\ufffdt{ zltk\ufffd kzn\n|{wk~t\ufffdk\ufffdt{z1 [mtoz\ufffdtqt mZo|{~\ufffd\ufffd1 534;? ;*4+>736=4 1s\ufffd\ufffd|\ufffd>22n{t1{~ r2431436 <2\ufffd~o|736=4 WTOJ> 5<3;7<;7\n641 Uo\ufffdykz \\W1\\~kmvtzr \ufffdso~owok\ufffdo {qOWII FZ9 {z\\\ufffdt\ufffd\ufffdo~> ]\ufffdo~\ufffd. m{yyoz\ufffd\ufffd .kzn\ufffd{\ufffd~mo\ufffd q{ww{\ufffdtzr\n\ufffdso~owok\ufffdo {q\ufffdso_{~vtzr M~{\ufffd| O[\ufffdyyk~\ufffd q{~W{wtm\ufffdykv o~\ufffd1W\ufffdlwtm ]zno~\ufffd\ufffdkzn tzr{q[mtozmo1 534;?\n5:*;+><49\u02d8 <591 s\ufffd\ufffd|\ufffd>22 n{t1{~r243144 ;;23=:6: :594::5<7;; WTOJ> 5:<:<35 9\n651 Rt~twozv{ FW.[\ufffdo|msoz v{\ufffdk [V1 W\ufffdlwtm ytm~{lw{r rtzr {zmwtyk\ufffdo mskzro> Vzo \ufffdok~ {q\\\ufffdt\ufffd\ufffdo~ \ufffd{~wn/\n\ufffdtno1 Mw{lkw Kz\ufffdt~{ zyoz\ufffdkw Iskzro1 5347? 5:>4;4\u02d84<51 s\ufffd\ufffd|\ufffd>22n{t1 {~r2431434:2u 1rw{oz\ufffdmsk 15347135 1\n33<\n661 Qkzr [T. Nk~\ufffd W[1W{wk~t\ufffdon q~kyo\ufffd {z\ufffdmwtyk\ufffdo mskzro\ufffd kzn\ufffdrw{lkw \ufffdk~ytzr\ufffd km~{\ufffd\ufffd m{\ufffdz\ufffd~to \ufffdkzn\n\ufffd\ufffdk\ufffdo\ufffd> K\ufffdtnozmo q~{y \\\ufffdt\ufffd\ufffdo ~ltrnk\ufffdk1 Mw{lkw Kz\ufffdt~{ zyoz\ufffdkw Iskzro1 5349? 65>44\u02d84;1 s\ufffd\ufffd|\ufffd>22n{t1{~ r2\n431434:2 u1rw{oz\ufffdms k153491351343\n671 V)Uotww [._twwtky\ufffd N\\W. R\ufffd~\ufffd \\._to~\ufffdyk G.G{\ufffdv{qq T1J{ytzkz\ufffd q~kyo\ufffd tzworkm\ufffd kzn\ufffd{mtkw yontk\nm{\ufffdo~kro {q\ufffdsoOWII Ltq\ufffds F\ufffd\ufffdo\ufffd\ufffdyo z\ufffdZo|{~\ufffd1 Uk\ufffd\ufffd~o Iwtyk\ufffd oIskzro1 5349? 91\n691 [ms\ufffdwn\ufffd QW.R{z~k\ufffds [N.[ms\ufffdk~\ufffd U1\ufffdMw{lkw \ufffdk~ytzr\ufffd {~\ufffdmwtyk\ufffdo mskzro\ufffd D_so\ufffdso~ \ufffdso|wkzo\ufffd t\ufffd\n\ufffdk~ytzr no|ozn\ufffd {z}\ufffdo\ufffd\ufffdt{z \ufffd{~ntzr 1W\ufffdlwtm V|tzt{z Y\ufffdk~\ufffdo~w\ufffd1 5344? ;9*4+>449\u02d8 4571 s\ufffd\ufffd|\ufffd>22n{t1{~ r2\n43143=62 |{}2zq}3;6\n6:1 Lk~~oww Q1Uo\ufffd\ufffd{~ v\ufffd\ufffd~\ufffdm\ufffd\ufffd~o kzntzqw\ufffdozmo {q\ufffdsomwtyk\ufffdo mskzro m{\ufffdz\ufffdo~/y {\ufffdoyoz\ufffd1 Uk\ufffd\ufffd~o Iwtyk\ufffdo\nIskzro1 534:? :*7+>6;3\u02d86 ;71s\ufffd\ufffd|\ufffd>2 2n{t1{~r243143 6<2zmwtyk\ufffdo 5<;9\n6;1 L~oow{z J.Tk~\ufffdtmv F.R~ot\ufffd\ufffd J1Lkw\ufffdo o}\ufffdt\ufffdkwo zmto\ufffd> Vzwtzo km\ufffdt\ufffdt\ufffdy q~{y woq\ufffd\ufffd{~trs\ufffd1 [mtozmo1 5353?\n6:=*:93<+ >44=;\u02d845341 s\ufffd\ufffd|\ufffd>22n{t1{~ r2431445:2 \ufffdmtozmo1kll5 75<WTOJ> 65<<6< :6\n6<1 [\ufffdk~lt~n R1K\ufffdkytztzr \ufffdsoFw\ufffdo~zk\ufffdt\ufffdo Tontk Km{\ufffd\ufffd \ufffd\ufffdoy \\s~{\ufffdrs \ufffdsoW~{n\ufffdm\ufffdt{z {qFw\ufffdo~zk\ufffd t\ufffdoUk~~k/\n\ufffdt\ufffdo\ufffd {qTk\ufffd\ufffd [s{{\ufffdtzr K\ufffdoz\ufffd\ufffd {z\\\ufffdt\ufffd\ufffdo~1 Oz>Oz\ufffdo~zk \ufffdt{zkw FFFO I{zqo~ozm o{z_ol kzn [{mtkw\nTontk? 534;1 |1563\u02d856= 1F\ufffdktwklwo q~{y> s\ufffd\ufffd|\ufffd>22kkk t1{~r2{m\ufffd2tz no\ufffd1|s|2O I_[T2OI_[ T4;2|k|o~2\n\ufffdto\ufffd249: 361\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 57259\n6=1 F~tqF.[skzkskz R.Is{\ufffd LQ.J{\ufffd{\ufffd \ufffd{b.[\ufffdk~lt~n R.[|t~{ K[1N{\ufffd Ozq{~yk\ufffdt{ z[z{\ufffdlkw w\ufffd>K\ufffd|w{~tzr\n\ufffdsoZ{wo {qK\ufffd|{\ufffd\ufffd~o tzVzwtzo Z\ufffdy{~ W~{|krk\ufffdt{ z1Oz>W~{moontzr\ufffd {q\ufffdso4=\ufffds FIT I{zqo~oz mo{z\nI{y|\ufffd\ufffdo~/ [\ufffd||{~\ufffdon I{{|o~k \ufffdt\ufffdo_{~v \u2019[{mtkw I{y|\ufffd\ufffdtzr? 534:1 |17::\u02d87;;1 F\ufffdktwkl woq~{y> s\ufffd\ufffd|>22\nn{t1kmy1{~r 24314479 25<4<37<15<4= =:71\n731 [msytn\ufffd FS.c{ww{ L.Jow^tmk~t{ T.Go\ufffd\ufffdt F.[mkwk F.Ikwnk~owwt M.o\ufffdkw1Fzk\ufffd{y\ufffd {qzo\ufffd\ufffd m{z\ufffd\ufffdy |\ufffdt{z\n{zLkmol{{v1 W~{moo ntzr\ufffd {q\ufffdsoUk\ufffdt{zkw Fmknoy\ufffd {q[mtozmo\ufffd1 534;? 447*45+>63 69\u02d8636=1 s\ufffd\ufffd|\ufffd>22\nn{t1{~r243143 ;62|zk\ufffd14: 4;395447 WTOJ> 5<5:93<5\n741 _twwtky\ufffd T.It{~{tkz\ufffd O._twwtky\ufffd N1Jtqqo~oz\ufffd Uo\ufffd\ufffd q{~Jtqqo~oz\ufffd ^to\ufffd\ufffd> W{wt\ufffdtmk wUo\ufffd\ufffd/[ sk~tzr I{yy\ufffd/\nzt\ufffdto\ufffd {z[{mtkw Tontk \\s~{\ufffdrs \ufffdso]RMozo~kw Kwom\ufffdt{z tz53491 Oz>Oz\ufffdo~zk\ufffdt{zkw FFFO I{zqo~oz mo{z\n_ol kzn[{mtkw Tontk? 534:1 |144<\u02d84591 F\ufffdktwkl woq~{y> s\ufffd\ufffd|\ufffd>22\ufffd\ufffd\ufffd 1kkkt1{~r2{ m\ufffd2tzno\ufffd1|s|2OI _[T2\nOI_[T 4:2|k|o~2\ufffdto\ufffd 246565245<9 31\n751 \\\ufffdt\ufffd\ufffdo~ [\ufffd~okytzr FWOn{m\ufffdyoz\ufffd k\ufffdt{z?1 s\ufffd\ufffd|\ufffd>22no\ufffd ow{|o~1\ufffd\ufffdt\ufffd \ufffdo~1m{y2oz2n{ m\ufffd2\ufffd\ufffdoo\ufffd\ufffd2q tw\ufffdo~/~okw\ufffdty o2k|t/\n~oqo~ozmo2 |{\ufffd\ufffd/\ufffd\ufffdk\ufffd\ufffd\ufffdo\ufffd/ qtw\ufffdo~1 Fmmo\ufffd\ufffdo n>3<236253 531\n761 Uo\ufffdykz TKQ1 [mtoz\ufffdtqtm m{wwkl{~ k\ufffdt{z zo\ufffd\ufffd{~v\ufffd1 OO1[s{~\ufffdo\ufffd\ufffd |k\ufffds\ufffd. \ufffdotrs\ufffdon zo\ufffd\ufffd{~v\ufffd. kznmoz\ufffd~kwt\ufffd\ufffd1\nWs\ufffd\ufffd Zo\ufffd K15334? :7>34:4651 s\ufffd\ufffd|\ufffd>22n{t1{~ r243144362 Ws\ufffd\ufffdZo\ufffdK1:71 34:465 WTOJ> 447:469:\n771 Ikzz \\QG. _ok\ufffdo~ O[._twwtky\ufffd N\\W1 O\ufffdt\ufffdm{~~om\ufffd \ufffd{|~{uom\ufffd kznno\ufffdom\ufffdD N{\ufffd \ufffdotrs\ufffdtzr \ufffdzt|k~\ufffdt\ufffdo |~{uom/\n\ufffdt{z\ufffd tzqw\ufffdoz mo\ufffdm{yy\ufffd zt\ufffd\ufffdno\ufffdom\ufffdt{z1 Uo\ufffd\ufffd{~v [mtozmo1 5353? |14\u02d84=1 s\ufffd\ufffd|\ufffd>22 n{t1{~r243143 4;2z\ufffd\ufffd1\n5353144\n791 Uo\ufffdykz TKQ. Mt~\ufffdkz T1Ltzntzr kzno\ufffdkw\ufffdk\ufffdtzr m{yy\ufffd zt\ufffd\ufffd\ufffd\ufffd~\ufffdm\ufffd\ufffd~o tzzo\ufffd\ufffd{~v\ufffd1 Ws\ufffd\ufffd Zo\ufffd K15337?\n:=1s\ufffd\ufffd|\ufffd>22n{t1 {~r243144362W s\ufffd\ufffdZo\ufffdK 1:=13::466 WTOJ> 47==995:\n7:1 Iwk\ufffd\ufffdo\ufffd F.Uo\ufffdykz TKQ. T{{~o I1Ltzntzr m{yy\ufffd zt\ufffd\ufffd\ufffd\ufffd~\ufffdm\ufffd\ufffd~o tz\ufffdo~\ufffd wk~ro zo\ufffd\ufffd{~v\ufffd 1Ws\ufffd\ufffd Zo\ufffd K1\n5337? ;3>3::4 441s\ufffd\ufffd|\ufffd>22n{t1{~ r2431443 62Ws\ufffd\ufffdZo\ufffd K1;313::444 WTOJ> 49:=;7 6<\n7;1 Jtqql{\ufffd Fzkw\ufffd\ufffdo FWOno\ufffdow{|o~ n{m\ufffdyoz\ufffdk\ufffdt {z?1s\ufffd\ufffd|\ufffd>22\ufffd\ufffd\ufffd 1ntqql{\ufffd1m{y2 no\ufffd2n{m\ufffd2kzkw \ufffd\ufffdo21 Fmmo\ufffd\ufffdo n>\n534</39/5 61\n7<1 W{~\ufffdo~ TL1 Fzkwr{~t\ufffdsy q{~\ufffd\ufffdqqt\ufffd \ufffd\ufffd~t||tzr1 W~{r~ky >owom\ufffd~{ztm wtl~k~\ufffd kzntzq{~yk\ufffdt {z\ufffd\ufffd\ufffd\ufffdoy\ufffd1 4=<3?\n47*6+>463\u02d8 46;1 s\ufffd\ufffd|\ufffd>22 n{t1{~r243144 3<2ol37: <47\n7=1 Tontk Gtk\ufffd2Lkm\ufffd Isomv?1 s\ufffd\ufffd|\ufffd>22 yontkltk\ufffd qkm\ufffdmsomv1m{y2 1Fmmo\ufffd\ufffdon> 534</3;/3 :1\n931 Qkm{y\ufffd T.^oz\ufffd\ufffd~tzt \\.No\ufffdykz z[.Gk\ufffd\ufffdtkz T1L{~moF\ufffdw k\ufffd5. kI{z\ufffdtz\ufffd{\ufffd \ufffdM~k|s Sk\ufffd{\ufffd\ufffd Fwr{~t\ufffdsy q{~\nNkzn\ufffd Uo\ufffd\ufffd{~v ^t\ufffd\ufffdkwt\ufffdk\ufffdt{z Jo\ufffdtrzon q{~\ufffdsoMo|st [{q\ufffd\ufffdk~o1 WSV[ VUK1 5347? =*:+>4\u02d8451 s\ufffd\ufffd|\ufffd>22\nn{t1{~r243146 ;42u{\ufffd~zkw1| {zo133=<:;= WTOJ> 57=47:;<\n941 Lownykz S.Tktlkms K_. Z{\ufffdo~/Z oz{\ufffdq I.Sot\ufffdo~{\ufffdt\ufffd \ufffdF1Iwtyk\ufffdo {zIklwo> \\so Uk\ufffd\ufffd~o kznOy|km\ufffd {q\nMw{lkw _k~ytzr I{\ufffdo~kro {zL{\ufffdUo\ufffd\ufffd. IUU. kznT[UGI1 \\so Oz\ufffdo~zk\ufffdt{zkw Q{\ufffd~zkw {qW~o\ufffd\ufffd2W{wt/\n\ufffdtm\ufffd1 5345? 4;*4+>6\u02d864 1s\ufffd\ufffd|\ufffd>22 n{t1{~r243144 ;;24=73 4:454475974 3\n951 N{~z\ufffdo\ufffd TQ.Nk~~t\ufffd KF.Ltowntzr R[1Zowk\ufffdt{z\ufffdst |\ufffdky{zr m{z\ufffd|t~k\ufffd{ ~tkwlowtoq\ufffd. m{z\ufffdo~\ufffdk\ufffdt\ufffd ykznmwt/\nyk\ufffdo \ufffdmo|\ufffdtmt\ufffdy km~{\ufffd\ufffd zk\ufffdt{z\ufffd1 Uk\ufffd\ufffd~o Iwtyk\ufffdo Iskzro1 534<? <*;+>:47\u02d8: 531s\ufffd\ufffd|\ufffd>22n{t1{~ r2431436< 2\n\ufffd7499</34< /349;/5\n961 Sk\ufffdk|\ufffd T1Tktz/yoy {~\ufffd\ufffd~tkzrwo m{y|\ufffd\ufffdk\ufffdt{z \ufffdq{~\ufffdo~\ufffd wk~ro *\ufffd|k~\ufffdo *|{\ufffdo~/wk\ufffd ++r~k|s\ufffd1 \\so{~o\ufffdtm kw\nI{y|\ufffd\ufffdo~ [mtozmo1 533<? 73;*4+>79< \u02d87;61 s\ufffd\ufffd|\ufffd>22n{t1{~ r2431434:2 u1\ufffdm\ufffd1533<13;13 4;\n971 I{wwo{zt K.Z{\ufffd\ufffdk F.F~\ufffdtn\ufffd\ufffd{z F1Kms{ Iskylo ~{~W\ufffdlwtm [|so~oD W~ontm\ufffd tzrW{wt\ufffdtmkw V~toz\ufffdk\ufffdt{z kzn\nTok\ufffd\ufffd~tz rW{wt\ufffdtmkw N{y{|stw\ufffd tz\\\ufffdt\ufffd\ufffdo ~]\ufffdtzr GtrJk\ufffdk1 Q{\ufffd~zkw {qI{yy\ufffdz tmk\ufffdt{z1 5347? :7*5+>64;\u02d8\n6651 s\ufffd\ufffd|\ufffd>22n{ t1{~r24314444 2um{y1453<7\n991 [sok~o~ K.Tk\ufffd\ufffdk RK1Uo\ufffd\ufffd ]\ufffdo Fm~{\ufffd\ufffd [{mtkw Tontk Wwk\ufffdq{~y\ufffd 534<1 Wo\ufffd Zo\ufffdok~ms Ioz\ufffdo~? 534<1\nF\ufffdktwklwo q~{y> s\ufffd\ufffd|\ufffd>22\ufffd\ufffd\ufffd 1u{\ufffd~zkwt\ufffdy 1{~r2534<23=24 32zo\ufffd\ufffd /\ufffd\ufffdo/km~{\ufffd\ufffd /\ufffd{mtkw/yo ntk/|wk\ufffdq{~y\ufffd/ 534<2\n1\n9:1 Rk\ufffd\ufffd K.Sk\ufffdk~\ufffdqown WL1Wo~\ufffd{zkw tzqw\ufffdozmo> \ufffdso|k~\ufffd |wk\ufffdon l\ufffd|o{|wo tz\ufffdsoqw{\ufffd {qyk\ufffd\ufffd m{yy\ufffdztm k/\n\ufffdt{z\ufffd1 Uo\ufffd b{~v. Ub.][> L~oo W~o\ufffd\ufffd? 4=991\n9;1 _{umtv [.N\ufffdrso\ufffd F1[t\ufffdtzr ]|\\\ufffdt\ufffd\ufffdo~ ]\ufffdo~\ufffd1 Wo\ufffd Zo\ufffdok~ms Ioz\ufffdo~? 534=1 F\ufffdktwklwo q~{y> s\ufffd\ufffd|\ufffd>22\n\ufffd\ufffd\ufffd1|o\ufffd ~o\ufffdok~ms1{~r2t z\ufffdo~zo\ufffd 2534=2372572 \ufffdt\ufffdtzr/\ufffd|/\ufffd\ufffd t\ufffd\ufffdo~/\ufffd\ufffdo~\ufffd21\nPLOS ONEOno{w{rtm kwltk\ufffdo\ufffd tz\ufffd{mtkw \ufffdsk~tzr {q{zwtzo tzq{~yk\ufffdt {zkl{\ufffd\ufffd mwtyk\ufffdo mskzro\nWSV[ VUK \ufffds\ufffd\ufffd|\ufffd>22n{t1{~ r243146; 42u{\ufffd~zkw1|{ zo13593: 9: F|~tw 56.5354 59259", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Ideological biases in social sharing of online information about climate change", "author": ["TJB Cann", "IS Weaver", "HTP Williams"], "pub_year": "2021", "venue": "Plos one", "abstract": "Exposure to media content is an important component of opinion formation around climate  change. Online social media such as Twitter, the focus of this study, provide an avenue to"}, "filled": false, "gsrank": 625, "pub_url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0250656", "author_id": ["TtllgDgAAAAJ", "Vtgh0DgAAAAJ", "vARIotQAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:nssB-RHxaa0J:scholar.google.com/&output=cite&scirp=624&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D620%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=nssB-RHxaa0J&ei=d7WsaKCIGLXCieoP4PfQ0A8&json=", "num_citations": 45, "citedby_url": "/scholar?cites=12495783700610534302&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:nssB-RHxaa0J:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0250656&type=printable"}}, {"title": "A survey on stance detection for mis-and disinformation identification", "year": "2021", "pdf_data": "A Survey on Stance Detection for Mis- and Disinformation Identi\ufb01cation\nMomchil Hardalov1;2Arnav Arora1;3Preslav Nakov1;4Isabelle Augenstein1;3\n1Checkstep Research\n2So\ufb01a University \u201cSt. Kliment Ohridski\u201d, Bulgaria\n3University of Copenhagen, Denmark\n4Qatar Computing Research Institute, HBKU, Doha, Qatar\n{momchil, arnav, preslav.nakov, isabelle}@checkstep.com\nAbstract\nUnderstanding attitudes expressed in texts,\nalso known as stance detection , plays an im-\nportant role in systems for detecting false infor-\nmation online, be it misinformation (uninten-\ntionally false) or disinformation (intentionally\nfalse information). Stance detection has been\nframed in different ways, including (a) as a\ncomponent of fact-checking, rumour detection,\nand detecting previously fact-checked claims,\nor (b) as a task in its own right. While there\nhave been prior efforts to contrast stance detec-\ntion with other related tasks such as argumenta-\ntion mining and sentiment analysis, there is no\nexisting survey on examining the relationship\nbetween stance detection and mis- and disin-\nformation detection. Here, we aim to bridge\nthis gap by reviewing and analysing existing\nwork in this area, with mis- and disinformation\nin focus, and discussing lessons learnt and fu-\nture challenges.\n1 Introduction\nThe past decade is characterized by a rapid growth\nin popularity of social media platforms such as\nFacebook, Twitter, Reddit, and more recently, Par-\nler. This, in turn, has led to a \ufb02ood of dubious con-\ntent, especially during controversial events such\nas Brexit and the US presidential election. More\nrecently, with the emergence of the COVID-19 pan-\ndemic, social media were at the center of the \ufb01rst\nglobal infodemic (Alam et al., 2021), thus raising\nyet another red \ufb02ag and a reminder of the need for\neffective mis- and disinformation detection online.\nIn this survey, we examine the relationship be-\ntween automatically detecting false information on-\nline \u2013 including fact-checking, and detecting fake\nnews, rumors, and hoaxes \u2013 and the core underlying\nNatural Language Processing (NLP) task needed\nto achieve this, namely stance detection . Therein,\nwe consider mis- and disinformation, which both\nrefer to false information, though disinformation\nhas an additional intention to harm.Detecting and aggregating the expressed stances\ntowards a piece of information can be a powerful\ntool for a variety of tasks including understanding\nideological debates (Hasan and Ng, 2014), gather-\ning different frames of a particular issue (Shurafa\net al., 2020) or determining the leanings of media\noutlets (Stefanov et al., 2020). The task of stance\ndetection has been studied from different angles,\ne.g., in political debates (Habernal et al., 2018),\nfor fact-checking (Thorne et al., 2018), or regard-\ning new products (Somasundaran et al., 2009).\nMoreover, different types of text have been studied,\nincluding social media posts (Zubiaga et al., 2016b)\nand news articles (Pomerleau and Rao, 2017). Fi-\nnally, stances expressed by different actors have\nbeen considered, such as politicians (Johnson et\nal., 2009), journalists (Hanselowski et al., 2019),\nand users on the web (Derczynski et al., 2017).\nThere are some recent surveys related to stance\ndetection. Zubiaga et al. (2018a) discuss the role of\nstance in rumour veri\ufb01cation, Aldayel and Magdy\n(2021) survey stance detection for social media,\nand K\u00fc\u00e7\u00fck and Can (2020) survey stance detection\nholistically, without a speci\ufb01c focus on veracity.\nThere are also surveys on fact-checking (Thorne\nand Vlachos, 2018; Guo et al., 2022), which men-\ntion, though do not exhaustively survey, stance.\nHowever, there is no existing overview of the\nrole that different formulations of stance detection\nplay in the detection of false content. In that re-\nspect, stance detection could be modelled as fact-\nchecking \u2014 to gather the stances of users or texts\ntowards a claim or a headline (and support fact-\nchecking or studying misinformation) \u2014, or as a\ncomponent of a system that uses stance as part of\nits process of judging the veracity of an input claim.\nHere, we aim to bridge this gap by surveying the\nresearch on stance for mis- and disinformation de-\ntection, including task formulations, datasets, and\nmethods, from which we draw conclusions and\nlessons, and we forecast future research trends.arXiv:2103.00242v3  [cs.CL]  8 May 2022\nDataset Source(s) Target Context Evidence #Instances Task\nEnglish Datasets\nRumour Has It (Qazvinian et al., 2011) /twitter Topic Tweet /th 10K Rumours\nPHEME (Zubiaga et al., 2016b) /twitter Claim Tweet /comments_alt 4.5K Rumours\nEmergent (Ferreira and Vlachos, 2016) /_460 Headline Article\u0003/th 2.6K Rumours\nFNC-1 (Pomerleau and Rao, 2017) /_460 Headline Article /file_text_alt 75K Fake news\nRumourEval \u201917 (Derczynski et al., 2017) /twitter Implicit1Tweet /comments_alt 7.1K Rumours\nFEVER (Thorne et al., 2018) /_576 Claim Facts /th 185K Fact-checking\nSnopes (Hanselowski et al., 2019) Snopes Claim Snippets /th 19.5K Fact-checking\nRumourEval \u201919 (Gorrell et al., 2019) /twitter/f1a1 Implicit1Post /comments_alt 8.5K Rumours\nCOVIDLies (Hossain et al., 2020) /twitter Claim Tweet /file_text_alt 6.8K Misconceptions\nTabFact (Chen et al., 2020) /_576 Statement WikiTable /th 118K Fact-checking\nNon-English Datasets\nArabic FC (Baly et al., 2018b) /_460 Claim Document /file_text_alt 3K Fact-checking\nDAST (Danish) (Lillie et al., 2019) /f1a1 Submission Comment /comments_alt 3K Rumour\nCroatian (Bo\u0161njak and Karan, 2019) /_460 Title Comment /file_text_alt 0.9K Claim veri\ufb01ability\nANS (Arabic) (Khouja, 2020) /_460 Claim Title /file_text_alt 3.8K Claim veri\ufb01cation\nAra(bic)Stance (Alhindi et al., 2021) /_460 Claim Title /file_text_alt 4K Claim veri\ufb01cation\nTable 1: Key characteristics of stance detection datasets for mis- and disinformation detection. #Instances denotes\ndataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds.\u0003the article\u2019s body is\nsummarised. Sources :/twitterTwitter,/_460News,/_576ikipedia, /f1a1Reddit. Evidence :/file_text_altSingle,/thMultiple, /comments_altThread.\n2 What is Stance?\nIn order to understand the task of stance detec-\ntion, we \ufb01rst provide de\ufb01nitions of stance and the\nstance-taking process. Biber and Finegan (1988)\nde\ufb01ne stance as the expression of a speaker\u2019s stand-\npoint and judgement towards a given proposition.\nFurther, Du Bois (2007)) de\ufb01ne stance as \u201c a pub-\nlic act by a social actor, achieved dialogically\nthrough overt communicative means, of simultane-\nously evaluating objects, positioning subjects (self\nand others), and aligning with other subjects, with\nrespect to any salient dimension of the sociocul-\ntural \ufb01eld \u201d, showing that the stance-taking process\nis affected not only by personal opinions, but also\nby other external factors such as cultural norms,\nroles in the institution of the family, etc. Here,\nwe adopt the general de\ufb01nition of stance detection\nby K\u00fc\u00e7\u00fck and Can (2020): \u201c for an input in the form\nof a piece of text and a target pair, stance detection\nis a classi\ufb01cation problem where the stance of the\nauthor of the text is sought in the form of a category\nlabel from this set: Favor, Against, Neither. Occa-\nsionally, the category label of Neutral is also added\nto the set of stance categories (Mohammad et al.,\n2016), and the target may or may not be explic-\nitly mentioned in the text \u201d (Augenstein et al., 2016;\nMohammad et al., 2016). Note that the stance de-\ntection de\ufb01nitions and the label inventories vary\nsomewhat, depending on the target application (see\nSection 3).Finally, stance detection can be distinguished\nfrom several other closely related NLP tasks: ( i)bi-\nased language detection , where the existence of\nan inclination or tendency towards a particular\nperspective within a text is explored, ( ii)emotion\nrecognition , where the goal is to recognise emo-\ntions such as love, anger, etc. in the text, ( iii)per-\nspective identi\ufb01cation , which aims to \ufb01nd the point-\nof-view of the author (e.g., Democrat vs. Repub-\nlican) and the target is always explicit, ( iv)sar-\ncasm detection , where the interest is in satirical or\nironic pieces of text, often written with the intent\nof ridicule or mockery, and ( v)sentiment analysis ,\nwhich checks the polarity of a piece of text.\n3 Stance and Factuality\nHere, we offer an overview of the settings for mis-\nand disinformation identi\ufb01cation to which stance\ndetection has been successfully applied. As shown\nin Figure 1, stance can be used (a) as a way to\nperform fact-checking, or more typically, (b) as\na component of a fact-checking pipeline. Table 1\nshows an overview of the key characteristics of the\navailable datasets. We include the source of the\ndata and the target1towards which the stance is\nexpressed in the provided textual context .\n1The target can either be explicit, e.g., a topic such as\nPublic Healthcare , or implicit, where only the context is\npresent and the target is not directly available and is usu-\nally a topic (Derczynski et al., 2017; Gorrell et al., 2019), e.g.,\nGermanwings , or \u2018 Prince to play in Toronto \u2019. When the target\nis implicit, the task becomes similar to sentiment analysis.\n(a) stance detection as fact-checking\n (b) stance detection as a component of a fact-checking pipeline\nFigure 1: Two stance detection formulations.\nFigure 2: Types of stance. The Target is the object of\nthe stance expressed in the Context .\nWe further show the type of evidence: Single is a\nsingle document/fact, Multiple is multiple pieces of\ntextual evidence, often facts or documents, Thread\nis a (conversational) sequence of posts or a discus-\nsion. The \ufb01nal column is the type of the target Task.\nFinally, we present a dataset-agnostic summary\nof the terminology used for the different types of\nstance (see Figure 2), which we describe in a four-\nlevel taxonomy: ( i) sources, i.e., where the dataset\nwas collected from, ( ii) inputs that represent the\nstance target (e.g., claim), and the accompanying\ncontext (e.g., news article), ( iii) categorisation \u2013\nmeta-level characteristics of the input, and ( iv) the\ntextual object types for a particular stance scenario\n(e.g., topic, tweet, etc.). Appendix A discusses dif-\nferent stance scenarios with corresponding contexts\nand targets, with illustrations in Table 3.\n3.1 Fact-Checking as Stance Detection\nAs stance detection is the core task within fact-\nchecking, prior work has studied it in isolation,\ne.g., predicting the stance towards one or more doc-\numents. More precisely, the stance of the textual\nevidence(s) toward the target claim is considered\nas a veracity label, as illustrated in Figure 1a.Fact-Checking with One Evidence Document\nPomerleau and Rao (2017) organised the \ufb01rst Fake\nNews Challenge (FNC-1) with the aim of auto-\nmatically detecting fake news. The goal was to\ndetect the relatedness of a news article\u2019s body w.r.t.\na headline (possibly from another news article),\nbased on the stance that the former takes regarding\nthe latter. The possible categories were positive ,\nnegative ,discuss , and unrelated . This was a stan-\ndalone task, as it provides stance annotations only,\nomitting the actual \u201ctruth labels\u201d, with the motiva-\ntion of assisting fact-checkers in gathering several\ndistinct arguments pertaining to a particular claim.\nFact-Checking with Multiple Evidence Docu-\nments The FEVER (Thorne et al., 2018, 2019)\nshared task was introduced in 2018, aiming to deter-\nmine the veracity of a claim based on a set of state-\nments from Wikipedia. Claims can be composite\nand can contain multiple (contradicting) statements,\nwhich requires multi-hop reasoning, and the claim\u2013\nevidence pairs are annotated as SUPPORTED ,RE-\nFUTED , and NOT ENOUGH INFO . The latter cat-\negory includes claims that are either too general or\ntoo speci\ufb01c, and cannot be supported or refuted by\nthe available information in Wikipedia. This setup\nmay help fact-checkers understand the decisions a\nmodel made in their assessment of the veracity of\na claim, or assist human fact-checkers.\nThe second edition (2019) of FEVER evaluated\nthe robustness of models to adversarial attacks,\nwhere the participants were asked to provide new\nexamples to \u201cbreak\u201d existing models, then to pro-\npose \u201c\ufb01xes\u201d for the system against such attacks.\nNote that FEVER slightly differs from typical\nstance detection, as it considers evidence support-\ning or refuting a claim, rather than the stance of an\nauthor towards a claim. An alternative way to look\nat this is in terms of argument reasoning, i.e., ex-\ntracting and providing factual evidence for a claim.\nFEVER also has a connection to Natural Lan-\nguage Inference, i.e., determining the relationship\nbetween two sentences. We view FEVER as requir-\ning stance detection as it resembles FNC, which is\ncommonly seen as a stance detection task.\nApart from FEVER, Hanselowski et al. (2019)\npresented a task constructed from manually fact-\nchecked claims on Snopes. For this task, a model\nhad to predict the stance of evidence sentences\nin articles written by journalists towards claims.\nUnlike FEVER, this task does not require multi-\nhop reasoning.\nChen et al. (2020) studied the veri\ufb01cation of\nclaims using tabular data. The TabFact dataset was\ngenerated by human annotators who created posi-\ntive and negative statements about Wikipedia tables.\nTwo different forms of reasoning in a statement are\nrequired: ( i) linguistic, i.e., semantic understand-\ning, and ( ii) symbolic, i.e., using the table structure.\n3.2 Stance as a (Mis-/Dis-)information\nDetection Component\nFully automated systems can assist in gauging the\nextent and studying the spread of false informa-\ntion online. This is in contrast to the previously\ndiscussed applications of stance detection \u2013 as a\nstand-alone system for detecting mis- and disinfor-\nmation. Here, we review its potency to serve as\na component in an automated pipeline. Figure 1b\nillustrates the setup, which can also include steps\nsuch as modelling the user or pro\ufb01ling the media\noutlet among others. We discuss in more detail me-\ndia pro\ufb01ling and misconceptions in Appendix B.\nRumors Stance detection can be used for rumour\ndetection and debunking, where the stance of the\ncrowd, media, or other sources towards a claim\nare used to determine the veracity of a currently\ncirculating story or report of uncertain or doubtful\nfactuality. More formally, for a textual input and\na rumour expressed as text, stance detection here\nis to determine the position of the text towards the\nrumour as a category label from the set {Support,\nDeny, Query, Comment}. Zubiaga et al. (2016b)\nde\ufb01ne these categories as whether the author: sup-\nports ( Support ) or denies ( Deny ) the veracity of\nthe rumour they are responding to, \u201casks for ad-\nditional evidence in relation to the veracity of the\nrumour\u201d ( Query ) or \u201cmakes their own comment\nwithout a clear contribution to assessing the verac-\nity of the rumour\u201d ( Comment ). This setup was\nwidely explored for microblogs and social media.Qazvinian et al. (2011) started with \ufb01ve rumours\nand classi\ufb01ed the user\u2019s stance as endorse ,deny ,\nunrelated ,question , orneutral . While they were\namong the \ufb01rst to demonstrate the feasibility of\nthis task formulation, the limited size of their study\nand the focus on assessing the stance of individual\nposts limited its real-world applicability.\nZubiaga et al. (2016b) analysed how people\nspread rumours on social media based on conver-\nsational threads. They included rumour threads\nassociated with nine newsworthy events, and users\u2019\nstance before and after the rumours were con\ufb01rmed\nor denied. Dungs et al. (2018) continued this line\nof research, but focused on the effectiveness of\nstance for predicting rumour veracity. Hartmann\net al. (2019) explored the \ufb02ow of (dis-)information\non Twitter after the MH17 Plane Crash.\nThe two RumourEval (Derczynski et al., 2017;\nGorrell et al., 2019) shared tasks on automated\nclaim validation aimed to identify and handle ru-\nmours based on user reactions and ensuing con-\nversations in social media, offering annotations for\nboth stance and veracity. The two editions of Ru-\nmourEval were similar in spirit, with the second\none providing more tweets and also additionally\nReddit posts. RumourEval demonstrated the impor-\ntance of modelling the context of a story instead of\ndrawing conclusions based on a single post.\nFerreira and Vlachos (2016) collected claims and\nnews articles from rumour sites with annotations\nfor stance and veracity by journalists as part of\nthe Emergent project. The goal was to use the\nstance of a news article, summarised into a single\nsentence, towards a claim as one of the components\nto determine its veracity. A downside is the need to\nsummarise, in contrast to FNC-1 (Pomerleau and\nRao, 2017), where entire news articles were used.\nMultiple languages While the above research\nhas focused exclusively or primarily on English,\ninterest in stance detection for other languages has\nstarted to emerge. Baly et al. (2018b) integrated\nstance detection and fact-checking for Arabic in a\nsingle corpus. Khouja (2020) proposed a dataset for\nArabic following the FEVER setup. Alhindi et al.\n(2021) introduced AraStance, a multi-country and\nmulti-domain dataset of Arabic stance detection\nfor fact-checking. Lillie et al. (2019) collected data\nfor stance and veracity from Danish Reddit threads\nZubiaga et al. (2016b). Bo\u0161njak and Karan (2019)\nstudied stance detection and claim veri\ufb01cation of\ncomments for Croatian news articles.\n4 Approaches\nIn this section, we discuss various ways to use\nstance detection for mis- and disinformation detec-\ntion, and list the state-of-the-art results in Table 2.\nFact-Checking as Stance Detection Here, we\ndiscuss approaches for stance detection in the con-\ntext of mis- and disinformation detection, where\nveracity is modelled as stance detection as outlined\nin Section 3.1. One such line of research is the\nFake News Challenge, which used weighted accu-\nracy as an evaluation measure (FNC score), to miti-\ngate the impact of class imbalance. Subsequently,\nHanselowski et al. (2018a) criticized the FNC score\nand F1-micro, and argued in favour of F1-macro\n(F1) instead. In the competition, most teams used\nhand-crafted features such as words, word embed-\ndings, and sentiment lexica (Riedel et al., 2017;\nHanselowski et al., 2018a). Hanselowski et al.\n(2018a) showed that the most important group of\nfeatures were the lexical ones, followed by features\nfrom topic models, while sentiment analysis did\nnot help. Ghanem et al. (2018) investigated the im-\nportance of lexical cues, and found that report and\nnegation are most bene\ufb01cial, while knowledge and\ndenial are least useful. All these models struggle to\nlearn the Disagree class, achieving up to 18 F1 due\nto major class imbalance. In contrast, Unrelated\nis detected almost perfectly by all models (over\n99 F1). Hanselowski et al. (2018a) showed that\nthese models exploit the lexical overlap between\nthe headline and the document, but fail when there\nis a need to model semantic relations or complex\nnegation, or to understand propositional content\nin general. This can be attributed to the use of\nn-grams, topic models, and lexica.\nMohtarami et al. (2018) investigated memory\nnetworks, aiming to mitigate the impact of irrele-\nvant and noisy information by learning a similarity\nmatrix and a stance \ufb01ltering component, and tak-\ning a step towards explaining the stance of a given\nclaim by extracting meaningful snippets from evi-\ndence documents. Like previous work, their model\nperforms poorly on the Agree/Disagree classes, due\nto the unsupervised way of training the memory\nnetworks, i.e., there are no gold snippets justifying\nthe document\u2019s stance w.r.t. the target claim.\nMore recently, transfer learning with pre-trained\nTransformers has been explored (Slovikovskaya\nand Attardi, 2020), signi\ufb01cantly improving the per-\nformance of previous state-of-the-art approaches.Guderlei and A\u00dfenmacher (2020) showed the most\nimportant hyper-parameter to be learning rate,\nwhile freezing layers did not help. In particular,\nusing the pre-trained Transformer RoBERTa im-\nproved F1 from 18 to 58 for Disagree , and from\n50 to 70 for Agree . The success of these models\nis also seen in cross-lingual settings. For Arabic,\nKhouja (2020) achieved 76.7 F1 for stance detec-\ntion on the ANS dataset using mBERT. Similarly,\nHardalov et al. (2022) applied pattern-exploiting\ntraining (PET) with sentiment pre-training in a\ncross-lingual setting showing sizeable improve-\nments on 15 datasets. Alhindi et al. (2021) showed\nthat language-speci\ufb01c pre-training was pivotal, out-\nperforming the state of the art on AraStance (52 F1)\nand Arabic FC (78 F1).\nSome formulations include an extra step for ev-\nidence retrieval, e.g., retrieving Wikipedia snip-\npets for FEVER (Thorne et al., 2018). To evaluate\nthe whole fact-checking pipeline, they introduced\nthe FEVER score \u2013 the proportion of claims for\nwhich both correct evidence is returned and a cor-\nrect label is predicted. The top systems that partici-\npated in the FEVER competition Hanselowski et al.\n(2018b); Yoneda et al. (2018); Nie et al. (2019)\nused LSTM-based models for natural language in-\nference, e.g., enhanced sequential inference model\n(ESIM Chen et al. (2017)). Nie et al. (2019) pro-\nposed a neural semantic matching network, which\nranked \ufb01rst in the competition, achieving 64.2\nFEVER score. They used page view frequency\nand WordNet features in addition to pre-trained\ncontextualized embeddings (Peters et al., 2018).\nMore recent approaches used bi-directional\nattention (Li et al., 2018), a GPT language\nmodel (Malon, 2018; Yang et al., 2019), and graph\nneural networks (Zhou et al., 2019; Atanasov et al.,\n2019; Liu et al., 2020b; Wang et al., 2020; Zhong\net al., 2020; Weinzierl et al., 2021; Si et al., 2021).\nZhou et al. (2019) showed that adding graph net-\nworks on top of BERT can improve performance,\nreaching 67.1 FEVER score. Yet, the retrieval\nmodel is also important, e.g., using the gold ev-\nidence set adds 1.4 points. Liu et al. (2020b);\nZhong et al. (2020) replaced the retrieval model\nwith a BERT-based one, in addition to using an\nimproved mechanism to propagate the information\nbetween nodes in the graph, boosting the score to\n70. Recently, Ye et al. (2020) experimented with a\nretriever that incorporates co-reference in distant-\nsupervised pre-training, namely, CorefRoBERTa.\nWang et al. (2020) added external knowledge to\nbuild a contextualized semantic graph, setting a\nnew SOTA on Snopes. Si et al. (2021) and Os-\ntrowski et al. (2021) improved multi-hop reasoning\nusing a model with eXtra Hop attention (Zhao et al.,\n2020), a capsule network aggregation layer, and\nLDA topic information. Atanasova et al. (2022)\nintroduced the task of evidence suf\ufb01ciency predic-\ntion to more reliably predict the NOT ENOUGH\nINFO class.\nAnother notable idea is to use pre-trained lan-\nguage models as fact-checkers based on a masked\nlanguage modelling objective (Lee et al., 2020), or\nto use the perplexity of the entire claim with respect\nto the target document (Lee et al., 2021). Such\nmodels do not require a retrieval step, as they use\nthe knowledge stored in language models. How-\never, they are prone to biases in the patterns used,\ne.g., they can predict date instead of city/country\nand vice-versa when using \u201cborn in/on\u201d. More-\nover, the insuf\ufb01cient context can seriously confuse\nthem, e.g., for short claims with uncommon words\nsuch as \u201cSarawak is a ...\u201d, where it is hard to detect\nthe entity type. Finally, the performance of such\nmodels remains well below supervised approaches;\neven though recent work shows that few-shot train-\ningcan improve results (Lee et al., 2021).\nError analysis suggests that the main challenges\nare ( i) confusing semantics at the sentence level,\ne.g., \u201c Andrea Pirlo is an American professional\nfootballer. \u201d vs. \u201c Andrea Pirlo is an Italian profes-\nsional footballer who plays for an American club. \u201d,\n(ii) sensitivity to spelling errors, ( iii) lack of rela-\ntion between the article and the entities in the claim,\n(vi) dependence on syntactic overlaps, e.g., \u201c Terry\nCrews played on the Los Angeles Chargers. \u201d (NotE-\nnoughInfo ) is classi\ufb01ed as refuted , given the sen-\ntence \u201c In football, Crews played ... for the Los\nAngeles Rams, San Diego Chargers and Washing-\nton Redskins, ... \u201d, (v) embedding-level confusion,\ne.g., numbers tend to have similar embeddings,\n\u201cThe heart beats at a resting rate close to 22 bpm. \u201d\nis not classi\ufb01ed as refuted based on the evidence\nsentence \u201c The heart beats at a resting rate close to\n72 bpm. \u201d, and similarly for months.\nThreaded Stance In the setting of conversa-\ntional threads (Zubiaga et al., 2016b; Derczynski\net al., 2017; Gorrell et al., 2019), in contrast to\nthe single-task setup, which ignores or does not\nprovide further context, important knowledge can\nbe gained from the structure of user interactions.These approaches are mostly applied as part of a\nlarger system, e.g., for detecting and debunking ru-\nmours (see Section 3.2, Rumours ). A common pat-\ntern is to use tree-like structured models, fed with\nlexicon-based content formatting (Zubiaga et al.,\n2016a) or dictionary-based token scores (Aker\net al., 2017). Kumar and Carley (2019) replaced\nCRFs with Binarised Constituency Tree LSTMs,\nand used pre-trained embeddings to encode the\ntweets. More recently, Tree (Ma and Gao, 2020)\nand Hierarchical (Yu et al., 2020) Transformers\nwere proposed, which combine post- and thread-\nlevel representations for rumour debunking, im-\nproving previous results on RumourEval \u201917 (Yu\net al., 2020). Kochkina et al. (2017, 2018) split con-\nversations into branches, modelling each branch\nwith branched-LSTM and hand-crafted features,\noutperforming other systems at RumourEval \u201917\non stance detection (43.4 F1). Li et al. (2020) devi-\nated from this structure and modelled the conver-\nsations as a graph. Tian et al. (2020) showed that\npre-training on stance data yielded better represen-\ntations for threaded tweets for downstream rumour\ndetection. Yang et al. (2019) took a step further\nand curated per-class pre-training data by adapting\nexamples, not only from stance datasets, but also\nfrom tasks such as question answering, achieving\nthe highest F1 (57.9) on the RumourEval \u201919 stance\ndetection task. Li et al. (2019a,b) additionally in-\ncorporated user credibility information, conversa-\ntion structure, and other content-related features to\npredict the rumour veracity, ranking 3rd on stance\ndetection and 1st on veracity classi\ufb01cation (Ru-\nmourEval \u201919). Finally, the stance of a post might\nnot be expressed directly towards the root of the\nthread, thus the preceding posts must be also taken\ninto account (Gorrell et al., 2019).\nA major challenge for all rumour detection\ndatasets is the class distribution (Zubiaga et al.,\n2016b; Derczynski et al., 2017; Gorrell et al., 2019),\ne.g., the minority class denying is extremely hard\nfor models to learn, as even for strong systems such\nas Kochkina et al. (2017) the F1 for it is 0. Label se-\nmantics also appears to play a role as the querying\nlabel has a similar distribution, but much higher F1.\nYet another factor is thread depth, as performance\ndrops signi\ufb01cant at higher depth, especially for the\nsupporting class. On the positive side, using multi-\ntask learning and incorporating stance detection\nlabels into veracity detection yields a huge boost in\nperformance (Gorrell et al., 2019; Yu et al., 2020).\nAnother factor, which goes hand in hand with\nthe threaded structure, is the temporal dimension\nof posts in a thread (Lukasik et al., 2016; Veyseh\net al., 2017; Dungs et al., 2018; Wei et al., 2019).\nIn-depth data analysis (Zubiaga et al. (2016a,b);\nKochkina et al. (2017); Wei et al. (2019); Ma and\nGao (2020); Li et al. (2020); among others) shows\ninteresting patterns along the temporal dimension:\n(i) source tweets (at zero depth) usually support the\nrumour and models often learn to detect that, ( ii) it\ntakes time for denying tweets to emerge, afterwards\nfor false rumors their number increases quite sub-\nstantially, ( iii) the proportion of querying tweets\ntowards unveri\ufb01ed rumors also shows an upward\ntrend over time, but their overall number decreases.\nMulti-Dataset Learning (MDL) Mixing data\nfrom different domains and sources can improve\nrobustness. However, setups that combine mis- and\ndisinformation identi\ufb01cation with stance detection,\noutlined in Section 3, vary in their annotation and\nlabelling schemes, which poses many challenges.\nEarlier approaches focused on pre-training mod-\nels on multiple tasks, e.g., Fang et al. (2019)\nachieved state-of-the-art results on FNC-1 by \ufb01ne-\ntuning on multiple tasks such as question answer-\ning, natural language inference, etc., which are\nweakly related to stance. Recently, Schiller et al.\n(2021) proposed a benchmark to evaluate the ro-\nbustness of stance detection models. They lever-\naged a pre-trained multi-task deep neural network,\nMT-DNN (Liu et al., 2019), and continued its\ntraining on all datasets simultaneously using multi-\ntask learning, showing sizeable improvements over\nmodels trained on individual datasets. Hardalov\net al. (2021) experimented with cross-domain learn-\ning from 16 stance detection datasets. They pro-\nposed a novel architecture (MoLE) that applies\ndomain adaptation at different stages of the mod-\nelling process (Luo et al., 2002): feature-level (Guo\net al., 2018; Wright and Augenstein, 2020) and\ndecision-level (Ganin and Lempitsky, 2015). They\nfurther integrated label embeddings (Augenstein\net al., 2018), and eventually developed an end-to-\nend unsupervised framework for predicting stance\nfrom a set of unseen target labels. Hardalov et al.\n(2022) explored PET (Schick and Sch\u00fctze, 2021)\nin a cross-lingual setting, combining datasets with\ndifferent label inventories by modelling the task as\na cloze question answering one.\n1The result from dominiks can be found at https://\ncompetitions.codalab.org/competitions/18814#resultsPaper Dataset Score Metric\nHardalov et al. (2021) Rumour Has It 71.2 F1 macro\nKumar et al. (2019) PHEME 53.2 F1 macro\nHardalov et al. (2021) Emergent 86.2 F1 macro\nGuderlei et al. (2020) FNC-1 78.2 F1 macro\nYu et al. (2020) RumourEval \u201917 50.9 F1 macro\nDominiks (2021)\u0003FEVER 76.8 FEVER\nWang et al. (2020) Snopes 78.3 F1 macro\nYang et al. (2019) RumourEval \u201919 61.9 F1 macro\nWeinzierl et al. (2021) COVIDLies 74.3 F1 macro\nLiu et al. (2020a) TabFact 84.2 Accuracy\nAlhindi et al. (2021) Arabic FC 52.? F1 macro\nLillie et al. (2019) DAST 42.1 F1 macro\nBo\u0161njak and Karan (2019) Croatian 25.8 F1 macro\nAlhindi et al. (2021) ANS 90.? F1 macro\nAlhindi et al. (2021) AraStance 78.? F1 macro\nTable 2: State-of-the-art results on the stance detection\ndatasets. Note that some papers round their results to\nintegers, and thus we put \u2018 ?\u2019 for them.\u0003Extracted from\nthe FEVER leaderboard.2\nThey showed that MDL helps for low-resource\nand substantively for full-resource scenarios. More-\nover, transferring knowledge from English stance\ndatasets and noisily generated sentiment-based\nstance data can further boost performance.\nState of the Art Table 2 shows the state-of-the-\nart (SOTA) results for each dataset discussed in\nSection 3 and Table 1. The datasets vary in their\ntask formulation and composition in terms of size,\nnumber of classes, class imbalance, topics, evalua-\ntion measures, etc. Each of these factors impacts\nthe performance, leading to sizable differences in\nthe \ufb01nal score, as discussed in Section 4, and hence\nrendering the reported results hard to compare di-\nrectly across these datasets.\n5 Lessons Learned and Future Trends\nDataset Size A major limitation holding back the\nperformance of machine learning for stance detec-\ntion is the size of the existing stance datasets, the\nvast majority of which contain at most a few thou-\nsand examples. Contrasted with the related task of\nNatural Language Inference, where datasets such\nas SNLI (Bowman et al., 2015) of more than half\na million samples have been collected, this is far\nfrom optimal. Moreover, the small dataset sizes are\noften accompanied with skewed class distribution\nwith very few examples from the minority classes,\nincluding many of the datasets in this study (Zubi-\naga et al., 2016b; Derczynski et al., 2017; Pomer-\nleau and Rao, 2017; Baly et al., 2018b; Gorrell\net al., 2019; Lillie et al., 2019; Alhindi et al., 2021).\nThis can lead to a signi\ufb01cant disparity for label per-\nformance (see Section 4). Several techniques have\nbeen proposed to mitigate this, such as sampling\nstrategies (Nie et al., 2019), weighting classes (Vey-\nseh et al., 2017),3crafting arti\ufb01cial examples from\nauxiliary tasks (Yang et al., 2019; Hardalov et al.,\n2022), or training on multiple datasets (Schiller\net al., 2021; Hardalov et al., 2021, 2022).\nData Mixing A potential way of overcoming lim-\nitations in terms of dataset size and focus is to\ncombine multiple datasets. Yet, as we previously\ndiscussed (see Section 3), task de\ufb01nitions and label\ninventories vary across stance datasets. Further,\nlarge-scale studies of approaches that leverage the\nrelationships between label inventories, or the sim-\nilarity between datasets are still largely lacking.\nOne promising direction is the use of label em-\nbeddings (Augenstein et al., 2018), as they offer a\nconvenient way to learn interactions between dis-\njoint label sets that carry semantic relations. One\nsuch \ufb01rst study was recently presented by Hardalov\net al. (2021), which explored different strategies\nfor leveraging inter-dataset signals and label inter-\nactions in both in- (seen targets) and out-of-domain\n(unseen targets) settings. This could help to over-\ncome challenges faced by models trained on small-\nsize datasets, and even for smaller minority classes.\nMultilinguality Multi-linguality is important for\nseveral reasons: ( i) the content may originate in\nvarious languages, ( ii) the evidence or the stance\nmay not be expressed in the same language, thus\n(iii) posing a challenge for fact-checkers, who\nmight not be speakers of the language the claim\nwas originally made in, and ( iv) it adds more data\nthat can be leveraged for modelling stance. Cur-\nrently, only a handful of datasets for factuality and\nstance cover languages other than English (see Ta-\nble 1), and they are small in size and do not offer\na cross-lingual setup. Recently, Vamvas and Sen-\nnrich (2020) proposed such a setup for three lan-\nguages for stance in debates, Schick and Sch\u00fctze\n(2021) explored few-shot learning, and Hardalov\net al. (2022) extended that paradigm with sentiment\nand stance pre-training and evaluated on twelve lan-\nguages from various domains. Since cultural norms\nand expressed linguistic phenomena play a crucial\nrole in understanding the context of a claim (Sap\net al., 2019), we do not argue for a completely\n3Weighting is not trivial for some setups, e.g., threaded\nstance (Zubiaga et al., 2018b)language-agnostic framework. Yet, empirically,\ntraining in cross-lingual setups improves perfor-\nmance by leveraging better representations learned\non a similar language or by acting as a regulariser.\nModelling the Context Modelling the context is\na particularly important, yet challenging task. In\nmany cases, there is a need to consider the back-\nground of the stance-taker as well as the character-\nistics of the targeted object. In particular, in the\ncontext of social media, one can provide informa-\ntion about the users such as their previous activity,\nother users they interact most with, the threads they\nparticipate in, or even their interests (Zubiaga et al.,\n2016b; Gorrell et al., 2019; Li et al., 2019b). The\ncontext of the stance expressed in news articles is\nrelated to the features of the media outlets, such\nas source of funding, previously known biases, or\ncredibility (Baly et al., 2019; Darwish et al., 2020;\nStefanov et al., 2020; Baly et al., 2020). When us-\ning contextual information about the object, factual\ninformation about the real world, and the time of\nposting are all important. Incorporating these into a\nstance detection pipeline, while challenging, paves\nthe way towards a robust detection process.\nMultimodal Content Spreading mis- and disin-\nformation through multiple modalities is becoming\nincreasingly popular. One such example are deep-\nfakes , i.e., synthetically created images or videos,\nin which (usually) the face of one person is re-\nplaced with another person\u2019s face. Another exam-\nple are information propagation techniques such as\nmemetic warfare . Hence, it is increasingly impor-\ntant to combine different modalities to understand\nthe full context stance is being expressed in. Some\nwork in this area is on fake news detection for im-\nages (Nakamura et al., 2020), claim veri\ufb01cation\nfor images (Zlatkova et al., 2019), or searching for\nfact-checked information to alleviate the spread of\nfake news (V o and Lee, 2020). There has been\nwork on meme analysis for related tasks: detecting\nhateful (Kiela et al., 2020), harmful (Pramanick\net al., 2021; Sharma et al., 2022a), and propagan-\ndistic memes (Dimitrov et al., 2021a,b); see also\na recent survey of harmful memes (Sharma et al.,\n2022b). This line of research is especially rele-\nvant for mis- and disinformation tasks that depend\non the wisdom of the crowd in social media as\nit adds additional information sources (Qazvinian\net al., 2011; Zubiaga et al., 2016b; Derczynski et al.,\n2017; Hossain et al., 2020); see Section 5.\nShades of Truth The notion of shades of truth is\nimportant in mis- and disinformation detection. For\nexample, fact-checking often goes beyond binary\ntrue/false labels, e.g., Nakov et al. (2018) used a\nthird category half-true , Rashkin et al. (2017) in-\ncluded mixed andno factual evidence , and Wang\n(2017); Santia and Williams (2018) adopted an\neven \ufb01ner-grained schema with six labels, includ-\ningbarely true andutterly false . We believe that\nsuch shades could be applied to stance and used\nin a larger pipeline. In fact, \ufb01ne-grained labels are\ncommon for the related task of Sentiment Analy-\nsis (Pang and Lee, 2005; Rosenthal et al., 2017).\nLabel Semantics As research in stance detection\nhas evolved, so has the de\ufb01nition of the task and\nthe label inventories, but they still do not capture\nthe strength of the expressed stance. As shown in\nSection 3 (also Appendix 2, labels can vary based\non the use case and the setting they are used in.\nMost researchers have adopted a variant of the\nFavour ,Against , and Neither labels, or an extended\nschema such as (S)upport ,(Q)uery ,(D)eny , and\n(C)omment (Mohammad et al., 2016), but that is\nnot enough to accurately assess stance. Moreover,\nadding label granularity can further improve the\ntransfer between datasets, as the stance labels al-\nready share some semantic similarities, but there\ncan be mismatches in the label de\ufb01nitions (Schiller\net al., 2021; Hardalov et al., 2021, 2022).\nExplainability The ability for a model to be able\nto explain its decisions is getting increasingly im-\nportant, especially for mis- and disinformation de-\ntection, as one could argue that it is a crucial step\ntowards adopting fully automated fact-checking.\nThe FEVER 2.0 task formulation (Thorne et al.,\n2019) can be viewed as a step towards obtaining\nsuch explanations, e.g., there have been efforts to\nidentify adversarial triggers that offer explanations\nfor the vulnerabilities at the model level (Atanasova\net al., 2020b). However, FEVER is arti\ufb01cially cre-\nated and is limited to Wikipedia, which may not\nre\ufb02ect real-world settings. To mitigate this, expla-\nnation by professional journalists can be found on\nfact-checking websites, and can be further com-\nbined with stance detection in an automated sys-\ntem. In a step in this direction, Atanasova et al.\n(2020a) generated natural language explanations\nfor claims from PolitiFact4given gold evidence\ndocument summaries by journalists.\n4http://www.politifact.com/Moreover, partial explanations can be ob-\ntained automatically from the underlying models,\ne.g., from memory networks (Mohtarami et al.,\n2018), attention weights (Zhou et al., 2019; Liu\net al., 2020b), or topic relations (Si et al., 2021).\nHowever, such approaches are limited as they can\nrequire gold snippets justifying the document\u2019s\nstance, attention weights can be misleading (Jain\nand Wallace, 2019), and topics might be noisy due\nto their unsupervised nature. Other existing sys-\ntems (Popat et al., 2017, 2018; Nadeem et al., 2019)\noffer explanations to a more limited extent, high-\nlighting span overlaps between the target text and\nthe evidence documents. Overall, there is a need\nfor holistic and realistic explanations of how a fact-\nchecking model arrived at its prediction.\nIntegration People question false information\nmore and tend to con\ufb01rm true information (Men-\ndoza et al., 2010). Thus, stance can play a vital\nrole in verifying dubious content. In Appendix C,\nwe discuss existing systems and real-world ap-\nplications of stance for mis- and disinformation\nidenti\ufb01cation in more detail. However, we argue\nthat a tighter integration between stance and fact-\nchecking is needed. Stance can be expressed in dif-\nferent forms, e.g., tweets, news articles, user posts,\nsentences in Wikipedia, and Wiki tables, among\nothers and can have different formulations as part\nof the fact-checking pipeline (see Section 3). All\nthese can guide human fact-checkers through the\nprocess of fact-checking, and can point them to\nrelevant evidence. Moreover, the wisdom of the\ncrowd can be a powerful instrument in the \ufb01ght\nagainst mis- and disinformation (Pennycook and\nRand, 2019), but we should note that vocal mi-\nnorities can derail public discourse (Scannell et al.,\n2021). Nevertheless, these risks can be mitigated\nby taking into account the credibility of the user or\nof the information source, which can be done auto-\nmatically or with the help of human fact-checkers.\n6 Conclusion\nWe surveyed the current state-of-the-art in stance\ndetection for mis- and disinformation detection.\nWe explored applications of stance for detecting\nfake news, verifying rumours, identifying miscon-\nceptions, and fact-checking. We also discussed\nexisting approaches used in different aspects of\nthe aforementioned tasks, and we outlined several\ninteresting phenomena, which we summarised as\nlessons learned and promising future trends.\nAcknowledgements\nWe would like to thank the anonymous reviewers\nfor their useful feedback. Isabelle Augenstein\u2019s\nresearch is partially funded by a DFF Sapere Aude\nresearch leader grant with grant number 0171-\n00034B. The work is also part of the Tanbih mega-\nproject, which is developed at the Qatar Computing\nResearch Institute, HBKU, and aims to limit the\nimpact of \u201cfake news,\u201d propaganda, and media bias\nby making users aware of what they are reading,\nthus promoting media literacy and critical thinking.\nReferences\nAhmet Aker, Leon Derczynski, and Kalina Bontcheva.\n2017. Simple open stance classi\ufb01cation for rumour\nanalysis. In Proceedings of the International Con-\nference Recent Advances in Natural Language Pro-\ncessing , RANLP \u201917, pages 31\u201339, Varna, Bulgaria.\nFiroj Alam, Fahim Dalvi, Shaden Shaar, Nadir Dur-\nrani, Hamdy Mubarak, Alex Nikolov, Giovanni\nDa San Martino, Ahmed Abdelali, Hassan Sajjad,\nKareem Darwish, and Preslav Nakov. 2021. Fight-\ning the COVID-19 infodemic in social media: A\nholistic perspective and a call to arms. Proceedings\nof the International AAAI Conference on Web and\nSocial Media , 15(1):913\u2013922.\nAbeer Aldayel and Walid Magdy. 2021. Stance\ndetection on social media: State of the art and\ntrends. Information Processing & Management ,\n58(4):102597.\nTariq Alhindi, Amal Alabdulkarim, Ali Alshehri,\nMuhammad Abdul-Mageed, and Preslav Nakov.\n2021. AraStance: A multi-country and multi-\ndomain dataset of Arabic stance detection for fact\nchecking. In Proceedings of the Fourth Workshop\non NLP for Internet Freedom: Censorship, Disinfor-\nmation, and Propaganda , NLP4IF \u201921, pages 57\u201365.\nAtanas Atanasov, Gianmarco De Francisci Morales,\nand Preslav Nakov. 2019. Predicting the role of\npolitical trolls in social media. In Proceedings\nof the 23rd Conference on Computational Natural\nLanguage Learning , CoNLL \u201919, pages 1023\u20131034,\nHong Kong, China.\nPepa Atanasova, Jakob Grue Simonsen, Christina Li-\noma, and Isabelle Augenstein. 2020a. Generating\nfact checking explanations. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics , ACL \u201920, pages 7352\u20137364.\nPepa Atanasova, Jakob Grue Simonsen, Christina Li-\noma, and Isabelle Augenstein. 2022. Fact checking\nwith insuf\ufb01cient evidence. Transactions of the Asso-\nciation for Computational Linguistics .Pepa Atanasova, Dustin Wright, and Isabelle Augen-\nstein. 2020b. Generating label cohesive and well-\nformed adversarial claims. In Proceedings of the\nConference on Empirical Methods in Natural Lan-\nguage Processing , EMNLP \u201920, pages 3168\u20133177.\nIsabelle Augenstein, Tim Rockt\u00e4schel, Andreas Vla-\nchos, and Kalina Bontcheva. 2016. Stance detec-\ntion with bidirectional conditional encoding. In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing , EMNLP \u201916,\npages 876\u2013885, Austin, Texas, USA.\nIsabelle Augenstein, Sebastian Ruder, and Anders\nS\u00f8gaard. 2018. Multi-task learning of pairwise\nsequence classi\ufb01cation tasks over disparate label\nspaces. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies , NAACL-HLT \u201918, pages 1896\u20131906, New\nOrleans, Louisiana, USA.\nRamy Baly, Georgi Karadzhov, Dimitar Alexandrov,\nJames Glass, and Preslav Nakov. 2018a. Predict-\ning factuality of reporting and bias of news media\nsources. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing ,\npages 3528\u20133539, Brussels, Belgium.\nRamy Baly, Georgi Karadzhov, Jisun An, Haewoon\nKwak, Yoan Dinkov, Ahmed Ali, James Glass, and\nPreslav Nakov. 2020. What was written vs. who\nread it: News media pro\ufb01ling using text analysis and\nsocial media context. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics , ACL \u201920, pages 3364\u20133374, Online.\nRamy Baly, Georgi Karadzhov, Abdelrhman Saleh,\nJames Glass, and Preslav Nakov. 2019. Multi-task\nordinal regression for jointly predicting the trustwor-\nthiness and the leading political ideology of news\nmedia. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies , NAACL-HLT \u201919, pages 2109\u20132116, Min-\nneapolis, Minnesota, USA.\nRamy Baly, Mitra Mohtarami, James Glass, Llu\u00eds\nM\u00e0rquez, Alessandro Moschitti, and Preslav Nakov.\n2018b. Integrating stance detection and fact check-\ning in a uni\ufb01ed corpus. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , NAACL-HLT \u201918, pages\n21\u201327, New Orleans, Louisiana, USA.\nDouglas Biber and Edward Finegan. 1988. Adver-\nbial stance types in English. Discourse Processes ,\n11(1):1\u201334.\nMihaela Bo\u0161njak and Mladen Karan. 2019. Data set for\nstance and sentiment analysis from user comments\non Croatian news. In Proceedings of the 7th Work-\nshop on Balto-Slavic Natural Language Processing ,\nBSNLP \u201919, pages 50\u201355, Florence, Italy.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large an-\nnotated corpus for learning natural language infer-\nence. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing ,\nEMNLP \u201915, pages 632\u2013642, Lisbon, Portugal.\nCheng Chen, Kui Wu, Venkatesh Srinivasan, and\nXudong Zhang. 2013. Battling the internet water\narmy: Detection of hidden paid posters. In Pro-\nceedings of the 2013 IEEE/ACM International Con-\nference on Advances in Social Networks Analysis\nand Mining , ASONAM \u201913, pages 116\u2013120, Nia-\ngara, Ontario, Canada.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui\nJiang, and Diana Inkpen. 2017. Enhanced LSTM\nfor natural language inference. In Proceedings of\nthe 55th Annual Meeting of the Association for Com-\nputational Linguistics , ACL \u201917, pages 1657\u20131668,\nVancouver, Canada.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2020. Tabfact: A large-scale\ndataset for table-based fact veri\ufb01cation. In 8th Inter-\nnational Conference on Learning Representations ,\nICLR \u201920, Addis Ababa, Ethiopia.\nKareem Darwish, Dimitar Alexandrov, Preslav Nakov,\nand Yelena Mejova. 2017. Seminar users in the\nArabic Twitter sphere. In Proceedings of the\n9th International Conference on Social Informatics ,\nSocInfo \u201917, pages 91\u2013108, Oxford, UK.\nKareem Darwish, Peter Stefanov, Micha\u00ebl Aupetit, and\nPreslav Nakov. 2020. Unsupervised user stance de-\ntection on Twitter. In Proceedings of the Interna-\ntional AAAI Conference on Web and Social Media ,\nvolume 14 of ICWSM \u201920 , pages 141\u2013152.\nLeon Derczynski, Kalina Bontcheva, Maria Liakata,\nRob Procter, Geraldine Wong Sak Hoi, and Arkaitz\nZubiaga. 2017. SemEval-2017 task 8: RumourEval:\nDetermining rumour veracity and support for ru-\nmours. In Proceedings of the 11th International\nWorkshop on Semantic Evaluation , SemEval \u201917,\npages 69\u201376, Vancouver, Canada.\nDimitar Dimitrov, Bishr Bin Ali, Shaden Shaar, Firoj\nAlam, Fabrizio Silvestri, Hamed Firooz, Preslav\nNakov, and Giovanni Da San Martino. 2021a. De-\ntecting propaganda techniques in memes. In Pro-\nceedings of the Joint Conference of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing , ACL-IJCNLP \u201921,\npages 6603\u20136617.\nDimitar Dimitrov, Bishr Bin Ali, Shaden Shaar, Firoj\nAlam, Fabrizio Silvestri, Hamed Firooz, Preslav\nNakov, and Giovanni Da San Martino. 2021b.\nSemEval-2021 task 6: Detection of persuasion tech-\nniques in texts and images. In Proceedings of the\n15th International Workshop on Semantic Evalua-\ntion, SemEval \u201921, pages 70\u201398, Online.Xin Luna Dong, Evgeniy Gabrilovich, Kevin Murphy,\nVan Dang, Wilko Horn, Camillo Lugaresi, Shaohua\nSun, and Wei Zhang. 2015. Knowledge-based trust:\nEstimating the trustworthiness of web sources. Proc.\nVLDB Endow. , 8(9):938\u2013949.\nJohn W Du Bois. 2007. The stance triangle. Stanc-\netaking in discourse: Subjectivity, evaluation, inter-\naction , 164(3):139\u2013182.\nSebastian Dungs, Ahmet Aker, Norbert Fuhr, and\nKalina Bontcheva. 2018. Can rumour stance alone\npredict veracity? In Proceedings of the 27th In-\nternational Conference on Computational Linguis-\ntics, COLING \u201918, pages 3360\u20133370, Santa Fe, New\nMexico, USA.\nWei Fang, Moin Nadeem, Mitra Mohtarami, and James\nGlass. 2019. Neural multi-task learning for stance\nprediction. In Proceedings of the Second Workshop\non Fact Extraction and VERi\ufb01cation , FEVER \u201919,\npages 13\u201319, Hong Kong, China.\nWilliam Ferreira and Andreas Vlachos. 2016. Emer-\ngent: a novel data-set for stance classi\ufb01cation. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\nNAACL-HLT \u201916, pages 1163\u20131168, San Diego,\nCalifornia, USA.\nYaroslav Ganin and Victor S. Lempitsky. 2015. Un-\nsupervised domain adaptation by backpropagation.\nInProceedings of the 32nd International Confer-\nence on Machine Learning , volume 37 of ICML\u2018\u201915 ,\npages 1180\u20131189, Lille, France.\nBilal Ghanem, Paolo Rosso, and Francisco Rangel.\n2018. Stance detection in fake news a com-\nbined feature representation. In Proceedings of the\nFirst Workshop on Fact Extraction and VERi\ufb01cation ,\nFEVER \u201918, pages 66\u201371, Brussels, Belgium.\nGenevieve Gorrell, Elena Kochkina, Maria Liakata,\nAhmet Aker, Arkaitz Zubiaga, Kalina Bontcheva,\nand Leon Derczynski. 2019. SemEval-2019 task 7:\nRumourEval, determining rumour veracity and sup-\nport for rumours. In Proceedings of the 13th In-\nternational Workshop on Semantic Evaluation , Se-\nmEval \u201917, pages 845\u2013854, Minneapolis, Minnesota,\nUSA.\nMaike Guderlei and Matthias A\u00dfenmacher. 2020. Eval-\nuating unsupervised representation learning for de-\ntecting stances of fake news. In Proceedings\nof the 28th International Conference on Computa-\ntional Linguistics , COLING \u201920, pages 6339\u20136349,\nBarcelona, Spain (Online).\nJiang Guo, Darsh Shah, and Regina Barzilay. 2018.\nMulti-source domain adaptation with mixture of ex-\nperts. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing ,\nEMNLP \u201918, pages 4694\u20134703, Brussels, Belgium.\nZhijiang Guo, Michael Schlichtkrull, and Andreas Vla-\nchos. 2022. A Survey on Automated Fact-Checking.\nTransactions of the Association for Computational\nLinguistics , 10:178\u2013206.\nIvan Habernal, Henning Wachsmuth, Iryna Gurevych,\nand Benno Stein. 2018. The argument reasoning\ncomprehension task: Identi\ufb01cation and reconstruc-\ntion of implicit warrants. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , NAACL-HLT \u201918, pages\n1930\u20131940, New Orleans, Louisiana, USA.\nAndreas Hanselowski, Avinesh PVS, Benjamin\nSchiller, Felix Caspelherr, Debanjan Chaudhuri,\nChristian M. Meyer, and Iryna Gurevych. 2018a. A\nretrospective analysis of the fake news challenge\nstance-detection task. In Proceedings of the 27th\nInternational Conference on Computational Lin-\nguistics , COLING \u201918, pages 1859\u20131874, Santa Fe,\nNew Mexico, USA.\nAndreas Hanselowski, Christian Stab, Claudia Schulz,\nZile Li, and Iryna Gurevych. 2019. A richly anno-\ntated corpus for different tasks in automated fact-\nchecking. In Proceedings of the 23rd Confer-\nence on Computational Natural Language Learning ,\nCoNLL \u201919, pages 493\u2013503, Hong Kong, China.\nAndreas Hanselowski, Hao Zhang, Zile Li, Daniil\nSorokin, Benjamin Schiller, Claudia Schulz, and\nIryna Gurevych. 2018b. UKP-Athene: Multi-\nsentence textual entailment for claim veri\ufb01cation. In\nProceedings of the First Workshop on Fact Extrac-\ntion and VERi\ufb01cation , FEVER \u201918, pages 103\u2013108,\nBrussels, Belgium.\nMomchil Hardalov, Arnav Arora, Preslav Nakov, and\nIsabelle Augenstein. 2021. Cross-domain label-\nadaptive stance detection. In Proceedings of the\n2021 Conference on Empirical Methods in Natu-\nral Language Processing , EMNLP \u201921, pages 9011\u2013\n9028, Online and Punta Cana, Dominican Republic.\nMomchil Hardalov, Arnav Arora, Preslav Nakov, and\nIsabelle Augenstein. 2022. Few-shot cross-lingual\nstance detection with sentiment-based pre-training.\nInProceedings of the Thirty-Sixth AAAI Conference\non Arti\ufb01cial Intelligence , Online.\nMareike Hartmann, Yevgeniy Golovchenko, and\nIsabelle Augenstein. 2019. Mapping (dis-\n)information \ufb02ow about the MH17 plane crash.\nInProceedings of the Second Workshop on Natural\nLanguage Processing for Internet Freedom: Censor-\nship, Disinformation, and Propaganda , NLP4IF \u201919,\npages 45\u201355, Hong Kong, China.\nKazi Saidul Hasan and Vincent Ng. 2014. Why are\nyou taking this stance? Identifying and classifying\nreasons in ideological debates. In Proceedings of the\n2014 Conference on Empirical Methods in Natural\nLanguage Processing , EMNLP \u201914, pages 751\u2013762,\nDoha, Qatar.Tamanna Hossain, Robert L. Logan IV , Arjuna Ugarte,\nYoshitomo Matsubara, Sean Young, and Sameer\nSingh. 2020. COVIDLies: Detecting COVID-19\nmisinformation on social media. In Proceedings of\nthe 1st Workshop on NLP for COVID-19 (Part 2) at\nEMNLP 2020 , NLP-COVID19 \u201920, Online.\nAustin Hounsel, Jordan Holland, Ben Kaiser, Kevin\nBorgolte, Nick Feamster, and Jonathan Mayer. 2020.\nIdentifying disinformation websites using infrastruc-\nture features. In Proceedings of the 10th USENIX\nWorkshop on Free and Open Communications on the\nInternet , FOCI \u201920.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies , NAACL-HLT \u201919, pages 3543\u2013\n3556, Minneapolis, Minnesota, USA.\nJude Khouja. 2020. Stance prediction and claim veri-\n\ufb01cation: An Arabic perspective. In Proceedings of\nthe Third Workshop on Fact Extraction and VERi\ufb01-\ncation , FEVER \u201920, pages 8\u201317, Online.\nDouwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj\nGoswami, Amanpreet Singh, Pratik Ringshia, and\nDavide Testuggine. 2020. The hateful memes chal-\nlenge: Detecting hate speech in multimodal memes.\nInAdvances in Neural Information Processing Sys-\ntems 33: Annual Conference on Neural Informa-\ntion Processing Systems , volume 33 of NeurIPS \u201920 ,\npages 2611\u20132624.\nElena Kochkina, Maria Liakata, and Isabelle Augen-\nstein. 2017. Turing at SemEval-2017 task 8: Se-\nquential approach to rumour stance classi\ufb01cation\nwith branch-LSTM. In Proceedings of the 11th In-\nternational Workshop on Semantic Evaluation , Se-\nmEval \u201917, pages 475\u2013480, Vancouver, Canada.\nElena Kochkina, Maria Liakata, and Arkaitz Zubi-\naga. 2018. All-in-one: Multi-task learning for ru-\nmour veri\ufb01cation. In Proceedings of the 27th In-\nternational Conference on Computational Linguis-\ntics, COLING \u201918, pages 3402\u20133413, Santa Fe, New\nMexico, USA.\nDilek K\u00fc\u00e7\u00fck and Fazli Can. 2020. Stance detection: A\nsurvey. ACM Comput. Surv. , 53(1).\nSrijan Kumar, Justin Cheng, Jure Leskovec, and V . S.\nSubrahmanian. 2017. An army of me: Sockpuppets\nin online discussion communities. In Proceedings\nof the 26th International Conference on World Wide\nWeb, WWW \u201917, pages 857\u2013866, Perth, Australia.\nSumeet Kumar and Kathleen Carley. 2019. Tree\nLSTMs with convolution units to predict stance and\nrumor veracity in social media conversations. In\nProceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics , ACL \u201919,\npages 5047\u20135058, Florence, Italy.\nNayeon Lee, Yejin Bang, Andrea Madotto, and Pas-\ncale Fung. 2021. Towards few-shot fact-checking\nvia perplexity. In Proceedings of the Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies , NAACL-HLT \u201921, pages 1971\u20131981.\nNayeon Lee, Belinda Z. Li, Sinong Wang, Wen-tau\nYih, Hao Ma, and Madian Khabsa. 2020. Language\nmodels as fact checkers? In Proceedings of the\nThird Workshop on Fact Extraction and VERi\ufb01ca-\ntion, FEVER \u201920, pages 36\u201341, Online.\nJiawen Li, Yudianto Sujana, and Hung-Yu Kao. 2020.\nExploiting microblog conversation structures to de-\ntect rumors. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics ,\nCOLING \u201920, pages 5420\u20135429, Barcelona, Spai.\nQuanzhi Li, Qiong Zhang, and Luo Si. 2019a. even-\ntAI at SemEval-2019 task 7: Rumor detection on so-\ncial media by exploiting content, user credibility and\npropagation information. In Proceedings of the 13th\nInternational Workshop on Semantic Evaluation , Se-\nmEval \u201919, pages 855\u2013859, Minneapolis, Minnesota,\nUSA.\nQuanzhi Li, Qiong Zhang, and Luo Si. 2019b. Rumor\ndetection by exploiting user credibility information,\nattention and multi-task learning. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics , ACL \u201919, pages 1173\u20131179,\nFlorence, Italy.\nSizhen Li, Shuai Zhao, Bo Cheng, and Hao Yang. 2018.\nAn end-to-end multi-task learning model for fact\nchecking. In Proceedings of the First Workshop\non Fact Extraction and VERi\ufb01cation , FEVER \u201918,\npages 138\u2013144, Brussels, Belgium.\nAnders Edelbo Lillie, Emil Refsgaard Middelboe, and\nLeon Derczynski. 2019. Joint rumour stance and ve-\nracity prediction. In Proceedings of the 22nd Nordic\nConference on Computational Linguistics , NoDaL-\niDa \u201919, pages 208\u2013221, Turku, Finland.\nQian Liu, Bei Chen, Jiaqi Guo, Zeqi Lin, and Jian-\nguang Lou. 2020a. TAPEX: Table pre-training via\nlearning a neural SQL executor. In Proceedings of\nthe 10th International Conference on Learning Rep-\nresentations , ICLR \u201922, Virtual.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019. Multi-task deep neural networks\nfor natural language understanding. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics , ACL \u201919, pages 4487\u2013\n4496, Florence, Italy.\nZhenghao Liu, Chenyan Xiong, Maosong Sun, and\nZhiyuan Liu. 2020b. Fine-grained fact veri\ufb01cation\nwith kernel graph attention network. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics , ACL \u201920, pages 7342\u2013\n7351, Online.Michal Lukasik, P. K. Srijith, Duy Vu, Kalina\nBontcheva, Arkaitz Zubiaga, and Trevor Cohn. 2016.\nHawkes processes for continuous time sequence\nclassi\ufb01cation: an application to rumour stance clas-\nsi\ufb01cation in Twitter. In Proceedings of the Annual\nMeeting of the Association for Computational Lin-\nguistics , ACL \u201916, pages 393\u2013398, Berlin, Germany.\nR.C. Luo, Chih-Chen Yih, and Kuo Lan Su. 2002. Mul-\ntisensor fusion and integration: approaches, applica-\ntions, and future research directions. IEEE Sensors\nJournal , 2(2):107\u2013119.\nJing Ma and Wei Gao. 2020. Debunking rumors\non Twitter with tree transformer. In Proceedings\nof the 28th International Conference on Computa-\ntional Linguistics , COLING \u201920, pages 5455\u20135466,\nBarcelona, Spain (Online).\nSuman Kalyan Maity, Aishik Chakraborty, Pawan\nGoyal, and Animesh Mukherjee. 2017. Detection\nof sockpuppets in social media. In Proceedings of\nthe ACM Conference on Computer Supported Co-\noperative Work and Social Computing , CSCW \u201917,\nPortland, Oregon, USA.\nChristopher Malon. 2018. Team Papelo: Trans-\nformer networks at FEVER. In Proceedings of the\nFirst Workshop on Fact Extraction and VERi\ufb01cation ,\nFEVER \u201918, pages 109\u2013113, Brussels, Belgium.\nMarcelo Mendoza, Barbara Poblete, and Carlos\nCastillo. 2010. Twitter under crisis: Can we trust\nwhat we RT? In Proceedings of the First Workshop\non Social Media Analytics , SOMA \u201910, pages 71\u2014-\n79, Washington D.C., District of Columbia.\nTodor Mihaylov, Georgi Georgiev, and Preslav Nakov.\n2015a. Finding opinion manipulation trolls in news\ncommunity forums. In Proceedings of the Nine-\nteenth Conference on Computational Natural Lan-\nguage Learning , pages 310\u2013314, Beijing, China.\nTodor Mihaylov, Ivan Koychev, Georgi Georgiev, and\nPreslav Nakov. 2015b. Exposing paid opinion ma-\nnipulation trolls. In Proceedings of the International\nConference Recent Advances in Natural Language\nProcessing , pages 443\u2013450, Hissar, Bulgaria.\nSaif Mohammad, Svetlana Kiritchenko, Parinaz Sob-\nhani, Xiaodan Zhu, and Colin Cherry. 2016.\nSemEval-2016 task 6: Detecting stance in tweets.\nInProceedings of the 10th International Workshop\non Semantic Evaluation , SemEval \u201916, pages 31\u201341,\nSan Diego, California, USA.\nMitra Mohtarami, Ramy Baly, James Glass, Preslav\nNakov, Llu\u00eds M\u00e0rquez, and Alessandro Moschitti.\n2018. Automatic stance detection using end-to-end\nmemory networks. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies , NAACL-HLT \u201918, pages 767\u2013\n776, New Orleans, Louisiana, USA.\nSubhabrata Mukherjee and Gerhard Weikum. 2015.\nLeveraging joint interactions for credibility analysis\nin news communities. In Proceedings of the 24th\nACM International Conference on Information and\nKnowledge Management , CIKM \u201915, pages 353\u2013\n362, Melbourne, Australia.\nMoin Nadeem, Wei Fang, Brian Xu, Mitra Mohtarami,\nand James Glass. 2019. FAKTA: An automatic end-\nto-end fact checking system. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations) , NAACL-HLT \u201919, pages 78\u201383,\nMinneapolis, Minnesota, USA.\nKai Nakamura, Sharon Levy, and William Yang Wang.\n2020. Fakeddit: A new multimodal benchmark\ndataset for \ufb01ne-grained fake news detection. In Pro-\nceedings of the 12th Language Resources and Eval-\nuation Conference , LREC \u201920, pages 6149\u20136157,\nMarseille, France.\nPreslav Nakov, Alberto Barr\u00f3n-Cede\u00f1o, Tamer El-\nsayed, Reem Suwaileh, Llu\u00eds M\u00e0rquez, Wajdi Za-\nghouani, Pepa Atanasova, Spas Kyuchukov, and\nGiovanni Da San Martino. 2018. Overview of the\nCLEF-2018 CheckThat! lab on automatic identi\ufb01ca-\ntion and veri\ufb01cation of political claims. In Experi-\nmental IR Meets Multilinguality, Multimodality, and\nInteraction , CLEF \u201918, pages 372\u2013387.\nPreslav Nakov, Husrev Taha Sencar, Jisun An, and Hae-\nwoon Kwak. 2021. A survey on predicting the factu-\nality and the bias of news media. arXiv/2103.12506 .\nAn T. Nguyen, Aditya Kharosekar, Matthew Lease,\nand Byron C. Wallace. 2018. An interpretable joint\ngraphical model for fact-checking from crowds. In\nProceedings of the Thirty-Second AAAI Conference\non Arti\ufb01cial Intelligence , AAAI \u201918, pages 1511\u2013\n1518, New Orleans, Louisiana, USA.\nVan-Hoang Nguyen, Kazunari Sugiyama, Preslav\nNakov, and Min-Yen Kan. 2020. FANG: leveraging\nsocial context for fake news detection using graph\nrepresentation. In Proceedings of the ACM Inter-\nnational Conference on Information and Knowledge\nManagement , CIKM \u201920, pages 1165\u20131174, Virtual\nEvent, Ireland.\nYixin Nie, Haonan Chen, and Mohit Bansal. 2019.\nCombining fact extraction and veri\ufb01cation with neu-\nral semantic matching networks. In Proceedings\nof the Thirty-Third Conference on Arti\ufb01cial Intel-\nligence AAAI 2019 , AAAI \u201919, pages 6859\u20136866,\nHonolulu, Hawaii, USA.\nWojciech Ostrowski, Arnav Arora, Pepa Atanasova,\nand Isabelle Augenstein. 2021. Multi-hop fact\nchecking of political claims. In Proceedings of the\nThirtieth International Joint Conference on Arti\ufb01cial\nIntelligence , pages 3892\u20133898.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploit-\ning class relationships for sentiment categorizationwith respect to rating scales. In Proceedings of the\n43rd Annual Meeting of the Association for Compu-\ntational Linguistics , ACL \u201905, pages 115\u2013124, Ann\nArbor, Michigan, USA.\nGordon Pennycook and David G. Rand. 2019. Fighting\nmisinformation on social media using crowdsourced\njudgments of news source quality. Proceedings\nof the National Academy of Sciences , 116(7):2521\u2013\n2526.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies , NAACL-HLT \u201918, pages 2227\u20132237,\nNew Orleans, Louisiana, USA.\nDean Pomerleau and Delip Rao. 2017. Fake\nnews challenge stage 1 (FNC-I): Stance detection.\nhttps://www.fakenewschallenge.org/.\nKashyap Popat, Subhabrata Mukherjee, Jannik Str\u00f6t-\ngen, and Gerhard Weikum. 2017. Where the\ntruth lies: Explaining the credibility of emerging\nclaims on the Web and social media. In Proceed-\nings of the 26th International Conference on World\nWide Web Companion , WWW \u201917 Companion, page\n1003\u20131012, Perth, Australia.\nKashyap Popat, Subhabrata Mukherjee, Jannik Str\u00f6t-\ngen, and Gerhard Weikum. 2018. CredEye: A credi-\nbility lens for analyzing and explaining misinforma-\ntion. In Companion Proceedings of The Web Confer-\nence 2018 , WWW \u201918, page 155\u2013158, Lyon, France.\nMartin Potthast, Johannes Kiesel, Kevin Reinartz,\nJanek Bevendorff, and Benno Stein. 2018. A stylo-\nmetric inquiry into hyperpartisan and fake news. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics , ACL \u201918,\npages 231\u2013240, Melbourne, Australia.\nShraman Pramanick, Dimitar Dimitrov, Rituparna\nMukherjee, Shivam Sharma, Md. Shad Akhtar,\nPreslav Nakov, and Tanmoy Chakraborty. 2021. De-\ntecting harmful memes and their targets. In Findings\nof ACL-IJCNLP , pages 2783\u20132796.\nVahed Qazvinian, Emily Rosengren, Dragomir R.\nRadev, and Qiaozhu Mei. 2011. Rumor has it: Iden-\ntifying misinformation in microblogs. In Proceed-\nings of the 2011 Conference on Empirical Meth-\nods in Natural Language Processing , EMNLP \u201911,\npages 1589\u20131599, Edinburgh, Scotland, UK.\nHannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana\nV olkova, and Yejin Choi. 2017. Truth of varying\nshades: Analyzing language in fake news and politi-\ncal fact-checking. In Proceedings of the Conference\non Empirical Methods in Natural Language Process-\ning, pages 2931\u20132937, Copenhagen, Denmark.\nBenjamin Riedel, Isabelle Augenstein, Georgios P Sp-\nithourakis, and Sebastian Riedel. 2017. A sim-\nple but tough-to-beat baseline for the Fake News\nChallenge stance detection task. arXiv preprint\narXiv:1707.03264 .\nSara Rosenthal, Noura Farra, and Preslav Nakov. 2017.\nSemEval-2017 task 4: Sentiment analysis in Twitter.\nInProceedings of the 11th International Workshop\non Semantic Evaluation , SemEval \u201917, pages 502\u2013\n518, Vancouver, Canada.\nGiovanni Santia and Jake Williams. 2018. BuzzFace:\nA news veracity dataset with Facebook user com-\nmentary and egos. In Proceedings of the Twelfth in-\nternational AAAI conference on web and social me-\ndia, volume 12 of ICWSM \u201918 , pages 531\u2013540, Palo\nAlto, California, USA.\nMaarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,\nand Noah A. Smith. 2019. The risk of racial bias\nin hate speech detection. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics , ACL \u201919, pages 1668\u20131678,\nFlorence, Italy.\nDenise Scannell, Linda Desens, Marie Guadagno,\nYolande Tra, Emily Acker, Kate Sheridan, Margo\nRosner, Jennifer Mathieu, and Mike Fulk. 2021.\nCOVID-19 vaccine discourse on Twitter: A content\nanalysis of persuasion techniques, sentiment and\nmis/disinformation. Journal of Health Communica-\ntion, 26(7):443\u2013459.\nTimo Schick and Hinrich Sch\u00fctze. 2021. Exploiting\ncloze-questions for few-shot text classi\ufb01cation and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics , ACL \u201921,\npages 255\u2013269, Online.\nBenjamin Schiller, Johannes Daxenberger, and Iryna\nGurevych. 2021. Stance detection benchmark: How\nrobust is your stance detection? KI-K\u00fcnstliche Intel-\nligenz , pages 1\u201313.\nShaden Shaar, Nikolay Babulkov, Giovanni\nDa San Martino, and Preslav Nakov. 2020. That\nis a known lie: Detecting previously fact-checked\nclaims. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\nACL \u201920, pages 3607\u20133618.\nShivam Sharma, Md Shad Akhtar, Preslav Nakov, and\nTanmoy Chakraborty. 2022a. DISARM: Detecting\nthe victims targeted by harmful memes. In Findings\nof NAACL 2022 , Seattle, Washington, USA.\nShivam Sharma, Firoj Alam, Md. Shad Akhtar, Dimitar\nDimitrov, Giovanni Da San Martino, Hamed Firooz,\nAlon Halevy, Fabrizio Silvestri, Preslav Nakov, and\nTanmoy Chakraborty. 2022b. Detecting and under-\nstanding harmful memes: A survey. In Proceedings\nof the 31st International Joint Conference on Arti\ufb01-\ncial Intelligence , IJCAI-ECAI \u201922, Vienna, Austria.Chereen Shurafa, Kareem Darwish, and Wajdi Za-\nghouani. 2020. Political framing: US COVID19\nblame game. In Social Informatics , pages 333\u2013351.\nJiasheng Si, Deyu Zhou, Tongzhe Li, Xingyu Shi, and\nYulan He. 2021. Topic-aware evidence reasoning\nand stance-aware aggregation for fact veri\ufb01cation.\nInProceedings of the Annual Meeting of the Asso-\nciation for Computational Linguistics and the Inter-\nnational Joint Conference on Natural Language Pro-\ncessing , ACL-IJCNLP \u201921, pages 1612\u20131622.\nValeriya Slovikovskaya and Giuseppe Attardi. 2020.\nTransfer learning from transformers to fake news\nchallenge stance detection (FNC-1) task. In Pro-\nceedings of the 12th Language Resources and Eval-\nuation Conference , LREC \u201920, pages 1211\u20131218,\nMarseille, France.\nPeter Stefanov, Kareem Darwish, Atanas Atanasov,\nand Preslav Nakov. 2020. Predicting the topical\nstance and political leaning of media using tweets.\nInProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , ACL \u201920,\npages 527\u2013537, Online.\nJames Thorne and Andreas Vlachos. 2018. Automated\nfact checking: Task formulations, methods and fu-\nture directions. In Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics ,\nCOLING \u201918, pages 3346\u20133359, Santa Fe, New\nMexico, USA.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERi\ufb01cation. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , NAACL-HLT \u201918, pages\n809\u2013819, New Orleans, Louisiana, USA.\nJames Thorne, Andreas Vlachos, Oana Cocarascu,\nChristos Christodoulopoulos, and Arpit Mittal. 2019.\nThe FEVER2.0 shared task. In Proceedings of the\nSecond Workshop on Fact Extraction and VERi\ufb01ca-\ntion, FEVER \u201919, pages 1\u20136, Hong Kong, China.\nLin Tian, Xiuzhen Zhang, Yan Wang, and Huan Liu.\n2020. Early detection of rumours on Twitter via\nstance transfer learning. In Advances in Information\nRetrieval , ECIR \u201920, pages 575\u2013588, Cham.\nJannis Vamvas and Rico Sennrich. 2020. X-Stance:\nA multilingual multi-target dataset for stance detec-\ntion. In Proceedings of the 5th Swiss Text Analytics\nConference (SwissText) & the 16th Conference on\nNatural Language Processing (KONVENS) , Zurich,\nSwitzerland.\nAmir Pouran Ben Veyseh, Javid Ebrahimi, Dejing Dou,\nand Daniel Lowd. 2017. A temporal attentional\nmodel for rumor stance classi\ufb01cation. In Proceed-\nings of the 2017 ACM on Conference on Informa-\ntion and Knowledge Management , CIKM \u201917, pages\n2335\u20132338, Singapore.\nNguyen V o and Kyumin Lee. 2020. Where are the\nfacts? Searching for fact-checked information to\nalleviate the spread of fake news. In Proceedings\nof the Conference on Empirical Methods in Natu-\nral Language Processing , EMNLP \u201920, pages 7717\u2013\n7731, Online.\nWilliam Yang Wang. 2017. \u201cLiar, Liar Pants on Fire\u201d:\nA new benchmark dataset for fake news detection.\nInProceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics , ACL \u201917,\npages 422\u2013426, Vancouver, Canada.\nYongyue Wang, Chunhe Xia, Chengxiang Si, Beitong\nYao, and Tianbo Wang. 2020. Robust reasoning over\nheterogeneous textual information for fact veri\ufb01ca-\ntion. IEEE Access , 8:157140\u2013157150.\nPenghui Wei, Nan Xu, and Wenji Mao. 2019. Mod-\neling conversation structure and temporal dynam-\nics for jointly predicting rumor stance and veracity.\nInProceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing , EMNLP-IJCNLP \u201919, pages\n4787\u20134798, Hong Kong, China.\nMaxwell Weinzierl, Suellen Hopfer, and Sanda M.\nHarabagiu. 2021. Misinformation adoption or re-\njection in the era of COVID-19. In Proceedings of\nthe Fifteenth International AAAI Conference on Web\nand Social Media , volume 15 of ICWSM\u2019 21 , pages\n787\u2013795, Virtual.\nWeiming Wen, Songwen Su, and Zhou Yu. 2018.\nCross-lingual cross-platform rumor veri\ufb01cation piv-\noting on multimedia content. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing , EMNLP \u201918, pages 3487\u2013\n3496, Brussels, Belgium.\nDustin Wright and Isabelle Augenstein. 2020. Trans-\nformer based multi-source domain adaptation. In\nProceedings of the 2020 Conference on Empir-\nical Methods in Natural Language Processing ,\nEMNLP \u201920, pages 7963\u20137974, Online.\nRuoyao Yang, Wanying Xie, Chunhua Liu, and Dong\nYu. 2019. BLCU_NLP at SemEval-2019 task 7: An\ninference chain-based GPT model for rumour eval-\nuation. In Proceedings of the 13th International\nWorkshop on Semantic Evaluation , SemEval \u201919,\npages 1090\u20131096, Minneapolis, Minnesota, USA.\nDeming Ye, Yankai Lin, Jiaju Du, Zhenghao Liu, Peng\nLi, Maosong Sun, and Zhiyuan Liu. 2020. Coref-\nerential reasoning learning for language representa-\ntion. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing ,\nEMNLP \u201920, pages 7170\u20137186, Online.\nTakuma Yoneda, Jeff Mitchell, Johannes Welbl, Pon-\ntus Stenetorp, and Sebastian Riedel. 2018. UCL\nmachine reading group: Four factor framework\nfor fact \ufb01nding (HexaF). In Proceedings of theFirst Workshop on Fact Extraction and VERi\ufb01cation ,\nFEVER \u201918, pages 97\u2013102, Brussels, Belgium.\nJianfei Yu, Jing Jiang, Ling Min Serena Khoo,\nHai Leong Chieu, and Rui Xia. 2020. Coupled hi-\nerarchical transformer for stance-aware rumor veri\ufb01-\ncation in social media conversations. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing , EMNLP \u201920, pages\n1392\u20131401, Online.\nChen Zhao, Chenyan Xiong, Corby Rosset, Xia\nSong, Paul N. Bennett, and Saurabh Tiwary. 2020.\nTransformer-XH: Multi-evidence reasoning with ex-\ntra hop attention. In Proceedings of the 8th Inter-\nnational Conference on Learning Representations ,\nICLR \u201920, Addis Ababa, Ethiopia.\nWanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu,\nNan Duan, Ming Zhou, Jiahai Wang, and Jian Yin.\n2020. Reasoning over semantic-level graph for fact\nchecking. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics ,\nACL \u201920, pages 6170\u20136180, Online.\nJie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng\nWang, Changcheng Li, and Maosong Sun. 2019.\nGEAR: Graph-based evidence aggregating and rea-\nsoning for fact veri\ufb01cation. In Proceedings of the\nAnnual Meeting of the Association for Computa-\ntional Linguistics , ACL \u201919, pages 892\u2013901, Flo-\nrence, Italy.\nDimitrina Zlatkova, Preslav Nakov, and Ivan Koychev.\n2019. Fact-checking meets fauxtography: Verify-\ning claims about images. In Proceedings of the\nConference on Empirical Methods in Natural Lan-\nguage Processing and the International Joint Con-\nference on Natural Language Processing , EMNLP-\nIJCNLP \u201919, pages 2099\u20132108, Hong Kong, China.\nArkaitz Zubiaga, Ahmet Aker, Kalina Bontcheva,\nMaria Liakata, and Rob Procter. 2018a. Detection\nand resolution of rumours in social media: A survey.\nACM Comput. Surv. , 51(2).\nArkaitz Zubiaga, Elena Kochkina, Maria Liakata, Rob\nProcter, and Michal Lukasik. 2016a. Stance classi-\n\ufb01cation in rumours as a sequential task exploiting\nthe tree structure of social media conversations. In\nProceedings of the 26th International Conference\non Computational Linguistics , COLING \u201916, pages\n2438\u20132448, Osaka, Japan.\nArkaitz Zubiaga, Elena Kochkina, Maria Liakata, Rob\nProcter, Michal Lukasik, Kalina Bontcheva, Trevor\nCohn, and Isabelle Augenstein. 2018b. Discourse-\naware rumour stance classi\ufb01cation in social media\nusing sequential classi\ufb01ers. Information Processing\n& Management , 54(2):273\u2013290.\nArkaitz Zubiaga, Maria Liakata, Rob Procter, Geral-\ndine Wong Sak Hoi, and Peter Tolmie. 2016b.\nAnalysing how people orient to and spread rumours\nin social media by looking at conversational threads.\nPLOS ONE , 11(3):1\u201329.\nA Examples of Stance\nAs outlined in Section 3, there are different for-\nmulations in which the task of stance de\ufb01nition\nis materialised. In Table 3, we present some in-\nstances of these as exempli\ufb01ed by different stance\ndetection datasets. The target with respect to which\nthe stance is assessed can vary, e.g., a headline, a\ncomment, a claim, a topic, etc., which in turn can\ndiffer in length and form. Moreover, the context\nwhere the stance is expressed can vary not only\nin its domain, e.g., News in (Ferreira and Vlachos,\n2016) and Twitter in (Qazvinian et al., 2011), but\nalso in its structure, as seen in the example of multi-\nple evidence sentences in (Thorne et al., 2018) and\nthreaded comments in (Gorrell et al., 2019).\nIn a more detailed view of Table 3, we see that\neach group of examples has its own important\nspeci\ufb01cs that alter the task of stance detection for\nmis- and disinformation detection.\nFigure 3a shows an example from the News do-\nmain, where we have a headline and an entire ar-\nticle body, and the goal is to \ufb01nd how the two are\nrelated in terms of the body\u2019s stance(s) towards the\nheadline. In this scenario, the models need to be\nable to handle very long documents, on one hand,\nand on the other to reason over multiple fragments\nof the input text, which might potentially express\ndifferent stances. It is possible to simplify the task\nby extracting a summary of the news article be-\nforehand, and evaluating only the stance of that\nsummary, as shown in Figure 3d. However, obtain-\ning such summaries is not a trivial task: (a) they\ncan be extracted by a human annotator (e.g., a jour-\nnalist), which is time-consuming and expensive,\nand can require a priori knowledge about the head-\nline/topic of interest as the article might have more\nthan one highlight or viewpoint, or (b) they can be\nautomatically generated using text summarisation\nmethods, but the result can be noisy.\nStance is often expressed in social media such\nas Twitter, Facebook, Reddit, etc. We illustrate two\nsuch scenarios in Figures 3b and 3e. In contrast to\nthe usually long and well-written news documents,\nsocial media posts are mostly short and depend\non additional context such as the previous posts\nin a conversational thread (Figure 3e), or external\nURLs and implicit topics (Figure 3b). Moreover,\nthese texts also need normalisation, as users tend\nto use slurs, emojis, and other informal language.\n4For illustrative purposes the text is trimmed to include\nonly the relevant passage.Next, in Figure 3c we highlight another interest-\ning setup: claim veri\ufb01cation using multiple pieces\nof evidence. Here, the reasoning is carried in multi-\nple hops over a set of texts. In particular, there\nmight not exists a single passage from a docu-\nment/post that supports/refutes the claim directly.\nIn that case, a large enough chain of evidence might\nbe needed, which can cover enough contextual\nknowledge in order to allow the model (or a person)\nto assess the veracity of the input claim.\nFinally, the examples in Figure 3 demonstrate\nthat stance can be used for mis- and disinformation\ndetection in different ways: ( i) directly, as in the\nexamples in Figures 3a and 3b, or ( ii) as multiple\nviewpoints, which are later aggregated into a \ufb01nal\ndecision, as in Figure 3c, 3d and 3e.\nWe thoroughly discussed all of the aforemen-\ntioned setups in Section 3, including the publicly\navailable datasets that focus on stance in the context\nof mis- and disinformation identi\ufb01cation.\nB Additional Formulations of Stance as a\nComponent for Fact-Checking\nBeyond the approaches that we outlined in Sec-\ntion 3.2, stance has also been used for detecting\nmisconceptions and for pro\ufb01ling media sources as\npart of a fact-checking pipeline. Below, we de-\nscribe some work that follows these formulations.\nMisconceptions Hossain et al. (2020) focused\non detecting misinformation related to COVID-\n19, based on known misconceptions listed in\nWikipedia. They evaluated the veracity of a tweet\ndepending on whether it agrees ,disagrees , or has\nno stance with respect to a set of misconceptions.\nA related formulation of the task is detecting previ-\nously fact-checked claims (Shaar et al., 2020). This\nallows to assess the veracity of dubious content by\nevaluating the stance of a claim regarding already\nchecked stories, known misconceptions, and facts.\nMedia Pro\ufb01ling Stance detection has also been\nused for media pro\ufb01ling. Stefanov et al. (2020) ex-\nplored the feasibility of an unsupervised approach\nfor identifying the political leanings (left, center, or\nright bias) of media outlets and in\ufb02uential people\non Twitter based on their stance on controversial\ntopics. They built clusters of users around core vo-\ncal ones based on their behaviour on Twitter such\nas retweeting, using the procedure proposed by\nDarwish et al. (2020). This is an important step\ntowards understanding media biases.\nHeadline :Robert Plant Ripped up $800M Led Zeppelin\nReunion Contract\n/_460Body : ...Led Zeppelin\u2019s Robert Plant turned down \u00a3500\nMILLION to reform supergroup.. /thumbs_up_alt\n(a) Example from Pomerleau and Rao (2017)Topic :Sarah Palin getting divorced?\n/twitterTweet : OneRiot.com - Palin Denies First Dude Divorce\nRumors http://url /thumbs_down_alt\nTopic :N/A (Implicit)\n/twitterTweet : Wow, that is fascinating! I hope you never mock\nour proud Scandi heritage again. /comment_alt\n(b) Examples from Qazvinian et al. (2011) and Derczynski et al.\n(2017)\nClaim :The Rodney King riots took place in the most\npopulous county in the USA.\n/_576ikiEvidence 1 : The 1992 Los Angeles riots, also\nknown as the Rodney King riots were a series of riots,\nlootings, arsons, and civil disturbances that occurred in Los\nAngeles County , California in April and May 1992.\n/_576ikiEvidence 2 : Los Angeles County, of\ufb01cially the\nCounty of Los Angeles, is the most populous county in the\nUSA./thumbs_up_alt\n(c) Example from Thorne et al. (2018)Headline :Jess Smith of Chatham, Kent was the smiling\nsun baby in the Teletubbies TV show\n/_460Summary 1 : Canterbury Christ Church University\nstudent Jess Smith, from Chatham, starred as Teletubbies\nsun/thumbs_up_alt\n/_460Summary 2 : This College Student Claims She Was\nThe Teletubbies Sun Baby /thumbs_down_alt\n(d) Example from Ferreira and Vlachos (2016)\n/twitter/f1a1\nu1: We understand that there are two gunmen and up to a dozen hostages inside the cafe under siege at Sydney.. ISIS \ufb02ags\nremain on display #7News /thumbs_up_alt\nu2: @u1 not ISIS \ufb02ags /thumbs_down_alt\nu3: @u1 sorry - how do you know its an ISIS \ufb02ag? Can you actually con\ufb01rm that? /_627\nu4: @u3 no she cant cos its actually not /thumbs_down_alt\nu5: @u1 More on situation at Martin Place in Sydney, AU LINK /comment_alt\nu6: @u1 Have you actually con\ufb01rmed its an ISIS \ufb02ag or are you talking shit /_627\n(e) Example from Gorrell et al. (2019)\nTable 3: Illustrative examples for different stance detection scenarios included in our survey. We annotate the\nexpressed stance with /thumbs_up_alt(support, for ),/thumbs_down_alt(deny, against ),/_627(query ), and/comment_alt(comment ).\nThe reliability of entire news media sources has\nbeen automatically rated based on their stance with\nrespect to manually fact-checked claims, without\naccess to gold labels for the overall medium-level\nfactuality of reporting (Mukherjee and Weikum,\n2015; Popat et al., 2017, 2018). The assumption\nin such methods is that reliable media agree with\ntrue claims and disagree with false ones, while for\nunreliable media, the situation is reversed. The\ntrustworthiness of Web sources has also been stud-\nied from a data analytics perspective, e.g., Dong\net al. (2015) proposed that a trustworthy source is\none that contains very few false claims.\nMore recently, Baly et al. (2018a) used gold\nlabels from Media Bias/Fact Check,5and a vari-\nety of information sources: articles published by\nthe medium, what is said about the medium on\nWikipedia, metadata from its Twitter pro\ufb01le, URL\nstructure, and traf\ufb01c information. In follow-up\nwork, Baly et al. (2019) used the same represen-\ntation to jointly predict a medium\u2019s factuality of\nreporting ( high vs.mixed vs.low) and its bias\n(leftvs.center vs.right ) on an ordinal scale, in a\nmulti-task ordinal regression setup.\n5http://mediabiasfactcheck.comBaly et al. (2020) extended the information\nsources to include Facebook followers and speech\nsignals from the news medium\u2019s channel on\nYouTube. Finally, Hounsel et al. (2020) proposed\nto use domain, certi\ufb01cate, and hosting information\nabout the infrastructure of the website. See (Nakov\net al., 2021) for a recent survey on media pro\ufb01ling.\nThere is a well-known connection between factu-\nality and bias.6For example, hyper-partisanship is\noften linked to low trustworthiness (Potthast et al.,\n2018), e.g., appealing to emotions rather than stick-\ning to the facts, while center media tend to be gen-\nerally more impartial and also more trustworthy.\nUser Pro\ufb01ling In the case of social media and\ncommunity fora, it is important to model the trust-\nworthiness of the user. In particular, there has been\nresearch on \ufb01nding opinion manipulation trolls ,\npaid (Mihaylov et al., 2015b) or just perceived (Mi-\nhaylov et al., 2015a), sockpuppets (Maity et al.,\n2017; Kumar et al., 2017), Internet water army\n(Chen et al., 2013), and seminar users (Darwish\net al., 2017).\n6http://www.poynter.org/fact-checking/media-literacy/\n2021/should-you-trust-media-bias-charts/\nC Systems and Applications\nThe systems and applications below use stance de-\ntection as part of a pipeline for identifying mis-\nand disinformation, see Section 4 for more details\nabout the methods.\nWen et al. (2018) worked in a cross-lingual cross-\nplatform rumour veri\ufb01cation setup. They included\nmultimodal content from fake and from real posts\nwith images or videos shared on Twitter. They then\ncollected supporting documents from two search\nengines, Google and Baidu, in English and Chi-\nnese, which they used for veracity evaluation. They\ntrained their stance detection model on English data\n(FNC-1) using pre-trained multilingual sentence\nembeddings, and further added cross-platform fea-\ntures in their \ufb01nal neural model.\nPopat et al. (2018) proposed CredEye,7a sys-\ntem for automatic credibility assessment of tex-\ntual claims. The system takes a claim as an input\nand analyses its credibility by considering relevant\narticles it retrieved from the Web, by combining\nthe predicted stance of the articles regarding the\nclaim with linguistic features to obtain a credibility\nscore (Popat et al., 2017).\nNguyen et al. (2018) designed a prototype fact-\nchecker Web tool.8Their system leverages a proba-\nbilistic graphical model to assess a claim\u2019s veracity\ntaking into consideration the stance of multiple arti-\ncles regarding this claim, the reputation of the news\nsources, and the annotators\u2019 reliability. In addition,\nit offers explanations to the fact-checkers based on\nthe aforementioned features, which was shown to\nimprove the overall user satisfaction and trust in\nthe predictions.\nZubiaga et al. (2018a) considered a four-step\ntracking process as a pipeline for rumour veri\ufb01-\ncatioon: ( i)rumour detection , i.e., given a stream\nof claims, determine whether they are worth veri-\nfying or they do contain no rumours, ( ii)rumour\ntracking for \ufb01nding relevant information about the\nrumour using social media posts, sentence descrip-\ntions, and keywords, ( iii)stance classi\ufb01cation to\ncollect stances towards the rumour, and ( iv)ve-\nracity classi\ufb01cation to aggregate the information\nfrom the tracking component, the collected stances,\nand optionally other relevant information about the\nsources, metadata about the users, etc., to predict a\ntruth value for the rumour.\n7https://gate.d5.mpi-inf.mpg.de/credeye/\n8http://fcweb.pythonanywhere.com/Nadeem et al. (2019) developed FAKTA, a sys-\ntem for automatic end-to-end fact-checking of\nclaims. It retrieves relevant articles from Wikipedia\nand as well as from selected media sources, which\nit then uses for veri\ufb01cation. FAKTA uses a stance\ndetection model, trained in a FEVER setting, to pre-\ndict the stance and to obtain entailed spans. These\npredictions, combined with linguistic analysis, are\nused to provide both document- and sentence-level\nexplanations and a factuality score.\nNguyen et al. (2020) proposed the Factual News\nGraph (FANG) model, which models the social\ncontext for fake news detection. In particular,\nFANG uses the stance of user comments with re-\nspect to the target news article as an integral compo-\nnent of its model, together with temporality, user\u2013\nuser interactions, article\u2013source interactions, as\nwell a source reliability information.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "A survey on stance detection for mis-and disinformation identification", "author": ["M Hardalov", "A Arora", "P Nakov", "I Augenstein"], "pub_year": "2021", "venue": "arXiv preprint arXiv \u2026", "abstract": "Understanding attitudes expressed in texts, also known as stance detection, plays an  important role in systems for detecting false information online, be it misinformation (unintentionally"}, "filled": false, "gsrank": 626, "pub_url": "https://arxiv.org/abs/2103.00242", "author_id": ["rzJnkjgAAAAJ", "EQUUUUoAAAAJ", "DfXsKZ4AAAAJ", "DjJp0dcAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:0WxwQZ0kCdcJ:scholar.google.com/&output=cite&scirp=625&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D620%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=0WxwQZ0kCdcJ&ei=d7WsaKCIGLXCieoP4PfQ0A8&json=", "num_citations": 165, "citedby_url": "/scholar?cites=15494956250771254481&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:0WxwQZ0kCdcJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2103.00242"}}]